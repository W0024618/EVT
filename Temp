Go Very step by step and fix this errors carefully if need suggest me Which file need to update and fix this issue .

File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 3252, in _run_ddl_visitor
    conn._run_ddl_visitor(visitorcallable, element, **kwargs)
    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2459, in _run_ddl_visitor
    ).traverse_single(element)
      ~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\sql\visitors.py", line 661, in traverse_single
    return meth(obj, **kw)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\sql\ddl.py", line 963, in visit_metadata
    [t for t in tables if self._can_create_table(t)]
                          ~~~~~~~~~~~~~~~~~~~~~~^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\sql\ddl.py", line 928, in _can_create_table
    return not self.checkfirst or not self.dialect.has_table(
                                      ~~~~~~~~~~~~~~~~~~~~~~^
        self.connection, table.name, schema=effective_schema
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "<string>", line 2, in has_table
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\engine\reflection.py", line 89, in cache
    return fn(self, con, *args, **kw)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\dialects\sqlite\base.py", line 2304, in has_table
    info = self._get_table_pragma(
        connection, "table_info", table_name, schema=schema
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\dialects\sqlite\base.py", line 2942, in _get_table_pragma
    cursor = connection.exec_driver_sql(statement)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 1779, in exec_driver_sql
    ret = self._execute_context(
        dialect,
    ...<5 lines>...
        distilled_parameters,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 1846, in _execute_context
    return self._exec_single_context(
           ~~~~~~~~~~~~~~~~~~~~~~~~~^
        dialect, context, statement, parameters
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        e, str_statement, effective_parameters, cursor, context
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 2355, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\engine\base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
    ~~~~~~~~~~~~~~~~~~~~~~~^
        cursor, str_statement, effective_parameters, context
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\sqlalchemy\engine\default.py", line 951, in do_execute
    cursor.execute(statement, parameters)
    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.DatabaseError: (sqlite3.DatabaseError) database disk image is malformed
[SQL: PRAGMA main.table_info("active_employees")]
(Background on this error at: https://sqlalche.me/e/20/4xp6)
INFO:     Stopping reloader process [32192]
(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> cd "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics"
(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> Copy-Item .\attendance.db .\attendance.db.bak -Force
(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> sqlite3 attendance.db "PRAGMA integrity_check;"
>> 
sqlite3 : The term 'sqlite3' is not recognized as the name of a cmdlet, function, script file, or operable program. 
Check the spelling of the name, or if a path was included, verify that the path is correct and try again.
At line:1 char:1
+ sqlite3 attendance.db "PRAGMA integrity_check;"
+ ~~~~~~~
    + CategoryInfo          : ObjectNotFound: (sqlite3:String) [], CommandNotFoundException
    + FullyQualifiedErrorId : CommandNotFoundException

(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> # Try normal dump
>> sqlite3 attendance.db .dump > dump.sql
>> # If dump succeeded, create a fresh DB
>> sqlite3 new_attendance.db < dump.sql
>> # Replace original (after verifying new_attendance.db)
>> Move-Item .\attendance.db .\attendance.db.corrupt -Force
>> Move-Item .\new_attendance.db .\attendance.db
At line:4 char:27
+ sqlite3 new_attendance.db < dump.sql
+                           ~
The '<' operator is reserved for future use.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
    + FullyQualifiedErrorId : RedirectionNotSupported

(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> sqlite3 attendance.db .dump > dump.sql
sqlite3 : The term 'sqlite3' is not recognized as the name of a cmdlet, function, script file, or operable program. 
Check the spelling of the name, or if a path was included, verify that the path is correct and try again.
At line:1 char:1
+ sqlite3 attendance.db .dump > dump.sql
+ ~~~~~~~
    + CategoryInfo          : ObjectNotFound: (sqlite3:String) [], CommandNotFoundException
    + FullyQualifiedErrorId : CommandNotFoundException

(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> sqlite3 new_attendance.db < dump.sql    
At line:1 char:27
+ sqlite3 new_attendance.db < dump.sql
+                           ~
The '<' operator is reserved for future use.

(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> sqlite3 new_attendance.db < dump.sql    
At line:1 char:27
+ sqlite3 new_attendance.db < dump.sql
+                           ~
The '<' operator is reserved for future use.
(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> sqlite3 new_attendance.db < dump.sql    
At line:1 char:27
+ sqlite3 new_attendance.db < dump.sql
+                           ~
The '<' operator is reserved for future use.
At line:1 char:27
+ sqlite3 new_attendance.db < dump.sql
+                           ~
The '<' operator is reserved for future use.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
+ sqlite3 new_attendance.db < dump.sql
+                           ~
The '<' operator is reserved for future use.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
+                           ~
The '<' operator is reserved for future use.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
    + FullyQualifiedErrorId : RedirectionNotSupported
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
    + FullyQualifiedErrorId : RedirectionNotSupported
    + FullyQualifiedErrorId : RedirectionNotSupported


(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> sqlite3 attendance.db ".recover" > dump.sql
>> # then recreate
>> sqlite3 new_attendance.db < dump.sql
>>
At line:3 char:27
+ sqlite3 new_attendance.db < dump.sql
+                           ~
The '<' operator is reserved for future use.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
    + FullyQualifiedErrorId : RedirectionNotSupported
                                                                               (.venv) python tools\sqlite_iterdump.py
>> env) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics>
At line:1 char:9
+ (.venv) python tools\sqlite_iterdump.py
+         ~~~~~~
Unexpected token 'python' in expression or statement.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
    + FullyQualifiedErrorId : UnexpectedToken

(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> (.venv) python tools\sqlite_iterdump.py
>>
At line:1 char:9
+ (.venv) python tools\sqlite_iterdump.py
+         ~~~~~~
Unexpected token 'python' in expression or statement.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
    + FullyQualifiedErrorId : UnexpectedToken

(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> sqlite3 new_attendance.db < dump_iter.sql
>> Move-Item .\attendance.db .\attendance.db.corrupt -Force
>> Move-Item .\new_attendance.db .\attendance.db
>>
At line:1 char:27
+ sqlite3 new_attendance.db < dump_iter.sql
+                           ~
The '<' operator is reserved for future use.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
    + FullyQualifiedErrorId : RedirectionNotSupported

(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> sqlite3 new_attendance.db < dump_iter.sql
>>
>>
At line:1 char:27
+ sqlite3 new_attendance.db < dump_iter.sql
+                           ~
The '<' operator is reserved for future use.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
    + FullyQualifiedErrorId : RedirectionNotSupported

(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> Move-Item .\attendance.db .\attendance.db.corrupt -Force
>>
(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> Move-Item .\new_attendance.db .\attendance.db
>>
Move-Item : Cannot find path 'C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\new_attendance.db' 
because it does not exist.
At line:1 char:1
+ Move-Item .\new_attendance.db .\attendance.db
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (C:\Users\W00246...w_attendance.db:String) [Move-Item], ItemNotFoundExc  
   eption
    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.MoveItemCommand

(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> # rename corrupted DB
>> Move-Item .\attendance.db .\attendance.db.corrupt -Force
>> # Create new empty DB using SQLAlchemy metadata (models.Base)
>> (.venv) python - <<'PY'
>> from db import engine
>> from models import Base
>> Base.metadata.create_all(bind=engine)
>> print("Created new database")
>> PY
>>
>>
At line:4 char:9
+ (.venv) python - <<'PY'
+         ~~~~~~
Unexpected token 'python' in expression or statement.
At line:4 char:19
+ (.venv) python - <<'PY'
+                   ~
Missing file specification after redirection operator.
At line:4 char:18
+ (.venv) python - <<'PY'
+                  ~
The '<' operator is reserved for future use.
At line:4 char:19
+ (.venv) python - <<'PY'
+                   ~
The '<' operator is reserved for future use.
At line:5 char:1
+ from db import engine
+ ~~~~
The 'from' keyword is not supported in this version of the language.
At line:6 char:1
+ from models import Base
+ ~~~~
The 'from' keyword is not supported in this version of the language.
    + CategoryInfo          : ParserError: (:) [], ParentContainsErrorRecordException
    + FullyQualifiedErrorId : UnexpectedToken

(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics>




for more information check below file carefully..


C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py


# app.py
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.responses import JSONResponse
import shutil, uuid, json
from settings import UPLOAD_DIR, OUTPUT_DIR
from ingest_excel import ingest_employee_excel, ingest_contractor_excel
from compare_service import ingest_live_details_list, compute_daily_attendance, compare_with_active
import os

app = FastAPI(title="Attendance Analytics")

@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = UPLOAD_DIR / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    ingest_employee_excel(dest)
    return {"status":"ok", "path": str(dest)}

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    dest = UPLOAD_DIR / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    ingest_contractor_excel(dest)
    return {"status":"ok", "path": str(dest)}



@app.post("/ingest/live-details")
async def ingest_live(request: Request):
    """
    Accepts:
      - Raw JSON array in body (preferred)
      - JSON object with {"details": [...]} in body
      - multipart/form-data where a form field 'details' contains a JSON string
    """
    # 1) Try to parse JSON body first
    details = None
    try:
        body = await request.json()
        if isinstance(body, dict) and 'details' in body:
            details = body['details']
        else:
            details = body
    except Exception:
        # not a JSON body, try form data
        try:
            form = await request.form()
            if 'details' in form:
                raw = form['details']
                if isinstance(raw, str):
                    details = json.loads(raw)
                else:
                    # file upload or other; try to read if file-like
                    try:
                        details = json.loads((await raw.read()).decode('utf-8'))
                    except Exception:
                        details = list(form.getlist('details'))
            else:
                # try first field if it looks like JSON string
                first = None
                for v in form.values():
                    first = v
                    break
                if isinstance(first, str):
                    details = json.loads(first)
                else:
                    raise HTTPException(status_code=400, detail="No JSON payload found")
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Could not parse request body as JSON or form: {e}")

    if not isinstance(details, (list, tuple)):
        raise HTTPException(status_code=400, detail="Expected top-level array (JSON list) of detail objects")

    try:
        ingest_live_details_list(details)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to ingest details: {e}")

    return {"status": "ok", "inserted": len(details)}





@app.get("/ingest/fetch-all")
def fetch_all_and_ingest():
    """
    Convenience: call region_clients.fetch_all_details() and ingest them.
    Uses local import to avoid circular import issues.
    """
    try:
        import region_clients
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"region_clients unavailable: {e}")

    details = region_clients.fetch_all_details()
    if not isinstance(details, list):
        raise HTTPException(status_code=500, detail="Unexpected data from region_clients.fetch_all_details")
    ingest_live_details_list(details)
    return {"status":"ok", "inserted": len(details)}




@app.get("/reports/daily/{yyyymmdd}")
def daily_report(yyyymmdd: str):
    import datetime
    try:
        dt = datetime.datetime.strptime(yyyymmdd, "%Y%m%d").date()
    except Exception:
        raise HTTPException(status_code=400, detail="Date must be in YYYYMMDD format")
    compute_daily_attendance(dt)
    summary = compare_with_active(dt)
    # summary is JSON-safe (compare_service ensures conversion)
    return JSONResponse(summary)



C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\compare_service.py



# compare_service.py
import pandas as pd
import numpy as np
from datetime import datetime, date, timezone, timedelta
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary

# --- Helpers -----------------------------------------------------------------

def _to_native(value):
    """Convert numpy/pandas types and datetimes to plain Python types (JSON safe)."""
    if value is None:
        return None
    # handle pandas / numpy NA
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    if isinstance(value, (np.integer,)):
        return int(value)
    if isinstance(value, (np.floating,)):
        # should not be nan here (handled above)
        return float(value)
    if isinstance(value, (np.bool_, bool)):
        return bool(value)
    # datetimes -> isoformat string
    try:
        import datetime as _dt
        if isinstance(value, _dt.datetime):
            # if timezone-aware convert to iso in UTC
            try:
                if value.tzinfo is not None:
                    utc = value.astimezone(timezone.utc)
                    return utc.replace(tzinfo=None).isoformat() + "Z"
                else:
                    return value.isoformat()
            except Exception:
                return str(value)
        # pandas Timestamp
        if hasattr(value, 'isoformat'):
            try:
                return value.isoformat()
            except Exception:
                return str(value)
    except Exception:
        pass
    return value

def _normalize_employee_key(x):
    """Return a normalized string key for joining employee ids/card numbers."""
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na"):
            return None
        return s
    except Exception:
        return None

def _parse_timestamp_from_value(val):
    """
    Parse a timestamp value robustly and return naive UTC datetime (no tzinfo).
    Accepts ISO strings with Z/offset, epoch seconds (int), epoch ms (int), or datetime objects.
    Returns None if it cannot be parsed.
    """
    if val is None:
        return None
    # if already a datetime
    if isinstance(val, datetime):
        dt = val
        # convert timezone-aware -> UTC, then drop tzinfo (store naive UTC consistently)
        try:
            if dt.tzinfo is not None:
                dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            try:
                return dt.replace(tzinfo=None)
            except Exception:
                return None
    # numeric epoch
    if isinstance(val, (int, float, np.integer, np.floating)):
        # heuristics: > 1e12 -> ms; > 1e9 -> seconds
        try:
            v = int(val)
            if v > 1e12:  # milliseconds
                return datetime.fromtimestamp(v / 1000.0, tz=timezone.utc).replace(tzinfo=None)
            else:
                return datetime.fromtimestamp(v, tz=timezone.utc).replace(tzinfo=None)
        except Exception:
            return None
    # string parsing
    if isinstance(val, str):
        s = val.strip()
        if s == "":
            return None
        # try plain ISO with Z
        try:
            # handle common "Z" suffix
            if s.endswith("Z"):
                s2 = s.replace("Z", "+00:00")
                dt = datetime.fromisoformat(s2)
                # normalize to naive UTC
                if dt.tzinfo is not None:
                    return dt.astimezone(timezone.utc).replace(tzinfo=None)
                return dt
            # try fromisoformat directly (handles offset)
            try:
                dt = datetime.fromisoformat(s)
                if dt.tzinfo is not None:
                    return dt.astimezone(timezone.utc).replace(tzinfo=None)
                return dt
            except Exception:
                pass
            # try int-like epoch inside string
            if s.isdigit():
                v = int(s)
                if v > 1e12:
                    return datetime.fromtimestamp(v / 1000.0, tz=timezone.utc).replace(tzinfo=None)
                else:
                    return datetime.fromtimestamp(v, tz=timezone.utc).replace(tzinfo=None)
            # try common alternate formats
            for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M:%S.%f",
                        "%d/%m/%Y %H:%M:%S", "%d-%m-%Y %H:%M:%S"):
                try:
                    dt = datetime.strptime(s, fmt)
                    # assume this is local/naive; convert to UTC naive by interpreting as UTC (consistent)
                    return dt
                except Exception:
                    pass
        except Exception:
            pass
    return None

def _extract_timestamp_from_detail(detail):
    """
    Try multiple likely keys from the incoming detail dict;
    return naive-UTC datetime or None.
    """
    # candidate keys in order of likelihood
    keys = [
        "LocaleMessageDateTime", "LocalMessageDateTime", "LocaleMessageTime", "LocalMessageTime",
        "LocaleMessageDate", "Timestamp", "timestamp", "Time", "LocaleTime", "LocalTime",
        "time", "date", "LocaleMessageDateTimeUtc", "LocalMessageDateTimeUtc"
    ]
    # if detail itself is a timestamp string/object (rare)
    if not isinstance(detail, dict):
        return _parse_timestamp_from_value(detail)
    for k in keys:
        if k in detail:
            ts = detail.get(k)
            parsed = _parse_timestamp_from_value(ts)
            if parsed is not None:
                return parsed
    # some payloads nest date in other fields
    # try these fallback fields
    for k in ("LocalMessageDateTime", "LocaleMessageTime", "LocaleMessageDateTime", "LocaleMessageDate"):
        if k in detail:
            p = _parse_timestamp_from_value(detail.get(k))
            if p is not None:
                return p
    # as last resort try to find any value that looks like a timestamp
    for v in detail.values():
        p = _parse_timestamp_from_value(v)
        if p is not None:
            return p
    return None

# --- Main functions ----------------------------------------------------------

def ingest_live_details_list(details_list):
    """Persist details_list (array of detail dicts) into LiveSwipe table."""
    from db import SessionLocal as _SessionLocal
    inserted = 0
    skipped = 0
    with _SessionLocal() as db:
        for d in details_list:
            try:
                ts_parsed = _extract_timestamp_from_detail(d)
            except Exception:
                ts_parsed = None
            # if no parseable timestamp, skip this record (avoid polluting DB with 'now')
            if ts_parsed is None:
                skipped += 1
                continue

            # normalize employee/card strings
            emp = _normalize_employee_key(d.get("EmployeeID") or d.get("employee_id") or d.get("employeeId"))
            card = _normalize_employee_key(d.get("CardNumber") or d.get("card_number") or d.get("Card"))
            full_name = d.get("ObjectName1") or d.get("FullName") or d.get("Fullname") or d.get("full_name")
            partition = d.get("PartitionName2") or d.get("PartitionName1") or d.get("Partition")
            floor = d.get("Floor") or d.get("floor")
            door = d.get("Door") or d.get("DoorName") or d.get("door")
            region = d.get("PartitionName2") or d.get("Region") or d.get("region")

            rec = LiveSwipe(
                timestamp=ts_parsed,
                employee_id=emp,
                card_number=card,
                full_name=full_name,
                partition=partition,
                floor=floor,
                door=door,
                region=region,
                raw=d
            )
            db.add(rec)
            inserted += 1
        db.commit()
    # return counts for diagnostics (caller can log if desired)
    return {"inserted": inserted, "skipped_invalid_timestamp": skipped}

def compute_daily_attendance(target_date: date):
    """Build AttendanceSummary rows for target_date by reading LiveSwipe rows on that date."""
    with SessionLocal() as db:
        # target_date is a date object; our timestamps are naive UTC datetimes (stored that way)
        start = datetime.combine(target_date, datetime.min.time())
        end = datetime.combine(target_date, datetime.max.time())
        # Query live swipes for that date range
        swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
        if not swipes:
            return []

        rows = []
        for s in swipes:
            rows.append({
                "id": s.id,
                "timestamp": s.timestamp,
                "employee_id": _normalize_employee_key(s.employee_id),
                "card_number": _normalize_employee_key(s.card_number),
                "full_name": s.full_name,
                "partition": s.partition,
                "floor": s.floor,
                "door": s.door
            })
        df = pd.DataFrame(rows)
        if df.empty:
            return []

        # prefer employee_id, fallback to card_number
        df['key'] = df['employee_id'].fillna(df['card_number'])
        # drop rows with no usable key
        df = df[df['key'].notna()]
        if df.empty:
            return []

        grouped = df.groupby('key', dropna=False).agg(
            presence_count=('id', 'count'),
            first_seen=('timestamp', 'min'),
            last_seen=('timestamp', 'max'),
            full_name=('full_name', 'first'),
            partition=('partition', 'first')
        ).reset_index().rename(columns={'key': 'employee_id'})

        # Upsert AttendanceSummary (employee_id + date uniqueness assumed)
        for _, row in grouped.iterrows():
            try:
                rec = AttendanceSummary(
                    employee_id=str(row['employee_id']) if pd.notna(row['employee_id']) else None,
                    date=target_date,
                    presence_count=int(row['presence_count']),
                    first_seen=row['first_seen'],
                    last_seen=row['last_seen'],
                    derived={"partition": (row.get('partition') or None), "full_name": (row.get('full_name') or None)}
                )
                db.merge(rec)
            except Exception:
                # if one row fails, skip but continue (avoid aborting entire update)
                continue
        db.commit()
        return grouped.to_dict(orient='records')

def compare_with_active(target_date: date):
    """
    Compare AttendanceSummary (for target_date) with ActiveEmployee table.
    Returns a dict with JSON-safe Python types.
    """
    with SessionLocal() as db:
        # Attendance summary rows for date
        att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == target_date).all()
        if not att_rows:
            att_df = pd.DataFrame(columns=["employee_id", "presence_count", "first_seen", "last_seen"])
        else:
            att_df = pd.DataFrame([{
                "employee_id": _normalize_employee_key(a.employee_id),
                "presence_count": a.presence_count,
                "first_seen": a.first_seen,
                "last_seen": a.last_seen,
                **(a.derived or {})
            } for a in att_rows])

        # Active employees
        act_rows = db.query(ActiveEmployee).all()
        if not act_rows:
            act_df = pd.DataFrame(columns=["employee_id", "full_name", "location_city", "status"])
        else:
            act_df = pd.DataFrame([{
                "employee_id": _normalize_employee_key(e.employee_id),
                "full_name": e.full_name,
                "location_city": e.location_city,
                "status": e.current_status
            } for e in act_rows])

        # Ensure employee_id columns exist and are comparable strings
        if 'employee_id' not in act_df.columns:
            act_df['employee_id'] = pd.NA
        if 'employee_id' not in att_df.columns:
            att_df['employee_id'] = pd.NA

        # Make both employee_id columns strings for deterministic join
        act_df['employee_id'] = act_df['employee_id'].astype(object).apply(_normalize_employee_key)
        att_df['employee_id'] = att_df['employee_id'].astype(object).apply(_normalize_employee_key)

        # Merge left (active employees -> attendance)
        merged = pd.merge(act_df, att_df, on='employee_id', how='left', suffixes=('', '_att'))

        # Fill presence_count safely
        if 'presence_count' in merged.columns:
            merged['presence_count'] = merged['presence_count'].fillna(0)
            # convert floats like 0.0 to int where safe
            merged['presence_count'] = merged['presence_count'].apply(lambda x: int(x) if (pd.notnull(x) and float(x).is_integer()) else x)
        else:
            merged['presence_count'] = 0

        # present_today boolean
        merged['present_today'] = merged['presence_count'].apply(lambda x: bool(x and x != 0))

        # Location summary - ensure a column exists
        if 'location_city' not in merged.columns:
            merged['location_city'] = 'Unknown'
        else:
            merged['location_city'] = merged['location_city'].fillna('Unknown')

        loc_group = merged.groupby('location_city', dropna=False).agg(
            total_n=('employee_id', 'count'),
            present_n=('present_today', 'sum')
        ).reset_index()

        # safe percent
        def safe_percent(row):
            try:
                if row['total_n'] and row['total_n'] > 0:
                    return round((row['present_n'] / row['total_n']) * 100, 2)
            except Exception:
                pass
            return 0.0
        loc_group['percent_present'] = loc_group.apply(safe_percent, axis=1)

        by_location = []
        for r in loc_group.to_dict(orient='records'):
            clean = {k: _to_native(v) for k, v in r.items()}
            if 'location_city' not in clean and 'index' in clean:
                clean['location_city'] = clean.pop('index')
            by_location.append(clean)

        merged_list = []
        for r in merged.to_dict(orient='records'):
            clean = {}
            for k, v in r.items():
                clean[k] = _to_native(v)
            if 'employee_id' not in clean:
                clean['employee_id'] = None
            merged_list.append(clean)

        return {"by_location": by_location, "merged": merged_list}








C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\db.py


from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from settings import DB_URL

engine = create_engine(DB_URL, connect_args={"check_same_thread": False} if DB_URL.startswith("sqlite") else {})
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
Base = declarative_base()



C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\models.py



from sqlalchemy import Column, Integer, String, DateTime, JSON, Boolean, Date
from sqlalchemy import ForeignKey, UniqueConstraint
from sqlalchemy.orm import relationship
from db import Base

class ActiveEmployee(Base):
    __tablename__ = "active_employees"
    id = Column(Integer, primary_key=True)
    employee_id = Column(String, index=True, unique=True, nullable=False)
    full_name = Column(String, index=True)
    email = Column(String)
    location_city = Column(String, index=True)
    location_desc = Column(String)
    current_status = Column(String)
    raw_row = Column(JSON)  # store original row for reference
    uploaded_at = Column(DateTime)

class ActiveContractor(Base):
    __tablename__ = "active_contractors"
    id = Column(Integer, primary_key=True)
    worker_system_id = Column(String, index=True, unique=True, nullable=False)
    ipass_id = Column(String, index=True)
    full_name = Column(String, index=True)
    vendor = Column(String)
    location = Column(String)
    status = Column(String)
    raw_row = Column(JSON)
    uploaded_at = Column(DateTime)

class LiveSwipe(Base):
    __tablename__ = "live_swipes"
    id = Column(Integer, primary_key=True)
    timestamp = Column(DateTime, index=True)
    employee_id = Column(String, index=True, nullable=True)
    card_number = Column(String, index=True, nullable=True)
    full_name = Column(String)
    partition = Column(String, index=True)
    floor = Column(String)
    door = Column(String)
    region = Column(String, index=True)
    raw = Column(JSON)

class AttendanceSummary(Base):
    __tablename__ = "attendance_summary"
    id = Column(Integer, primary_key=True)
    employee_id = Column(String, index=True)
    date = Column(Date, index=True)
    presence_count = Column(Integer)
    first_seen = Column(DateTime)
    last_seen = Column(DateTime)
    derived = Column(JSON)  # extra stats

