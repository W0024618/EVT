# data_compare_service_v2.py
"""
Comparison service (v2) with broadened matching heuristics, safer prefetch/cache,
and explicit Sheet vs AttendanceSummary comparison diagnostics.

Drop-in replacement for your existing data_compare_service_v2.py
"""

import sys
import re
import uuid
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
from typing import Optional, Dict, Any, List, Tuple

import pandas as pd

# HTTP client for region histories
try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except Exception:
    requests = None

# DB imports (same as your project)
from db import SessionLocal
from models import AttendanceSummary

# Settings / defaults
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR, REGION_HISTORY_URLS as SETTINGS_REGION_HISTORY_URLS
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
    REGION_HISTORY_URLS = SETTINGS_REGION_HISTORY_URLS
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"
    REGION_HISTORY_URLS = [
        "http://10.199.22.57:3008/api/occupancy/history",  # APAC
        "http://10.199.22.57:3006/api/occupancy/history",  # NAMER
        "http://10.199.22.57:3007/api/occupancy/history",  # EMEA
        "http://10.199.22.57:4000/api/occupancy/history"   # LACA
    ]

DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger("data_compare_service_v2")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# In-memory cache for prefetched region history entries
REGION_HISTORY_CACHE = None
REGION_HISTORY_CACHE_FETCHED_AT = None
REGION_HISTORY_CACHE_TTL_SECONDS = 300  # 5 minutes

# Matching config
ID_FIELD_CANDIDATES = [
    "EmployeeID","employeeId","Employee Id","EmpID","Emp Id","EmpNo","EmployeeNo","Employee_Number",
    "PersonID","PersonId","personId","person_id","employee_id","id","Id","employeeNumber","EmployeeNumber",
    "worker_system_id","wsid","WorkerID","Worker System Id","workerId","WorkerSystemId"
]
CARD_FIELD_CANDIDATES = [
    "CardNumber","Card","cardNumber","card_number","BadgeNumber","BadgeNo","Badge","badgeNumber","badge_no",
    "iPassID","IPassID","iPass","i_pass_id","CardNo","card_no","card","IPASSID","IPass"
]
NAME_FIELD_CANDIDATES = [
    "FullName","Full Name","EmpName","Name","full_name","displayName","personName","PersonName"
]

# ----------------------------
# Utilities
# ----------------------------
def _find_active_employee_file():
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_employee{ext}"
        if p.exists():
            return p
    for p in DATA_DIR.iterdir():
        if p.is_file() and "active_employee" in p.name.lower():
            return p
    return None

def _normalize_key(k):
    if k is None:
        return None
    try:
        s = str(k).strip()
        return s if s != "" else None
    except Exception:
        return None

def _digits_only(s):
    if s is None:
        return ""
    return re.sub(r'\D+', '', str(s))

def _safe_int(v, default=0):
    try:
        if v is None or v == "":
            return default
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return default

def _week_monday_and_friday(ref_date: Optional[date] = None):
    if ref_date is None:
        ref_date = date.today()
    iso = ref_date.isoweekday()
    monday = ref_date - timedelta(days=(iso - 1))
    friday = monday + timedelta(days=4)
    return monday, friday

def _maybe_mark_on_leave(status_str: Optional[str]) -> bool:
    if not status_str:
        return False
    s = str(status_str).strip().lower()
    for tok in ("leave", "vacation", "on leave", "holiday", "sabbatical", "furlough", "loa"):
        if tok in s:
            return True
    return False

# ----------------------------
# Load active sheet
# ----------------------------
def load_active_employees_dataframe() -> pd.DataFrame:
    src = _find_active_employee_file()
    if not src:
        raise FileNotFoundError(f"Active employee canonical file not found in {DATA_DIR}")
    ext = src.suffix.lower()
    if ext in (".xls", ".xlsx"):
        df = pd.read_excel(src, sheet_name=0, dtype=str)
    else:
        df = pd.read_csv(src, dtype=str)
    df.columns = [c.strip() for c in df.columns]

    def _first_present(row, candidates):
        for c in candidates:
            if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
                return row[c]
        return None

    rows = []
    for _, row in df.iterrows():
        emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id','EmployeeNo','Employee No'])
        if emp_id is None:
            continue
        full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or (
            f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
        )
        location_city = _first_present(row, ['Location City','Location','City'])
        location_desc = _first_present(row, ['Location Description','LocationDescription'])
        location_state = _first_present(row, ['Location State / Province','Location State','State','Province'])
        region_code = _first_present(row, ['Region Code','Region','RegionCode'])
        current_status = _first_present(row, ['Current Status','Status','Employee Status'])
        employee_type = _first_present(row, ['Employee Type','Type','Time Type'])
        rows.append({
            "employee_id": _normalize_key(emp_id),
            "full_name": _normalize_key(full_name),
            "location_city": _normalize_key(location_city),
            "location_desc": _normalize_key(location_desc),
            "location_state": _normalize_key(location_state),
            "region_code": (str(region_code).strip() if region_code is not None else None),
            "current_status": _normalize_key(current_status),
            "employee_type": _normalize_key(employee_type),
            "raw_row": row.to_dict()
        })
    ndf = pd.DataFrame(rows)
    for col in ("employee_id","full_name","location_city","location_desc","location_state","region_code","current_status","employee_type","raw_row"):
        if col not in ndf:
            ndf[col] = None
    ndf = ndf[ndf["employee_id"].notna() & (ndf["employee_id"].str.strip() != "")]
    ndf.reset_index(drop=True, inplace=True)
    return ndf

# ----------------------------
# HTTP Session factory (retries/backoff)
# ----------------------------
def _build_requests_session():
    if requests is None:
        return None
    s = requests.Session()
    allowed = frozenset(['GET', 'HEAD'])
    try:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=allowed
        )
    except TypeError:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            method_whitelist=allowed
        )
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

# ----------------------------
# Prefetch / cache helpers
# ----------------------------
def prefetch_region_history(timeout: int = 10, force: bool = False):
    """
    Fetch region history entries (cached). Returns list of raw entries.
    """
    global REGION_HISTORY_CACHE, REGION_HISTORY_CACHE_FETCHED_AT
    try:
        now = datetime.utcnow()
        if not force and REGION_HISTORY_CACHE is not None and REGION_HISTORY_CACHE_FETCHED_AT:
            elapsed = (now - REGION_HISTORY_CACHE_FETCHED_AT).total_seconds()
            if elapsed < REGION_HISTORY_CACHE_TTL_SECONDS:
                logger.info("[region_cache] Using cached region history (age %.1fs)", elapsed)
                return REGION_HISTORY_CACHE

        entries = []
        # Prefer region_clients when available
        try:
            import region_clients
            logger.info("[region_cache] fetching region history via region_clients.fetch_all_history()")
            try:
                got = region_clients.fetch_all_history(timeout=timeout)
            except TypeError:
                got = region_clients.fetch_all_history()
            entries = got or []
        except Exception:
            entries = []

        # If empty, try direct requests to configured URLs
        if not entries and requests is not None:
            logger.info("[region_cache] fetching region history directly from endpoints")
            session = _build_requests_session() or requests
            for url in REGION_HISTORY_URLS:
                if not url:
                    continue
                try:
                    resp = session.get(url, timeout=(3, max(5, timeout)))
                    if not resp or resp.status_code != 200:
                        continue
                    try:
                        payload = resp.json()
                    except Exception:
                        continue
                    # flatten possible lists
                    if isinstance(payload, list):
                        for p in payload:
                            if isinstance(p, dict):
                                p['_source_url'] = url
                                entries.append(p)
                    elif isinstance(payload, dict):
                        found_list = False
                        for k in ("results","summaryByDate","details","data","entries","list","people","items"):
                            if k in payload and isinstance(payload[k], list):
                                for p in payload[k]:
                                    if isinstance(p, dict):
                                        p['_source_url'] = url
                                        entries.append(p)
                                found_list = True
                                break
                        if not found_list:
                            payload['_source_url'] = url
                            entries.append(payload)
                except requests.exceptions.RequestException as e:
                    logger.warning("[region_cache] fetch failed for %s: %s", url, str(e))
                    continue

        REGION_HISTORY_CACHE = entries or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        logger.info("[region_cache] prefetched %d region history entries", len(REGION_HISTORY_CACHE))
        return REGION_HISTORY_CACHE
    except Exception:
        logger.exception("[region_cache] prefetch failed")
        REGION_HISTORY_CACHE = REGION_HISTORY_CACHE or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        return REGION_HISTORY_CACHE

# ----------------------------
# Payload helpers (deep search)
# ----------------------------
def _iter_scalars_in_obj(obj, parent_key=""):
    """
    Yield all scalar key->value pairs inside nested dict/list structures as (key_path, value).
    """
    if isinstance(obj, dict):
        for k, v in obj.items():
            new_key = f"{parent_key}.{k}" if parent_key else str(k)
            if isinstance(v, (dict, list)):
                yield from _iter_scalars_in_obj(v, parent_key=new_key)
            else:
                yield new_key, v
    elif isinstance(obj, list):
        for idx, it in enumerate(obj):
            new_key = f"{parent_key}[{idx}]" if parent_key else f"[{idx}]"
            if isinstance(it, (dict, list)):
                yield from _iter_scalars_in_obj(it, parent_key=new_key)
            else:
                yield new_key, it

def _extract_details_from_payload(payload):
    """
    Normalize to list of dict rows that look like detail records.
    """
    if payload is None:
        return []
    if isinstance(payload, list):
        return [p for p in payload if isinstance(p, dict)]
    if isinstance(payload, dict):
        for k in ("details","results","data","entries","list","people","items"):
            v = payload.get(k)
            if isinstance(v, list):
                return [x for x in v if isinstance(x, dict)]
        # single-record-like payload (date-summary) -> return as single element to allow higher-level scanner to inspect fields
        if any(k in payload for k in ("date","Employee","Contractor","total","day")) and len(payload.keys()) <= 40:
            return [payload]
    return []

# ----------------------------
# ID matching helpers
# ----------------------------
def _match_candidate_to_employees(candidate_raw, orig_ids_set, orig_ids_list, digits_map):
    """
    Try many heuristics to map candidate_raw to one of the orig_ids_list.
    Returns matched_orig_id or None.
    """
    if candidate_raw is None:
        return None
    cand_str = str(candidate_raw).strip()
    if not cand_str:
        return None

    # direct exact match
    if cand_str in orig_ids_set:
        return cand_str

    # direct case-insensitive match
    for o in orig_ids_list:
        if isinstance(o, str) and cand_str.lower() == o.lower():
            return o

    # numeric transformations
    cand_digits = _digits_only(cand_str)
    if cand_digits:
        cand_nolead = cand_digits.lstrip('0') or cand_digits
        # direct digits match to original ids
        for o in orig_ids_list:
            if not isinstance(o, str):
                continue
            if o == cand_digits or o == cand_nolead:
                return o
            od = _digits_only(o)
            if od == cand_digits or od == cand_nolead:
                return o
        # numeric equality by int
        try:
            ci = int(cand_nolead)
            for o in orig_ids_list:
                try:
                    od = _digits_only(o)
                    if not od:
                        continue
                    oi = int(od.lstrip('0') or od)
                    if oi == ci:
                        return o
                except Exception:
                    continue
        except Exception:
            pass

        # last-n digits heuristics (conservative)
        if len(cand_digits) >= 3:
            for n in (6, 4):
                suf = cand_digits[-n:]
                if not suf:
                    continue
                for o in orig_ids_list:
                    od = _digits_only(o)
                    if od and od.endswith(suf):
                        return o

    # strip common prefixes and retry
    up = cand_str.upper()
    for pref in ("W", "IPASS", "IPASSID", "IPass"):
        if up.startswith(pref):
            stripped = cand_str[len(pref):]
            m = _match_candidate_to_employees(stripped, orig_ids_set, orig_ids_list, digits_map)
            if m:
                return m

    # fallback: find numeric substrings inside string and try mapping
    for match in re.finditer(r'(\d{3,})', cand_str):
        ssub = match.group(1)
        m = _match_candidate_to_employees(ssub, orig_ids_set, orig_ids_list, digits_map)
        if m:
            return m

    return None

# ----------------------------
# Region history scanning -> build presence map
# ----------------------------
def _fetch_presence_from_region_histories(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None, preloaded_entries: Optional[List[dict]] = None):
    """
    Scans preloaded_entries (or global cache) and returns presence mapping: {employee_id -> {date: 0/1}}
    """
    presence = {eid: {} for eid in employee_ids}
    if not employee_ids:
        return presence

    if preloaded_entries is None:
        global REGION_HISTORY_CACHE
        preloaded_entries = REGION_HISTORY_CACHE or []

    if not preloaded_entries:
        logger.info("[region_history] no preloaded region history entries to scan")
    else:
        orig_ids_list = [str(e).strip() for e in employee_ids]
        orig_ids_set = set(orig_ids_list)
        digits_map = {e: _digits_only(e) for e in orig_ids_list}

        scanned = 0
        matched = 0
        examples_matched = []

        for entry in preloaded_entries:
            detail_rows = []
            if isinstance(entry, dict):
                # prefer explicit lists
                for key in ("details","people","items","list","results","entries","data"):
                    if key in entry and isinstance(entry.get(key), list):
                        detail_rows = [r for r in entry.get(key) if isinstance(r, dict)]
                        break
                if not detail_rows:
                    # treat entry itself as a candidate detail row if it has plausible fields (timestamp or id-like)
                    candidate_keys = set(ID_FIELD_CANDIDATES + CARD_FIELD_CANDIDATES + ["date","timestamp","time","SwipeDate","LocaleMessageTime","day"])
                    if any(k in entry for k in candidate_keys):
                        detail_rows = [entry]
                    else:
                        # try extract via helper (covers nested payload shapes)
                        detail_rows = _extract_details_from_payload(entry) or []
            else:
                continue

            scanned += len(detail_rows)
            for d in detail_rows:
                try:
                    # find a timestamp / date for row
                    ts = None
                    # common keys first
                    for tkey in ("LocaleMessageTime","LocaleMessageDateTime","SwipeDate","SwipeTime","timestamp","time","DateTime","date","swipeDate","day"):
                        if tkey in d and d.get(tkey):
                            ts = d.get(tkey)
                            break

                    # fallback: scan scalar values in row for iso-like date
                    if ts is None:
                        for k, v in _iter_scalars_in_obj(d):
                            if v is None:
                                continue
                            try:
                                s = str(v)
                            except Exception:
                                continue
                            # quick heuristic
                            if re.search(r'\d{4}-\d{2}-\d{2}', s) or re.search(r'\d{2}/\d{2}/\d{4}', s):
                                ts = s
                                break

                    if ts is None:
                        # skip row (no date info)
                        continue

                    # parse timestamp into date (robust)
                    t = None
                    if isinstance(ts, (int, float)):
                        try:
                            t = datetime.fromtimestamp(int(ts))
                        except Exception:
                            try:
                                t = datetime.utcfromtimestamp(int(ts) / 1000.0)
                            except Exception:
                                t = None
                    elif isinstance(ts, str):
                        s = ts.strip()
                        if s == "":
                            t = None
                        else:
                            parsed = None
                            # try dateutil if available (best)
                            try:
                                from dateutil import parser as _parser
                                parsed = _parser.parse(s)
                            except Exception:
                                parsed = None
                            if parsed:
                                try:
                                    if parsed.tzinfo is not None:
                                        parsed = parsed.astimezone(tz=None).replace(tzinfo=None)
                                except Exception:
                                    pass
                                t = parsed
                            else:
                                # try common formats
                                fmts = [
                                    "%Y-%m-%dT%H:%M:%S.%fZ",
                                    "%Y-%m-%dT%H:%M:%S.%f",
                                    "%Y-%m-%dT%H:%M:%S",
                                    "%Y-%m-%d %H:%M:%S",
                                    "%Y-%m-%d",
                                    "%d/%m/%Y %H:%M:%S",
                                    "%d/%m/%Y"
                                ]
                                for f in fmts:
                                    try:
                                        t = datetime.strptime(s, f)
                                        break
                                    except Exception:
                                        t = None
                    if t is None:
                        continue

                    dt = t.date()
                    if dt < start_date or dt > end_date:
                        continue

                    # partition filter (if provided)
                    if partition_filter:
                        part_values = []
                        for k in ("PartitionNameFriendly","PartitionName","PrimaryLocation","partition","location","Partition","Location","Site","PartitionName1","PartitionName2"):
                            v = d.get(k)
                            if v:
                                part_values.append(str(v))
                        if part_values:
                            ok = any(part_filter_match(part, partition_filter) for part in part_values)
                            if not ok:
                                continue
                        else:
                            # no partition info -> skip when filter exists
                            continue

                    # attempt to match explicit id keys first
                    matched_key = None
                    for k in ID_FIELD_CANDIDATES:
                        if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                            m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                            if m:
                                matched_key = m
                                break

                    # try card fields
                    if not matched_key:
                        for k in CARD_FIELD_CANDIDATES:
                            if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                                m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break

                    # deep-scan scalars for numeric substrings and name fields
                    if not matched_key:
                        for key_path, val in _iter_scalars_in_obj(d):
                            if val is None:
                                continue
                            sval = str(val)
                            # numeric substring preference
                            if re.search(r'\d{3,}', sval):
                                m = _match_candidate_to_employees(sval, orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break
                        # check name fields (disabled by default to avoid false positives)

                    if matched_key:
                        matched += 1
                        # coerce matched_key to exact string from orig list
                        matched_key = next((o for o in orig_ids_list if str(o).strip() == str(matched_key).strip()), str(matched_key).strip())
                        presence.setdefault(matched_key, {})
                        presence[matched_key][dt] = 1
                        if len(examples_matched) < 10:
                            examples_matched.append({"matched": matched_key, "date": dt.isoformat(), "sample_row_keys": list(d.keys())[:8]})
                except Exception:
                    continue

        logger.info("[region_history] scanned %d detail rows from preloaded entries; matched %d presence entries", scanned, matched)
        if examples_matched:
            logger.debug("[region_history] example matches (up to 10): %s", examples_matched)

    # fill zeros for any missing date
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            presence.setdefault(eid, {})
            if cur not in presence[eid]:
                presence[eid][cur] = 0
        cur = cur + timedelta(days=1)

    return presence

def part_filter_match(src_val, partition_filter):
    try:
        if not src_val:
            return False
        return partition_filter.strip().lower() in str(src_val).strip().lower()
    except Exception:
        return False

# ----------------------------
# DB / combined fetch
# ----------------------------
def _fetch_presence_for_employees(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None):
    """
    1) chunked DB IN queries (AttendanceSummary)
    2) fallback broad DB query
    3) fallback region history cache scan (prefetch_region_history)
    Returns mapping {employee_id: {date: 0/1}}
    """
    if not employee_ids:
        return {}

    orig_ids = [str(e).strip() for e in employee_ids]
    norm_id_set = set([s for s in orig_ids if s])
    result = {eid: {} for eid in orig_ids}

    # 1) chunked DB fetch
    rows = []
    chunk_size = 500
    try:
        with SessionLocal() as db:
            for i in range(0, len(orig_ids), chunk_size):
                chunk = orig_ids[i:i+chunk_size]
                try:
                    q = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date,
                        AttendanceSummary.employee_id.in_(chunk)
                    )
                    rows_chunk = q.all()
                    if rows_chunk:
                        rows.extend(rows_chunk)
                except Exception:
                    logger.exception("chunked query failed (continuing)")
                    continue

            # fallback broad query if none found
            if not rows:
                try:
                    rows = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date
                    ).all()
                    logger.info("[presence_fetch] fallback broad DB query returned %d rows for %s -> %s", len(rows), start_date, end_date)
                except Exception:
                    logger.exception("fallback broad DB query failed")
                    rows = []
    except Exception:
        logger.exception("DB session error in _fetch_presence_for_employees")
        rows = []

    # map DB rows to provided employee ids
    for r in rows:
        try:
            raw = r.employee_id
            if raw is None:
                continue
            db_key = str(raw).strip()
            match_key = None
            if db_key in norm_id_set:
                match_key = db_key
            else:
                digits = _digits_only(db_key)
                if digits:
                    cand = digits.lstrip('0') or digits
                    if cand in norm_id_set:
                        match_key = cand
                if match_key is None:
                    for o in orig_ids:
                        if o == db_key or o.lstrip('0') == db_key or db_key.lstrip('0') == o:
                            match_key = o
                            break
            if not match_key:
                continue
            d = r.date
            present = 0
            try:
                present = int(r.presence_count or 0)
            except Exception:
                present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
            result.setdefault(match_key, {})
            prev = result[match_key].get(d, 0)
            result[match_key][d] = 1 if (prev == 1 or present > 0) else 0
        except Exception:
            continue

    # fill zeros
    cur = start_date
    while cur <= end_date:
        for eid in orig_ids:
            result.setdefault(eid, {})
            if cur not in result[eid]:
                result[eid][cur] = 0
        cur = cur + timedelta(days=1)

    db_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] DB-derived presence found for %d/%d employees", db_positive, len(orig_ids))

    # fallback to region details/history if needed
    if db_positive == 0 or db_positive < max(10, int(0.1 * len(orig_ids))):
        try:
            logger.info("[presence_fetch] DB coverage low (%d/%d) - trying region occupancy detail/history fallback", db_positive, len(orig_ids))

            # 1) Try region_clients.fetch_all_details() first (per-person rows are most useful)
            try:
                import region_clients
                details = []
                if hasattr(region_clients, "fetch_all_details"):
                    try:
                        details = region_clients.fetch_all_details(timeout=10)
                    except TypeError:
                        details = region_clients.fetch_all_details()
                details = details or []
                if details:
                    logger.info("[presence_fetch] fetched %d detail rows from region_clients.fetch_all_details()", len(details))
                    # attempt to match details to employee ids and fill presence
                    for d in details:
                        try:
                            # extract candidate id fields (fast checks)
                            cand_fields = []
                            for k in ("EmployeeID","employee_id","EmpID","CardNumber","Card","CardNo","IPassID","iPassID","PersonGUID","PersonId"):
                                v = d.get(k) if isinstance(d, dict) else None
                                if v and str(v).strip():
                                    cand_fields.append(v)
                            # if no explicit candidate id, attempt to deep-scan for numeric substring
                            if not cand_fields:
                                for key_path, val in _iter_scalars_in_obj(d):
                                    if val is None:
                                        continue
                                    sval = str(val)
                                    if re.search(r'\d{3,}', sval):
                                        cand_fields.append(sval)
                            # determine date for this row (fast heuristics)
                            ts = None
                            for tkey in ("LocaleMessageTime","SwipeDate","SwipeTime","timestamp","time","DateTime","date"):
                                if tkey in d and d.get(tkey):
                                    ts = d.get(tkey)
                                    break
                            if ts is None:
                                # quick scalar scan for iso date fragment
                                for key_path, val in _iter_scalars_in_obj(d):
                                    if val is None:
                                        continue
                                    s = str(val)
                                    if re.search(r'\d{4}-\d{2}-\d{2}', s) or re.search(r'\d{2}/\d{2}/\d{4}', s):
                                        ts = s
                                        break
                            if ts is None:
                                continue
                            # parse to date - handle common string formats without heavy dateutil
                            parsed_dt = None
                            if isinstance(ts, (int, float)):
                                try:
                                    parsed_dt = datetime.fromtimestamp(int(ts))
                                except Exception:
                                    try:
                                        parsed_dt = datetime.utcfromtimestamp(int(ts) / 1000.0)
                                    except Exception:
                                        parsed_dt = None
                            elif isinstance(ts, str):
                                s = ts.strip()
                                # try ISO-like fast parse
                                m = re.search(r'(\d{4}-\d{2}-\d{2})', s)
                                if m:
                                    try:
                                        parsed_dt = datetime.fromisoformat(m.group(1))
                                    except Exception:
                                        try:
                                            parsed_dt = datetime.strptime(m.group(1), "%Y-%m-%d")
                                        except Exception:
                                            parsed_dt = None
                                else:
                                    # fallback: try a couple common formats
                                    for f in ("%Y-%m-%dT%H:%M:%S.%fZ","%Y-%m-%dT%H:%M:%S","%d/%m/%Y %H:%M:%S","%d/%m/%Y"):
                                        try:
                                            parsed_dt = datetime.strptime(s, f)
                                            break
                                        except Exception:
                                            parsed_dt = None
                            if parsed_dt is None:
                                continue
                            dt = parsed_dt.date()
                            if dt < start_date or dt > end_date:
                                continue

                            # attempt to map candidate fields to orig_ids using the same matching helper
                            for cand in cand_fields:
                                try:
                                    m = _match_candidate_to_employees(cand, set(orig_ids), orig_ids, {})  # re-use existing matcher
                                    if m:
                                        # coerce matched to orig string
                                        matched_key = next((o for o in orig_ids if str(o).strip() == str(m).strip()), str(m).strip())
                                        result.setdefault(matched_key, {})
                                        prev = result[matched_key].get(dt, 0)
                                        result[matched_key][dt] = 1 if (prev == 1 or 1) else prev
                                        break
                                except Exception:
                                    continue
                        except Exception:
                            continue
                else:
                    logger.info("[presence_fetch] region_clients.fetch_all_details returned no details")
            except Exception:
                logger.exception("region_clients.fetch_all_details fallback failed (continuing)")

            # 2) ensure region history cache is populated and try scanning preloaded history (less precise)
            try:
                prefetch_region_history()
                region_presence = _fetch_presence_from_region_histories(orig_ids, start_date, end_date, partition_filter=partition_filter, preloaded_entries=REGION_HISTORY_CACHE)
                for eid in orig_ids:
                    rp = region_presence.get(eid, {})
                    for d, v in rp.items():
                        if v and result.setdefault(eid, {}).get(d, 0) == 0:
                            result[eid][d] = 1
            except Exception:
                logger.exception("region_history fallback failed")
        except Exception:
            logger.exception("detail/history fallback collapsed")

    final_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] final presence coverage: %d/%d employees have at least one positive day", final_positive, len(orig_ids))
    return result

# ----------------------------
# Sheet vs AttendanceSummary comparison
# ----------------------------
def _compare_sheet_vs_db_summary(sel_df: pd.DataFrame, start_date: date, end_date: date) -> Dict[str, Any]:
    """
    For each employee in sel_df compute DB-derived weekly presence totals and produce mismatch diagnostics.
    Returns dict with:
      - per_employee_summary (employee_id -> {sheet:..., db_total:..., db_by_date: {...}})
      - missing_in_summary (list of employee rows with zero DB presence)
      - mismatches (list where sheet vs DB total differ)
      - extra_summary_ids (IDs present in DB during week but not in sheet selection)
    """
    out = {
        "per_employee_summary": {},
        "missing_in_summary": [],
        "mismatches": [],
        "extra_summary_ids": []
    }
    employee_ids = sel_df["employee_id"].astype(str).str.strip().tolist()
    employee_set = set(employee_ids)

    # fetch attendance rows for the range and for relevant employee ids (DB query)
    try:
        with SessionLocal() as db:
            rows = db.query(AttendanceSummary).filter(
                AttendanceSummary.date >= start_date,
                AttendanceSummary.date <= end_date
            ).all()
    except Exception:
        logger.exception("Failed to query AttendanceSummary for sheet-vs-db comparison; will attempt per-id chunked queries")
        rows = []

    # Map DB rows by normalized employee id
    db_map = {}
    for r in rows:
        try:
            rid = r.employee_id
            if rid is None:
                continue
            rk = str(rid).strip()
            # prefer exact string match to sheet ids
            candidates = [rk]
            digits = _digits_only(rk)
            if digits:
                candidates.append(digits.lstrip('0') or digits)
            # find best matching sheet id
            matched = None
            for c in candidates:
                if c in employee_set:
                    matched = c
                    break
            if not matched:
                # try more expensive matching
                for s in employee_ids:
                    if s == rk or rk.lstrip('0') == s or s.lstrip('0') == rk:
                        matched = s
                        break
            if not matched:
                # add as extra (db-only) under its raw key
                db_map.setdefault(rk, {})
                db_map[rk].setdefault(r.date, 0)
                try:
                    present = int(r.presence_count or 0)
                except Exception:
                    present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
                db_map[rk][r.date] = 1 if (db_map[rk].get(r.date, 0) == 1 or present > 0) else db_map[rk].get(r.date, 0)
            else:
                db_map.setdefault(matched, {})
                db_map[matched].setdefault(r.date, 0)
                try:
                    present = int(r.presence_count or 0)
                except Exception:
                    present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
                db_map[matched][r.date] = 1 if (db_map[matched].get(r.date, 0) == 1 or present > 0) else db_map[matched].get(r.date, 0)
        except Exception:
            continue

    # ensure all sheet employees have entries per day (0 default)
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            db_map.setdefault(eid, {})
            if cur not in db_map[eid]:
                db_map[eid][cur] = 0
        cur = cur + timedelta(days=1)

    # compute per-employee summary and mismatches
    for eid in employee_ids:
        try:
            db_by_date = {d.isoformat(): int(v) for d, v in db_map.get(eid, {}).items()}
            db_total = sum(int(v) for v in db_map.get(eid, {}).values())
            # sheet-side: we don't have presence per-day on the sheet; so sheet's weekly presence is unknown.
            # But we can at least mark employees missing from DB or with low coverage.
            entry = {
                "employee_id": eid,
                "full_name": sel_df[sel_df["employee_id"] == eid]["full_name"].iloc[0] if not sel_df[sel_df["employee_id"] == eid].empty else None,
                "db_total_week_presence": db_total,
                "db_by_date": db_by_date
            }
            out["per_employee_summary"][eid] = entry
            if db_total == 0:
                out["missing_in_summary"].append(entry)
            # If you had a target expected presence from sheet metadata you could compare here; for now, put thresholds:
            # We'll treat employees with db_total < 3 (for regulars) as defaulters -> included in mismatches so they are visible
            out["mismatches"].append(entry) if db_total < 3 else None
        except Exception:
            continue

    # compute extras: DB IDs with presence >0 that are not in sheet employee_ids
    extras = []
    for dbid, bydates in db_map.items():
        try:
            if dbid not in employee_set:
                total = sum(int(v) for v in bydates.values())
                if total > 0:
                    extras.append({"employee_id": dbid, "db_total_week_presence": total, "db_by_date": {d.isoformat(): v for d, v in bydates.items()}})
        except Exception:
            continue
    out["extra_summary_ids"] = extras

    return out

# ----------------------------
# Main compare function (public)
# ----------------------------
def compare_ccure_vs_sheets(
    mode: str = "full",
    stats_detail: str = "ActiveProfiles",
    limit_list: int = 200,
    export: bool = False,
    region_filter: Optional[str] = None,
    location_city: Optional[str] = None,
    location_state: Optional[str] = None,
    location_description: Optional[str] = None,
    week_ref_date: Optional[str] = None
):
    # compute week window
    if week_ref_date:
        monday, friday = _week_monday_and_friday(date.fromisoformat(week_ref_date))
    else:
        monday, friday = _week_monday_and_friday(date.today())

    try:
        df = load_active_employees_dataframe()
    except Exception as e:
        logger.exception("Failed to load active employees sheet")
        return {"error": f"active sheet load failed: {e}"}

    rf = region_filter.strip().lower() if region_filter else None
    lc = location_city.strip().lower() if location_city else None
    ls = location_state.strip().lower() if location_state else None
    ld = location_description.strip().lower() if location_description else None

    sel = df.copy()
    if rf:
        sel = sel[sel["region_code"].fillna("").str.strip().str.lower() == rf]
    if lc:
        sel = sel[sel["location_city"].fillna("").str.strip().str.lower() == lc]
    if ls:
        sel = sel[sel["location_state"].fillna("").str.strip().str.lower() == ls]
    if ld:
        sel = sel[sel["location_desc"].fillna("").str.strip().str.lower() == ld]

    total_active = len(sel)
    employee_ids = sel["employee_id"].astype(str).str.strip().tolist()

    # Ensure region history cache is primed (should be done by caller/app)
    try:
        prefetch_region_history(timeout=10)
    except Exception:
        logger.exception("prefetch_region_history failed (continuing)")

    # presence_map: best-effort using DB + region history fallback
    presence_map = _fetch_presence_for_employees(employee_ids, monday, friday, partition_filter=location_city)

    # compute today count (today inside week may differ; we compute today's presence using presence_map and DB fallback)
    today = date.today()
    today_count = 0
    for eid in employee_ids:
        pm = presence_map.get(eid, {})
        if pm.get(today, 0) > 0:
            today_count += 1
        else:
            # fallback DB check
            try:
                with SessionLocal() as db:
                    row = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == eid, AttendanceSummary.date == today).first()
                    if row and getattr(row, "presence_count", 0) > 0:
                        today_count += 1
                        continue
                    digits = _digits_only(eid)
                    if digits:
                        cand = digits.lstrip('0') or digits
                        row2 = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == cand, AttendanceSummary.date == today).first()
                        if row2 and getattr(row2, "presence_count", 0) > 0:
                            today_count += 1
            except Exception:
                continue

    today_pct = round((today_count / float(total_active)) * 100.0, 2) if total_active > 0 else None

    sel["on_leave"] = sel["current_status"].apply(lambda x: _maybe_mark_on_leave(x))
    leave_count = int(sel["on_leave"].sum())
    sel["employee_type_norm"] = sel["employee_type"].fillna("").str.strip().str.lower()
    type_counts = sel["employee_type_norm"].value_counts().to_dict()

    regular_df = sel[sel["employee_type_norm"].str.contains("regular", na=False)]
    regular_ids = regular_df["employee_id"].astype(str).str.strip().tolist()

    regular_presence = {}
    for eid in regular_ids:
        week_map = presence_map.get(eid, {})
        days_present = sum(1 for d, v in week_map.items() if v and (monday <= d <= friday))
        days_present = int(days_present)
        regular_presence[eid] = {
            "days_present": days_present,
            "on_leave": bool(sel[sel["employee_id"] == eid]["on_leave"].any()),
            "full_name": sel[sel["employee_id"] == eid]["full_name"].iloc[0] if not sel[sel["employee_id"] == eid].empty else None
        }

    present_5_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 5)
    present_3_or_more_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 3)
    present_less_than_3_count = sum(1 for v in regular_presence.values() if v["days_present"] < 3)
    present_only_1_count = sum(1 for v in regular_presence.values() if v["days_present"] == 1)

    present_5_list = []
    present_3_list = []
    defaulters_list = []

    for eid, info in regular_presence.items():
        entry = {
            "employee_id": eid,
            "full_name": info["full_name"],
            "days_present": info["days_present"],
            "on_leave": info["on_leave"]
        }
        if info["days_present"] >= 5:
            present_5_list.append(entry)
        if info["days_present"] >= 3:
            present_3_list.append(entry)
        if info["days_present"] < 3:
            defaulters_list.append(entry)

    present_5_list = sorted(present_5_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    present_3_list = sorted(present_3_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    defaulters_list = sorted(defaulters_list, key=lambda x: (x["days_present"], x["on_leave"], x["full_name"] or ""))[:limit_list]

    # NEW: explicit sheet vs DB summary comparison diagnostics
    try:
        sheet_vs_summary = _compare_sheet_vs_db_summary(sel, monday, friday)
    except Exception:
        logger.exception("sheet vs summary comparison failed")
        sheet_vs_summary = {"error": "comparison failure"}

    summary = {
        "filters": {
            "region": region_filter,
            "location_city": location_city,
            "location_state": location_state,
            "location_description": location_description,
            "week_monday": monday.isoformat(),
            "week_friday": friday.isoformat()
        },
        "counts": {
            "total_active_in_sheet": total_active,
            "today_headcount_from_summary": today_count,
            "today_headcount_pct_vs_sheet": today_pct,
            "on_leave_count_in_sheet": leave_count,
            "employee_type_counts": type_counts
        },
        "regular_attendance_summary": {
            "regular_total": len(regular_ids),
            "present_5_day_count": present_5_count,
            "present_3_or_more_count": present_3_or_more_count,
            "present_less_than_3_count": present_less_than_3_count,
            "present_only_1_day_count": present_only_1_count
        },
        # attach diagnostics
        "sheet_vs_summary": sheet_vs_summary
    }

    details = {
        "present_5_days": present_5_list,
        "present_3_or_more_days": present_3_list,
        "defaulters_less_than_3_days": defaulters_list,
        "sheet_vs_summary_rows": sheet_vs_summary.get("per_employee_summary") if isinstance(sheet_vs_summary, dict) else {}
    }

    report_path = None
    if export:
        try:
            report_name = f"attendance_compare_{(region_filter or 'all')}_{(location_city or 'all')}_{uuid.uuid4().hex[:8]}.xlsx"
            report_file = OUTPUT_DIR / report_name
            with pd.ExcelWriter(report_file, engine="openpyxl") as writer:
                sel_df_for_export = sel.copy()
                sel_df_for_export["raw_row_str"] = sel_df_for_export["raw_row"].apply(lambda r: str(r) if r is not None else "")
                sel_df_for_export.to_excel(writer, sheet_name="active_sheet_selection", index=False)
                pd.DataFrame([summary["counts"]]).to_excel(writer, sheet_name="summary_counts", index=False)
                pd.DataFrame([summary["regular_attendance_summary"]]).to_excel(writer, sheet_name="regular_summary", index=False)
                pd.DataFrame(details["present_5_days"]).to_excel(writer, sheet_name="present_5_days", index=False)
                pd.DataFrame(details["present_3_or_more_days"]).to_excel(writer, sheet_name="present_3_plus", index=False)
                pd.DataFrame(details["defaulters_less_than_3_days"]).to_excel(writer, sheet_name="defaulters_lt3", index=False)
                # sheet vs summary diagnostics
                try:
                    # per_employee_summary -> long table
                    per_emp = sheet_vs_summary.get("per_employee_summary") or {}
                    per_emp_rows = []
                    for k, v in per_emp.items():
                        r = {"employee_id": k, "full_name": v.get("full_name"), "db_total_week_presence": v.get("db_total_week_presence")}
                        # flatten db_by_date
                        for d, val in sorted((v.get("db_by_date") or {}).items()):
                            r[f"day_{d}"] = val
                        per_emp_rows.append(r)
                    if per_emp_rows:
                        pd.DataFrame(per_emp_rows).to_excel(writer, sheet_name="sheet_vs_db_per_employee", index=False)
                    if sheet_vs_summary.get("missing_in_summary"):
                        pd.DataFrame(sheet_vs_summary.get("missing_in_summary")).to_excel(writer, sheet_name="missing_in_summary", index=False)
                    if sheet_vs_summary.get("extra_summary_ids"):
                        pd.DataFrame(sheet_vs_summary.get("extra_summary_ids")).to_excel(writer, sheet_name="extra_summary_ids", index=False)
                except Exception:
                    logger.exception("Failed writing sheet_vs_summary sections to export")
            report_path = str(report_file.name)
        except Exception:
            logger.exception("Failed to write export report")
            report_path = None

    out = {
        "mode": mode,
        "stats_detail": stats_detail,
        "summary": summary,
        "details": details
    }
    if report_path:
        out["report_path"] = report_path
    return out


if __name__ == "__main__":
    res = compare_ccure_vs_sheets(region_filter="APAC", location_city="Pune", export=False, limit_list=20)
    import json as _json
    print(_json.dumps(res, indent=2, default=str))
















Iniotailly resolve this issue carefully and share me fully Updatedd file carefully...

In Below API Only Active Employee Sheet reads .and Sheet data Count is Correct but they are not compare Sheet data With Summary each API 
I want to do that comparision carefully so how We can solve this issue ...
And Share me Fully Updated file carefully


http://localhost:8000/ccure/compare_v2
{
  "mode": "full",
  "stats_detail": "ActiveProfiles",
  "summary": {
    "filters": {
      "region": null,
      "location_city": null,
      "location_state": null,
      "location_description": null,
      "week_monday": "2025-09-01",
      "week_friday": "2025-09-05"
    },
    "counts": {
      "total_active_in_sheet": 8625,
      "today_headcount_from_summary": 1,
      "today_headcount_pct_vs_sheet": 0.01,
      "on_leave_count_in_sheet": 294,
      "employee_type_counts": {
        "regular": 8448,
        "fixed term": 146,
        "temporary": 13,
        "casual": 12,
        "intern": 5,
        "trainee": 1
      }
    },
    "regular_attendance_summary": {
      "regular_total": 8448,
      "present_5_day_count": 0,
      "present_3_or_more_count": 0,
      "present_less_than_3_count": 8448,
      "present_only_1_day_count": 3
    }
  },
  "details": {
    "present_5_days": [],
    "present_3_or_more_days": [],
    "defaulters_less_than_3_days": [
      {
        "employee_id": "319473",
        "full_name": "., Anushka",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "324002",
        "full_name": "., Diwakar",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "324422",
        "full_name": "., LEONARDO Tertuliano Monsani",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "323879",
        "full_name": "., Vikas",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "322825",
        "full_name": "AKTER, SULTANA",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "320818",
        "full_name": "ANAND, NUPUR",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "328795",
        "full_name": "ARRIOLA BORDON, ALICIA",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "312895",
        "full_name": "Aassar, Maisa",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "321269",
        "full_name": "Abaca Paez, Agustin Marcelo",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "325862",
        "full_name": "Abaca, Melisa Ayelen",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "325299",
        "full_name": "Abadilla, Ma. Erika",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "328996",
        "full_name": "Abalos, Larry",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "326980",
        "full_name": "Abancio, Mechel",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "328945",
        "full_name": "Abang, Emmanuel E.",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "321388",
        "full_name": "Abanto Reyes, Christian",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "307689",
        "full_name": "Abarca Torres, Kevin Andrey",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "322554",
        "full_name": "Abarca, Joshua",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "309123",
        "full_name": "Abarzosa, Rowegine",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "323056",
        "full_name": "Abastos Manyari, Erika Natalia",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "250864",
        "full_name": "Abate, Miriam Bibiana",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "328869",
        "full_name": "Abbude, Tcio",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "195823",
        "full_name": "Abdelali, Tarik",
        "days_present": 0,
        "on_leave": false
      },










# # app.py (top portion) - minor cleanup (remove duplicate import)
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import shutil
import uuid
import json
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
import re
import asyncio
from typing import Optional, Dict, Any

# --- DB / models imports (kept for endpoints that still use DB like headcount/attendance fallback) ---
from db import SessionLocal
from models import LiveSwipe, AttendanceSummary

# --- settings (optional override) ---
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"

import sys  # ensure logging stream available
RAW_UPLOADS_DIR = DATA_DIR / "raw_uploads"
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)




REGION_TIMEOUT_SECONDS = 30
# ----------------------------------------------------------

_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

_broadcaster_clients = set()

def broadcast_ccure_update(payload: dict):
    if not _broadcaster_clients:
        return
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = None
    for q in list(_broadcaster_clients):
        try:
            if loop and loop.is_running():
                loop.call_soon_threadsafe(q.put_nowait, payload)
            else:
                q.put_nowait(payload)
        except Exception:
            logger.exception("Failed to push payload to SSE client (will remove client)")
            try:
                _broadcaster_clients.discard(q)
            except Exception:
                pass

async def _sse_event_generator(client_queue: asyncio.Queue):
    try:
        while True:
            payload = await client_queue.get()
            try:
                data = json.dumps(payload, default=str)
            except Exception:
                data = json.dumps({"error": "serialization error", "payload": str(payload)})
            yield f"data: {data}\n\n"
    finally:
        try:
            _broadcaster_clients.discard(client_queue)
        except Exception:
            pass
        return

@app.get("/ccure/stream")
async def ccure_stream():
    q = asyncio.Queue()
    _broadcaster_clients.add(q)
    generator = _sse_event_generator(q)
    headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    return StreamingResponse(generator, media_type="text/event-stream", headers=headers)

def _guess_region_from_text(txt: str) -> str:
    if not txt:
        return "unknown"
    s = str(txt).strip().lower()
    s = re.sub(r"[,\-/()]", " ", s)
    if any(k in s for k in ("pune","quezon city","taguig city","bengaluru","hyderabad","chennai","manila","singapore","hong kong","beijing","shanghai","jakarta","kuala","osaka","tokyo","seoul","bangkok")):
        return "apac"
    if any(k in s for k in ("london","dublin","paris","frankfurt","amsterdam","stockholm","cape town","johannesburg","berlin","brussels","madrid","rome","milan")):
        return "emea"
    if any(k in s for k in ("mexico","bogota","buenos","santiago","sao","salvador","lima","caracas")):
        return "laca"
    if any(k in s for k in ("denver","new york","ny","chicago","toronto","vancouver","los angeles","san francisco","boston","houston","atlanta","miami")):
        return "namer"
    return "unknown"

@app.get("/headcount")
def api_headcount():
    try:
        totals = {"apac": 0, "emea": 0, "laca": 0, "namer": 0, "unknown": 0}
        with SessionLocal() as db:
            try:
                today = date.today()
                rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                if rows:
                    for r in rows:
                        try:
                            partition = None
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                            loc = partition or "unknown"
                            region = _guess_region_from_text(loc)
                            totals[region] = totals.get(region, 0) + 1
                        except Exception:
                            totals["unknown"] += 1
                else:
                    start = datetime.combine(today, datetime.min.time())
                    end = datetime.combine(today, datetime.max.time())
                    swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
                    for s in swipes:
                        loc = s.partition or "unknown"
                        region = _guess_region_from_text(loc)
                        totals[region] = totals.get(region, 0) + 1
            except Exception:
                logger.exception("Failed to compute headcount regions")
        out = {
            "apac": int(totals.get("apac", 0)),
            "emea": int(totals.get("emea", 0)),
            "laca": int(totals.get("laca", 0)),
            "namer": int(totals.get("namer", 0))
        }
        return JSONResponse(out)
    except Exception as exc:
        logger.exception("api_headcount failed")
        raise HTTPException(status_code=500, detail=f"headcount error: {exc}")

# ---------- Helpers retained (normalize / safe conversions) -------------
def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- build_ccure_averages (fallback) ------------------------------
def build_ccure_averages(start_date: Optional[str] = None, end_date: Optional[str] = None):
    """
    Fallback averages computation using AttendanceSummary only.
    Returns a compact shape (live_today, ccure_active, averages, notes).
    """
    try:
        def _parse_date_param(s):
            if not s:
                return None
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                try:
                    return date.fromisoformat(s)
                except Exception:
                    return None

        today = date.today()
        start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
        end_obj = _parse_date_param(end_date) if end_date else today
        if start_obj is None or end_obj is None or start_obj > end_obj:
            start_obj = today - timedelta(days=6)
            end_obj = today

        with SessionLocal() as db:
            try:
                att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
            except Exception:
                logger.exception("Failed to query AttendanceSummary")
                att_rows = []

            live_emp = 0
            live_contr = 0
            unknown_count = 0
            seen_keys = set()

            def classify_from_derived(derived):
                try:
                    if not derived or not isinstance(derived, dict):
                        return "contractor"
                    for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                        v = derived.get(k)
                        if v and "employee" in str(v).strip().lower():
                            return "employee"
                    for k in ("Employee_Status","Employee Status","Status"):
                        v = derived.get(k)
                        if v and "terminated" in str(v).strip().lower():
                            return "employee"
                    return "contractor"
                except Exception:
                    return "contractor"

            if att_rows:
                for a in att_rows:
                    key = None
                    try:
                        key = _normalize_employee_key(a.employee_id)
                    except Exception:
                        key = None
                    if not key:
                        try:
                            key = _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None)
                        except Exception:
                            key = None
                    if not key:
                        unknown_count += 1
                        continue
                    if key in seen_keys:
                        continue
                    seen_keys.add(key)
                    cls = classify_from_derived(a.derived)
                    if cls == "employee":
                        live_emp += 1
                    elif cls == "contractor":
                        live_contr += 1
                    else:
                        unknown_count += 1

                live_total_reported = live_emp + live_contr + unknown_count
                live_total_details = len(att_rows)
            else:
                live_total_reported = 0
                live_total_details = 0
                try:
                    import region_clients
                    details = []
                    try:
                        details = region_clients.fetch_all_details(timeout=REGION_TIMEOUT_SECONDS) or []
                    except Exception:
                        logger.exception("region_clients.fetch_all_details failed in build_ccure_averages()")
                        details = []
                    if details:
                        for d in details:
                            try:
                                cls = "contractor"
                                for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                                    v = d.get(k)
                                    if v and "employee" in str(v).strip().lower():
                                        cls = "employee"
                                        break
                                if cls == "employee":
                                    live_emp += 1
                                else:
                                    live_contr += 1
                                live_total_details += 1
                            except Exception:
                                continue
                        live_total_reported = live_emp + live_contr
                    else:
                        try:
                            regions = region_clients.fetch_all_regions(timeout=REGION_TIMEOUT_SECONDS) or []
                            for r in regions:
                                try:
                                    c = r.get("count")
                                    if isinstance(c, (int, float)):
                                        live_total_reported += int(c)
                                except Exception:
                                    continue
                        except Exception:
                            logger.exception("region_clients.fetch_all_regions failed in build_ccure_averages()")
                except Exception:
                    logger.exception("region_clients not importable in build_ccure_averages()")

            # compute avg range using AttendanceSummary if possible
            avg_range = None
            try:
                q = db.query(AttendanceSummary.date, AttendanceSummary.employee_id, AttendanceSummary.presence_count)\
                      .filter(AttendanceSummary.date >= start_obj, AttendanceSummary.date <= end_obj).all()
                by_date = {}
                for row in q:
                    d = row[0]
                    key = (row[1] or "").strip() if row[1] else None
                    if not key:
                        continue
                    if d not in by_date:
                        by_date[d] = set()
                    try:
                        presence_val = getattr(row, 'presence_count', row[2])
                        if int(presence_val) > 0:
                            by_date[d].add(key)
                    except Exception:
                        by_date[d].add(key)
                days_count = (end_obj - start_obj).days + 1
                daily_counts = [len(by_date.get(start_obj + timedelta(days=i), set())) for i in range(days_count)]
                if days_count and any(daily_counts):
                    avg_range = int(round(sum(daily_counts) / float(days_count)))
                else:
                    avg_range = None
            except Exception:
                logger.exception("Failed computing range average from AttendanceSummary")
                avg_range = None

        # fallback: use region history to compute avg_range if still None
        if avg_range is None:
            try:
                import region_clients
                entries = region_clients.fetch_all_history(timeout=REGION_TIMEOUT_SECONDS) or []
                agg = {}
                for e in entries:
                    try:
                        dstr = e.get("date")
                        if not dstr:
                            continue
                        region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                        emp = None
                        con = None
                        tot = None
                        if region_obj:
                            emp = _safe_int(region_obj.get("Employee"))
                            con = _safe_int(region_obj.get("Contractor"))
                            tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                        else:
                            emp = _safe_int(e.get("Employee"))
                            con = _safe_int(e.get("Contractor"))
                            tot = _safe_int(e.get("total"))
                        if emp is None and con is None:
                            continue
                        if tot is None:
                            tot = (emp or 0) + (con or 0)
                        if dstr not in agg:
                            agg[dstr] = {"total": 0, "count": 0}
                        agg[dstr]["total"] += tot or 0
                        agg[dstr]["count"] += 1
                    except Exception:
                        continue
                per_date_totals = []
                days_count = (end_obj - start_obj).days + 1
                for i in range(days_count):
                    dcheck = (start_obj + timedelta(days=i)).isoformat()
                    if dcheck in agg and agg[dcheck]["count"] > 0:
                        per_day_avg = float(agg[dcheck]["total"]) / float(agg[dcheck]["count"])
                        per_date_totals.append(per_day_avg)
                if per_date_totals:
                    avg_range = int(round(sum(per_date_totals) / float(len(per_date_totals))))
            except Exception:
                logger.exception("Failed computing avg_range from region history in fallback")

        # get ccure stats if available
        ccure_stats = {}
        try:
            import ccure_client
            if hasattr(ccure_client, "get_global_stats"):
                ccure_stats = ccure_client.get_global_stats() or {}
        except Exception:
            logger.debug("ccure_client.get_global_stats not available", exc_info=True)

        cc_active_emps = None
        cc_active_contractors = None
        try:
            if isinstance(ccure_stats, dict):
                a = ccure_stats.get("ActiveEmployees") or ccure_stats.get("active_employees") or None
                b = ccure_stats.get("ActiveContractors") or ccure_stats.get("active_contractors") or None
                if a is not None and str(a).strip() != "":
                    cc_active_emps = int(a)
                if b is not None and str(b).strip() != "":
                    cc_active_contractors = int(b)
        except Exception:
            cc_active_emps = cc_active_contractors = None

        emp_pct = None
        contr_pct = None
        overall_pct = None
        try:
            if isinstance(cc_active_emps, int) and cc_active_emps > 0:
                emp_pct = round((live_emp / float(cc_active_emps)) * 100.0, 2)
            if isinstance(cc_active_contractors, int) and cc_active_contractors > 0:
                contr_pct = round((live_contr / float(cc_active_contractors)) * 100.0, 2)
            if isinstance(cc_active_emps, int) and isinstance(cc_active_contractors, int) and (cc_active_emps + cc_active_contractors) > 0:
                overall_pct = round(((live_emp + live_contr) / float(cc_active_emps + cc_active_contractors)) * 100.0, 2)
        except Exception:
            emp_pct = contr_pct = overall_pct = None

        resp = {
            "date": today.isoformat(),
            "notes": f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}" if (start_date or end_date) else None,
            "live_today": {
                "employee": live_emp,
                "contractor": live_contr,
                "total_reported": live_total_reported,
                "total_from_details": live_total_details
            },
            "ccure_active": {
                "active_employees": cc_active_emps,
                "active_contractors": cc_active_contractors,
                "ccure_active_employees_reported": cc_active_emps,
                "ccure_active_contractors_reported": cc_active_contractors
            },
            "averages": {
                "employee_pct": emp_pct,
                "contractor_pct": contr_pct,
                "overall_pct": overall_pct,
                "avg_headcount_last_7_days": avg_range,
                "head_emp_pct_vs_ccure_today": emp_pct,
                "head_contractor_pct_vs_ccure_today": contr_pct,
                "headcount_overall_pct_vs_ccure_today": overall_pct,
                "history_avg_overall_last_7_days": avg_range
            }
        }

        return resp
    except Exception:
        logger.exception("build_ccure_averages failed")
        raise

# -------------------------
# Upload endpoints (store uploads under data/ and data/raw_uploads/ and rotate)
# -------------------------
ALLOWED_EXT = (".xls", ".xlsx", ".csv")

def _remove_old_files_for_kind(kind: str):
    """
    Remove previous canonical files and previous raw uploads that include kind in filename.
    """
    try:
        # canonical in DATA_DIR
        for p in DATA_DIR.iterdir():
            if p.is_file() and f"active_{kind}" in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old canonical file %s", p)
        # remove raw uploads for same kind to keep only latest raw (user requested)
        for p in RAW_UPLOADS_DIR.iterdir():
            if p.is_file() and kind in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old raw file %s", p)
    except Exception:
        logger.exception("error while removing old files for kind=%s", kind)

def _save_upload_and_rotate(upload_file: UploadFile, kind: str) -> dict:
    """
    Save the uploaded file as:
      - data/active_<kind>.<ext>     (canonical)
      - data/raw_uploads/<timestamp>_<kind>_<origname>  (raw trace)
    Remove previous files for same kind (both canonical & raw).
    Returns metadata dict.
    """
    fname = Path(upload_file.filename).name
    ext = Path(fname).suffix.lower()
    if ext not in ALLOWED_EXT:
        raise HTTPException(status_code=400, detail="Only .xls .xlsx .csv allowed")

    # rotate old files
    _remove_old_files_for_kind(kind)

    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    raw_name = f"{ts}_{kind}_{fname}"
    raw_path = RAW_UPLOADS_DIR / raw_name

    # Save raw
    try:
        with raw_path.open("wb") as out_f:
            shutil.copyfileobj(upload_file.file, out_f)
    finally:
        try:
            upload_file.file.close()
        except Exception:
            pass

    # Write canonical: keep same extension as original (simpler)
    canonical_path = DATA_DIR / f"active_{kind}{ext}"
    try:
        shutil.copy(raw_path, canonical_path)
    except Exception:
        logger.exception("Failed to write canonical copy for %s", canonical_path)

    info = {
        "raw_saved": str(raw_path),
        "canonical_saved": str(canonical_path),
        "original_filename": fname,
        "size_bytes": raw_path.stat().st_size if raw_path.exists() else None
    }
    logger.info("Uploaded %s: %s", kind, info)
    return info

# (Upload endpoints continue unchanged - already present above)
@app.post("/api/upload/active-employees")
async def upload_active_employees_api(file: UploadFile = File(...)):
    return await upload_active_employees(file)

@app.post("/api/upload/active-contractors")
async def upload_active_contractors_api(file: UploadFile = File(...)):
    return await upload_active_contractors(file)

@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    """
    Upload Active Employee sheet:
      - stores raw to data/raw_uploads and canonical to data/active_employee.*
      - removes previous uploaded employee sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="employee")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("employee upload failed")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    """
    Upload Active Contractor sheet:
      - stores raw to data/raw_uploads and canonical to data/active_contractor.*
      - removes previous uploaded contractor sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="contractor")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("contractor upload failed")
        raise HTTPException(status_code=500, detail=str(e))

# remaining utility endpoints and compute wrappers (unchanged)
def _map_detailed_to_resp(detailed: Dict[str, Any]) -> Dict[str, Any]:
    live_h = detailed.get("live_headcount", {}) or {}
    head_h = detailed.get("headcount", {}) or {}
    ccure_active_obj = detailed.get("ccure_active", {}) or {}
    averages_obj = detailed.get("averages", {}) or {}

    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    live_employee = int(live_h.get("employee") or head_h.get("employee") or 0)
    live_contractor = int(live_h.get("contractor") or head_h.get("contractor") or 0)
    total_reported = int(
        live_h.get("currently_present_total")
        or head_h.get("total_visited_today")
        or (live_employee + live_contractor)
        or 0
    )
    total_from_details = int(head_h.get("total_visited_today") or 0)

    mapped_headcount = {
        "total_visited_today": int(head_h.get("total_visited_today") or 0),
        "employee": int(head_h.get("employee") or 0),
        "contractor": int(head_h.get("contractor") or 0),
        "by_location": head_h.get("by_location") or {}
    }

    resp = {
        "date": detailed.get("date"),
        "notes": detailed.get("notes"),
        "live_today": {
            "employee": live_employee,
            "contractor": live_contractor,
            "total_reported": total_reported,
            "total_from_details": total_from_details
        },
        "headcount": mapped_headcount,
        "live_headcount": live_h,
        "ccure_active": {
            "active_employees": ccure_active_obj.get("ccure_active_employees_reported")
                             or ccure_active_obj.get("active_employees"),
            "active_contractors": ccure_active_obj.get("ccure_active_contractors_reported")
                               or ccure_active_obj.get("active_contractors"),
            "ccure_active_employees_reported": ccure_active_obj.get("ccure_active_employees_reported"),
            "ccure_active_contractors_reported": ccure_active_obj.get("ccure_active_contractors_reported")
        },
        "averages": averages_obj
    }
    return resp

def _build_verify_like_summary_from_mapped(mapped: Dict[str, Any], include_raw: bool = False) -> Dict[str, Any]:
    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    cc = mapped.get("ccure_active", {}) or {}
    head = mapped.get("headcount", {}) or {}
    live_head = mapped.get("live_headcount", {}) or {}
    averages = mapped.get("averages", {}) or {}

    cc_emp = to_int(cc.get("ccure_active_employees_reported") or cc.get("active_employees"))
    cc_con = to_int(cc.get("ccure_active_contractors_reported") or cc.get("active_contractors"))

    head_total = to_int(head.get("total_visited_today") or mapped.get("live_today", {}).get("total_from_details"))
    head_emp = to_int(head.get("employee") or mapped.get("live_today", {}).get("employee"))
    head_con = to_int(head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    live_total = to_int(live_head.get("currently_present_total") or mapped.get("live_today", {}).get("total_reported"))
    live_emp = to_int(live_head.get("employee") or mapped.get("live_today", {}).get("employee"))
    live_con = to_int(live_head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    history_emp_avg = averages.get("history_avg_employee_last_7_days")
    history_con_avg = averages.get("history_avg_contractor_last_7_days")
    history_overall_avg = averages.get("history_avg_overall_last_7_days")

    def pct(n, d):
        try:
            if n is None or d is None:
                return None
            if float(d) == 0:
                return None
            return round((float(n) / float(d)) * 100.0, 2)
        except Exception:
            return None

    summary = {
        "date": mapped.get("date"),
        "ccure_reported": {
            "employees": cc_emp,
            "contractors": cc_con,
            "total_reported": (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None
        },
        "headcount_attendance_summary": {
            "total_visited_today": head_total,
            "employee": head_emp,
            "contractor": head_con,
        },
        "live_headcount_region_clients": {
            "currently_present_total": live_total,
            "employee": live_emp,
            "contractor": live_con,
        },
        "percentages_vs_ccure": {
            "head_employee_pct_vs_ccure_today": pct(head_emp, cc_emp),
            "head_contractor_pct_vs_ccure_today": pct(head_con, cc_con),
            "head_overall_pct_vs_ccure_today": pct(head_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "live_employee_pct_vs_ccure_today": pct(live_emp, cc_emp),
            "live_contractor_pct_vs_ccure_today": pct(live_con, cc_con),
            "live_overall_pct_vs_ccure_today": pct(live_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "history_employee_pct_vs_ccure": pct(history_emp_avg, cc_emp),
            "history_contractor_pct_vs_ccure": pct(history_con_avg, cc_con),
            "history_overall_pct_vs_ccure": pct(history_overall_avg, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None)
        },
        "averages": {
            "history_avg_employee_last_7_days": history_emp_avg,
            "history_avg_contractor_last_7_days": history_con_avg,
            "history_avg_overall_last_7_days": history_overall_avg,
            "avg_headcount_last_7_days_db": averages.get("avg_headcount_last_7_days") or averages.get("avg_headcount_last_7_days_db"),
            "avg_headcount_per_site_last_7_days": averages.get("avg_headcount_per_site_last_7_days"),
            "employee_pct": averages.get("employee_pct"),
            "contractor_pct": averages.get("contractor_pct"),
            "overall_pct": averages.get("overall_pct"),
            **({k: v for k, v in averages.items() if k not in (
                "history_avg_employee_last_7_days",
                "history_avg_contractor_last_7_days",
                "history_avg_overall_last_7_days",
                "avg_headcount_last_7_days",
                "avg_headcount_last_7_days_db",
                "avg_headcount_per_site_last_7_days",
                "employee_pct","contractor_pct","overall_pct"
            )})
        },
        "notes": mapped.get("notes")
    }

    summary["headcount_details"] = {
        "total_visited_today": head_total,
        "employee": head_emp,
        "contractor": head_con,
        "by_location": head.get("by_location") if isinstance(head.get("by_location"), dict) else {}
    }
    summary["live_headcount_details"] = {
        "currently_present_total": live_total,
        "employee": live_emp,
        "contractor": live_con,
        "by_location": live_head.get("by_location") if isinstance(live_head.get("by_location"), dict) else {}
    }

    summary["ccure_active"] = {
        "active_employees": cc.get("active_employees") or cc.get("ccure_active_employees_reported"),
        "active_contractors": cc.get("active_contractors") or cc.get("ccure_active_contractors_reported"),
        "ccure_active_employees_reported": cc.get("ccure_active_employees_reported"),
        "ccure_active_contractors_reported": cc.get("ccure_active_contractors_reported")
    }

    return summary

# ---------- /ccure/verify (already present in your original) -----
@app.get("/ccure/verify")
def ccure_verify(
    raw: bool = Query(False, description="if true, include the raw compute payload for debugging"),
    start_date: Optional[str] = Query(None, description="YYYY-MM-DD start date (inclusive)"),
    end_date: Optional[str] = Query(None, description="YYYY-MM-DD end date (inclusive)")
):
    """
    Synchronous verification endpoint. Prefer compute_visit_averages() (synchronous call).
    If compute raises or fails, fall back to build_ccure_averages() so output shape remains consistent.
    """
    try:
        detailed = None
        try:
            from ccure_compare_service import compute_visit_averages
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
        except Exception:
            logger.exception("compute_visit_averages() failed inside /ccure/verify; falling back")
            detailed = None

        if isinstance(detailed, dict):
            mapped = _map_detailed_to_resp(detailed)
            summary = _build_verify_like_summary_from_mapped(mapped, include_raw=raw)
            if raw and isinstance(detailed, dict):
                summary["raw"] = detailed
            return JSONResponse(summary)
        else:
            fallback = build_ccure_averages(start_date, end_date)
            mapped_fallback = {
                "date": fallback.get("date"),
                "notes": fallback.get("notes"),
                "live_today": fallback.get("live_today", {}),
                "headcount": {
                    "total_visited_today": fallback.get("live_today", {}).get("total_from_details") or fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "live_headcount": {
                    "currently_present_total": fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "ccure_active": fallback.get("ccure_active", {}),
                "averages": fallback.get("averages", {})
            }

            summary = _build_verify_like_summary_from_mapped(mapped_fallback, include_raw=raw)
            if raw:
                summary["raw"] = mapped_fallback
       
            return JSONResponse(summary)
    except Exception as e:
        logger.exception("ccure_verify failed")
        raise HTTPException(status_code=500, detail=f"ccure verify error: {e}")

# ---------- /ccure/compare_v2 (calls data_compare_service_v2) ----------
@app.get("/ccure/compare_v2")
def ccure_compare_v2(
    mode: str = Query("full"),
    stats_detail: str = Query("ActiveProfiles"),
    limit_list: int = Query(200, ge=1, le=5000),
    export: bool = Query(False),
    region_filter: Optional[str] = Query(None, description="Region code e.g. APAC"),
    location_city: Optional[str] = Query(None, description="Location city e.g. Pune"),
    location_state: Optional[str] = Query(None, description="Location state/province"),
    location_description: Optional[str] = Query(None, description="Location Description"),
    week_ref_date: Optional[str] = Query(None, description="YYYY-MM-DD for week (Mon-Fri)")
):
    try:
        from data_compare_service_v2 import compare_ccure_vs_sheets, prefetch_region_history
    except Exception as e:
        logger.exception("data_compare_service_v2 import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    # 1) prefetch region history (cache) - do this before loading sheet to avoid repeated network calls
    try:
        # use REGION_TIMEOUT_SECONDS from this module to control how long prefetch should wait
        prefetch_region_history(timeout=REGION_TIMEOUT_SECONDS)
    except Exception:
        logger.exception("prefetch_region_history failed (continuing)")

    res = compare_ccure_vs_sheets(
        mode=mode,
        stats_detail=stats_detail,
        limit_list=limit_list,
        export=export,
        region_filter=region_filter,
        location_city=location_city,
        location_state=location_state,
        location_description=location_description,
        week_ref_date=week_ref_date
    )
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)

@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = OUTPUT_DIR / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(str(full),
                            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            filename=safe_name)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")

# End of app.py











# data_compare_service_v2.py
"""
Comparison service (v2) with broadened matching heuristics and safer prefetch/cache.
Improved: deeper scalar iteration, robust timestamp parsing, clearer logging and example matches.
Drop-in replacement for your existing data_compare_service_v2.py
"""

import sys
import re
import uuid
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
from typing import Optional, Dict, Any, List, Tuple

import pandas as pd

# HTTP client for region histories
try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except Exception:
    requests = None

# DB imports (same as your project)
from db import SessionLocal
from models import AttendanceSummary

# Settings / defaults
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR, REGION_HISTORY_URLS as SETTINGS_REGION_HISTORY_URLS
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
    REGION_HISTORY_URLS = SETTINGS_REGION_HISTORY_URLS
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"
    REGION_HISTORY_URLS = [
        "http://10.199.22.57:3008/api/occupancy/history",  # APAC
        "http://10.199.22.57:3006/api/occupancy/history",  # NAMER
        "http://10.199.22.57:3007/api/occupancy/history",  # EMEA
        "http://10.199.22.57:4000/api/occupancy/history"   # LACA
    ]

DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger("data_compare_service_v2")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# In-memory cache for prefetched region history entries
REGION_HISTORY_CACHE = None
REGION_HISTORY_CACHE_FETCHED_AT = None
REGION_HISTORY_CACHE_TTL_SECONDS = 300  # 5 minutes

# Matching config
ID_FIELD_CANDIDATES = [
    "EmployeeID","employeeId","Employee Id","EmpID","Emp Id","EmpNo","EmployeeNo","Employee_Number",
    "PersonID","PersonId","personId","person_id","employee_id","id","Id","employeeNumber","EmployeeNumber",
    "worker_system_id","wsid","WorkerID","Worker System Id","workerId","WorkerSystemId"
]
CARD_FIELD_CANDIDATES = [
    "CardNumber","Card","cardNumber","card_number","BadgeNumber","BadgeNo","Badge","badgeNumber","badge_no",
    "iPassID","IPassID","iPass","i_pass_id","CardNo","card_no","card","IPASSID","IPass"
]
NAME_FIELD_CANDIDATES = [
    "FullName","Full Name","EmpName","Name","full_name","displayName","personName","PersonName"
]

# ----------------------------
# Utilities
# ----------------------------
def _find_active_employee_file():
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_employee{ext}"
        if p.exists():
            return p
    for p in DATA_DIR.iterdir():
        if p.is_file() and "active_employee" in p.name.lower():
            return p
    return None

def _normalize_key(k):
    if k is None:
        return None
    try:
        s = str(k).strip()
        return s if s != "" else None
    except Exception:
        return None

def _digits_only(s):
    if s is None:
        return ""
    return re.sub(r'\D+', '', str(s))

def _safe_int(v, default=0):
    try:
        if v is None or v == "":
            return default
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return default

def _week_monday_and_friday(ref_date: Optional[date] = None):
    if ref_date is None:
        ref_date = date.today()
    iso = ref_date.isoweekday()
    monday = ref_date - timedelta(days=(iso - 1))
    friday = monday + timedelta(days=4)
    return monday, friday

def _maybe_mark_on_leave(status_str: Optional[str]) -> bool:
    if not status_str:
        return False
    s = str(status_str).strip().lower()
    for tok in ("leave", "vacation", "on leave", "holiday", "sabbatical", "furlough", "loa"):
        if tok in s:
            return True
    return False

# ----------------------------
# Load active sheet
# ----------------------------
def load_active_employees_dataframe() -> pd.DataFrame:
    src = _find_active_employee_file()
    if not src:
        raise FileNotFoundError(f"Active employee canonical file not found in {DATA_DIR}")
    ext = src.suffix.lower()
    if ext in (".xls", ".xlsx"):
        df = pd.read_excel(src, sheet_name=0, dtype=str)
    else:
        df = pd.read_csv(src, dtype=str)
    df.columns = [c.strip() for c in df.columns]

    def _first_present(row, candidates):
        for c in candidates:
            if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
                return row[c]
        return None

    rows = []
    for _, row in df.iterrows():
        emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id','EmployeeNo','Employee No'])
        if emp_id is None:
            continue
        full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or (
            f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
        )
        location_city = _first_present(row, ['Location City','Location','City'])
        location_desc = _first_present(row, ['Location Description','LocationDescription'])
        location_state = _first_present(row, ['Location State / Province','Location State','State','Province'])
        region_code = _first_present(row, ['Region Code','Region','RegionCode'])
        current_status = _first_present(row, ['Current Status','Status','Employee Status'])
        employee_type = _first_present(row, ['Employee Type','Type','Time Type'])
        rows.append({
            "employee_id": _normalize_key(emp_id),
            "full_name": _normalize_key(full_name),
            "location_city": _normalize_key(location_city),
            "location_desc": _normalize_key(location_desc),
            "location_state": _normalize_key(location_state),
            "region_code": (str(region_code).strip() if region_code is not None else None),
            "current_status": _normalize_key(current_status),
            "employee_type": _normalize_key(employee_type),
            "raw_row": row.to_dict()
        })
    ndf = pd.DataFrame(rows)
    for col in ("employee_id","full_name","location_city","location_desc","location_state","region_code","current_status","employee_type","raw_row"):
        if col not in ndf:
            ndf[col] = None
    ndf = ndf[ndf["employee_id"].notna() & (ndf["employee_id"].str.strip() != "")]
    ndf.reset_index(drop=True, inplace=True)
    return ndf

# ----------------------------
# HTTP Session factory (retries/backoff)
# ----------------------------
def _build_requests_session():
    if requests is None:
        return None
    s = requests.Session()
    allowed = frozenset(['GET', 'HEAD'])
    try:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=allowed
        )
    except TypeError:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            method_whitelist=allowed
        )
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

# ----------------------------
# Prefetch / cache helpers
# ----------------------------
def prefetch_region_history(timeout: int = 10, force: bool = False):
    """
    Fetch region history entries (cached). Returns list of raw entries.
    """
    global REGION_HISTORY_CACHE, REGION_HISTORY_CACHE_FETCHED_AT
    try:
        now = datetime.utcnow()
        if not force and REGION_HISTORY_CACHE is not None and REGION_HISTORY_CACHE_FETCHED_AT:
            elapsed = (now - REGION_HISTORY_CACHE_FETCHED_AT).total_seconds()
            if elapsed < REGION_HISTORY_CACHE_TTL_SECONDS:
                logger.info("[region_cache] Using cached region history (age %.1fs)", elapsed)
                return REGION_HISTORY_CACHE

        entries = []
        # Prefer region_clients when available
        try:
            import region_clients
            logger.info("[region_cache] fetching region history via region_clients.fetch_all_history()")
            try:
                got = region_clients.fetch_all_history(timeout=timeout)
            except TypeError:
                got = region_clients.fetch_all_history()
            entries = got or []
        except Exception:
            entries = []

        # If empty, try direct requests to configured URLs
        if not entries and requests is not None:
            logger.info("[region_cache] fetching region history directly from endpoints")
            session = _build_requests_session() or requests
            for url in REGION_HISTORY_URLS:
                if not url:
                    continue
                try:
                    resp = session.get(url, timeout=(3, max(5, timeout)))
                    if not resp or resp.status_code != 200:
                        continue
                    try:
                        payload = resp.json()
                    except Exception:
                        continue
                    # flatten possible lists
                    if isinstance(payload, list):
                        for p in payload:
                            if isinstance(p, dict):
                                p['_source_url'] = url
                                entries.append(p)
                    elif isinstance(payload, dict):
                        found_list = False
                        for k in ("results","summaryByDate","details","data","entries","list","people","items"):
                            if k in payload and isinstance(payload[k], list):
                                for p in payload[k]:
                                    if isinstance(p, dict):
                                        p['_source_url'] = url
                                        entries.append(p)
                                found_list = True
                                break
                        if not found_list:
                            payload['_source_url'] = url
                            entries.append(payload)
                except requests.exceptions.RequestException as e:
                    logger.warning("[region_cache] fetch failed for %s: %s", url, str(e))
                    continue

        REGION_HISTORY_CACHE = entries or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        logger.info("[region_cache] prefetched %d region history entries", len(REGION_HISTORY_CACHE))
        return REGION_HISTORY_CACHE
    except Exception:
        logger.exception("[region_cache] prefetch failed")
        REGION_HISTORY_CACHE = REGION_HISTORY_CACHE or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        return REGION_HISTORY_CACHE

# ----------------------------
# Payload helpers (deep search)
# ----------------------------
def _iter_scalars_in_obj(obj, parent_key=""):
    """
    Yield all scalar key->value pairs inside nested dict/list structures as (key_path, value).
    """
    if isinstance(obj, dict):
        for k, v in obj.items():
            new_key = f"{parent_key}.{k}" if parent_key else str(k)
            if isinstance(v, (dict, list)):
                yield from _iter_scalars_in_obj(v, parent_key=new_key)
            else:
                yield new_key, v
    elif isinstance(obj, list):
        for idx, it in enumerate(obj):
            new_key = f"{parent_key}[{idx}]" if parent_key else f"[{idx}]"
            if isinstance(it, (dict, list)):
                yield from _iter_scalars_in_obj(it, parent_key=new_key)
            else:
                yield new_key, it

def _extract_details_from_payload(payload):
    """
    Normalize to list of dict rows that look like detail records.
    """
    if payload is None:
        return []
    if isinstance(payload, list):
        return [p for p in payload if isinstance(p, dict)]
    if isinstance(payload, dict):
        for k in ("details","results","data","entries","list","people","items"):
            v = payload.get(k)
            if isinstance(v, list):
                return [x for x in v if isinstance(x, dict)]
        # single-record-like payload (date-summary) -> return as single element to allow higher-level scanner to inspect fields
        if any(k in payload for k in ("date","Employee","Contractor","total","day")) and len(payload.keys()) <= 40:
            return [payload]
    return []

# ----------------------------
# ID matching helpers
# ----------------------------
def _match_candidate_to_employees(candidate_raw, orig_ids_set, orig_ids_list, digits_map):
    """
    Try many heuristics to map candidate_raw to one of the orig_ids_list.
    Returns matched_orig_id or None.
    """
    if candidate_raw is None:
        return None
    cand_str = str(candidate_raw).strip()
    if not cand_str:
        return None

    # direct exact match
    if cand_str in orig_ids_set:
        return cand_str

    # direct case-insensitive match
    for o in orig_ids_list:
        if isinstance(o, str) and cand_str.lower() == o.lower():
            return o

    # numeric transformations
    cand_digits = _digits_only(cand_str)
    if cand_digits:
        cand_nolead = cand_digits.lstrip('0') or cand_digits
        # direct digits match to original ids
        for o in orig_ids_list:
            if not isinstance(o, str):
                continue
            if o == cand_digits or o == cand_nolead:
                return o
            od = _digits_only(o)
            if od == cand_digits or od == cand_nolead:
                return o
        # numeric equality by int
        try:
            ci = int(cand_nolead)
            for o in orig_ids_list:
                try:
                    od = _digits_only(o)
                    if not od:
                        continue
                    oi = int(od.lstrip('0') or od)
                    if oi == ci:
                        return o
                except Exception:
                    continue
        except Exception:
            pass

        # last-n digits heuristics (conservative)
        if len(cand_digits) >= 3:
            for n in (6, 4):
                suf = cand_digits[-n:]
                if not suf:
                    continue
                for o in orig_ids_list:
                    od = _digits_only(o)
                    if od and od.endswith(suf):
                        return o

    # strip common prefixes and retry
    up = cand_str.upper()
    for pref in ("W", "IPASS", "IPASSID", "IPass"):
        if up.startswith(pref):
            stripped = cand_str[len(pref):]
            m = _match_candidate_to_employees(stripped, orig_ids_set, orig_ids_list, digits_map)
            if m:
                return m

    # fallback: find numeric substrings inside string and try mapping
    for match in re.finditer(r'(\d{3,})', cand_str):
        ssub = match.group(1)
        m = _match_candidate_to_employees(ssub, orig_ids_set, orig_ids_list, digits_map)
        if m:
            return m

    return None

# ----------------------------
# Region history scanning -> build presence map
# ----------------------------
def _fetch_presence_from_region_histories(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None, preloaded_entries: Optional[List[dict]] = None):
    """
    Scans preloaded_entries (or global cache) and returns presence mapping: {employee_id -> {date: 0/1}}
    """
    presence = {eid: {} for eid in employee_ids}
    if not employee_ids:
        return presence

    if preloaded_entries is None:
        global REGION_HISTORY_CACHE
        preloaded_entries = REGION_HISTORY_CACHE or []

    if not preloaded_entries:
        logger.info("[region_history] no preloaded region history entries to scan")
    else:
        orig_ids_list = [str(e).strip() for e in employee_ids]
        orig_ids_set = set(orig_ids_list)
        digits_map = {e: _digits_only(e) for e in orig_ids_list}

        scanned = 0
        matched = 0
        examples_matched = []

        for entry in preloaded_entries:
            detail_rows = []
            if isinstance(entry, dict):
                # prefer explicit lists
                for key in ("details","people","items","list","results","entries","data"):
                    if key in entry and isinstance(entry.get(key), list):
                        detail_rows = [r for r in entry.get(key) if isinstance(r, dict)]
                        break
                if not detail_rows:
                    # treat entry itself as a candidate detail row if it has plausible fields (timestamp or id-like)
                    candidate_keys = set(ID_FIELD_CANDIDATES + CARD_FIELD_CANDIDATES + ["date","timestamp","time","SwipeDate","LocaleMessageTime","day"])
                    if any(k in entry for k in candidate_keys):
                        detail_rows = [entry]
                    else:
                        # try extract via helper (covers nested payload shapes)
                        detail_rows = _extract_details_from_payload(entry) or []
            else:
                continue

            scanned += len(detail_rows)
            for d in detail_rows:
                try:
                    # find a timestamp / date for row
                    ts = None
                    # common keys first
                    for tkey in ("LocaleMessageTime","LocaleMessageDateTime","SwipeDate","SwipeTime","timestamp","time","DateTime","date","swipeDate","day"):
                        if tkey in d and d.get(tkey):
                            ts = d.get(tkey)
                            break

                    # fallback: scan scalar values in row for iso-like date
                    if ts is None:
                        for k, v in _iter_scalars_in_obj(d):
                            if v is None:
                                continue
                            try:
                                s = str(v)
                            except Exception:
                                continue
                            # quick heuristic
                            if re.search(r'\d{4}-\d{2}-\d{2}', s) or re.search(r'\d{2}/\d{2}/\d{4}', s):
                                ts = s
                                break

                    if ts is None:
                        # skip row (no date info)
                        continue

                    # parse timestamp into date (robust)
                    t = None
                    if isinstance(ts, (int, float)):
                        try:
                            t = datetime.fromtimestamp(int(ts))
                        except Exception:
                            try:
                                t = datetime.utcfromtimestamp(int(ts) / 1000.0)
                            except Exception:
                                t = None
                    elif isinstance(ts, str):
                        s = ts.strip()
                        if s == "":
                            t = None
                        else:
                            parsed = None
                            # try dateutil if available (best)
                            try:
                                from dateutil import parser as _parser
                                parsed = _parser.parse(s)
                            except Exception:
                                parsed = None
                            if parsed:
                                try:
                                    if parsed.tzinfo is not None:
                                        parsed = parsed.astimezone(tz=None).replace(tzinfo=None)
                                except Exception:
                                    pass
                                t = parsed
                            else:
                                # try common formats
                                fmts = [
                                    "%Y-%m-%dT%H:%M:%S.%fZ",
                                    "%Y-%m-%dT%H:%M:%S.%f",
                                    "%Y-%m-%dT%H:%M:%S",
                                    "%Y-%m-%d %H:%M:%S",
                                    "%Y-%m-%d",
                                    "%d/%m/%Y %H:%M:%S",
                                    "%d/%m/%Y"
                                ]
                                for f in fmts:
                                    try:
                                        t = datetime.strptime(s, f)
                                        break
                                    except Exception:
                                        t = None
                    if t is None:
                        continue

                    dt = t.date()
                    if dt < start_date or dt > end_date:
                        continue

                    # partition filter (if provided)
                    if partition_filter:
                        part_values = []
                        for k in ("PartitionNameFriendly","PartitionName","PrimaryLocation","partition","location","Partition","Location","Site","PartitionName1","PartitionName2"):
                            v = d.get(k)
                            if v:
                                part_values.append(str(v))
                        if part_values:
                            ok = any(part_filter_match(part, partition_filter) for part in part_values)
                            if not ok:
                                continue
                        else:
                            # no partition info -> skip when filter exists
                            continue

                    # attempt to match explicit id keys first
                    matched_key = None
                    for k in ID_FIELD_CANDIDATES:
                        if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                            m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                            if m:
                                matched_key = m
                                break

                    # try card fields
                    if not matched_key:
                        for k in CARD_FIELD_CANDIDATES:
                            if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                                m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break

                    # deep-scan scalars for numeric substrings and name fields
                    if not matched_key:
                        for key_path, val in _iter_scalars_in_obj(d):
                            if val is None:
                                continue
                            sval = str(val)
                            # numeric substring preference
                            if re.search(r'\d{3,}', sval):
                                m = _match_candidate_to_employees(sval, orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break
                        # check name fields that may contain last name -> (left disabled by default; can be enabled later)
                        # (conservative) if matched_key still None we skip name mapping to avoid false positives

                    if matched_key:
                        matched += 1
                        # coerce matched_key to exact string from orig list
                        matched_key = next((o for o in orig_ids_list if str(o).strip() == str(matched_key).strip()), str(matched_key).strip())
                        presence.setdefault(matched_key, {})
                        presence[matched_key][dt] = 1
                        if len(examples_matched) < 10:
                            examples_matched.append({"matched": matched_key, "date": dt.isoformat(), "sample_row_keys": list(d.keys())[:8]})
                except Exception:
                    continue

        logger.info("[region_history] scanned %d detail rows from preloaded entries; matched %d presence entries", scanned, matched)
        if examples_matched:
            logger.debug("[region_history] example matches (up to 10): %s", examples_matched)

    # fill zeros for any missing date
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            presence.setdefault(eid, {})
            if cur not in presence[eid]:
                presence[eid][cur] = 0
        cur = cur + timedelta(days=1)

    return presence

def part_filter_match(src_val, partition_filter):
    try:
        if not src_val:
            return False
        return partition_filter.strip().lower() in str(src_val).strip().lower()
    except Exception:
        return False

# ----------------------------
# DB / combined fetch
# ----------------------------
def _fetch_presence_for_employees(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None):
    """
    1) chunked DB IN queries (AttendanceSummary)
    2) fallback broad DB query
    3) fallback region history cache scan (prefetch_region_history)
    """
    if not employee_ids:
        return {}

    orig_ids = [str(e).strip() for e in employee_ids]
    norm_id_set = set([s for s in orig_ids if s])
    result = {eid: {} for eid in orig_ids}

    # 1) chunked DB fetch
    rows = []
    chunk_size = 500
    try:
        with SessionLocal() as db:
            for i in range(0, len(orig_ids), chunk_size):
                chunk = orig_ids[i:i+chunk_size]
                try:
                    q = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date,
                        AttendanceSummary.employee_id.in_(chunk)
                    )
                    rows_chunk = q.all()
                    if rows_chunk:
                        rows.extend(rows_chunk)
                except Exception:
                    logger.exception("chunked query failed (continuing)")
                    continue

            # fallback broad query if none found
            if not rows:
                try:
                    rows = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date
                    ).all()
                    logger.info("[presence_fetch] fallback broad DB query returned %d rows for %s -> %s", len(rows), start_date, end_date)
                except Exception:
                    logger.exception("fallback broad DB query failed")
                    rows = []
    except Exception:
        logger.exception("DB session error in _fetch_presence_for_employees")
        rows = []

    # map DB rows to provided employee ids
    for r in rows:
        try:
            raw = r.employee_id
            if raw is None:
                continue
            db_key = str(raw).strip()
            match_key = None
            if db_key in norm_id_set:
                match_key = db_key
            else:
                digits = _digits_only(db_key)
                if digits:
                    cand = digits.lstrip('0') or digits
                    if cand in norm_id_set:
                        match_key = cand
                if match_key is None:
                    for o in orig_ids:
                        if o == db_key or o.lstrip('0') == db_key or db_key.lstrip('0') == o:
                            match_key = o
                            break
            if not match_key:
                continue
            d = r.date
            present = 0
            try:
                present = int(r.presence_count or 0)
            except Exception:
                present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
            result.setdefault(match_key, {})
            prev = result[match_key].get(d, 0)
            result[match_key][d] = 1 if (prev == 1 or present > 0) else 0
        except Exception:
            continue

    # fill zeros
    cur = start_date
    while cur <= end_date:
        for eid in orig_ids:
            result.setdefault(eid, {})
            if cur not in result[eid]:
                result[eid][cur] = 0
        cur = cur + timedelta(days=1)

    db_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] DB-derived presence found for %d/%d employees", db_positive, len(orig_ids))







    # fallback to region details/history if needed
    if db_positive == 0 or db_positive < max(10, int(0.1 * len(orig_ids))):
        try:
            logger.info("[presence_fetch] DB coverage low (%d/%d) - trying region occupancy detail/history fallback", db_positive, len(orig_ids))

            # 1) Try region_clients.fetch_all_details() first (per-person rows are most useful)
            try:
                import region_clients
                details = []
                if hasattr(region_clients, "fetch_all_details"):
                    try:
                        details = region_clients.fetch_all_details(timeout=timeout)  # region_clients handles retries
                    except TypeError:
                        details = region_clients.fetch_all_details()
                details = details or []
                if details:
                    logger.info("[presence_fetch] fetched %d detail rows from region_clients.fetch_all_details()", len(details))
                    # attempt to match details to employee ids and fill presence
                    for d in details:
                        try:
                            # extract candidate id fields (fast checks)
                            cand_fields = []
                            for k in ("EmployeeID","employee_id","EmpID","CardNumber","Card","CardNo","IPassID","iPassID","PersonGUID","PersonId"):
                                v = d.get(k) if isinstance(d, dict) else None
                                if v and str(v).strip():
                                    cand_fields.append(v)
                            # if no explicit candidate id, attempt to deep-scan for numeric substring
                            if not cand_fields:
                                for key_path, val in _iter_scalars_in_obj(d):
                                    if val is None:
                                        continue
                                    sval = str(val)
                                    if re.search(r'\d{3,}', sval):
                                        cand_fields.append(sval)
                            # determine date for this row (fast heuristics)
                            ts = None
                            for tkey in ("LocaleMessageTime","SwipeDate","SwipeTime","timestamp","time","DateTime","date"):
                                if tkey in d and d.get(tkey):
                                    ts = d.get(tkey)
                                    break
                            if ts is None:
                                # quick scalar scan for iso date fragment
                                for key_path, val in _iter_scalars_in_obj(d):
                                    if val is None:
                                        continue
                                    s = str(val)
                                    if re.search(r'\d{4}-\d{2}-\d{2}', s) or re.search(r'\d{2}/\d{2}/\d{4}', s):
                                        ts = s
                                        break
                            if ts is None:
                                continue
                            # parse to date - handle common string formats without heavy dateutil
                            parsed_dt = None
                            if isinstance(ts, (int, float)):
                                try:
                                    parsed_dt = datetime.fromtimestamp(int(ts))
                                except Exception:
                                    try:
                                        parsed_dt = datetime.utcfromtimestamp(int(ts) / 1000.0)
                                    except Exception:
                                        parsed_dt = None
                            elif isinstance(ts, str):
                                s = ts.strip()
                                # try ISO-like fast parse
                                m = re.search(r'(\d{4}-\d{2}-\d{2})', s)
                                if m:
                                    try:
                                        parsed_dt = datetime.fromisoformat(m.group(1))
                                    except Exception:
                                        try:
                                            parsed_dt = datetime.strptime(m.group(1), "%Y-%m-%d")
                                        except Exception:
                                            parsed_dt = None
                                else:
                                    # fallback: try a couple common formats
                                    for f in ("%Y-%m-%dT%H:%M:%S.%fZ","%Y-%m-%dT%H:%M:%S","%d/%m/%Y %H:%M:%S","%d/%m/%Y"):
                                        try:
                                            parsed_dt = datetime.strptime(s, f)
                                            break
                                        except Exception:
                                            parsed_dt = None
                            if parsed_dt is None:
                                continue
                            dt = parsed_dt.date()
                            if dt < start_date or dt > end_date:
                                continue

                            # attempt to map candidate fields to orig_ids using the same matching helper
                            for cand in cand_fields:
                                try:
                                    m = _match_candidate_to_employees(cand, set(orig_ids), orig_ids, {})  # re-use existing matcher
                                    if m:
                                        # coerce matched to orig string
                                        matched_key = next((o for o in orig_ids if str(o).strip() == str(m).strip()), str(m).strip())
                                        result.setdefault(matched_key, {})
                                        prev = result[matched_key].get(dt, 0)
                                        result[matched_key][dt] = 1 if (prev == 1 or 1) else prev
                                        break
                                except Exception:
                                    continue
                        except Exception:
                            continue
                else:
                    logger.info("[presence_fetch] region_clients.fetch_all_details returned no details")
            except Exception:
                logger.exception("region_clients.fetch_all_details fallback failed (continuing)")

            # 2) ensure region history cache is populated and try scanning preloaded history (less precise)
            try:
                prefetch_region_history()
                region_presence = _fetch_presence_from_region_histories(orig_ids, start_date, end_date, partition_filter=partition_filter, preloaded_entries=REGION_HISTORY_CACHE)
                for eid in orig_ids:
                    rp = region_presence.get(eid, {})
                    for d, v in rp.items():
                        if v and result.setdefault(eid, {}).get(d, 0) == 0:
                            result[eid][d] = 1
            except Exception:
                logger.exception("region_history fallback failed")
        except Exception:
            logger.exception("detail/history fallback collapsed")









    final_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] final presence coverage: %d/%d employees have at least one positive day", final_positive, len(orig_ids))
    return result

# ----------------------------
# Main compare function (public)
# ----------------------------
def compare_ccure_vs_sheets(
    mode: str = "full",
    stats_detail: str = "ActiveProfiles",
    limit_list: int = 200,
    export: bool = False,
    region_filter: Optional[str] = None,
    location_city: Optional[str] = None,
    location_state: Optional[str] = None,
    location_description: Optional[str] = None,
    week_ref_date: Optional[str] = None
):
    # compute week window
    if week_ref_date:
        monday, friday = _week_monday_and_friday(date.fromisoformat(week_ref_date))
    else:
        monday, friday = _week_monday_and_friday(date.today())

    try:
        df = load_active_employees_dataframe()
    except Exception as e:
        logger.exception("Failed to load active employees sheet")
        return {"error": f"active sheet load failed: {e}"}

    rf = region_filter.strip().lower() if region_filter else None
    lc = location_city.strip().lower() if location_city else None
    ls = location_state.strip().lower() if location_state else None
    ld = location_description.strip().lower() if location_description else None

    sel = df.copy()
    if rf:
        sel = sel[sel["region_code"].fillna("").str.strip().str.lower() == rf]
    if lc:
        sel = sel[sel["location_city"].fillna("").str.strip().str.lower() == lc]
    if ls:
        sel = sel[sel["location_state"].fillna("").str.strip().str.lower() == ls]
    if ld:
        sel = sel[sel["location_desc"].fillna("").str.strip().str.lower() == ld]

    total_active = len(sel)
    employee_ids = sel["employee_id"].astype(str).str.strip().tolist()

    presence_map = _fetch_presence_for_employees(employee_ids, monday, friday, partition_filter=location_city)

    # compute today count
    today = date.today()
    today_count = 0
    for eid in employee_ids:
        pm = presence_map.get(eid, {})
        if pm.get(today, 0) > 0:
            today_count += 1
        else:
            # fallback DB check
            try:
                with SessionLocal() as db:
                    row = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == eid, AttendanceSummary.date == today).first()
                    if row and getattr(row, "presence_count", 0) > 0:
                        today_count += 1
                        continue
                    digits = _digits_only(eid)
                    if digits:
                        cand = digits.lstrip('0') or digits
                        row2 = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == cand, AttendanceSummary.date == today).first()
                        if row2 and getattr(row2, "presence_count", 0) > 0:
                            today_count += 1
            except Exception:
                continue

    today_pct = round((today_count / float(total_active)) * 100.0, 2) if total_active > 0 else None

    sel["on_leave"] = sel["current_status"].apply(lambda x: _maybe_mark_on_leave(x))
    leave_count = int(sel["on_leave"].sum())
    sel["employee_type_norm"] = sel["employee_type"].fillna("").str.strip().str.lower()
    type_counts = sel["employee_type_norm"].value_counts().to_dict()

    regular_df = sel[sel["employee_type_norm"].str.contains("regular", na=False)]
    regular_ids = regular_df["employee_id"].astype(str).str.strip().tolist()

    regular_presence = {}
    for eid in regular_ids:
        week_map = presence_map.get(eid, {})
        days_present = sum(1 for d, v in week_map.items() if v and (monday <= d <= friday))
        days_present = int(days_present)
        regular_presence[eid] = {
            "days_present": days_present,
            "on_leave": bool(sel[sel["employee_id"] == eid]["on_leave"].any()),
            "full_name": sel[sel["employee_id"] == eid]["full_name"].iloc[0] if not sel[sel["employee_id"] == eid].empty else None
        }

    present_5_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 5)
    present_3_or_more_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 3)
    present_less_than_3_count = sum(1 for v in regular_presence.values() if v["days_present"] < 3)
    present_only_1_count = sum(1 for v in regular_presence.values() if v["days_present"] == 1)

    present_5_list = []
    present_3_list = []
    defaulters_list = []

    for eid, info in regular_presence.items():
        entry = {
            "employee_id": eid,
            "full_name": info["full_name"],
            "days_present": info["days_present"],
            "on_leave": info["on_leave"]
        }
        if info["days_present"] >= 5:
            present_5_list.append(entry)
        if info["days_present"] >= 3:
            present_3_list.append(entry)
        if info["days_present"] < 3:
            defaulters_list.append(entry)

    present_5_list = sorted(present_5_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    present_3_list = sorted(present_3_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    defaulters_list = sorted(defaulters_list, key=lambda x: (x["days_present"], x["on_leave"], x["full_name"] or ""))[:limit_list]

    summary = {
        "filters": {
            "region": region_filter,
            "location_city": location_city,
            "location_state": location_state,
            "location_description": location_description,
            "week_monday": monday.isoformat(),
            "week_friday": friday.isoformat()
        },
        "counts": {
            "total_active_in_sheet": total_active,
            "today_headcount_from_summary": today_count,
            "today_headcount_pct_vs_sheet": today_pct,
            "on_leave_count_in_sheet": leave_count,
            "employee_type_counts": type_counts
        },
        "regular_attendance_summary": {
            "regular_total": len(regular_ids),
            "present_5_day_count": present_5_count,
            "present_3_or_more_count": present_3_or_more_count,
            "present_less_than_3_count": present_less_than_3_count,
            "present_only_1_day_count": present_only_1_count
        }
    }

    details = {
        "present_5_days": present_5_list,
        "present_3_or_more_days": present_3_list,
        "defaulters_less_than_3_days": defaulters_list
    }

    report_path = None
    if export:
        try:
            report_name = f"attendance_compare_{(region_filter or 'all')}_{(location_city or 'all')}_{uuid.uuid4().hex[:8]}.xlsx"
            report_file = OUTPUT_DIR / report_name
            with pd.ExcelWriter(report_file, engine="openpyxl") as writer:
                sel_df_for_export = sel.copy()
                sel_df_for_export["raw_row_str"] = sel_df_for_export["raw_row"].apply(lambda r: str(r) if r is not None else "")
                sel_df_for_export.to_excel(writer, sheet_name="active_sheet_selection", index=False)
                pd.DataFrame([summary["counts"]]).to_excel(writer, sheet_name="summary_counts", index=False)
                pd.DataFrame([summary["regular_attendance_summary"]]).to_excel(writer, sheet_name="regular_summary", index=False)
                pd.DataFrame(details["present_5_days"]).to_excel(writer, sheet_name="present_5_days", index=False)
                pd.DataFrame(details["present_3_or_more_days"]).to_excel(writer, sheet_name="present_3_plus", index=False)
                pd.DataFrame(details["defaulters_less_than_3_days"]).to_excel(writer, sheet_name="defaulters_lt3", index=False)
            report_path = str(report_file.name)
        except Exception:
            logger.exception("Failed to write export report")
            report_path = None

    out = {
        "mode": mode,
        "stats_detail": stats_detail,
        "summary": summary,
        "details": details
    }
    if report_path:
        out["report_path"] = report_path
    return out


if __name__ == "__main__":
    res = compare_ccure_vs_sheets(region_filter="APAC", location_city="Pune", export=False, limit_list=20)
    import json as _json
    print(_json.dumps(res, indent=2, default=str))












# region_clients.py
"""
HTTP helpers for region occupancy endpoints.
Returns:
 - fetch_all_regions() -> list of {"region": <name>, "count": <int>}
 - fetch_all_details(timeout=...) -> list of person-detail dicts (where available)
 - fetch_all_history(timeout=...) -> list of date-or-detail dicts (history endpoints)

Drop-in replacement for your region_clients.py.
"""

import requests
from requests.exceptions import RequestException
from datetime import datetime
import logging
import time
import sys

logger = logging.getLogger("region_clients")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# endpoints (edit if your hosts differ)
endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/live-summary",
    "emea":  "http://10.199.22.57:3007/api/occupancy/live-summary",
    "laca":  "http://10.199.22.57:4000/api/occupancy/live-summary",
    "apac":  "http://10.199.22.57:3008/api/occupancy/live-summary"
}

history_endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/history",
    "emea":  "http://10.199.22.57:3007/api/occupancy/history",
    "apac":  "http://10.199.22.57:3008/api/occupancy/history",
    "laca":  "http://10.199.22.57:4000/api/occupancy/history"
}

DEFAULT_ATTEMPTS = 3
DEFAULT_BACKOFF = 0.6

def _do_get_with_retries(url, timeout, attempts=DEFAULT_ATTEMPTS, backoff=DEFAULT_BACKOFF):
    last_err = None
    for attempt in range(1, attempts + 1):
        try:
            # use connect,read timeouts tuple for clarity
            # r = requests.get(url, timeout=(3, max(5, timeout)))

            r = requests.get(url, timeout=(3, max(10, timeout)))

            r.raise_for_status()
            try:
                return r.json()
            except ValueError:
                try:
                    return {"_raw_text": r.text}
                except Exception:
                    return None
        except RequestException as e:
            last_err = e
            logger.warning(f"[region_clients] attempt {attempt}/{attempts} failed for {url}: {e}")
            if attempt < attempts:
                time.sleep(backoff * attempt)
            continue
    logger.warning(f"[region_clients] all {attempts} attempts failed for {url}: {last_err}")
    return None

def fetch_all_regions(timeout=6):
    results = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            realtime = {}
            if isinstance(data, dict):
                realtime = data.get("realtime", {}) or {}
            total = 0
            for site in realtime.values():
                try:
                    total += int(site.get("total", 0))
                except Exception:
                    pass
            if total == 0 and isinstance(data, dict):
                for k, v in data.items():
                    if isinstance(v, dict) and "total" in v:
                        try:
                            total += int(v.get("total", 0))
                        except Exception:
                            pass
            results.append({"region": region, "count": total})
        except Exception as e:
            logger.exception(f"[region_clients] unexpected error fetching live-summary for {region}: {e}")
            results.append({"region": region, "count": None})
    return results

def _extract_list_candidates(data):
    """
    Helper to find lists that may contain detail rows inside a payload.
    """
    if isinstance(data, list):
        return [x for x in data if isinstance(x, dict)]
    if isinstance(data, dict):
        for k in ("details","people","list","items","results","data","entries"):
            if k in data and isinstance(data[k], list):
                return [x for x in data[k] if isinstance(x, dict)]
        # top-level dict might be single row; return empty here (caller will inspect)
        return []
    return []

def fetch_all_details(timeout=6):
    """
    Attempt to fetch person-level detail records from live-summary endpoints.
    Returns a list of dicts (flattened detail rows). Regions that don't provide details will be skipped.
    """
    all_details = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            details = _extract_list_candidates(data)
            # also attempt to find nested partitions that contain details
            if not details and isinstance(data, dict):
                for v in data.values():
                    if isinstance(v, dict):
                        candidates = _extract_list_candidates(v)
                        if candidates:
                            details.extend(candidates)
            for d in details:
                try:
                    d2 = dict(d)
                    d2["_region"] = region
                    d2["_source_url"] = url
                    all_details.append(d2)
                except Exception:
                    continue
        except Exception as e:
            logger.warning(f"[region_clients] warning: cannot fetch details from {region}@{url}: {e}")
            continue

    # try history endpoints as second chance (they sometimes include per-person rows)
    if not all_details:
        for region, url in history_endpoints.items():
            try:
                data = _do_get_with_retries(url, timeout=timeout) or {}
                details = _extract_list_candidates(data)
                for d in details:
                    try:
                        d2 = dict(d)
                        d2["_region"] = region
                        d2["_source_url"] = url
                        all_details.append(d2)
                    except Exception:
                        continue
            except Exception:
                logger.debug(f"[region_clients] history details fetch for {region} failed", exc_info=True)
                continue

    logger.info("[region_clients] fetched %d detail rows across endpoints", len(all_details))
    return all_details

def fetch_history_for_region(region, timeout=6):
    url = history_endpoints.get(region)
    if not url:
        logger.debug(f"[fetch_history_for_region] no history endpoint for {region}")
        return []
    try:
        data = _do_get_with_retries(url, timeout=timeout) or {}
        summary = []
        if isinstance(data, dict):
            # try common key names
            for key in ("summaryByDate","summary","data","entries","results","details","items"):
                if key in data and isinstance(data.get(key), list):
                    for s in data.get(key):
                        if isinstance(s, dict):
                            s2 = dict(s)
                            s2["_region"] = region
                            s2["_source_url"] = url
                            summary.append(s2)
                    break
            else:
                # maybe single dict date-summary
                if "date" in data or "day" in data:
                    s2 = dict(data)
                    s2["_region"] = region
                    s2["_source_url"] = url
                    summary.append(s2)
        elif isinstance(data, list):
            for s in data:
                if isinstance(s, dict):
                    s2 = dict(s)
                    s2["_region"] = region
                    s2["_source_url"] = url
                    summary.append(s2)
        return summary
    except Exception as e:
        logger.warning(f"[region_clients] cannot fetch history for {region}@{url}: {e}")
        return []

def fetch_all_history(timeout=6):
    all_entries = []
    for region in history_endpoints.keys():
        try:
            entries = fetch_history_for_region(region, timeout=timeout)
            if entries:
                all_entries.extend(entries)
        except Exception:
            logger.exception(f"[region_clients] error aggregating history for {region}")
            continue
    logger.info("[region_clients] fetched %d history entries", len(all_entries))
    return all_entries



