# from C:\...\Trend Analysis\backend
python scripts/generate_90_days.py --end 2025-11-23 --window 90 --outdir ./outputs --all-cities

python scripts/build_90day_training.py --end 2025-11-23 --outdir ./outputs --window 90 --all-cities --start 2025-10-01 --force

python scripts/train_all_models.py --outdir ./outputs --models_dir ./models














python backend/scripts/generate_90_days.py --end 2025-11-23 --window 90 --outdir ./outputs --all-cities


python backend/scripts/build_90day_training.py --end 2025-11-23 --outdir ./outputs --window 90 --all-cities --start 2025-10-01 --force

python backend/scripts/train_all_models.py --outdir ./outputs --models_dir ./models


When i run  above we dont the console shows below erroe need to fix..


C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Scripts\python.exe: can't open file 'C:\\Users\\W0024618\\Desktop\\Trend Analysis\\backend\\backend\\scripts\\generate_90_days.py': [Errno 2] No such file or directory        
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python backend/scripts/build_90day_training.py --end 2025-11-23 --outdir ./outputs --window 90 --all-cities --start 2025-10-01 --force
>>
>>
C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Scripts\python.exe: can't open file 'C:\\Users\\W0024618\\Desktop\\Trend Analysis\\backend\\backend\\scripts\\build_90day_training.py': [Errno 2] No such file or directory    
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> 
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> 
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> 
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> 
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python backend/scripts/train_all_models.py --outdir ./outputs --models_dir ./models
>>
C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Scripts\python.exe: can't open file 'C:\\Users\\W0024618\\Desktop\\Trend Analysis\\backend\\backend\\scripts\\train_all_models.py': [Errno 2] No such file or directory        
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> 




C:\Users\W0024618\Desktop\Trend Analysis\backend\scripts\build_90day_training.py




#!/usr/bin/env python3
"""
Create a 90-day training CSV by calling into trend_runner.

New features:
 - --all-cities: discover cities from duration CSVs in outdir (since --start date),
   and build a separate 90-day training CSV into outdir/<city_slug>/ for each city.
 - per-city outputs are placed under outdir/<city_slug>/ to avoid overwriting.
 - Default start for discovery is 2025-10-01 (train-from Oct 1).

Usage examples:
  # single city (old behavior)
  python build_90day_training.py --end 2025-11-09 --outdir ./outputs --window 90 --city Pune --force

  # build for all cities discovered in ./outputs since 2025-10-01
  python build_90day_training.py --end 2025-11-09 --outdir ./outputs --window 90 --all-cities --start 2025-10-01 --force
"""
from datetime import date, datetime
from pathlib import Path
import sys
import argparse
import os
import logging
import pandas as pd
import re

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

# Defensive: make backend project root importable
HERE = Path(__file__).resolve()
PROJECT_ROOT = HERE.parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
os.chdir(str(PROJECT_ROOT))

# try to import helpers from trend_runner
try:
    import trend_runner as tr_mod
    build_90day_fn = getattr(tr_mod, "build_90day_training", None)
    build_monthly_fn = getattr(tr_mod, "build_monthly_training", None)
    TREND_OUTDIR = getattr(tr_mod, "OUTDIR", None)
except Exception:
    tr_mod = None
    build_90day_fn = None
    build_monthly_fn = None
    TREND_OUTDIR = None

# small slug helper for dir names
def _slug(s: str) -> str:
    return re.sub(r'[^a-z0-9]+', '_', str(s or "").strip().lower()).strip('_') or "unknown"

def find_cities_from_duration_files(outdir: Path, start_date: date) -> list:
    """
    Scan outdir for files like *_duration_YYYYMMDD.csv and extract unique PartitionName2 values
    from files whose date >= start_date. Returns sorted list of unique city names (non-empty).
    """
    cities = set()
    pattern = re.compile(r'.*_duration_(\d{8})\.csv$', flags=re.IGNORECASE)
    if not outdir.exists():
        logging.warning("Outdir %s does not exist for scanning duration files", outdir)
        return []
    for p in outdir.glob("*_duration_*.csv"):
        m = pattern.match(p.name)
        if not m:
            continue
        try:
            dt = datetime.strptime(m.group(1), "%Y%m%d").date()
        except Exception:
            continue
        if dt < start_date:
            continue
        # read only PartitionName2 column (fast)
        try:
            df = pd.read_csv(p, usecols=[c for c in ['PartitionName2'] if c in pd.read_csv(p, nrows=0).columns], dtype=str, low_memory=True)
            if 'PartitionName2' in df.columns:
                for v in df['PartitionName2'].dropna().unique():
                    vs = str(v).strip()
                    if vs:
                        cities.add(vs)
        except Exception:
            # fallback: try full read (defensive)
            try:
                df = pd.read_csv(p, dtype=str, low_memory=True)
                if 'PartitionName2' in df.columns:
                    for v in df['PartitionName2'].dropna().unique():
                        vs = str(v).strip()
                        if vs:
                            cities.add(vs)
            except Exception:
                logging.debug("Failed reading %s for city discovery", p)
                continue
    return sorted(cities)

def remove_cache_file(outdir: Path):
    candidates = [
        outdir / "trend_pune_90day_cache.csv",
        outdir / "trend_pune_cache.csv",
        outdir / "trend_90day_cache.csv"
    ]
    removed = []
    for p in candidates:
        try:
            if p.exists():
                p.unlink()
                removed.append(str(p.name))
        except Exception:
            logging.exception("Failed removing cache file %s", p)
    if removed:
        logging.info("Removed cache files: %s", ", ".join(removed))

def build_for_city(end_date, out_path_dir: Path, window_days, min_unique_employees, city, force):
    if force:
        remove_cache_file(out_path_dir)
    out_path_dir.mkdir(parents=True, exist_ok=True)
    if build_90day_fn:
        logging.info("Calling trend_runner.build_90day_training(end_date=%s, window_days=%s, outdir=%s, min_unique_employees=%s, city=%s)",
                     end_date, window_days, out_path_dir, min_unique_employees, city)
        res = build_90day_fn(end_date=end_date, window_days=window_days, outdir=str(out_path_dir),
                             min_unique_employees=min_unique_employees, city=city)
        return res
    elif build_monthly_fn:
        approx_months = max(1, int(round(float(window_days) / 30.0)))
        logging.info("trend_runner.build_90day_training not found; falling back to build_monthly_training(months=%s) for city=%s", approx_months, city)
        res = build_monthly_fn(end_date=end_date, months=approx_months, min_unique_employees=min_unique_employees, outdir=str(out_path_dir))
        return res
    else:
        raise RuntimeError("trend_runner missing build_90day_training / build_monthly_training")

def main(end_date_str=None, outdir="./outputs", window_days=90, force=False, all_cities=False, start_date_str=None, min_unique_employees=1000, city=None):
    if end_date_str:
        end_date = date.fromisoformat(end_date_str)
    else:
        end_date = date.today()

    # default discovery start for city scanning
    if start_date_str:
        start_date = date.fromisoformat(start_date_str)
    else:
        start_date = date.fromisoformat("2025-10-01")

    out_path_dir = Path(outdir)
    # prefer TREND_OUTDIR constant from trend_runner if user did not supply explicit outdir
    if TREND_OUTDIR and (outdir in (None, "./outputs", "") or str(out_path_dir) == "./outputs"):
        try:
            out_path_dir = Path(TREND_OUTDIR)
        except Exception:
            pass

    if all_cities:
        logging.info("Discovering cities in %s (duration files since %s)", out_path_dir, start_date.isoformat())
        cities = find_cities_from_duration_files(out_path_dir, start_date)
        if not cities:
            logging.warning("No cities discovered from duration files in %s. Exiting.", out_path_dir)
            return
        logging.info("Discovered %d cities: %s", len(cities), ", ".join(cities[:10]))
        results = {}
        for c in cities:
            slug = _slug(c)
            city_outdir = out_path_dir / slug
            try:
                res = build_for_city(end_date, city_outdir, window_days, min_unique_employees, c, force)
                results[c] = res
            except Exception:
                logging.exception("Failed building training for city %s", c)
        return results
    else:
        # single city behavior
        target_city = city or "Pune"
        if force:
            remove_cache_file(out_path_dir)
        out_path_dir.mkdir(parents=True, exist_ok=True)
        return build_for_city(end_date, out_path_dir, window_days, min_unique_employees, target_city, force)

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--end", help="end date (YYYY-MM-DD). default = today", default=None)
    p.add_argument("--outdir", help="outputs dir", default="./outputs")
    p.add_argument("--min_unique_employees", type=int, default=1000)
    p.add_argument("--window", type=int, default=90, help="window days (default 90)")
    p.add_argument("--city", default="Pune")
    p.add_argument("--all-cities", action="store_true", help="discover cities from duration files and run for each")
    p.add_argument("--start", help="start date for discovering duration files (YYYY-MM-DD). default 2025-10-01", default="2025-10-01")
    p.add_argument("--force", action="store_true", help="force refresh (remove cache files before building)")
    args = p.parse_args()
    main(end_date_str=args.end, outdir=args.outdir, window_days=args.window,
         force=args.force, all_cities=args.all_cities, start_date_str=args.start,
         min_unique_employees=args.min_unique_employees, city=args.city)







C:\Users\W0024618\Desktop\Trend Analysis\backend\scripts\generate_90_days.py








#!/usr/bin/env python3
"""
Generate trend CSVs for a sliding window of days by calling trend_runner.run_trend_for_date.

New features:
 - --all-cities: discover cities from duration CSVs (since --start) and generate trend CSVs for each city into outdir/<city_slug>/.
 - Keeps retry logic and permissive fallback to positional call if run_trend_for_date doesn't accept kwargs.

Usage:
  # single city (old behavior)
  python generate_90_days.py --end 2025-11-09 --window 90 --city Pune --outdir ./outputs

  # generate for all cities discovered in ./outputs since 2025-10-01
  python generate_90_days.py --end 2025-11-09 --window 90 --all-cities --outdir ./outputs --start 2025-10-01
"""
from datetime import date, timedelta, datetime
from pathlib import Path
import sys
import os
import argparse
import logging
import time
import pandas as pd
import re

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

# Make the backend project root importable when running the script directly.
HERE = Path(__file__).resolve()
PROJECT_ROOT = HERE.parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

os.chdir(str(PROJECT_ROOT))

try:
    from trend_runner import run_trend_for_date, OUTDIR as TREND_OUTDIR
except Exception:
    run_trend_for_date = None
    TREND_OUTDIR = None

def _slug(s: str) -> str:
    return re.sub(r'[^a-z0-9]+', '_', str(s or "").strip().lower()).strip('_') or "unknown"

def find_cities_from_duration_files(outdir: Path, start_date: date) -> list:
    pattern = re.compile(r'.*_duration_(\d{8})\.csv$', flags=re.IGNORECASE)
    cities = set()
    if not outdir.exists():
        return []
    for p in outdir.glob("*_duration_*.csv"):
        m = pattern.match(p.name)
        if not m:
            continue
        try:
            dt = datetime.strptime(m.group(1), "%Y%m%d").date()
        except Exception:
            continue
        if dt < start_date:
            continue
        try:
            # attempt to read PartitionName2 column first
            df = pd.read_csv(p, usecols=[c for c in ['PartitionName2'] if c in pd.read_csv(p, nrows=0).columns], dtype=str, low_memory=True)
            if 'PartitionName2' in df.columns:
                for v in df['PartitionName2'].dropna().unique():
                    vs = str(v).strip()
                    if vs:
                        cities.add(vs)
        except Exception:
            try:
                df = pd.read_csv(p, dtype=str, low_memory=True)
                if 'PartitionName2' in df.columns:
                    for v in df['PartitionName2'].dropna().unique():
                        vs = str(v).strip()
                        if vs:
                            cities.add(vs)
            except Exception:
                logging.debug("Failed reading %s for city discovery", p)
                continue
    return sorted(cities)

def process_dates(start_date: date, end_date: date, outdir: Path, city: str, max_retries: int = 1, pause_seconds: float = 1.0):
    d = start_date
    while d <= end_date:
        attempt = 0
        success = False
        while attempt <= max_retries and not success:
            try:
                logging.info("Processing %s (city=%s) attempt %s", d.isoformat(), city, attempt + 1)
                if run_trend_for_date is None:
                    raise RuntimeError("trend_runner.run_trend_for_date not importable. Ensure trend_runner is on PYTHONPATH.")
                run_trend_for_date(d, outdir=str(outdir), city=city)
                success = True
            except TypeError as te:
                # try positional fallback
                logging.warning("run_trend_for_date signature mismatch (%s). Trying positional call.", te)
                try:
                    run_trend_for_date(d)
                    success = True
                except Exception as e2:
                    logging.exception("Positional run_trend_for_date call also failed: %s", e2)
            except Exception as e:
                logging.exception("Failed for %s (city=%s): %s", d.isoformat(), city, e)
            attempt += 1
            if not success and attempt <= max_retries:
                time.sleep(pause_seconds)
        if not success:
            logging.error("Giving up for %s after %s attempts (city=%s)", d.isoformat(), max_retries + 1, city)
        d = d + timedelta(days=1)

def main(end_date=None, window_days=90, outdir="./outputs", city="Pune", start_date_override=None, all_cities=False):
    if end_date:
        end_dt = date.fromisoformat(end_date)
    else:
        end_dt = date.today()

    if start_date_override:
        start_dt = date.fromisoformat(start_date_override)
    else:
        start_dt = end_dt - timedelta(days=window_days - 1)

    out_path = Path(outdir)
    if TREND_OUTDIR and outdir in ("./outputs", None, ""):
        try:
            out_path = Path(TREND_OUTDIR)
        except Exception:
            pass

    out_path.mkdir(parents=True, exist_ok=True)

    if all_cities:
        # discover cities using duration files since provided start (default Oct 1)
        discovery_start = date.fromisoformat("2025-10-01")
        logging.info("Discovering cities in %s (duration files since %s)", out_path, discovery_start.isoformat())
        cities = find_cities_from_duration_files(out_path, discovery_start)
        if not cities:
            logging.warning("No cities discovered. Exiting.")
            return
        logging.info("Discovered %d cities: %s", len(cities), ", ".join(cities[:10]))
        for c in cities:
            slug = _slug(c)
            city_outdir = out_path / slug
            logging.info("Generating for city %s -> outputs in %s", c, city_outdir)
            process_dates(start_dt, end_dt, city_outdir, c, max_retries=1, pause_seconds=1.0)
    else:
        out_path.mkdir(parents=True, exist_ok=True)
        process_dates(start_dt, end_dt, out_path, city, max_retries=1, pause_seconds=1.0)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--end", help="end date (YYYY-MM-DD). default = today", default=None)
    parser.add_argument("--start", help="start date (YYYY-MM-DD). optional, overrides window", default=None)
    parser.add_argument("--outdir", help="outputs dir", default="./outputs")
    parser.add_argument("--window", type=int, default=90, help="window days (default 90)")
    parser.add_argument("--city", default="Pune")
    parser.add_argument("--all-cities", action="store_true", help="discover cities from duration files and generate for each")
    args = parser.parse_args()

    main(end_date=args.end, window_days=args.window, outdir=args.outdir, city=args.city, start_date_override=args.start, all_cities=args.all_cities)










C:\Users\W0024618\Desktop\Trend Analysis\backend\scripts\train_all_models.py


#!/usr/bin/env python3
"""
Scan output directories for training CSV(s) and run ml_training.main for each.

Usage:
  python train_all_models.py --outdir ./outputs --models_dir ./models --pattern training_person_*.csv

Notes:
 - This imports ml_training as a module and calls its main() function programmatically, so
   ml_training must be importable (backend on PYTHONPATH).
 - If ml_training cannot be imported, this script will attempt to call the CLI via subprocess as fallback.
"""
from pathlib import Path
import sys
import argparse
import logging
import subprocess
import importlib
import glob

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

HERE = Path(__file__).resolve()
PROJECT_ROOT = HERE.parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

def find_training_csvs(outdir: Path, pattern="training_person_*.csv"):
    found = list(outdir.glob(f"**/{pattern}"))
    return sorted(found)

def try_import_ml_training():
    try:
        import ml_training as ml_mod
        return ml_mod
    except Exception:
        return None

def run_ml_training_cli(csv_path: Path, models_dir: Path, features=None):
    cmd = [sys.executable, str(PROJECT_ROOT / "backend" / "ml_training.py"), "--input", str(csv_path), "--models_dir", str(models_dir)]
    if features:
        cmd += ["--features", features]
    logging.info("Running ML training CLI: %s", " ".join(cmd))
    subprocess.check_call(cmd)

def main(outdir="./outputs", models_dir="./models", pattern="training_person_*.csv", features=None):
    out_path = Path(outdir)
    models_path = Path(models_dir)
    models_path.mkdir(parents=True, exist_ok=True)

    csvs = find_training_csvs(out_path, pattern)
    if not csvs:
        logging.warning("No training CSVs found under %s matching %s", out_path, pattern)
        return

    ml_mod = try_import_ml_training()
    for csv in csvs:
        logging.info("Training models for %s", csv)
        per_city_models_dir = models_path / csv.parent.name
        per_city_models_dir.mkdir(parents=True, exist_ok=True)
        try:
            if ml_mod and hasattr(ml_mod, "main"):
                # call programmatically
                ml_mod.main(csv, per_city_models_dir, feature_cols=None)
            else:
                run_ml_training_cli(csv, per_city_models_dir, features)
        except Exception:
            logging.exception("Training failed for %s", csv)

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--outdir", default="./outputs")
    p.add_argument("--models_dir", default="./models")
    p.add_argument("--pattern", default="training_person_*.csv")
    p.add_argument("--features", default=None)
    args = p.parse_args()
    main(outdir=args.outdir, models_dir=args.models_dir, pattern=args.pattern, features=args.features)







