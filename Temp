(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> ^C
(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> ^C
(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> python -m uvicorn app:app --host 0.0.0.0 --port 8000
INFO:     Started server process [36948]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:62318 - "GET /ccure/stream HTTP/1.1" 200 OK
2025-09-04 13:14:17,198 INFO data_compare_service_v2: [region_cache] fetching region history via region_clients.fetch_all_history()
[compute_daily_attendance] no swipes for 2025-09-04
2025-09-04 13:14:28,823 INFO region_clients: [region_clients] fetched 1075 detail rows across endpoints
2025-09-04 13:14:58,752 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:15:12,085 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:15:19,376 WARNING region_clients: [region_clients] attempt 2/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:15:32,704 WARNING region_clients: [region_clients] attempt 2/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:15:40,602 WARNING region_clients: [region_clients] attempt 3/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:15:40,602 WARNING region_clients: [region_clients] all 3 attempts failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)    
2025-09-04 13:15:53,920 WARNING region_clients: [region_clients] attempt 3/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:15:53,922 WARNING region_clients: [region_clients] all 3 attempts failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)    
2025-09-04 13:16:00,634 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:4000/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)       
2025-09-04 13:16:07,337 INFO region_clients: [region_clients] fetched 51 history entries
INFO:     127.0.0.1:56124 - "GET /ccure/verify?raw=true HTTP/1.1" 200 OK
2025-09-04 13:16:19,079 INFO region_clients: [region_clients] fetched 51 history entries
2025-09-04 13:16:19,080 INFO data_compare_service_v2: [region_cache] prefetched 51 region history entries
2025-09-04 13:16:27,956 INFO data_compare_service_v2: [presence_fetch] fallback broad DB query returned 0 rows for 2025-08-25 -> 2025-08-29
2025-09-04 13:16:27,959 INFO data_compare_service_v2: [presence_fetch] DB-derived presence found for 0/1132 employees
2025-09-04 13:16:27,959 INFO data_compare_service_v2: [presence_fetch] DB coverage low (0/1132) - trying region occupancy history fallback
2025-09-04 13:16:27,959 INFO data_compare_service_v2: [region_cache] Using cached region history (age 8.9s)       
2025-09-04 13:16:27,965 INFO data_compare_service_v2: [region_history] scanned 51 detail rows from preloaded entries; matched 0 presence entries
2025-09-04 13:16:27,968 INFO data_compare_service_v2: [presence_fetch] final presence coverage: 0/1132 employees have at least one positive day
INFO:     127.0.0.1:62598 - "GET /ccure/compare_v2?region_filter=APAC&location_city=Pune&week_ref_date=2025-08-25 HTTP/1.1" 200 OK
INFO:     127.0.0.1:57488 - "GET /ccure/stream HTTP/1.1" 200 OK
[compute_daily_attendance] no swipes for 2025-09-04
2025-09-04 13:17:16,662 INFO region_clients: [region_clients] fetched 1081 detail rows across endpoints
2025-09-04 13:17:18,015 INFO data_compare_service_v2: [region_cache] Using cached region history (age 58.9s)
2025-09-04 13:17:32,271 INFO data_compare_service_v2: [presence_fetch] fallback broad DB query returned 0 rows for 2025-09-01 -> 2025-09-05
2025-09-04 13:17:32,334 INFO data_compare_service_v2: [presence_fetch] DB-derived presence found for 0/8625 employees
2025-09-04 13:17:32,335 INFO data_compare_service_v2: [presence_fetch] DB coverage low (0/8625) - trying region occupancy history fallback
2025-09-04 13:17:32,335 INFO data_compare_service_v2: [region_cache] Using cached region history (age 73.3s)      
2025-09-04 13:17:47,847 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:18:08,642 WARNING region_clients: [region_clients] attempt 2/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:18:30,133 WARNING region_clients: [region_clients] attempt 3/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:18:30,150 WARNING region_clients: [region_clients] all 3 attempts failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)    
2025-09-04 13:18:38,282 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:4000/api/occupancy/history: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))
2025-09-04 13:19:03,319 INFO region_clients: [region_clients] fetched 51 history entries
INFO:     127.0.0.1:59495 - "GET /ccure/verify?raw=true HTTP/1.1" 200 OK
INFO:     127.0.0.1:57931 - "GET /ccure/stream HTTP/1.1" 200 OK
[compute_daily_attendance] no swipes for 2025-09-04
2025-09-04 13:19:39,189 INFO region_clients: [region_clients] fetched 1075 detail rows across endpoints
2025-09-04 13:20:36,613 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:20:57,480 WARNING region_clients: [region_clients] attempt 2/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:21:18,963 WARNING region_clients: [region_clients] attempt 3/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:21:18,978 WARNING region_clients: [region_clients] all 3 attempts failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)    
[compute_daily_attendance] no swipes for 2025-09-04
2025-09-04 13:21:39,309 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:4000/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)       
2025-09-04 13:21:44,588 INFO region_clients: [region_clients] fetched 1078 detail rows across endpoints
2025-09-04 13:21:49,236 INFO data_compare_service_v2: [region_history] scanned 51 detail rows from preloaded entries; matched 9 presence entries
2025-09-04 13:21:49,266 INFO data_compare_service_v2: [presence_fetch] final presence coverage: 3/8625 employees have at least one positive day
2025-09-04 13:21:52,602 WARNING region_clients: [region_clients] attempt 2/3 failed for http://10.199.22.57:4000/api/occupancy/history: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))
2025-09-04 13:22:10,362 WARNING region_clients: [region_clients] attempt 3/3 failed for http://10.199.22.57:4000/api/occupancy/history: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))
2025-09-04 13:22:10,366 WARNING region_clients: [region_clients] all 3 attempts failed for http://10.199.22.57:4000/api/occupancy/history: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))
2025-09-04 13:22:10,373 INFO region_clients: [region_clients] fetched 35 history entries
INFO:     127.0.0.1:60702 - "GET /ccure/verify?raw=true HTTP/1.1" 200 OK
2025-09-04 13:22:20,922 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
INFO:     127.0.0.1:51946 - "GET /ccure/compare_v2 HTTP/1.1" 200 OK
2025-09-04 13:22:41,671 WARNING region_clients: [region_clients] attempt 2/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:23:02,901 WARNING region_clients: [region_clients] attempt 3/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 13:23:02,902 WARNING region_clients: [region_clients] all 3 attempts failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)    
2025-09-04 13:23:15,935 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:4000/api/occupancy/history: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))
2025-09-04 13:23:36,670 INFO region_clients: [region_clients] fetched 51 history entries
INFO:     127.0.0.1:56133 - "GET /ccure/verify?raw=true HTTP/1.1" 200 OK
INFO:     127.0.0.1:61748 - "GET /ccure/stream HTTP/1.1" 200 OK
[compute_daily_attendance] no swipes for 2025-09-04
2025-09-04 14:19:44,537 INFO region_clients: [region_clients] fetched 1268 detail rows across endpoints
2025-09-04 14:19:48,607 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:3006/api/occupancy/history: 500 Server Error: Internal Server Error for url: http://10.199.22.57:3006/api/occupancy/history
2025-09-04 14:20:09,237 WARNING region_clients: [region_clients] attempt 2/3 failed for http://10.199.22.57:3006/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=20)       
INFO:     127.0.0.1:55144 - "GET /ccure/stream HTTP/1.1" 200 OK
[compute_daily_attendance] no swipes for 2025-09-04
2025-09-04 14:20:34,909 INFO region_clients: [region_clients] fetched 1271 detail rows across endpoints
2025-09-04 14:20:53,172 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 14:21:12,167 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 14:21:13,800 WARNING region_clients: [region_clients] attempt 2/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 14:21:32,812 WARNING region_clients: [region_clients] attempt 2/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 14:21:35,024 WARNING region_clients: [region_clients] attempt 3/3 failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)       
2025-09-04 14:21:35,024 WARNING region_clients: [region_clients] all 3 attempts failed for http://10.199.22.57:3008/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)    





Read Above Console Details and check below each file line by line and help me to fix all issue caredfully..

1) Calculate Average logic  take more time for calculation .. sometime we got failed to calculate average error also..

2)http://localhost:8000/ccure/compare_v2  This Api not Comparing Data ..like this File fetch only exce sheet data but not comparing data with history api..
so check below API responce and Fix all issue caredfully...


{
  "mode": "full",
  "stats_detail": "ActiveProfiles",
  "summary": {
    "filters": {
      "region": null,
      "location_city": null,
      "location_state": null,
      "location_description": null,
      "week_monday": "2025-09-01",
      "week_friday": "2025-09-05"
    },
    "counts": {
      "total_active_in_sheet": 8625,
      "today_headcount_from_summary": 1,
      "today_headcount_pct_vs_sheet": 0.01,
      "on_leave_count_in_sheet": 294,
      "employee_type_counts": {
        "regular": 8448,
        "fixed term": 146,
        "temporary": 13,
        "casual": 12,
        "intern": 5,
        "trainee": 1
      }
    },
    "regular_attendance_summary": {
      "regular_total": 8448,
      "present_5_day_count": 0,
      "present_3_or_more_count": 0,
      "present_less_than_3_count": 8448,
      "present_only_1_day_count": 3
    }
  },
  "details": {
    "present_5_days": [],
    "present_3_or_more_days": [],
    "defaulters_less_than_3_days": [
      {
        "employee_id": "319473",
        "full_name": "., Anushka",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "324002",
        "full_name": "., Diwakar",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "324422",
        "full_name": "., LEONARDO Tertuliano Monsani",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "323879",
        "full_name": "., Vikas",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "322825",
        "full_name": "AKTER, SULTANA",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "320818",
        "full_name": "ANAND, NUPUR",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "328795",
        "full_name": "ARRIOLA BORDON, ALICIA",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "312895",
        "full_name": "Aassar, Maisa",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "321269",
        "full_name": "Abaca Paez, Agustin Marcelo",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "325862",
        "full_name": "Abaca, Melisa Ayelen",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "325299",
        "full_name": "Abadilla, Ma. Erika",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "328996",
        "full_name": "Abalos, Larry",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "326980",
        "full_name": "Abancio, Mechel",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "328945",
        "full_name": "Abang, Emmanuel E.",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "321388",
        "full_name": "Abanto Reyes, Christian",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "307689",
        "full_name": "Abarca Torres, Kevin Andrey",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "322554",
        "full_name": "Abarca, Joshua",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "309123",
        "full_name": "Abarzosa, Rowegine",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "323056",
        "full_name": "Abastos Manyari, Erika Natalia",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "250864",
        "full_name": "Abate, Miriam Bibiana",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "328869",
        "full_name": "Abbude, TÃ¡cio",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "195823",
        "full_name": "Abdelali, Tarik",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "323420",
        "full_name": "Abdrabou, Ahmed Atef Abdelrahman",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "317710",
        "full_name": "Abdul Hafeez, Sameera Begum",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "323542",
        "full_name": "Abdullah, Hasan",
        "days_present": 0,
        "on_leave": false
      },









Once Check http://10.199.22.57:3007/api/occupancy/history History APi Responce for Comparision..

{
  "success": true,
  "summaryByDate": [
    {
      "date": "2025-08-20",
      "day": "Wednesday",
      "region": {
        "name": "EMEA",
        "total": 775,
        "Employee": 733,
        "Contractor": 42
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 555,
          "Employee": 528,
          "Contractor": 27
        },
        "IE.Dublin": {
          "total": 38,
          "Employee": 35,
          "Contractor": 3
        },
        "AUT.Vienna": {
          "total": 46,
          "Employee": 42,
          "Contractor": 4
        },
        "ES.Madrid": {
          "total": 50,
          "Employee": 48,
          "Contractor": 2
        },
        "UK.London": {
          "total": 25,
          "Employee": 23,
          "Contractor": 2
        },
        "DU.Abu Dhab": {
          "total": 31,
          "Employee": 30,
          "Contractor": 1
        },
        "IT.Rome": {
          "total": 19,
          "Employee": 17,
          "Contractor": 2
        },
        "MA.Casablanca": {
          "total": 2,
          "Employee": 2,
          "Contractor": 0
        },
        "RU.Moscow": {
          "total": 9,
          "Employee": 8,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-21",
      "day": "Thursday",
      "region": {
        "name": "EMEA",
        "total": 604,
        "Employee": 568,
        "Contractor": 36
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 419,
          "Employee": 397,
          "Contractor": 22
        },
        "UK.London": {
          "total": 21,
          "Employee": 19,
          "Contractor": 2
        },
        "AUT.Vienna": {
          "total": 39,
          "Employee": 35,
          "Contractor": 4
        },
        "IE.Dublin": {
          "total": 24,
          "Employee": 22,
          "Contractor": 2
        },
        "ES.Madrid": {
          "total": 39,
          "Employee": 36,
          "Contractor": 3
        },
        "DU.Abu Dhab": {
          "total": 37,
          "Employee": 36,
          "Contractor": 1
        },
        "IT.Rome": {
          "total": 18,
          "Employee": 17,
          "Contractor": 1
        },
        "RU.Moscow": {
          "total": 7,
          "Employee": 6,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-22",
      "day": "Friday",
      "region": {
        "name": "EMEA",
        "total": 383,
        "Employee": 349,
        "Contractor": 34
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 253,
          "Employee": 234,
          "Contractor": 19
        },
        "MA.Casablanca": {
          "total": 12,
          "Employee": 11,
          "Contractor": 1
        },
        "AUT.Vienna": {
          "total": 22,
          "Employee": 20,
          "Contractor": 2
        },
        "IE.Dublin": {
          "total": 17,
          "Employee": 15,
          "Contractor": 2
        },
        "ES.Madrid": {
          "total": 24,
          "Employee": 21,
          "Contractor": 3
        },
        "UK.London": {
          "total": 15,
          "Employee": 13,
          "Contractor": 2
        },
        "IT.Rome": {
          "total": 13,
          "Employee": 11,
          "Contractor": 2
        },
        "DU.Abu Dhab": {
          "total": 21,
          "Employee": 20,
          "Contractor": 1
        },
        "RU.Moscow": {
          "total": 6,
          "Employee": 4,
          "Contractor": 2
        }
      }
    },
    {
      "date": "2025-08-23",
      "day": "Saturday",
      "region": {
        "name": "EMEA",
        "total": 39,
        "Employee": 34,
        "Contractor": 5
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 34,
          "Employee": 31,
          "Contractor": 3
        },
        "AUT.Vienna": {
          "total": 2,
          "Employee": 2,
          "Contractor": 0
        },
        "IE.Dublin": {
          "total": 2,
          "Employee": 1,
          "Contractor": 1
        },
        "MA.Casablanca": {
          "total": 1,
          "Employee": 0,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-24",
      "day": "Sunday",
      "region": {
        "name": "EMEA",
        "total": 28,
        "Employee": 23,
        "Contractor": 5
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 23,
          "Employee": 20,
          "Contractor": 3
        },
        "AUT.Vienna": {
          "total": 2,
          "Employee": 2,
          "Contractor": 0
        },
        "IE.Dublin": {
          "total": 1,
          "Employee": 0,
          "Contractor": 1
        },
        "MA.Casablanca": {
          "total": 1,
          "Employee": 1,
          "Contractor": 0
        },
        "RU.Moscow": {
          "total": 1,
          "Employee": 0,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-25",
      "day": "Monday",
      "region": {
        "name": "EMEA",
        "total": 612,
        "Employee": 569,
        "Contractor": 43
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 407,
          "Employee": 380,
          "Contractor": 27
        },
        "MA.Casablanca": {
          "total": 19,
          "Employee": 18,
          "Contractor": 1
        },
        "IE.Dublin": {
          "total": 33,
          "Employee": 30,
          "Contractor": 3
        },
        "UK.London": {
          "total": 1,
          "Employee": 0,
          "Contractor": 1
        },
        "AUT.Vienna": {
          "total": 43,
          "Employee": 39,
          "Contractor": 4
        },
        "ES.Madrid": {
          "total": 46,
          "Employee": 43,
          "Contractor": 3
        },
        "IT.Rome": {
          "total": 22,
          "Employee": 21,
          "Contractor": 1
        },
        "DU.Abu Dhab": {
          "total": 34,
          "Employee": 33,
          "Contractor": 1
        },
        "RU.Moscow": {
          "total": 7,
          "Employee": 5,
          "Contractor": 2
        }
      }
    },
    {
      "date": "2025-08-26",
      "day": "Tuesday",
      "region": {
        "name": "EMEA",
        "total": 792,
        "Employee": 748,
        "Contractor": 44
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 544,
          "Employee": 517,
          "Contractor": 27
        },
        "MA.Casablanca": {
          "total": 21,
          "Employee": 20,
          "Contractor": 1
        },
        "AUT.Vienna": {
          "total": 46,
          "Employee": 40,
          "Contractor": 6
        },
        "IE.Dublin": {
          "total": 37,
          "Employee": 35,
          "Contractor": 2
        },
        "ES.Madrid": {
          "total": 60,
          "Employee": 57,
          "Contractor": 3
        },
        "UK.London": {
          "total": 16,
          "Employee": 15,
          "Contractor": 1
        },
        "IT.Rome": {
          "total": 27,
          "Employee": 25,
          "Contractor": 2
        },
        "DU.Abu Dhab": {
          "total": 35,
          "Employee": 34,
          "Contractor": 1
        },
        "RU.Moscow": {
          "total": 6,
          "Employee": 5,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-27",
      "day": "Wednesday",
      "region": {
        "name": "EMEA",
        "total": 847,
        "Employee": 800,
        "Contractor": 47
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 567,
          "Employee": 542,
          "Contractor": 25
        },
        "MA.Casablanca": {
          "total": 23,
          "Employee": 22,
          "Contractor": 1
        },
        "AUT.Vienna": {
          "total": 60,
          "Employee": 53,
          "Contractor": 7
        },
        "IE.Dublin": {
          "total": 31,
          "Employee": 28,
          "Contractor": 3
        },
        "UK.London": {
          "total": 29,
          "Employee": 27,
          "Contractor": 2
        },
        "ES.Madrid": {
          "total": 67,
          "Employee": 64,
          "Contractor": 3
        },
        "IT.Rome": {
          "total": 27,
          "Employee": 25,
          "Contractor": 2
        },
        "DU.Abu Dhab": {
          "total": 35,
          "Employee": 33,
          "Contractor": 2
        },
        "RU.Moscow": {
          "total": 8,
          "Employee": 6,
          "Contractor": 2
        }
      }
    },
    {
      "date": "2025-08-28",
      "day": "Thursday",
      "region": {
        "name": "EMEA",
        "total": 652,
        "Employee": 614,
        "Contractor": 38
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 426,
          "Employee": 405,
          "Contractor": 21
        },
        "UK.London": {
          "total": 28,
          "Employee": 26,
          "Contractor": 2
        },
        "MA.Casablanca": {
          "total": 17,
          "Employee": 16,
          "Contractor": 1
        },
        "AUT.Vienna": {
          "total": 50,
          "Employee": 45,
          "Contractor": 5
        },
        "IE.Dublin": {
          "total": 24,
          "Employee": 22,
          "Contractor": 2
        },
        "ES.Madrid": {
          "total": 43,
          "Employee": 40,
          "Contractor": 3
        },
        "DU.Abu Dhab": {
          "total": 35,
          "Employee": 34,
          "Contractor": 1
        },
        "IT.Rome": {
          "total": 19,
          "Employee": 18,
          "Contractor": 1
        },
        "RU.Moscow": {
          "total": 10,
          "Employee": 8,
          "Contractor": 2
        }
      }
    },
    {
      "date": "2025-08-29",
      "day": "Friday",
      "region": {
        "name": "EMEA",
        "total": 381,
        "Employee": 347,
        "Contractor": 34
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 246,
          "Employee": 226,
          "Contractor": 20
        },
        "UK.London": {
          "total": 17,
          "Employee": 15,
          "Contractor": 2
        },
        "MA.Casablanca": {
          "total": 15,
          "Employee": 14,
          "Contractor": 1
        },
        "AUT.Vienna": {
          "total": 30,
          "Employee": 27,
          "Contractor": 3
        },
        "ES.Madrid": {
          "total": 17,
          "Employee": 15,
          "Contractor": 2
        },
        "DU.Abu Dhab": {
          "total": 20,
          "Employee": 19,
          "Contractor": 1
        },
        "IE.Dublin": {
          "total": 15,
          "Employee": 13,
          "Contractor": 2
        },
        "IT.Rome": {
          "total": 11,
          "Employee": 9,
          "Contractor": 2
        },
        "RU.Moscow": {
          "total": 10,
          "Employee": 9,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-30",
      "day": "Saturday",
      "region": {
        "name": "EMEA",
        "total": 30,
        "Employee": 24,
        "Contractor": 6
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 25,
          "Employee": 21,
          "Contractor": 4
        },
        "AUT.Vienna": {
          "total": 1,
          "Employee": 1,
          "Contractor": 0
        },
        "IE.Dublin": {
          "total": 1,
          "Employee": 1,
          "Contractor": 0
        },
        "MA.Casablanca": {
          "total": 2,
          "Employee": 1,
          "Contractor": 1
        },
        "RU.Moscow": {
          "total": 1,
          "Employee": 0,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-31",
      "day": "Sunday",
      "region": {
        "name": "EMEA",
        "total": 32,
        "Employee": 25,
        "Contractor": 7
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 28,
          "Employee": 23,
          "Contractor": 5
        },
        "IE.Dublin": {
          "total": 1,
          "Employee": 0,
          "Contractor": 1
        },
        "MA.Casablanca": {
          "total": 1,
          "Employee": 1,
          "Contractor": 0
        },
        "RU.Moscow": {
          "total": 1,
          "Employee": 1,
          "Contractor": 0
        },
        "UK.London": {
          "total": 1,
          "Employee": 0,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-09-01",
      "day": "Monday",
      "region": {
        "name": "EMEA",
        "total": 585,
        "Employee": 545,
        "Contractor": 40
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 354,
          "Employee": 330,
          "Contractor": 24
        },
        "UK.London": {
          "total": 22,
          "Employee": 21,
          "Contractor": 1
        },
        "MA.Casablanca": {
          "total": 21,
          "Employee": 20,
          "Contractor": 1
        },
        "AUT.Vienna": {
          "total": 44,
          "Employee": 39,
          "Contractor": 5
        },
        "IE.Dublin": {
          "total": 29,
          "Employee": 26,
          "Contractor": 3
        },
        "ES.Madrid": {
          "total": 42,
          "Employee": 40,
          "Contractor": 2
        },
        "IT.Rome": {
          "total": 27,
          "Employee": 25,
          "Contractor": 2
        },
        "DU.Abu Dhab": {
          "total": 37,
          "Employee": 36,
          "Contractor": 1
        },
        "RU.Moscow": {
          "total": 9,
          "Employee": 8,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-09-02",
      "day": "Tuesday",
      "region": {
        "name": "EMEA",
        "total": 867,
        "Employee": 825,
        "Contractor": 42
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 581,
          "Employee": 555,
          "Contractor": 26
        },
        "MA.Casablanca": {
          "total": 28,
          "Employee": 27,
          "Contractor": 1
        },
        "IE.Dublin": {
          "total": 36,
          "Employee": 34,
          "Contractor": 2
        },
        "AUT.Vienna": {
          "total": 56,
          "Employee": 52,
          "Contractor": 4
        },
        "IT.Rome": {
          "total": 27,
          "Employee": 25,
          "Contractor": 2
        },
        "UK.London": {
          "total": 29,
          "Employee": 27,
          "Contractor": 2
        },
        "ES.Madrid": {
          "total": 61,
          "Employee": 59,
          "Contractor": 2
        },
        "DU.Abu Dhab": {
          "total": 39,
          "Employee": 37,
          "Contractor": 2
        },
        "RU.Moscow": {
          "total": 10,
          "Employee": 9,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-09-03",
      "day": "Wednesday",
      "region": {
        "name": "EMEA",
        "total": 880,
        "Employee": 837,
        "Contractor": 43
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 596,
          "Employee": 569,
          "Contractor": 27
        },
        "UK.London": {
          "total": 30,
          "Employee": 29,
          "Contractor": 1
        },
        "MA.Casablanca": {
          "total": 30,
          "Employee": 29,
          "Contractor": 1
        },
        "IE.Dublin": {
          "total": 29,
          "Employee": 27,
          "Contractor": 2
        },
        "AUT.Vienna": {
          "total": 59,
          "Employee": 53,
          "Contractor": 6
        },
        "ES.Madrid": {
          "total": 58,
          "Employee": 56,
          "Contractor": 2
        },
        "IT.Rome": {
          "total": 31,
          "Employee": 29,
          "Contractor": 2
        },
        "DU.Abu Dhab": {
          "total": 37,
          "Employee": 36,
          "Contractor": 1
        },
        "RU.Moscow": {
          "total": 10,
          "Employee": 9,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-09-04",
      "day": "Thursday",
      "region": {
        "name": "EMEA",
        "total": 451,
        "Employee": 428,
        "Contractor": 23
      },
      "partitions": {
        "LT.Vilnius": {
          "total": 376,
          "Employee": 361,
          "Contractor": 15
        },
        "DU.Abu Dhab": {
          "total": 24,
          "Employee": 23,
          "Contractor": 1
        },
        "UK.London": {
          "total": 4,
          "Employee": 2,
          "Contractor": 2
        },
        "MA.Casablanca": {
          "total": 2,
          "Employee": 1,
          "Contractor": 1
        },
        "AUT.Vienna": {
          "total": 19,
          "Employee": 17,
          "Contractor": 2
        },
        "IE.Dublin": {
          "total": 7,
          "Employee": 6,
          "Contractor": 1
        },
        "ES.Madrid": {
          "total": 6,
          "Employee": 5,
          "Contractor": 1
        },
        "IT.Rome": {
          "total": 12,
          "Employee": 12,
          "Contractor": 0
        },
        "RU.Moscow": {
          "total": 1,
          "Employee": 1,
          "Contractor": 0
        }
      }
    }
  ],
  "details": [
    {
      "LocaleMessageTime": "2025-08-20T00:01:52.000Z",
      "ObjectName1": "Kolesovaite, Karina",
      "Door": "EMEA_LT_VNO_GAMA_1st Flr_Turnstile3 Exit",
      "EmployeeID": "318415",
      "Text5": "Vilnius - Gama Business Center",
      "PartitionName2": "LT.Vilnius",
      "PersonGUID": "7F5ABE07-70D4-4FB9-ADFF-A8DDC5624000",
      "PersonnelType": "Terminated Personnel",
      "CompanyName": "WU Processing Lithuania, UAB",
      "PrimaryLocation": "Vilnius - Gama Business Center",
      "CardNumber": "229791",
      "AdmitCode": "Admit",
      "Direction": "OutDirection",
      "SwipeDate": "2025-08-20T00:00:00.000Z"
    },
    {
      "LocaleMessageTime": "2025-08-20T00:03:36.000Z",
      "ObjectName1": "Olowe, Omofolawe",
      "Door": "EMEA_LT_VNO_GAMA_1st Flr_Turnstile1 Exit",
      "EmployeeID": "327432",
      "Text5": "Vilnius - Technopolis",
      "PartitionName2": "LT.Vilnius",
      "PersonGUID": "91926116-6289-41C1-99E5-D26CF65140BF",
      "PersonnelType": "Employee",
      "CompanyName": "WU Processing Lithuania, UAB",
      "PrimaryLocation": "Vilnius - Technopolis",
      "CardNumber": "615364",
      "AdmitCode": "Admit",
      "Direction": "OutDirection",
      "SwipeDate": "2025-08-20T00:00:00.000Z"
    },
    {
      "LocaleMessageTime": "2025-08-20T00:04:34.000Z",
      "ObjectName1": "Ntoh Yuh, Juliette",
      "Door": "EMEA_LT_VNO_GAMA_1st Flr_Turnstile1 Exit",
      "EmployeeID": "322295",
      "Text5": "Vilnius - Gama Business Center",
      "PartitionName2": "LT.Vilnius",
      "PersonGUID": "2E9EB940-E128-4AC5-8DAF-F2811A69EED5",
      "PersonnelType": "Employee",
      "CompanyName": "WU Processing Lithuania, UAB",
      "PrimaryLocation": "Vilnius - Gama Business Center",
      "CardNumber": "607294",
      "AdmitCode": "Admit",
      "Direction": "OutDirection",
      "SwipeDate": "2025-08-20T00:00:00.000Z"
    },
    {
      "LocaleMessageTime": "2025-08-20T00:34:01.000Z",
      "ObjectName1": "Pelanis, Vilgirdas",
      "Door": "EMEA_LT_VNO_GAMA_1st Flr_Turnstile1 Exit",
      "EmployeeID": "323145",
      "Text5": "Vilnius - Gama Business Center",
      "PartitionName2": "LT.Vilnius",
      "PersonGUID": "E658E8DA-8A35-4DC3-8026-595641F1A368",
      "PersonnelType": "Employee",
      "CompanyName": "WU Processing Lithuania, UAB",
      "PrimaryLocation": "Vilnius - Gama Business Center",
      "CardNumber": "607682",
      "AdmitCode": "Admit",
      "Direction": "OutDirection",
      "SwipeDate": "2025-08-20T00:00:00.000Z"
    },
    {
      "LocaleMessageTime": "2025-08-20T00:34:30.000Z",
      "ObjectName1": "Elcin, Bunyamin",
      "Door": "EMEA_LTU_VNO_GAMA_1st Floor_Community Space",
      "EmployeeID": "325182",
      "Text5": "Vilnius - Gama Business Center",
      "PartitionName2": "LT.Vilnius",
      "PersonGUID": "B70B1E69-EA2F-4F6E-8124-5273C3686F17",
      "PersonnelType": "Employee",
      "CompanyName": "WU Processing Lithuania, UAB",
      "PrimaryLocation": "Vilnius - Gama Business Center",
      "CardNumber": "619139",
      "AdmitCode": "Admit",
      "Direction": "OutDirection",
      "SwipeDate": "2025-08-20T00:00:00.000Z"
    },
    {
      "LocaleMessageTime": "2025-08-20T00:41:12.000Z",
      "ObjectName1": "Jhorar, Kapil",
      "Door": "EMEA_LTU_VNO_GAMA_1st Floor_Community Space",
      "EmployeeID": "326022",
      "Text5": "Vilnius - Gama Business Center",
      "PartitionName2": "LT.Vilnius",
      "PersonGUID": "121818E7-1C6F-4133-952F-AEA4B69C6C49",
      "PersonnelType": "Employee",
      "CompanyName": "WU Processing Lithuania, UAB",
      "PrimaryLocation": "Vilnius - Gama Business Center",
      "CardNumber": "614631",
      "AdmitCode": "Admit",
      "Direction": "OutDirection",
      "SwipeDate": "2025-08-20T00:00:00.000Z"
    },
    {
      "LocaleMessageTime": "2025-08-20T01:03:07.000Z",
      "ObjectName1": "Moradi, Moei",
      "Door": "EMEA_LT_VNO_GAMA_1st Flr_Turnstile2 Exit",
      "EmployeeID": "326933",
      "Text5": "Vilnius - Technopolis",
      "PartitionName2": "LT.Vilnius",
      "PersonGUID": "27135A48-87C2-4D72-B755-6EE734AB4C4C",
      "PersonnelType": "Employee",
      "CompanyName": "WU Processing Lithuania, UAB",
      "PrimaryLocation": "Vilnius - Technopolis",
      "CardNumber": "615016",
      "AdmitCode": "Admit",
      "Direction": "OutDirection",
      "SwipeDate": "2025-08-20T00:00:00.000Z"
    },
    {
      "LocaleMessageTime": "2025-08-20T02:15:32.000Z",
      "ObjectName1": "Tamasiunas, Mindaugas",
      "Door": "EMEA_LT_VNO_GAMA_2nd Flr_Parking",
      "EmployeeID": "316197",
      "Text5": "Vilnius - Gama Business Center",
      "PartitionName2": "LT.Vilnius",
      "PersonGUID": "5AF4B170-98DC-46CF-AD30-F0A42B7C458A",
      "PersonnelType": "Employee",
      "CompanyName": "WU Processing Lithuania, UAB",
      "PrimaryLocation": "Vilnius - Gama Business Center",
      "CardNumber": "415771",
      "AdmitCode": "Admit",
      "Direction": "OutDirection",
      "SwipeDate": "2025-08-20T00:00:00.000Z"
    },
    {
      "LocaleMessageTime": "2025-08-20T03:01:53.000Z",
      "ObjectName1": "Tomasevic, Kazimez",
      "Door": "EMEA_LTU_VNO_GAMA_1st Floor_Community Space",
      "EmployeeID": "86140423",
      "Text5": "Vilnius - Technopolis",
      "PartitionName2": "LT.Vilnius",
      "PersonGUID": "7AF6EB75-1162-4A6B-B12C-C842DB112C90",
      "PersonnelType": "Contractor",
      "CompanyName": "G4S Lietuva, UAB (HCT)",
      "PrimaryLocation": "Vilnius - Technopolis",
      "CardNumber": "619063",
      "AdmitCode": "Admit",
      "Direction": "InDirection",
      "SwipeDate": "2025-08-20T00:00:00.000Z"
    },







Now Check Belopw Each file line by line and Fix the issue carefully...




# app.py
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import shutil
import uuid
import json
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
import re
import asyncio
from typing import Optional, Dict, Any
import sys

# --- DB / models imports (kept for endpoints that still use DB like headcount/attendance fallback) ---
from db import SessionLocal
from models import LiveSwipe, AttendanceSummary

# --- settings (optional override) ---
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"

RAW_UPLOADS_DIR = DATA_DIR / "raw_uploads"
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# ----------------- GLOBAL TIMEOUTS (UNIFY) -----------------
REGION_TIMEOUT_SECONDS = 20
COMPUTE_WAIT_TIMEOUT_SECONDS = 30
COMPUTE_SYNC_TIMEOUT_SECONDS = 60
# ----------------------------------------------------------

_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

_broadcaster_clients = set()

def broadcast_ccure_update(payload: dict):
    if not _broadcaster_clients:
        return
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = None
    for q in list(_broadcaster_clients):
        try:
            if loop and loop.is_running():
                loop.call_soon_threadsafe(q.put_nowait, payload)
            else:
                q.put_nowait(payload)
        except Exception:
            logger.exception("Failed to push payload to SSE client (will remove client)")
            try:
                _broadcaster_clients.discard(q)
            except Exception:
                pass

async def _sse_event_generator(client_queue: asyncio.Queue):
    try:
        while True:
            payload = await client_queue.get()
            try:
                data = json.dumps(payload, default=str)
            except Exception:
                data = json.dumps({"error": "serialization error", "payload": str(payload)})
            yield f"data: {data}\n\n"
    finally:
        try:
            _broadcaster_clients.discard(client_queue)
        except Exception:
            pass
        return

@app.get("/ccure/stream")
async def ccure_stream():
    q = asyncio.Queue()
    _broadcaster_clients.add(q)
    generator = _sse_event_generator(q)
    headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    return StreamingResponse(generator, media_type="text/event-stream", headers=headers)

def _guess_region_from_text(txt: str) -> str:
    if not txt:
        return "unknown"
    s = str(txt).strip().lower()
    s = re.sub(r"[,\-/()]", " ", s)
    if any(k in s for k in ("pune","quezon city","taguig city","bengaluru","hyderabad","chennai","manila","singapore","hong kong","beijing","shanghai","jakarta","kuala","osaka","tokyo","seoul","bangkok")):
        return "apac"
    if any(k in s for k in ("london","dublin","paris","frankfurt","amsterdam","stockholm","cape town","johannesburg","berlin","brussels","madrid","rome","milan")):
        return "emea"
    if any(k in s for k in ("mexico","bogota","buenos","santiago","sao","salvador","lima","caracas")):
        return "laca"
    if any(k in s for k in ("denver","new york","ny","chicago","toronto","vancouver","los angeles","san francisco","boston","houston","atlanta","miami")):
        return "namer"
    return "unknown"

@app.get("/headcount")
def api_headcount():
    try:
        totals = {"apac": 0, "emea": 0, "laca": 0, "namer": 0, "unknown": 0}
        with SessionLocal() as db:
            try:
                today = date.today()
                rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                if rows:
                    for r in rows:
                        try:
                            partition = None
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                            loc = partition or "unknown"
                            region = _guess_region_from_text(loc)
                            totals[region] = totals.get(region, 0) + 1
                        except Exception:
                            totals["unknown"] += 1
                else:
                    start = datetime.combine(today, datetime.min.time())
                    end = datetime.combine(today, datetime.max.time())
                    swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
                    for s in swipes:
                        loc = s.partition or "unknown"
                        region = _guess_region_from_text(loc)
                        totals[region] = totals.get(region, 0) + 1
            except Exception:
                logger.exception("Failed to compute headcount regions")
        out = {
            "apac": int(totals.get("apac", 0)),
            "emea": int(totals.get("emea", 0)),
            "laca": int(totals.get("laca", 0)),
            "namer": int(totals.get("namer", 0))
        }
        return JSONResponse(out)
    except Exception as exc:
        logger.exception("api_headcount failed")
        raise HTTPException(status_code=500, detail=f"headcount error: {exc}")

# ---------- Helpers retained (normalize / safe conversions) -------------
def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- build_ccure_averages (fallback) ------------------------------
def build_ccure_averages(start_date: Optional[str] = None, end_date: Optional[str] = None):
    """
    Fallback averages computation using AttendanceSummary only.
    Returns a compact shape (live_today, ccure_active, averages, notes).
    """
    try:
        def _parse_date_param(s):
            if not s:
                return None
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                try:
                    return date.fromisoformat(s)
                except Exception:
                    return None

        today = date.today()
        start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
        end_obj = _parse_date_param(end_date) if end_date else today
        if start_obj is None or end_obj is None or start_obj > end_obj:
            start_obj = today - timedelta(days=6)
            end_obj = today

        with SessionLocal() as db:
            try:
                att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
            except Exception:
                logger.exception("Failed to query AttendanceSummary")
                att_rows = []

            live_emp = 0
            live_contr = 0
            unknown_count = 0
            seen_keys = set()

            def classify_from_derived(derived):
                try:
                    if not derived or not isinstance(derived, dict):
                        return "contractor"
                    for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                        v = derived.get(k)
                        if v and "employee" in str(v).strip().lower():
                            return "employee"
                    for k in ("Employee_Status","Employee Status","Status"):
                        v = derived.get(k)
                        if v and "terminated" in str(v).strip().lower():
                            return "employee"
                    return "contractor"
                except Exception:
                    return "contractor"

            if att_rows:
                for a in att_rows:
                    key = None
                    try:
                        key = _normalize_employee_key(a.employee_id)
                    except Exception:
                        key = None
                    if not key:
                        try:
                            key = _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None)
                        except Exception:
                            key = None
                    if not key:
                        unknown_count += 1
                        continue
                    if key in seen_keys:
                        continue
                    seen_keys.add(key)
                    cls = classify_from_derived(a.derived)
                    if cls == "employee":
                        live_emp += 1
                    elif cls == "contractor":
                        live_contr += 1
                    else:
                        unknown_count += 1

                live_total_reported = live_emp + live_contr + unknown_count
                live_total_details = len(att_rows)
            else:
                live_total_reported = 0
                live_total_details = 0
                try:
                    import region_clients
                    details = []
                    try:
                        details = region_clients.fetch_all_details(timeout=REGION_TIMEOUT_SECONDS) or []
                    except Exception:
                        logger.exception("region_clients.fetch_all_details failed in build_ccure_averages()")
                        details = []
                    if details:
                        for d in details:
                            try:
                                cls = "contractor"
                                for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                                    v = d.get(k)
                                    if v and "employee" in str(v).strip().lower():
                                        cls = "employee"
                                        break
                                if cls == "employee":
                                    live_emp += 1
                                else:
                                    live_contr += 1
                                live_total_details += 1
                            except Exception:
                                continue
                        live_total_reported = live_emp + live_contr
                    else:
                        try:
                            regions = region_clients.fetch_all_regions(timeout=REGION_TIMEOUT_SECONDS) or []
                            for r in regions:
                                try:
                                    c = r.get("count")
                                    if isinstance(c, (int, float)):
                                        live_total_reported += int(c)
                                except Exception:
                                    continue
                        except Exception:
                            logger.exception("region_clients.fetch_all_regions failed in build_ccure_averages()")
                except Exception:
                    logger.exception("region_clients not importable in build_ccure_averages()")

            # compute avg range using AttendanceSummary if possible
            avg_range = None
            try:
                q = db.query(AttendanceSummary.date, AttendanceSummary.employee_id, AttendanceSummary.presence_count)\
                      .filter(AttendanceSummary.date >= start_obj, AttendanceSummary.date <= end_obj).all()
                by_date = {}
                for row in q:
                    d = row[0]
                    key = (row[1] or "").strip() if row[1] else None
                    if not key:
                        continue
                    if d not in by_date:
                        by_date[d] = set()
                    try:
                        presence_val = getattr(row, 'presence_count', row[2])
                        if int(presence_val) > 0:
                            by_date[d].add(key)
                    except Exception:
                        by_date[d].add(key)
                days_count = (end_obj - start_obj).days + 1
                daily_counts = [len(by_date.get(start_obj + timedelta(days=i), set())) for i in range(days_count)]
                if days_count and any(daily_counts):
                    avg_range = int(round(sum(daily_counts) / float(days_count)))
                else:
                    avg_range = None
            except Exception:
                logger.exception("Failed computing range average from AttendanceSummary")
                avg_range = None

        # fallback: use region history to compute avg_range if still None
        if avg_range is None:
            try:
                import region_clients
                entries = region_clients.fetch_all_history(timeout=REGION_TIMEOUT_SECONDS) or []
                agg = {}
                for e in entries:
                    try:
                        dstr = e.get("date")
                        if not dstr:
                            continue
                        region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                        emp = None
                        con = None
                        tot = None
                        if region_obj:
                            emp = _safe_int(region_obj.get("Employee"))
                            con = _safe_int(region_obj.get("Contractor"))
                            tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                        else:
                            emp = _safe_int(e.get("Employee"))
                            con = _safe_int(e.get("Contractor"))
                            tot = _safe_int(e.get("total"))
                        if emp is None and con is None:
                            continue
                        if tot is None:
                            tot = (emp or 0) + (con or 0)
                        if dstr not in agg:
                            agg[dstr] = {"total": 0, "count": 0}
                        agg[dstr]["total"] += tot or 0
                        agg[dstr]["count"] += 1
                    except Exception:
                        continue
                per_date_totals = []
                days_count = (end_obj - start_obj).days + 1
                for i in range(days_count):
                    dcheck = (start_obj + timedelta(days=i)).isoformat()
                    if dcheck in agg and agg[dcheck]["count"] > 0:
                        per_day_avg = float(agg[dcheck]["total"]) / float(agg[dcheck]["count"])
                        per_date_totals.append(per_day_avg)
                if per_date_totals:
                    avg_range = int(round(sum(per_date_totals) / float(len(per_date_totals))))
            except Exception:
                logger.exception("Failed computing avg_range from region history in fallback")

        # get ccure stats if available
        ccure_stats = {}
        try:
            import ccure_client
            if hasattr(ccure_client, "get_global_stats"):
                ccure_stats = ccure_client.get_global_stats() or {}
        except Exception:
            logger.debug("ccure_client.get_global_stats not available", exc_info=True)

        cc_active_emps = None
        cc_active_contractors = None
        try:
            if isinstance(ccure_stats, dict):
                a = ccure_stats.get("ActiveEmployees") or ccure_stats.get("active_employees") or None
                b = ccure_stats.get("ActiveContractors") or ccure_stats.get("active_contractors") or None
                if a is not None and str(a).strip() != "":
                    cc_active_emps = int(a)
                if b is not None and str(b).strip() != "":
                    cc_active_contractors = int(b)
        except Exception:
            cc_active_emps = cc_active_contractors = None

        emp_pct = None
        contr_pct = None
        overall_pct = None
        try:
            if isinstance(cc_active_emps, int) and cc_active_emps > 0:
                emp_pct = round((live_emp / float(cc_active_emps)) * 100.0, 2)
            if isinstance(cc_active_contractors, int) and cc_active_contractors > 0:
                contr_pct = round((live_contr / float(cc_active_contractors)) * 100.0, 2)
            if isinstance(cc_active_emps, int) and isinstance(cc_active_contractors, int) and (cc_active_emps + cc_active_contractors) > 0:
                overall_pct = round(((live_emp + live_contr) / float(cc_active_emps + cc_active_contractors)) * 100.0, 2)
        except Exception:
            emp_pct = contr_pct = overall_pct = None

        resp = {
            "date": today.isoformat(),
            "notes": f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}" if (start_date or end_date) else None,
            "live_today": {
                "employee": live_emp,
                "contractor": live_contr,
                "total_reported": live_total_reported,
                "total_from_details": live_total_details
            },
            "ccure_active": {
                "active_employees": cc_active_emps,
                "active_contractors": cc_active_contractors,
                "ccure_active_employees_reported": cc_active_emps,
                "ccure_active_contractors_reported": cc_active_contractors
            },
            "averages": {
                "employee_pct": emp_pct,
                "contractor_pct": contr_pct,
                "overall_pct": overall_pct,
                "avg_headcount_last_7_days": avg_range,
                "head_emp_pct_vs_ccure_today": emp_pct,
                "head_contractor_pct_vs_ccure_today": contr_pct,
                "headcount_overall_pct_vs_ccure_today": overall_pct,
                "history_avg_overall_last_7_days": avg_range
            }
        }

        return resp
    except Exception:
        logger.exception("build_ccure_averages failed")
        raise

# -------------------------
# Upload endpoints (store uploads under data/ and data/raw_uploads/ and rotate)
# -------------------------
ALLOWED_EXT = (".xls", ".xlsx", ".csv")

def _remove_old_files_for_kind(kind: str):
    """
    Remove previous canonical files and previous raw uploads that include kind in filename.
    """
    try:
        # canonical in DATA_DIR
        for p in DATA_DIR.iterdir():
            if p.is_file() and f"active_{kind}" in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old canonical file %s", p)
        # remove raw uploads for same kind to keep only latest raw (user requested)
        for p in RAW_UPLOADS_DIR.iterdir():
            if p.is_file() and kind in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old raw file %s", p)
    except Exception:
        logger.exception("error while removing old files for kind=%s", kind)

def _save_upload_and_rotate(upload_file: UploadFile, kind: str) -> dict:
    """
    Save the uploaded file as:
      - data/active_<kind>.<ext>     (canonical)
      - data/raw_uploads/<timestamp>_<kind>_<origname>  (raw trace)
    Remove previous files for same kind (both canonical & raw).
    Returns metadata dict.
    """
    fname = Path(upload_file.filename).name
    ext = Path(fname).suffix.lower()
    if ext not in ALLOWED_EXT:
        raise HTTPException(status_code=400, detail="Only .xls .xlsx .csv allowed")

    # rotate old files
    _remove_old_files_for_kind(kind)

    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    raw_name = f"{ts}_{kind}_{fname}"
    raw_path = RAW_UPLOADS_DIR / raw_name

    # Save raw
    try:
        with raw_path.open("wb") as out_f:
            shutil.copyfileobj(upload_file.file, out_f)
    finally:
        try:
            upload_file.file.close()
        except Exception:
            pass

    # Write canonical: keep same extension as original (simpler)
    canonical_path = DATA_DIR / f"active_{kind}{ext}"
    try:
        shutil.copy(raw_path, canonical_path)
    except Exception:
        logger.exception("Failed to write canonical copy for %s", canonical_path)

    info = {
        "raw_saved": str(raw_path),
        "canonical_saved": str(canonical_path),
        "original_filename": fname,
        "size_bytes": raw_path.stat().st_size if raw_path.exists() else None
    }
    logger.info("Uploaded %s: %s", kind, info)
    return info

# (Upload endpoints continue unchanged - already present above)
@app.post("/api/upload/active-employees")
async def upload_active_employees_api(file: UploadFile = File(...)):
    return await upload_active_employees(file)

@app.post("/api/upload/active-contractors")
async def upload_active_contractors_api(file: UploadFile = File(...)):
    return await upload_active_contractors(file)

@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    """
    Upload Active Employee sheet:
      - stores raw to data/raw_uploads and canonical to data/active_employee.*
      - removes previous uploaded employee sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="employee")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("employee upload failed")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    """
    Upload Active Contractor sheet:
      - stores raw to data/raw_uploads and canonical to data/active_contractor.*
      - removes previous uploaded contractor sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="contractor")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("contractor upload failed")
        raise HTTPException(status_code=500, detail=str(e))

# remaining utility endpoints and compute wrappers (unchanged)
def _map_detailed_to_resp(detailed: Dict[str, Any]) -> Dict[str, Any]:
    live_h = detailed.get("live_headcount", {}) or {}
    head_h = detailed.get("headcount", {}) or {}
    ccure_active_obj = detailed.get("ccure_active", {}) or {}
    averages_obj = detailed.get("averages", {}) or {}

    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    live_employee = int(live_h.get("employee") or head_h.get("employee") or 0)
    live_contractor = int(live_h.get("contractor") or head_h.get("contractor") or 0)
    total_reported = int(
        live_h.get("currently_present_total")
        or head_h.get("total_visited_today")
        or (live_employee + live_contractor)
        or 0
    )
    total_from_details = int(head_h.get("total_visited_today") or 0)

    mapped_headcount = {
        "total_visited_today": int(head_h.get("total_visited_today") or 0),
        "employee": int(head_h.get("employee") or 0),
        "contractor": int(head_h.get("contractor") or 0),
        "by_location": head_h.get("by_location") or {}
    }

    resp = {
        "date": detailed.get("date"),
        "notes": detailed.get("notes"),
        "live_today": {
            "employee": live_employee,
            "contractor": live_contractor,
            "total_reported": total_reported,
            "total_from_details": total_from_details
        },
        "headcount": mapped_headcount,
        "live_headcount": live_h,
        "ccure_active": {
            "active_employees": ccure_active_obj.get("ccure_active_employees_reported")
                             or ccure_active_obj.get("active_employees"),
            "active_contractors": ccure_active_obj.get("ccure_active_contractors_reported")
                               or ccure_active_obj.get("active_contractors"),
            "ccure_active_employees_reported": ccure_active_obj.get("ccure_active_employees_reported"),
            "ccure_active_contractors_reported": ccure_active_obj.get("ccure_active_contractors_reported")
        },
        "averages": averages_obj
    }
    return resp

def _build_verify_like_summary_from_mapped(mapped: Dict[str, Any], include_raw: bool = False) -> Dict[str, Any]:
    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    cc = mapped.get("ccure_active", {}) or {}
    head = mapped.get("headcount", {}) or {}
    live_head = mapped.get("live_headcount", {}) or {}
    averages = mapped.get("averages", {}) or {}

    cc_emp = to_int(cc.get("ccure_active_employees_reported") or cc.get("active_employees"))
    cc_con = to_int(cc.get("ccure_active_contractors_reported") or cc.get("active_contractors"))

    head_total = to_int(head.get("total_visited_today") or mapped.get("live_today", {}).get("total_from_details"))
    head_emp = to_int(head.get("employee") or mapped.get("live_today", {}).get("employee"))
    head_con = to_int(head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    live_total = to_int(live_head.get("currently_present_total") or mapped.get("live_today", {}).get("total_reported"))
    live_emp = to_int(live_head.get("employee") or mapped.get("live_today", {}).get("employee"))
    live_con = to_int(live_head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    history_emp_avg = averages.get("history_avg_employee_last_7_days")
    history_con_avg = averages.get("history_avg_contractor_last_7_days")
    history_overall_avg = averages.get("history_avg_overall_last_7_days")

    def pct(n, d):
        try:
            if n is None or d is None:
                return None
            if float(d) == 0:
                return None
            return round((float(n) / float(d)) * 100.0, 2)
        except Exception:
            return None

    summary = {
        "date": mapped.get("date"),
        "ccure_reported": {
            "employees": cc_emp,
            "contractors": cc_con,
            "total_reported": (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None
        },
        "headcount_attendance_summary": {
            "total_visited_today": head_total,
            "employee": head_emp,
            "contractor": head_con,
        },
        "live_headcount_region_clients": {
            "currently_present_total": live_total,
            "employee": live_emp,
            "contractor": live_con,
        },
        "percentages_vs_ccure": {
            "head_employee_pct_vs_ccure_today": pct(head_emp, cc_emp),
            "head_contractor_pct_vs_ccure_today": pct(head_con, cc_con),
            "head_overall_pct_vs_ccure_today": pct(head_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "live_employee_pct_vs_ccure_today": pct(live_emp, cc_emp),
            "live_contractor_pct_vs_ccure_today": pct(live_con, cc_con),
            "live_overall_pct_vs_ccure_today": pct(live_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "history_employee_pct_vs_ccure": pct(history_emp_avg, cc_emp),
            "history_contractor_pct_vs_ccure": pct(history_con_avg, cc_con),
            "history_overall_pct_vs_ccure": pct(history_overall_avg, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None)
        },
        "averages": {
            "history_avg_employee_last_7_days": history_emp_avg,
            "history_avg_contractor_last_7_days": history_con_avg,
            "history_avg_overall_last_7_days": history_overall_avg,
            "avg_headcount_last_7_days_db": averages.get("avg_headcount_last_7_days") or averages.get("avg_headcount_last_7_days_db"),
            "avg_headcount_per_site_last_7_days": averages.get("avg_headcount_per_site_last_7_days"),
            "employee_pct": averages.get("employee_pct"),
            "contractor_pct": averages.get("contractor_pct"),
            "overall_pct": averages.get("overall_pct"),
            **({k: v for k, v in averages.items() if k not in (
                "history_avg_employee_last_7_days",
                "history_avg_contractor_last_7_days",
                "history_avg_overall_last_7_days",
                "avg_headcount_last_7_days",
                "avg_headcount_last_7_days_db",
                "avg_headcount_per_site_last_7_days",
                "employee_pct","contractor_pct","overall_pct"
            )})
        },
        "notes": mapped.get("notes")
    }

    summary["headcount_details"] = {
        "total_visited_today": head_total,
        "employee": head_emp,
        "contractor": head_con,
        "by_location": head.get("by_location") if isinstance(head.get("by_location"), dict) else {}
    }
    summary["live_headcount_details"] = {
        "currently_present_total": live_total,
        "employee": live_emp,
        "contractor": live_con,
        "by_location": live_head.get("by_location") if isinstance(live_head.get("by_location"), dict) else {}
    }

    summary["ccure_active"] = {
        "active_employees": cc.get("active_employees") or cc.get("ccure_active_employees_reported"),
        "active_contractors": cc.get("active_contractors") or cc.get("ccure_active_contractors_reported"),
        "ccure_active_employees_reported": cc.get("ccure_active_employees_reported"),
        "ccure_active_contractors_reported": cc.get("ccure_active_contractors_reported")
    }

    return summary

# ---------- /ccure/verify (already present in your original) -----
@app.get("/ccure/verify")
def ccure_verify(
    raw: bool = Query(False, description="if true, include the raw compute payload for debugging"),
    start_date: Optional[str] = Query(None, description="YYYY-MM-DD start date (inclusive)"),
    end_date: Optional[str] = Query(None, description="YYYY-MM-DD end date (inclusive)")
):
    """
    Synchronous verification endpoint. Prefer compute_visit_averages() (synchronous call).
    If compute raises or fails, fall back to build_ccure_averages() so output shape remains consistent.
    """
    try:
        detailed = None
        try:
            from ccure_compare_service import compute_visit_averages
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
        except Exception:
            logger.exception("compute_visit_averages() failed inside /ccure/verify; falling back")
            detailed = None

        if isinstance(detailed, dict):
            mapped = _map_detailed_to_resp(detailed)
            summary = _build_verify_like_summary_from_mapped(mapped, include_raw=raw)
            if raw and isinstance(detailed, dict):
                summary["raw"] = detailed
            return JSONResponse(summary)
        else:
            fallback = build_ccure_averages(start_date, end_date)
            mapped_fallback = {
                "date": fallback.get("date"),
                "notes": fallback.get("notes"),
                "live_today": fallback.get("live_today", {}),
                "headcount": {
                    "total_visited_today": fallback.get("live_today", {}).get("total_from_details") or fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "live_headcount": {
                    "currently_present_total": fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "ccure_active": fallback.get("ccure_active", {}),
                "averages": fallback.get("averages", {})
            }

            summary = _build_verify_like_summary_from_mapped(mapped_fallback, include_raw=raw)
            if raw:
                summary["raw"] = mapped_fallback
       
            return JSONResponse(summary)
    except Exception as e:
        logger.exception("ccure_verify failed")
        raise HTTPException(status_code=500, detail=f"ccure verify error: {e}")

# ---------- /ccure/compare_v2 (calls data_compare_service_v2) ----------
@app.get("/ccure/compare_v2")
def ccure_compare_v2(
    mode: str = Query("full"),
    stats_detail: str = Query("ActiveProfiles"),
    limit_list: int = Query(200, ge=1, le=5000),
    export: bool = Query(False),
    region_filter: Optional[str] = Query(None, description="Region code e.g. APAC"),
    location_city: Optional[str] = Query(None, description="Location city e.g. Pune"),
    location_state: Optional[str] = Query(None, description="Location state/province"),
    location_description: Optional[str] = Query(None, description="Location Description"),
    week_ref_date: Optional[str] = Query(None, description="YYYY-MM-DD for week (Mon-Fri)")
):
    try:
        from data_compare_service_v2 import compare_ccure_vs_sheets, prefetch_region_history
    except Exception as e:
        logger.exception("data_compare_service_v2 import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    # 1) prefetch region history (cache) - do this before loading sheet to avoid repeated network calls
    try:
        # use REGION_TIMEOUT_SECONDS from this module to control how long prefetch should wait
        prefetch_region_history(timeout=REGION_TIMEOUT_SECONDS)
    except Exception:
        logger.exception("prefetch_region_history failed (continuing)")

    res = compare_ccure_vs_sheets(
        mode=mode,
        stats_detail=stats_detail,
        limit_list=limit_list,
        export=export,
        region_filter=region_filter,
        location_city=location_city,
        location_state=location_state,
        location_description=location_description,
        week_ref_date=week_ref_date
    )
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)

@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = OUTPUT_DIR / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(str(full),
                            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            filename=safe_name)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")

# End of app.py








2) # ccure_client.py
"""
Lightweight CCURE client wrappers used by compare service.
This file is defensive: missing 'requests' or network failures return None instead of raising.
"""

import math
import logging
from requests.exceptions import RequestException

logger = logging.getLogger("ccure_client")
logger.setLevel(logging.INFO)
if not logger.handlers:
    import sys
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Base URL for CCURE API - adjust if necessary
BASE = "http://10.199.22.57:5001"
DEFAULT_TIMEOUT = 10

HEADERS = {
    "Accept": "application/json"
}

# Defensive import of requests
try:
    import requests
except Exception:
    requests = None
    logger.warning("requests module not available; ccure_client will return None for HTTP calls")

def _safe_get(path, params=None, timeout=DEFAULT_TIMEOUT):
    """
    Safe GET wrapper. Returns parsed JSON on success or None on failure.
    path may include leading slash or not; we join safely.
    """
    if requests is None:
        logger.debug("_safe_get: requests not available")
        return None
    # ensure path begins with '/'
    if not path.startswith("/"):
        path = "/" + path
    url = BASE.rstrip("/") + path
    try:
        r = requests.get(url, params=params, headers=HEADERS, timeout=timeout)
        r.raise_for_status()
        return r.json()
    except RequestException as e:
        logger.warning(f"[ccure_client] request failed {url} params={params} -> {e}")
        return None
    except ValueError:
        logger.warning(f"[ccure_client] response JSON decode error for {url}")
        return None

def fetch_all_employees_full():
    """Try to fetch a full dump from /api/employees (may return list or None)."""
    return _safe_get("/api/employees")

def fetch_stats_page(detail, page=1, limit=500):
    """
    One page of /api/stats?details=detail&page=page&limit=limit
    Returns page dict or None.
    """
    params = {"details": detail, "page": page, "limit": limit}
    return _safe_get("/api/stats", params=params)

def fetch_all_stats(detail, limit=1000):
    """
    Iterate pages for /api/stats detail and return combined data list.
    Returns list or None.
    """
    first = fetch_stats_page(detail, page=1, limit=limit)
    if not first:
        return None
    data = first.get("data") or []
    total = int(first.get("total") or len(data) or 0)
    if total <= len(data):
        return data
    pages = int(math.ceil(total / float(limit)))
    for p in range(2, pages + 1):
        page_res = fetch_stats_page(detail, page=p, limit=limit)
        if not page_res:
            # stop early on error
            break
        data.extend(page_res.get("data") or [])
    return data

def get_global_stats():
    """
    Best-effort summary using /api/stats (preferred) or /api/employees (fallback).
    Returns dict or None.
    """
    # First: try to call /api/stats endpoints for canonical totals (preferred).
    details = ["TotalProfiles", "ActiveProfiles", "ActiveEmployees", "ActiveContractors",
               "TerminatedProfiles", "TerminatedEmployees", "TerminatedContractors"]
    out = {}

    # Try a single call to /api/stats with no detail (some CCURE deployments return a summary dict)
    try:
        summary = _safe_get("/api/stats")
        if isinstance(summary, dict) and any(k in summary for k in details):
            # normalize keys to expected names
            for k in details:
                # attempt case-insensitive lookup
                for key in summary.keys():
                    if key.lower() == k.lower():
                        out[k] = summary.get(key)
                        break
            if out:
                # convert numeric-like to int where possible
                safe_out = {}
                for k, v in out.items():
                    try:
                        safe_out[k] = int(v) if v is not None and str(v).strip() != "" else None
                    except Exception:
                        safe_out[k] = v
                return safe_out
    except Exception:
        pass

    # If that didn't work, try per-detail endpoints (some setups expose /api/stats?details=...)
    try:
        any_found = False
        for d in details:
            resp = fetch_stats_page(d, page=1, limit=1)
            if isinstance(resp, dict):
                # common patterns:
                # - { "total": 123, "data": [...] }
                # - { "TotalProfiles": 123, ... } (summary response)
                if 'total' in resp and isinstance(resp['total'], (int, float, str)):
                    out[d] = int(resp['total'])
                    any_found = True
                elif d in resp:
                    out[d] = resp.get(d)
                    any_found = True
                else:
                    # try case-insensitive key match
                    for key in resp.keys():
                        if key.lower() == d.lower() and isinstance(resp.get(key), (int, float, str)):
                            try:
                                out[d] = int(resp.get(key))
                                any_found = True
                            except Exception:
                                out[d] = resp.get(key)
                                any_found = True
                            break
        if any_found:
            return {k: (int(v) if (v is not None and str(v).strip() != "") else None) for k, v in out.items()}
    except Exception:
        logger.exception("fetch per-detail stats failed")

    # Fallback: try /api/employees full dump and compute counts locally.
    try:
        full = fetch_all_employees_full()
        if isinstance(full, list):
            total = len(full)
            active_profiles = sum(1 for r in full if (r.get("Employee_Status") or "").lower() == "active")
            active_emps = sum(1 for r in full if (r.get("PersonnelType") or "").lower().startswith("employee") and (r.get("Employee_Status") or "").lower() == "active")
            active_contractors = sum(1 for r in full if (r.get("PersonnelType") or "").lower().startswith("contractor") and (r.get("Employee_Status") or "").lower() == "active")
            terminated = sum(1 for r in full if (r.get("Employee_Status") or "").lower() in ("deactive", "deactivated", "inactive", "terminated"))
            return {
                "TotalProfiles": total,
                "ActiveProfiles": active_profiles,
                "ActiveEmployees": active_emps,
                "ActiveContractors": active_contractors,
                "TerminatedProfiles": terminated
            }
    except Exception:
        logger.exception("Error calculating global stats from full dump fallback")

    # nothing available
    return None






3) # C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\ccure_compare_service.py
import re
import json
from datetime import date, datetime, timedelta
from typing import List, Dict, Any, Optional, Set, Tuple
import time
import logging

logger = logging.getLogger("ccure_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

from db import SessionLocal
from models import AttendanceSummary, LiveSwipe
from settings import OUTPUT_DIR

# ---------- small helpers ----------------------------------------------------

def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

def _sanitize_for_json(value):
    try:
        import numpy as _np
    except Exception:
        _np = None
    if value is None:
        return None
    if isinstance(value, (str, bool, int)):
        return value
    if isinstance(value, float):
        if _np is not None and not _np.isfinite(value):
            return None
        return float(value)
    if _np is not None and isinstance(value, (_np.integer,)):
        return int(value)
    if isinstance(value, dict):
        out = {}
        for k, v in value.items():
            try:
                key = str(k)
            except Exception:
                key = repr(k)
            out[key] = _sanitize_for_json(v)
        return out
    if isinstance(value, (list, tuple, set)):
        return [_sanitize_for_json(v) for v in value]
    try:
        return str(value)
    except Exception:
        return None

# ---------- ccure helpers ---------------------------------------------------

def _fetch_ccure_stats():
    try:
        import ccure_client
        if hasattr(ccure_client, "get_global_stats"):
            return ccure_client.get_global_stats()
    except Exception:
        logger.debug("ccure_client.get_global_stats not available", exc_info=True)
    return None

def _fetch_ccure_profiles():
    try:
        import ccure_client
        for fn in ("fetch_all_employees_full", "fetch_all_employees", "fetch_all_profiles", "fetch_profiles", "fetch_all"):
            if hasattr(ccure_client, fn):
                try:
                    res = getattr(ccure_client, fn)()
                    if isinstance(res, list):
                        return res
                except Exception:
                    continue
    except Exception:
        pass
    return []

def _extract_ccure_locations_from_profiles(profiles: List[dict]) -> Set[str]:
    locs = set()
    for p in profiles:
        if not isinstance(p, dict):
            continue
        for k in ("Partition", "PartitionName", "Location", "Location City", "location_city", "location", "Site", "BaseLocation"):
            v = p.get(k) if isinstance(p, dict) else None
            if v and isinstance(v, str) and v.strip():
                locs.add(v.strip())
    return locs

# ---------- classification & partition helpers ------------------------------

def classify_personnel_from_detail(detail: dict) -> str:
    """
    Classify 'detail' dict as 'employee' or 'contractor'.
    Looks for several PersonnelType/status keys; default is contractor.
    """
    try:
        if not isinstance(detail, dict):
            return "contractor"
        candidate_keys = [
            "PersonnelType", "personnelType", "personnel_type", "Personnel Type",
            "PersonnelTypeName", "Personnel", "Type", "personnel", "PersonType", "personType"
        ]
        val = None
        for k in candidate_keys:
            if k in detail and detail.get(k) is not None:
                val = str(detail.get(k)).strip().lower()
                break
        status_keys = ["Employee_Status", "Employee Status", "Status", "Profile_Disabled"]
        status_val = None
        for k in status_keys:
            if k in detail and detail.get(k) is not None:
                status_val = str(detail.get(k)).strip().lower()
                break

        # terminated/disabled rows treated as employee (they were employees historically)
        if status_val is not None and "terminated" in status_val:
            return "employee"
        if val is None or val == "":
            return "contractor"
        if "employee" in val:
            return "employee"
        contractor_terms = ["contractor", "visitor", "property", "temp", "temp badge", "tempbadge"]
        for t in contractor_terms:
            if t in val:
                return "contractor"
        if "contract" in val or "visitor" in val:
            return "contractor"
        # default to contractor if unclear
        return "contractor"
    except Exception:
        return "contractor"

def pick_partition_from_detail(detail: dict) -> str:
    if not isinstance(detail, dict):
        return "Unknown"
    for k in ("PartitionName2","PartitionName1","Partition","PartitionName","Region","Location","Site","location_city","Location City"):
        if k in detail and detail.get(k):
            try:
                return str(detail.get(k)).strip()
            except Exception:
                continue
    if "__region" in detail and detail.get("__region"):
        return str(detail.get("__region")).strip()
    return "Unknown"

# ---------- small resilient fetch helper for region_clients -----------------
def _attempt_region_call(fn, timeout, attempts=2, backoff=0.5) -> Tuple[Optional[Any], Optional[Exception]]:
    """
    Attempt calling a region_clients function fn(timeout=...) multiple times.
    Returns (result, last_exception). On success last_exception is None.
    On persistent failure, returns (None, last_exception). This lets callers record errors
    and attach them to the compute payload instead of spamming WARNING logs.
    """
    last_exc = None
    fn_name = getattr(fn, "__name__", str(fn))
    for i in range(attempts):
        try:
            res = fn(timeout=timeout)
            # successful call
            return res, None
        except Exception as e:
            last_exc = e
            # classify error message â many remote 500s/timeouts are transient; log them at INFO,
            # keep full stack at DEBUG. This reduces WARNING spam seen in console.
            msg = str(e)
            low = msg.lower()
            if "500 server error" in low or "internal server error" in low or "read timed out" in low or "timeout" in low:
                logger.info("[region_clients] attempt %d/%d failed for %s: %s", i+1, attempts, fn_name, msg)
                logger.debug("[region_clients] full exception for %s attempt %d: ", fn_name, i+1, exc_info=True)
            else:
                # other exceptions might be more important
                logger.warning("[region_clients] attempt %d/%d failed for %s: %s", i+1, attempts, fn_name, msg)
                logger.debug("[region_clients] full exception for %s attempt %d: ", fn_name, i+1, exc_info=True)
            time.sleep(backoff * (i+1))
            continue
    # All attempts exhausted â record at INFO and provide last_exc to caller
    logger.info("[region_clients] all %d attempts failed for %s: last_exc=%s", attempts, fn_name, str(last_exc))
    logger.debug("[region_clients] last exception traceback for %s: ", fn_name, exc_info=True)
    return None, last_exc

# ---------- utility: fallback headcount builder from LiveSwipe --------------

def build_headcount_from_liveswipes_for_today(session) -> (int, Dict[str, Dict[str, int]]):
    start = datetime.combine(date.today(), datetime.min.time())
    end = datetime.combine(date.today(), datetime.max.time())
    swipes = session.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
    if not swipes:
        return 0, {}
    seen_keys = {}
    per_loc = {}
    for s in swipes:
        key = _normalize_employee_key(s.employee_id) or _normalize_card_like(s.card_number)
        if not key:
            key = f"nokey_{s.id}"
        rec = seen_keys.get(key)
        ts = s.timestamp
        if rec is None:
            seen_keys[key] = {"first_seen": ts, "last_seen": ts, "partition": (s.partition or "Unknown"), "class": None, "card": s.card_number, "raw": s.raw}
        else:
            if ts and rec.get("first_seen") and ts < rec["first_seen"]:
                rec["first_seen"] = ts
            if ts and rec.get("last_seen") and ts > rec["last_seen"]:
                rec["last_seen"] = ts
    for k, v in seen_keys.items():
        loc = v.get("partition") or "Unknown"
        if not isinstance(loc, str) or not loc.strip():
            loc = "Unknown"
        if loc not in per_loc:
            per_loc[loc] = {"total": 0, "employee": 0, "contractor": 0}
        per_loc[loc]["total"] += 1
        classified = "contractor"
        raw = v.get("raw")
        if isinstance(raw, dict):
            try:
                classified = classify_personnel_from_detail(raw)
            except Exception:
                classified = "contractor"
        per_loc[loc][classified] += 1
    total = sum(p["total"] for p in per_loc.values())
    return int(total), per_loc

# ---------- main compute function -----------------------------------------

def compute_visit_averages(start_date: Optional[str] = None, end_date: Optional[str] = None, timeout: int = 6) -> Dict[str, Any]:
    """
    Compute visit averages for an inclusive date range.
    Uses only AttendanceSummary, region_clients.fetch_all_details, region_clients.fetch_all_history and ccure stats.
    Defaults to last 7 days if no valid range provided.
    `timeout` is forwarded to region_clients functions (live/history). We use small retries internally.
    """
    notes = []
    region_client_errors: List[Dict[str, str]] = []
    today = date.today()

    # parse date strings
    def _parse_date_param(s):
        if not s:
            return None
        try:
            return datetime.strptime(s, "%Y-%m-%d").date()
        except Exception:
            try:
                return date.fromisoformat(s)
            except Exception:
                return None

    start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
    end_obj = _parse_date_param(end_date) if end_date else today
    if start_obj is None or end_obj is None or start_obj > end_obj:
        start_obj = today - timedelta(days=6)
        end_obj = today
        notes.append("Invalid or missing date range; defaulted to last 7 calendar days inclusive.")

    # gather CCURE stats & profiles (profiles only used if you want to filter â currently not used to drop partitions)
    ccure_stats = _fetch_ccure_stats()
    reported_active_emps = _safe_int(ccure_stats.get("ActiveEmployees")) if isinstance(ccure_stats, dict) else None
    reported_active_contractors = _safe_int(ccure_stats.get("ActiveContractors")) if isinstance(ccure_stats, dict) else None

    ccure_profiles = _fetch_ccure_profiles()
    ccure_locations = _extract_ccure_locations_from_profiles(ccure_profiles) if isinstance(ccure_profiles, list) else set()

    # ---------- HEADCOUNT (AttendanceSummary fallback logic) ----------
    head_total = 0
    head_per_location: Dict[str, Dict[str, int]] = {}
    key_map_head: Dict[str, Dict[str, Any]] = {}
    key_map_live: Dict[str, Dict[str, Any]] = {}

    try:
        session = SessionLocal()
        att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
        if not att_rows_today:
            # try to build from LiveSwipe compute_daily_attendance if available
            try:
                from compare_service import compute_daily_attendance as _compute_daily_attendance
                try:
                    built = _compute_daily_attendance(today)
                    if isinstance(built, list) and len(built) > 0:
                        att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                        notes.append("AttendanceSummary was missing; built from LiveSwipe via compute_daily_attendance().")
                except Exception:
                    logger.exception("compute_daily_attendance execution failed; falling back")
            except Exception:
                logger.debug("compare_service.compute_daily_attendance not importable; falling back", exc_info=True)

            if not att_rows_today:
                built_total, built_per_loc = build_headcount_from_liveswipes_for_today(session)
                head_total = built_total
                head_per_location = built_per_loc
                if head_total > 0:
                    notes.append("AttendanceSummary for today empty; built headcount from LiveSwipe rows (non-persistent fallback).")
        if att_rows_today:
            # classify using AttendanceSummary.derived if possible (don't rely on Active sheets)
            for a in att_rows_today:
                key = _normalize_employee_key(a.employee_id) or _normalize_card_like((a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None)) or None
                partition = None
                try:
                    if a.derived and isinstance(a.derived, dict):
                        partition = a.derived.get("partition")
                except Exception:
                    partition = None
                loc = partition or "Unknown"
                if not isinstance(loc, str) or not loc.strip():
                    loc = "Unknown"
                if (a.presence_count or 0) > 0:
                    cls = "contractor"
                    try:
                        if a.derived and isinstance(a.derived, dict):
                            cls = classify_personnel_from_detail(a.derived)
                    except Exception:
                        cls = "contractor"
                    if key:
                        key_map_head[key] = {"loc": loc, "cls": cls}
                    if loc not in head_per_location:
                        head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                    head_per_location[loc][cls] += 1
                    head_per_location[loc]["total"] += 1
                    head_total += 1
        session.expunge_all()
    except Exception:
        logger.exception("Error computing HeadCount")
        notes.append("Failed to compute HeadCount from DB; see server logs.")
    finally:
        try:
            session.close()
        except Exception:
            pass

    # --- LIVE HEADCOUNT via region_clients (realtime) ----------
    live_total = 0
    live_per_location: Dict[str, Dict[str, int]] = {}
    sites_queried = 0
    details = []
    try:
        import region_clients
        regions_info = []
        # Use resilient wrapper with retries/backoff and collect errors
        try:
            if hasattr(region_clients, "fetch_all_regions"):
                maybe_regions, exc_regions = _attempt_region_call(region_clients.fetch_all_regions, timeout=timeout, attempts=2, backoff=0.5)
                if exc_regions is not None:
                    region_client_errors.append({"fn": "fetch_all_regions", "error": str(exc_regions)})
                regions_info = maybe_regions or []
        except Exception:
            logger.exception("region_clients.fetch_all_regions failed")
        try:
            if hasattr(region_clients, "fetch_all_details"):
                maybe_details, exc_details = _attempt_region_call(region_clients.fetch_all_details, timeout=timeout, attempts=2, backoff=0.5)
                if exc_details is not None:
                    region_client_errors.append({"fn": "fetch_all_details", "error": str(exc_details)})
                details = maybe_details or []
        except Exception:
            logger.exception("region_clients.fetch_all_details failed")
        sites_queried = len(regions_info) if isinstance(regions_info, list) else 0
        if regions_info:
            for r in regions_info:
                try:
                    c = r.get("count") if isinstance(r, dict) else None
                    ci = _safe_int(c)
                    if ci is not None:
                        live_total += int(ci)
                except Exception:
                    continue
        derived_detail_sum = 0
        if details and isinstance(details, list):
            for d in details:
                try:
                    loc = pick_partition_from_detail(d) or "Unknown"
                    if not isinstance(loc, str) or not loc.strip():
                        loc = "Unknown"
                    pclass = classify_personnel_from_detail(d)
                    # dedupe key
                    key = _normalize_employee_key(d.get("EmployeeID")) or _normalize_card_like(d.get("CardNumber")) or (d.get("PersonGUID") if d.get("PersonGUID") else None)
                    if not key:
                        key = _normalize_employee_key(d.get("employee_id")) or _normalize_card_like(d.get("Card")) or None
                    if not key:
                        # synthetic key for anonymous row
                        key = f"detail_{derived_detail_sum}_{str(hash(json.dumps(d, default=str)))}"
                    key = str(key)
                    key_map_live[key] = {"loc": loc, "cls": pclass}
                    if loc not in live_per_location:
                        live_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                    live_per_location[loc]["total"] += 1
                    live_per_location[loc][pclass] += 1
                    derived_detail_sum += 1
                except Exception:
                    continue
            # decide final live_total: if regions provided prefer region totals for overall; details used for breakdown
            if live_total == 0 and derived_detail_sum > 0:
                live_total = derived_detail_sum
            else:
                if live_total != derived_detail_sum:
                    notes.append(f"Region totals ({live_total}) differ from detail rows ({derived_detail_sum}); using region totals for overall and details for breakdown.")
        else:
            notes.append("No per-person details available from region_clients; live breakdown unavailable.")
    except Exception:
        logger.exception("Error computing Live HeadCount")
        notes.append("Failed to compute Live HeadCount; see logs.")
        live_total = live_total or 0

    # ---------- Ensure headcount is union(head_keys, live_keys) ----------
    try:
        head_keys = set(k for k in key_map_head.keys() if k)
        live_keys = set(k for k in key_map_live.keys() if k)
        union_keys = head_keys.union(live_keys)

        unified_head_per_location: Dict[str, Dict[str, int]] = {}
        for k in union_keys:
            if k in key_map_head:
                loc = key_map_head[k].get("loc") or "Unknown"
                cls = key_map_head[k].get("cls") or "contractor"
            else:
                loc = key_map_live.get(k, {}).get("loc") or "Unknown"
                cls = key_map_live.get(k, {}).get("cls") or "contractor"
            if not isinstance(loc, str) or not loc.strip():
                loc = "Unknown"
            if loc not in unified_head_per_location:
                unified_head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
            unified_head_per_location[loc]["total"] += 1
            unified_head_per_location[loc][cls] += 1

        if union_keys:
            head_per_location = unified_head_per_location
            head_total = len(union_keys)
    except Exception:
        logger.exception("Error reconciling headcount union with live details")

    # --- Averages from AttendanceSummary (DB) - per-location & overall (range-based) ---
    avg_headcount_last_range = None
    avg_headcount_per_site_last_range = None
    avg_by_location_last_range: Dict[str, Dict[str, Any]] = {}

    try:
        session = SessionLocal()
        # build list of days in range
        days = []
        days_count = (end_obj - start_obj).days + 1
        for i in range(0, days_count):
            days.append(start_obj + timedelta(days=i))

        loc_day_vals: Dict[str, Dict[str, List[int]]] = {}
        for d in days:
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            per_loc_counts: Dict[str, Dict[str, int]] = {}
            if rows:
                for r in rows:
                    try:
                        if (r.presence_count or 0) <= 0:
                            continue
                        partition = None
                        try:
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                        except Exception:
                            partition = None
                        loc = partition or "Unknown"
                        if not isinstance(loc, str) or not loc.strip():
                            loc = "Unknown"
                        if loc not in per_loc_counts:
                            per_loc_counts[loc] = {"employee": 0, "contractor": 0, "total": 0}
                        # classify via derived if possible
                        cls = "contractor"
                        try:
                            if r.derived and isinstance(r.derived, dict):
                                cls = classify_personnel_from_detail(r.derived)
                        except Exception:
                            cls = "contractor"
                        per_loc_counts[loc][cls] += 1
                        per_loc_counts[loc]["total"] += 1
                    except Exception:
                        continue
            for loc, counts in per_loc_counts.items():
                if loc not in loc_day_vals:
                    loc_day_vals[loc] = {"employee": [], "contractor": [], "total": []}
                loc_day_vals[loc]["employee"].append(counts.get("employee", 0))
                loc_day_vals[loc]["contractor"].append(counts.get("contractor", 0))
                loc_day_vals[loc]["total"].append(counts.get("total", 0))

        for loc, lists in loc_day_vals.items():
            emp_list = lists.get("employee", [])
            con_list = lists.get("contractor", [])
            tot_list = lists.get("total", [])
            days_counted = len(tot_list)
            if days_counted == 0:
                continue
            avg_emp = round(sum(emp_list) / float(days_counted), 2)
            avg_con = round(sum(con_list) / float(days_counted), 2)
            avg_tot = round(sum(tot_list) / float(days_counted), 2)
            avg_by_location_last_range[loc] = {
                "history_days_counted": int(days_counted),
                "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
            }

        days_totals = []
        for d in days:
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            day_total = 0
            if rows:
                for r in rows:
                    if (r.presence_count or 0) > 0:
                        day_total += 1
            days_totals.append(day_total)
        if days_totals:
            avg_headcount_last_range = round(sum(days_totals) / float(len(days_totals)), 2)
            if sites_queried and sites_queried > 0:
                avg_headcount_per_site_last_range = round((sum(days_totals) / float(len(days_totals))) / float(sites_queried), 2)
        session.close()
    except Exception:
        logger.exception("Error computing averages from AttendanceSummary")
        notes.append("Failed to compute historical averages from AttendanceSummary; partial results only.")

    # --- HISTORY AVERAGES: use region_clients.fetch_all_history (range-based) ----------
    history_emp_avg = None
    history_contractor_avg = None
    history_overall_avg = None
    history_days = 0
    history_avg_by_location: Dict[str, Dict[str, Any]] = {}
    history_today_emp = None
    history_today_con = None

    try:
        import region_clients
        if hasattr(region_clients, "fetch_all_history"):
            entries, exc_history = _attempt_region_call(region_clients.fetch_all_history, timeout=timeout, attempts=2, backoff=0.5)
            if exc_history is not None:
                region_client_errors.append({"fn": "fetch_all_history", "error": str(exc_history)})
            entries = entries or []
            agg_by_date = {}
            agg_partitions_by_date = {}
            for e in entries:
                try:
                    dstr = e.get("date")
                    if not dstr:
                        dstr = e.get("day") or e.get("timestamp") or None
                        if isinstance(dstr, datetime):
                            dstr = dstr.date().isoformat()
                    if not dstr:
                        continue
                    # robust region-level extraction (some endpoints return region key, some return fields at top)
                    region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                    emp = None
                    con = None
                    tot = None
                    if region_obj and isinstance(region_obj, dict):
                        emp = _safe_int(region_obj.get("Employee"))
                        con = _safe_int(region_obj.get("Contractor"))
                        tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                    else:
                        emp = _safe_int(e.get("Employee") or (e.get("region") and e.get("region").get("Employee") if isinstance(e.get("region"), dict) else None))
                        con = _safe_int(e.get("Contractor") or (e.get("region") and e.get("region").get("Contractor") if isinstance(e.get("region"), dict) else None))
                        tot = _safe_int(e.get("total") or ((emp or 0) + (con or 0)))
                    if emp is None and con is None:
                        try:
                            robj = e.get("region") or {}
                            if isinstance(robj, dict):
                                emp = _safe_int(robj.get("Employee"))
                                con = _safe_int(robj.get("Contractor"))
                                tot = _safe_int(robj.get("total"))
                        except Exception:
                            pass
                    if emp is None and con is None:
                        continue
                    if tot is None:
                        tot = (emp or 0) + (con or 0)
                    if dstr not in agg_by_date:
                        agg_by_date[dstr] = {"employee": 0, "contractor": 0, "total": 0, "counted_regions": 0}
                    agg_by_date[dstr]["employee"] += (emp or 0)
                    agg_by_date[dstr]["contractor"] += (con or 0)
                    agg_by_date[dstr]["total"] += (tot or 0)
                    agg_by_date[dstr]["counted_regions"] += 1

                    parts = e.get("partitions") if isinstance(e.get("partitions"), dict) else {}
                    if dstr not in agg_partitions_by_date:
                        agg_partitions_by_date[dstr] = {}
                    for pname, pstat in parts.items():
                        try:
                            p_emp = _safe_int(pstat.get("Employee"))
                            p_con = _safe_int(pstat.get("Contractor"))
                            p_tot = _safe_int(pstat.get("total")) or ((p_emp or 0) + (p_con or 0))
                            if pname not in agg_partitions_by_date[dstr]:
                                agg_partitions_by_date[dstr][pname] = {"employee": 0, "contractor": 0, "total": 0}
                            agg_partitions_by_date[dstr][pname]["employee"] += (p_emp or 0)
                            agg_partitions_by_date[dstr][pname]["contractor"] += (p_con or 0)
                            agg_partitions_by_date[dstr][pname]["total"] += (p_tot or 0)
                        except Exception:
                            continue
                except Exception:
                    continue

            # history_today (if present)
            today_iso = today.isoformat()
            if today_iso in agg_by_date:
                history_today_emp = agg_by_date[today_iso].get("employee", 0)
                history_today_con = agg_by_date[today_iso].get("contractor", 0)

            # select dates within requested inclusive range
            day_vals_emp = []
            day_vals_con = []
            day_vals_tot = []
            selected_dates = []
            for i in range(0, (end_obj - start_obj).days + 1):
                dcheck = (start_obj + timedelta(days=i)).isoformat()
                entry = agg_by_date.get(dcheck)
                if entry:
                    day_vals_emp.append(entry.get("employee", 0))
                    day_vals_con.append(entry.get("contractor", 0))
                    day_vals_tot.append(entry.get("total", 0))
                    selected_dates.append(dcheck)

            if day_vals_emp:
                history_emp_avg = round(sum(day_vals_emp) / float(len(day_vals_emp)), 2)
            if day_vals_con:
                history_contractor_avg = round(sum(day_vals_con) / float(len(day_vals_con)), 2)
            if day_vals_tot:
                history_overall_avg = round(sum(day_vals_tot) / float(len(day_vals_tot)), 2)
            history_days = len(day_vals_tot)
            if history_days == 0:
                notes.append("History endpoints returned no usable rows in requested range; history averages not available.")

            # per-partition averages across the selected_dates
            partition_day_values = {}
            for d_iso in selected_dates:
                per_parts = agg_partitions_by_date.get(d_iso, {})
                for pname, pvals in per_parts.items():
                    if pname not in partition_day_values:
                        partition_day_values[pname] = {"employee": [], "contractor": [], "total": []}
                    partition_day_values[pname]["employee"].append(pvals.get("employee", 0))
                    partition_day_values[pname]["contractor"].append(pvals.get("contractor", 0))
                    partition_day_values[pname]["total"].append(pvals.get("total", 0))
            for pname, lists in partition_day_values.items():
                emp_list = lists.get("employee", [])
                con_list = lists.get("contractor", [])
                tot_list = lists.get("total", [])
                days_counted = len(tot_list)
                if days_counted == 0:
                    continue
                avg_emp = round(sum(emp_list) / float(days_counted), 2)
                avg_con = round(sum(con_list) / float(days_counted), 2)
                avg_tot = round(sum(tot_list) / float(days_counted), 2)
                history_avg_by_location[pname] = {
                    "history_days_counted": int(days_counted),
                    "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                    "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                    "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
                }

            logger.debug("history: dates collected=%d partitions_sample=%d", len(selected_dates), len(history_avg_by_location))
    except Exception:
        logger.exception("Error fetching/processing history endpoints")
        notes.append("Failed to compute history averages from region history endpoints; partial results.")

    # Merge DB-derived per-location (avg_by_location_last_range) with history per-location (history_avg_by_location)
    try:
        merged_history = dict(history_avg_by_location)  # prefer history where present
        for loc, dbvals in (avg_by_location_last_range or {}).items():
            if loc in merged_history:
                continue
            try:
                merged_history[loc] = {
                    "history_days_counted": int(dbvals.get("history_days_counted") or 0),
                    "avg_employee_last_7_days": _sanitize_for_json(dbvals.get("avg_employee_last_7_days")),
                    "avg_contractor_last_7_days": _sanitize_for_json(dbvals.get("avg_contractor_last_7_days")),
                    "avg_overall_last_7_days": _sanitize_for_json(dbvals.get("avg_overall_last_7_days"))
                }
            except Exception:
                merged_history[loc] = {
                    "history_days_counted": int(dbvals.get("history_days_counted") or 0),
                    "avg_employee_last_7_days": _sanitize_for_json(dbvals.get("avg_employee_last_7_days") or None),
                    "avg_contractor_last_7_days": _sanitize_for_json(dbvals.get("avg_contractor_last_7_days") or None),
                    "avg_overall_last_7_days": _sanitize_for_json(dbvals.get("avg_overall_last_7_days") or None)
                }
        history_avg_by_location = merged_history
    except Exception:
        logger.exception("Failed to normalize history_avg_by_location")

    # Fallback: if DB-based avg empty, use history_overall_avg
    if (not avg_headcount_last_range or avg_headcount_last_range == 0) and history_overall_avg:
        try:
            avg_headcount_last_range = history_overall_avg
            avg_headcount_per_site_last_range = round(history_overall_avg / float(sites_queried), 2) if sites_queried and sites_queried > 0 else None
            notes.append("avg_headcount_last_range derived from region history endpoints due to missing AttendanceSummary historical data.")
        except Exception:
            pass

    # --- compute percentages (head/live vs CCURE reported)
    def safe_pct(n, denom):
        try:
            if n is None or denom is None:
                return None
            if float(denom) == 0.0:
                return None
            return round((float(n) / float(denom)) * 100.0, 2)
        except Exception:
            return None

    cc_emp_denom = reported_active_emps
    cc_con_denom = reported_active_contractors
    cc_total_denom = None
    if isinstance(cc_emp_denom, int) and isinstance(cc_con_denom, int):
        cc_total_denom = cc_emp_denom + cc_con_denom

    head_emp_total = sum(v.get("employee", 0) for v in head_per_location.values())
    head_con_total = sum(v.get("contractor", 0) for v in head_per_location.values())
    live_emp_total = sum(v.get("employee", 0) for v in live_per_location.values())
    live_con_total = sum(v.get("contractor", 0) for v in live_per_location.values())

    head_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_emp_total, cc_emp_denom))
    head_contractor_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_con_total, cc_con_denom))
    head_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_total, cc_total_denom))

    live_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_emp_total, cc_emp_denom))
    live_contractor_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_con_total, cc_con_denom))
    live_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_total, cc_total_denom))

    history_emp_pct_vs_ccure = _sanitize_for_json(safe_pct(history_emp_avg, cc_emp_denom))
    history_contractor_pct_vs_ccure = _sanitize_for_json(safe_pct(history_contractor_avg, cc_con_denom))
    history_overall_pct_vs_ccure = _sanitize_for_json(safe_pct(history_overall_avg, cc_total_denom))

    history_today_emp_pct_vs_ccure = _sanitize_for_json(safe_pct(history_today_emp, cc_emp_denom))
    history_today_contractor_pct_vs_ccure = _sanitize_for_json(safe_pct(history_today_con, cc_con_denom))

    # Final result
    result = {
        "date": today.isoformat(),
        "headcount": {
            "total_visited_today": int(head_total),
            "employee": int(head_emp_total),
            "contractor": int(head_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in head_per_location.items() }
        },
        "live_headcount": {
            "currently_present_total": int(live_total),
            "employee": int(live_emp_total),
            "contractor": int(live_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in live_per_location.items() }
        },
        "ccure_active": {
            "ccure_active_employees_reported": _safe_int(reported_active_emps),
            "ccure_active_contractors_reported": _safe_int(reported_active_contractors)
        },
        "averages": {
            "head_emp_pct_vs_ccure_today": head_emp_pct_vs_ccure_today,
            "head_contractor_pct_vs_ccure_today": head_contractor_pct_vs_ccure_today,
            "headcount_overall_pct_vs_ccure_today": head_overall_pct_vs_ccure_today,
            "live_employee_pct_vs_ccure": live_emp_pct_vs_ccure_today,
            "live_contractor_pct_vs_ccure": _sanitize_for_json(safe_pct(live_con_total, cc_con_denom)),
            "live_overall_pct_vs_ccure": live_overall_pct_vs_ccure_today,
            # range-keys (kept for compatibility)
            "avg_headcount_last_7_days": _sanitize_for_json(avg_headcount_last_range),
            "avg_headcount_per_site_last_7_days": _sanitize_for_json(avg_headcount_per_site_last_range),
            "avg_live_per_site": _sanitize_for_json(round(live_total / sites_queried, 2) if sites_queried and sites_queried > 0 else None),

            # history endpoint range averages
            "history_avg_employee_last_7_days": _sanitize_for_json(history_emp_avg),
            "history_avg_contractor_last_7_days": _sanitize_for_json(history_contractor_avg),
            "history_avg_overall_last_7_days": _sanitize_for_json(history_overall_avg),
            "history_days_counted": int(history_days) if history_days is not None else None,
            "history_employee_pct_vs_ccure": history_emp_pct_vs_ccure,
            "history_contractor_pct_vs_ccure": history_contractor_pct_vs_ccure,
            "history_overall_pct_vs_ccure": history_overall_pct_vs_ccure,

            # history-today specific metrics (if present)
            "history_today_employee_count": int(history_today_emp) if history_today_emp is not None else None,
            "history_today_contractor_count": int(history_today_con) if history_today_con is not None else None,
            "history_today_employee_pct_vs_ccure": history_today_emp_pct_vs_ccure,
            "history_today_contractor_pct_vs_ccure": history_today_contractor_pct_vs_ccure,

            "avg_by_location_last_7_days": _sanitize_for_json(avg_by_location_last_range),
            "history_avg_by_location_last_7_days": _sanitize_for_json(history_avg_by_location)
        },
        "sites_queried": int(sites_queried),
        "notes": " | ".join(notes) if notes else f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}",
        # new optional fields that record region client failures
        "region_client_errors": region_client_errors,
        "region_clients_unavailable": True if region_client_errors else False
    }

    return _sanitize_for_json(result)







4) # data_compare_service_v2.py
"""
Comparison service (v2) with broadened matching heuristics and safer prefetch/cache.
Improved: deeper scalar iteration, robust timestamp parsing, clearer logging and example matches.
Drop-in replacement for your existing data_compare_service_v2.py
"""

import sys
import re
import uuid
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
from typing import Optional, Dict, Any, List, Tuple

import pandas as pd

# HTTP client for region histories
try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except Exception:
    requests = None

# DB imports (same as your project)
from db import SessionLocal
from models import AttendanceSummary

# Settings / defaults
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR, REGION_HISTORY_URLS as SETTINGS_REGION_HISTORY_URLS
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
    REGION_HISTORY_URLS = SETTINGS_REGION_HISTORY_URLS
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"
    REGION_HISTORY_URLS = [
        "http://10.199.22.57:3008/api/occupancy/history",  # APAC
        "http://10.199.22.57:3006/api/occupancy/history",  # NAMER
        "http://10.199.22.57:3007/api/occupancy/history",  # EMEA
        "http://10.199.22.57:4000/api/occupancy/history"   # LACA
    ]

DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger("data_compare_service_v2")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# In-memory cache for prefetched region history entries
REGION_HISTORY_CACHE = None
REGION_HISTORY_CACHE_FETCHED_AT = None
REGION_HISTORY_CACHE_TTL_SECONDS = 300  # 5 minutes

# Matching config
ID_FIELD_CANDIDATES = [
    "EmployeeID","employeeId","Employee Id","EmpID","Emp Id","EmpNo","EmployeeNo","Employee_Number",
    "PersonID","PersonId","personId","person_id","employee_id","id","Id","employeeNumber","EmployeeNumber",
    "worker_system_id","wsid","WorkerID","Worker System Id","workerId","WorkerSystemId"
]
CARD_FIELD_CANDIDATES = [
    "CardNumber","Card","cardNumber","card_number","BadgeNumber","BadgeNo","Badge","badgeNumber","badge_no",
    "iPassID","IPassID","iPass","i_pass_id","CardNo","card_no","card","IPASSID","IPass"
]
NAME_FIELD_CANDIDATES = [
    "FullName","Full Name","EmpName","Name","full_name","displayName","personName","PersonName"
]

# ----------------------------
# Utilities
# ----------------------------
def _find_active_employee_file():
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_employee{ext}"
        if p.exists():
            return p
    for p in DATA_DIR.iterdir():
        if p.is_file() and "active_employee" in p.name.lower():
            return p
    return None

def _normalize_key(k):
    if k is None:
        return None
    try:
        s = str(k).strip()
        return s if s != "" else None
    except Exception:
        return None

def _digits_only(s):
    if s is None:
        return ""
    return re.sub(r'\D+', '', str(s))

def _safe_int(v, default=0):
    try:
        if v is None or v == "":
            return default
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return default

def _week_monday_and_friday(ref_date: Optional[date] = None):
    if ref_date is None:
        ref_date = date.today()
    iso = ref_date.isoweekday()
    monday = ref_date - timedelta(days=(iso - 1))
    friday = monday + timedelta(days=4)
    return monday, friday

def _maybe_mark_on_leave(status_str: Optional[str]) -> bool:
    if not status_str:
        return False
    s = str(status_str).strip().lower()
    for tok in ("leave", "vacation", "on leave", "holiday", "sabbatical", "furlough", "loa"):
        if tok in s:
            return True
    return False

# ----------------------------
# Load active sheet
# ----------------------------
def load_active_employees_dataframe() -> pd.DataFrame:
    src = _find_active_employee_file()
    if not src:
        raise FileNotFoundError(f"Active employee canonical file not found in {DATA_DIR}")
    ext = src.suffix.lower()
    if ext in (".xls", ".xlsx"):
        df = pd.read_excel(src, sheet_name=0, dtype=str)
    else:
        df = pd.read_csv(src, dtype=str)
    df.columns = [c.strip() for c in df.columns]

    def _first_present(row, candidates):
        for c in candidates:
            if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
                return row[c]
        return None

    rows = []
    for _, row in df.iterrows():
        emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id','EmployeeNo','Employee No'])
        if emp_id is None:
            continue
        full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or (
            f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
        )
        location_city = _first_present(row, ['Location City','Location','City'])
        location_desc = _first_present(row, ['Location Description','LocationDescription'])
        location_state = _first_present(row, ['Location State / Province','Location State','State','Province'])
        region_code = _first_present(row, ['Region Code','Region','RegionCode'])
        current_status = _first_present(row, ['Current Status','Status','Employee Status'])
        employee_type = _first_present(row, ['Employee Type','Type','Time Type'])
        rows.append({
            "employee_id": _normalize_key(emp_id),
            "full_name": _normalize_key(full_name),
            "location_city": _normalize_key(location_city),
            "location_desc": _normalize_key(location_desc),
            "location_state": _normalize_key(location_state),
            "region_code": (str(region_code).strip() if region_code is not None else None),
            "current_status": _normalize_key(current_status),
            "employee_type": _normalize_key(employee_type),
            "raw_row": row.to_dict()
        })
    ndf = pd.DataFrame(rows)
    for col in ("employee_id","full_name","location_city","location_desc","location_state","region_code","current_status","employee_type","raw_row"):
        if col not in ndf:
            ndf[col] = None
    ndf = ndf[ndf["employee_id"].notna() & (ndf["employee_id"].str.strip() != "")]
    ndf.reset_index(drop=True, inplace=True)
    return ndf

# ----------------------------
# HTTP Session factory (retries/backoff)
# ----------------------------
def _build_requests_session():
    if requests is None:
        return None
    s = requests.Session()
    allowed = frozenset(['GET', 'HEAD'])
    try:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=allowed
        )
    except TypeError:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            method_whitelist=allowed
        )
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

# ----------------------------
# Prefetch / cache helpers
# ----------------------------
def prefetch_region_history(timeout: int = 10, force: bool = False):
    """
    Fetch region history entries (cached). Returns list of raw entries.
    """
    global REGION_HISTORY_CACHE, REGION_HISTORY_CACHE_FETCHED_AT
    try:
        now = datetime.utcnow()
        if not force and REGION_HISTORY_CACHE is not None and REGION_HISTORY_CACHE_FETCHED_AT:
            elapsed = (now - REGION_HISTORY_CACHE_FETCHED_AT).total_seconds()
            if elapsed < REGION_HISTORY_CACHE_TTL_SECONDS:
                logger.info("[region_cache] Using cached region history (age %.1fs)", elapsed)
                return REGION_HISTORY_CACHE

        entries = []
        # Prefer region_clients when available
        try:
            import region_clients
            logger.info("[region_cache] fetching region history via region_clients.fetch_all_history()")
            try:
                got = region_clients.fetch_all_history(timeout=timeout)
            except TypeError:
                got = region_clients.fetch_all_history()
            entries = got or []
        except Exception:
            entries = []

        # If empty, try direct requests to configured URLs
        if not entries and requests is not None:
            logger.info("[region_cache] fetching region history directly from endpoints")
            session = _build_requests_session() or requests
            for url in REGION_HISTORY_URLS:
                if not url:
                    continue
                try:
                    resp = session.get(url, timeout=(3, max(5, timeout)))
                    if not resp or resp.status_code != 200:
                        continue
                    try:
                        payload = resp.json()
                    except Exception:
                        continue
                    # flatten possible lists
                    if isinstance(payload, list):
                        for p in payload:
                            if isinstance(p, dict):
                                p['_source_url'] = url
                                entries.append(p)
                    elif isinstance(payload, dict):
                        found_list = False
                        for k in ("results","summaryByDate","details","data","entries","list","people","items"):
                            if k in payload and isinstance(payload[k], list):
                                for p in payload[k]:
                                    if isinstance(p, dict):
                                        p['_source_url'] = url
                                        entries.append(p)
                                found_list = True
                                break
                        if not found_list:
                            payload['_source_url'] = url
                            entries.append(payload)
                except requests.exceptions.RequestException as e:
                    logger.warning("[region_cache] fetch failed for %s: %s", url, str(e))
                    continue

        REGION_HISTORY_CACHE = entries or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        logger.info("[region_cache] prefetched %d region history entries", len(REGION_HISTORY_CACHE))
        return REGION_HISTORY_CACHE
    except Exception:
        logger.exception("[region_cache] prefetch failed")
        REGION_HISTORY_CACHE = REGION_HISTORY_CACHE or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        return REGION_HISTORY_CACHE

# ----------------------------
# Payload helpers (deep search)
# ----------------------------
def _iter_scalars_in_obj(obj, parent_key=""):
    """
    Yield all scalar key->value pairs inside nested dict/list structures as (key_path, value).
    """
    if isinstance(obj, dict):
        for k, v in obj.items():
            new_key = f"{parent_key}.{k}" if parent_key else str(k)
            if isinstance(v, (dict, list)):
                yield from _iter_scalars_in_obj(v, parent_key=new_key)
            else:
                yield new_key, v
    elif isinstance(obj, list):
        for idx, it in enumerate(obj):
            new_key = f"{parent_key}[{idx}]" if parent_key else f"[{idx}]"
            if isinstance(it, (dict, list)):
                yield from _iter_scalars_in_obj(it, parent_key=new_key)
            else:
                yield new_key, it

def _extract_details_from_payload(payload):
    """
    Normalize to list of dict rows that look like detail records.
    """
    if payload is None:
        return []
    if isinstance(payload, list):
        return [p for p in payload if isinstance(p, dict)]
    if isinstance(payload, dict):
        for k in ("details","results","data","entries","list","people","items"):
            v = payload.get(k)
            if isinstance(v, list):
                return [x for x in v if isinstance(x, dict)]
        # single-record-like payload (date-summary) -> return as single element to allow higher-level scanner to inspect fields
        if any(k in payload for k in ("date","Employee","Contractor","total","day")) and len(payload.keys()) <= 40:
            return [payload]
    return []

# ----------------------------
# ID matching helpers
# ----------------------------
def _match_candidate_to_employees(candidate_raw, orig_ids_set, orig_ids_list, digits_map):
    """
    Try many heuristics to map candidate_raw to one of the orig_ids_list.
    Returns matched_orig_id or None.
    """
    if candidate_raw is None:
        return None
    cand_str = str(candidate_raw).strip()
    if not cand_str:
        return None

    # direct exact match
    if cand_str in orig_ids_set:
        return cand_str

    # direct case-insensitive match
    for o in orig_ids_list:
        if isinstance(o, str) and cand_str.lower() == o.lower():
            return o

    # numeric transformations
    cand_digits = _digits_only(cand_str)
    if cand_digits:
        cand_nolead = cand_digits.lstrip('0') or cand_digits
        # direct digits match to original ids
        for o in orig_ids_list:
            if not isinstance(o, str):
                continue
            if o == cand_digits or o == cand_nolead:
                return o
            od = _digits_only(o)
            if od == cand_digits or od == cand_nolead:
                return o
        # numeric equality by int
        try:
            ci = int(cand_nolead)
            for o in orig_ids_list:
                try:
                    od = _digits_only(o)
                    if not od:
                        continue
                    oi = int(od.lstrip('0') or od)
                    if oi == ci:
                        return o
                except Exception:
                    continue
        except Exception:
            pass

        # last-n digits heuristics (conservative)
        if len(cand_digits) >= 3:
            for n in (6, 4):
                suf = cand_digits[-n:]
                if not suf:
                    continue
                for o in orig_ids_list:
                    od = _digits_only(o)
                    if od and od.endswith(suf):
                        return o

    # strip common prefixes and retry
    up = cand_str.upper()
    for pref in ("W", "IPASS", "IPASSID", "IPass"):
        if up.startswith(pref):
            stripped = cand_str[len(pref):]
            m = _match_candidate_to_employees(stripped, orig_ids_set, orig_ids_list, digits_map)
            if m:
                return m

    # fallback: find numeric substrings inside string and try mapping
    for match in re.finditer(r'(\d{3,})', cand_str):
        ssub = match.group(1)
        m = _match_candidate_to_employees(ssub, orig_ids_set, orig_ids_list, digits_map)
        if m:
            return m

    return None

# ----------------------------
# Region history scanning -> build presence map
# ----------------------------
def _fetch_presence_from_region_histories(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None, preloaded_entries: Optional[List[dict]] = None):
    """
    Scans preloaded_entries (or global cache) and returns presence mapping: {employee_id -> {date: 0/1}}
    """
    presence = {eid: {} for eid in employee_ids}
    if not employee_ids:
        return presence

    if preloaded_entries is None:
        global REGION_HISTORY_CACHE
        preloaded_entries = REGION_HISTORY_CACHE or []

    if not preloaded_entries:
        logger.info("[region_history] no preloaded region history entries to scan")
    else:
        orig_ids_list = [str(e).strip() for e in employee_ids]
        orig_ids_set = set(orig_ids_list)
        digits_map = {e: _digits_only(e) for e in orig_ids_list}

        scanned = 0
        matched = 0
        examples_matched = []

        for entry in preloaded_entries:
            detail_rows = []
            if isinstance(entry, dict):
                # prefer explicit lists
                for key in ("details","people","items","list","results","entries","data"):
                    if key in entry and isinstance(entry.get(key), list):
                        detail_rows = [r for r in entry.get(key) if isinstance(r, dict)]
                        break
                if not detail_rows:
                    # treat entry itself as a candidate detail row if it has plausible fields (timestamp or id-like)
                    candidate_keys = set(ID_FIELD_CANDIDATES + CARD_FIELD_CANDIDATES + ["date","timestamp","time","SwipeDate","LocaleMessageTime","day"])
                    if any(k in entry for k in candidate_keys):
                        detail_rows = [entry]
                    else:
                        # try extract via helper (covers nested payload shapes)
                        detail_rows = _extract_details_from_payload(entry) or []
            else:
                continue

            scanned += len(detail_rows)
            for d in detail_rows:
                try:
                    # find a timestamp / date for row
                    ts = None
                    # common keys first
                    for tkey in ("LocaleMessageTime","LocaleMessageDateTime","SwipeDate","SwipeTime","timestamp","time","DateTime","date","swipeDate","day"):
                        if tkey in d and d.get(tkey):
                            ts = d.get(tkey)
                            break

                    # fallback: scan scalar values in row for iso-like date
                    if ts is None:
                        for k, v in _iter_scalars_in_obj(d):
                            if v is None:
                                continue
                            try:
                                s = str(v)
                            except Exception:
                                continue
                            # quick heuristic
                            if re.search(r'\d{4}-\d{2}-\d{2}', s) or re.search(r'\d{2}/\d{2}/\d{4}', s):
                                ts = s
                                break

                    if ts is None:
                        # skip row (no date info)
                        continue

                    # parse timestamp into date (robust)
                    t = None
                    if isinstance(ts, (int, float)):
                        try:
                            t = datetime.fromtimestamp(int(ts))
                        except Exception:
                            try:
                                t = datetime.utcfromtimestamp(int(ts) / 1000.0)
                            except Exception:
                                t = None
                    elif isinstance(ts, str):
                        s = ts.strip()
                        if s == "":
                            t = None
                        else:
                            parsed = None
                            # try dateutil if available (best)
                            try:
                                from dateutil import parser as _parser
                                parsed = _parser.parse(s)
                            except Exception:
                                parsed = None
                            if parsed:
                                try:
                                    if parsed.tzinfo is not None:
                                        parsed = parsed.astimezone(tz=None).replace(tzinfo=None)
                                except Exception:
                                    pass
                                t = parsed
                            else:
                                # try common formats
                                fmts = [
                                    "%Y-%m-%dT%H:%M:%S.%fZ",
                                    "%Y-%m-%dT%H:%M:%S.%f",
                                    "%Y-%m-%dT%H:%M:%S",
                                    "%Y-%m-%d %H:%M:%S",
                                    "%Y-%m-%d",
                                    "%d/%m/%Y %H:%M:%S",
                                    "%d/%m/%Y"
                                ]
                                for f in fmts:
                                    try:
                                        t = datetime.strptime(s, f)
                                        break
                                    except Exception:
                                        t = None
                    if t is None:
                        continue

                    dt = t.date()
                    if dt < start_date or dt > end_date:
                        continue

                    # partition filter (if provided)
                    if partition_filter:
                        part_values = []
                        for k in ("PartitionNameFriendly","PartitionName","PrimaryLocation","partition","location","Partition","Location","Site","PartitionName1","PartitionName2"):
                            v = d.get(k)
                            if v:
                                part_values.append(str(v))
                        if part_values:
                            ok = any(part_filter_match(part, partition_filter) for part in part_values)
                            if not ok:
                                continue
                        else:
                            # no partition info -> skip when filter exists
                            continue

                    # attempt to match explicit id keys first
                    matched_key = None
                    for k in ID_FIELD_CANDIDATES:
                        if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                            m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                            if m:
                                matched_key = m
                                break

                    # try card fields
                    if not matched_key:
                        for k in CARD_FIELD_CANDIDATES:
                            if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                                m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break

                    # deep-scan scalars for numeric substrings and name fields
                    if not matched_key:
                        for key_path, val in _iter_scalars_in_obj(d):
                            if val is None:
                                continue
                            sval = str(val)
                            # numeric substring preference
                            if re.search(r'\d{3,}', sval):
                                m = _match_candidate_to_employees(sval, orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break
                        # check name fields that may contain last name -> (left disabled by default; can be enabled later)
                        # (conservative) if matched_key still None we skip name mapping to avoid false positives

                    if matched_key:
                        matched += 1
                        # coerce matched_key to exact string from orig list
                        matched_key = next((o for o in orig_ids_list if str(o).strip() == str(matched_key).strip()), str(matched_key).strip())
                        presence.setdefault(matched_key, {})
                        presence[matched_key][dt] = 1
                        if len(examples_matched) < 10:
                            examples_matched.append({"matched": matched_key, "date": dt.isoformat(), "sample_row_keys": list(d.keys())[:8]})
                except Exception:
                    continue

        logger.info("[region_history] scanned %d detail rows from preloaded entries; matched %d presence entries", scanned, matched)
        if examples_matched:
            logger.debug("[region_history] example matches (up to 10): %s", examples_matched)

    # fill zeros for any missing date
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            presence.setdefault(eid, {})
            if cur not in presence[eid]:
                presence[eid][cur] = 0
        cur = cur + timedelta(days=1)

    return presence

def part_filter_match(src_val, partition_filter):
    try:
        if not src_val:
            return False
        return partition_filter.strip().lower() in str(src_val).strip().lower()
    except Exception:
        return False

# ----------------------------
# DB / combined fetch
# ----------------------------
def _fetch_presence_for_employees(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None):
    """
    1) chunked DB IN queries (AttendanceSummary)
    2) fallback broad DB query
    3) fallback region history cache scan (prefetch_region_history)
    """
    if not employee_ids:
        return {}

    orig_ids = [str(e).strip() for e in employee_ids]
    norm_id_set = set([s for s in orig_ids if s])
    result = {eid: {} for eid in orig_ids}

    # 1) chunked DB fetch
    rows = []
    chunk_size = 500
    try:
        with SessionLocal() as db:
            for i in range(0, len(orig_ids), chunk_size):
                chunk = orig_ids[i:i+chunk_size]
                try:
                    q = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date,
                        AttendanceSummary.employee_id.in_(chunk)
                    )
                    rows_chunk = q.all()
                    if rows_chunk:
                        rows.extend(rows_chunk)
                except Exception:
                    logger.exception("chunked query failed (continuing)")
                    continue

            # fallback broad query if none found
            if not rows:
                try:
                    rows = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date
                    ).all()
                    logger.info("[presence_fetch] fallback broad DB query returned %d rows for %s -> %s", len(rows), start_date, end_date)
                except Exception:
                    logger.exception("fallback broad DB query failed")
                    rows = []
    except Exception:
        logger.exception("DB session error in _fetch_presence_for_employees")
        rows = []

    # map DB rows to provided employee ids
    for r in rows:
        try:
            raw = r.employee_id
            if raw is None:
                continue
            db_key = str(raw).strip()
            match_key = None
            if db_key in norm_id_set:
                match_key = db_key
            else:
                digits = _digits_only(db_key)
                if digits:
                    cand = digits.lstrip('0') or digits
                    if cand in norm_id_set:
                        match_key = cand
                if match_key is None:
                    for o in orig_ids:
                        if o == db_key or o.lstrip('0') == db_key or db_key.lstrip('0') == o:
                            match_key = o
                            break
            if not match_key:
                continue
            d = r.date
            present = 0
            try:
                present = int(r.presence_count or 0)
            except Exception:
                present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
            result.setdefault(match_key, {})
            prev = result[match_key].get(d, 0)
            result[match_key][d] = 1 if (prev == 1 or present > 0) else 0
        except Exception:
            continue

    # fill zeros
    cur = start_date
    while cur <= end_date:
        for eid in orig_ids:
            result.setdefault(eid, {})
            if cur not in result[eid]:
                result[eid][cur] = 0
        cur = cur + timedelta(days=1)

    db_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] DB-derived presence found for %d/%d employees", db_positive, len(orig_ids))

    # fallback to region history if needed
    if db_positive == 0 or db_positive < max(10, int(0.1 * len(orig_ids))):
        try:
            logger.info("[presence_fetch] DB coverage low (%d/%d) - trying region occupancy history fallback", db_positive, len(orig_ids))
            # ensure cache is populated
            prefetch_region_history()
            region_presence = _fetch_presence_from_region_histories(orig_ids, start_date, end_date, partition_filter=partition_filter)
            for eid in orig_ids:
                rp = region_presence.get(eid, {})
                for d, v in rp.items():
                    if v and result.setdefault(eid, {}).get(d, 0) == 0:
                        result[eid][d] = 1
        except Exception:
            logger.exception("region history fallback failed")

    final_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] final presence coverage: %d/%d employees have at least one positive day", final_positive, len(orig_ids))
    return result

# ----------------------------
# Main compare function (public)
# ----------------------------
def compare_ccure_vs_sheets(
    mode: str = "full",
    stats_detail: str = "ActiveProfiles",
    limit_list: int = 200,
    export: bool = False,
    region_filter: Optional[str] = None,
    location_city: Optional[str] = None,
    location_state: Optional[str] = None,
    location_description: Optional[str] = None,
    week_ref_date: Optional[str] = None
):
    # compute week window
    if week_ref_date:
        monday, friday = _week_monday_and_friday(date.fromisoformat(week_ref_date))
    else:
        monday, friday = _week_monday_and_friday(date.today())

    try:
        df = load_active_employees_dataframe()
    except Exception as e:
        logger.exception("Failed to load active employees sheet")
        return {"error": f"active sheet load failed: {e}"}

    rf = region_filter.strip().lower() if region_filter else None
    lc = location_city.strip().lower() if location_city else None
    ls = location_state.strip().lower() if location_state else None
    ld = location_description.strip().lower() if location_description else None

    sel = df.copy()
    if rf:
        sel = sel[sel["region_code"].fillna("").str.strip().str.lower() == rf]
    if lc:
        sel = sel[sel["location_city"].fillna("").str.strip().str.lower() == lc]
    if ls:
        sel = sel[sel["location_state"].fillna("").str.strip().str.lower() == ls]
    if ld:
        sel = sel[sel["location_desc"].fillna("").str.strip().str.lower() == ld]

    total_active = len(sel)
    employee_ids = sel["employee_id"].astype(str).str.strip().tolist()

    presence_map = _fetch_presence_for_employees(employee_ids, monday, friday, partition_filter=location_city)

    # compute today count
    today = date.today()
    today_count = 0
    for eid in employee_ids:
        pm = presence_map.get(eid, {})
        if pm.get(today, 0) > 0:
            today_count += 1
        else:
            # fallback DB check
            try:
                with SessionLocal() as db:
                    row = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == eid, AttendanceSummary.date == today).first()
                    if row and getattr(row, "presence_count", 0) > 0:
                        today_count += 1
                        continue
                    digits = _digits_only(eid)
                    if digits:
                        cand = digits.lstrip('0') or digits
                        row2 = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == cand, AttendanceSummary.date == today).first()
                        if row2 and getattr(row2, "presence_count", 0) > 0:
                            today_count += 1
            except Exception:
                continue

    today_pct = round((today_count / float(total_active)) * 100.0, 2) if total_active > 0 else None

    sel["on_leave"] = sel["current_status"].apply(lambda x: _maybe_mark_on_leave(x))
    leave_count = int(sel["on_leave"].sum())
    sel["employee_type_norm"] = sel["employee_type"].fillna("").str.strip().str.lower()
    type_counts = sel["employee_type_norm"].value_counts().to_dict()

    regular_df = sel[sel["employee_type_norm"].str.contains("regular", na=False)]
    regular_ids = regular_df["employee_id"].astype(str).str.strip().tolist()

    regular_presence = {}
    for eid in regular_ids:
        week_map = presence_map.get(eid, {})
        days_present = sum(1 for d, v in week_map.items() if v and (monday <= d <= friday))
        days_present = int(days_present)
        regular_presence[eid] = {
            "days_present": days_present,
            "on_leave": bool(sel[sel["employee_id"] == eid]["on_leave"].any()),
            "full_name": sel[sel["employee_id"] == eid]["full_name"].iloc[0] if not sel[sel["employee_id"] == eid].empty else None
        }

    present_5_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 5)
    present_3_or_more_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 3)
    present_less_than_3_count = sum(1 for v in regular_presence.values() if v["days_present"] < 3)
    present_only_1_count = sum(1 for v in regular_presence.values() if v["days_present"] == 1)

    present_5_list = []
    present_3_list = []
    defaulters_list = []

    for eid, info in regular_presence.items():
        entry = {
            "employee_id": eid,
            "full_name": info["full_name"],
            "days_present": info["days_present"],
            "on_leave": info["on_leave"]
        }
        if info["days_present"] >= 5:
            present_5_list.append(entry)
        if info["days_present"] >= 3:
            present_3_list.append(entry)
        if info["days_present"] < 3:
            defaulters_list.append(entry)

    present_5_list = sorted(present_5_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    present_3_list = sorted(present_3_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    defaulters_list = sorted(defaulters_list, key=lambda x: (x["days_present"], x["on_leave"], x["full_name"] or ""))[:limit_list]

    summary = {
        "filters": {
            "region": region_filter,
            "location_city": location_city,
            "location_state": location_state,
            "location_description": location_description,
            "week_monday": monday.isoformat(),
            "week_friday": friday.isoformat()
        },
        "counts": {
            "total_active_in_sheet": total_active,
            "today_headcount_from_summary": today_count,
            "today_headcount_pct_vs_sheet": today_pct,
            "on_leave_count_in_sheet": leave_count,
            "employee_type_counts": type_counts
        },
        "regular_attendance_summary": {
            "regular_total": len(regular_ids),
            "present_5_day_count": present_5_count,
            "present_3_or_more_count": present_3_or_more_count,
            "present_less_than_3_count": present_less_than_3_count,
            "present_only_1_day_count": present_only_1_count
        }
    }

    details = {
        "present_5_days": present_5_list,
        "present_3_or_more_days": present_3_list,
        "defaulters_less_than_3_days": defaulters_list
    }

    report_path = None
    if export:
        try:
            report_name = f"attendance_compare_{(region_filter or 'all')}_{(location_city or 'all')}_{uuid.uuid4().hex[:8]}.xlsx"
            report_file = OUTPUT_DIR / report_name
            with pd.ExcelWriter(report_file, engine="openpyxl") as writer:
                sel_df_for_export = sel.copy()
                sel_df_for_export["raw_row_str"] = sel_df_for_export["raw_row"].apply(lambda r: str(r) if r is not None else "")
                sel_df_for_export.to_excel(writer, sheet_name="active_sheet_selection", index=False)
                pd.DataFrame([summary["counts"]]).to_excel(writer, sheet_name="summary_counts", index=False)
                pd.DataFrame([summary["regular_attendance_summary"]]).to_excel(writer, sheet_name="regular_summary", index=False)
                pd.DataFrame(details["present_5_days"]).to_excel(writer, sheet_name="present_5_days", index=False)
                pd.DataFrame(details["present_3_or_more_days"]).to_excel(writer, sheet_name="present_3_plus", index=False)
                pd.DataFrame(details["defaulters_less_than_3_days"]).to_excel(writer, sheet_name="defaulters_lt3", index=False)
            report_path = str(report_file.name)
        except Exception:
            logger.exception("Failed to write export report")
            report_path = None

    out = {
        "mode": mode,
        "stats_detail": stats_detail,
        "summary": summary,
        "details": details
    }
    if report_path:
        out["report_path"] = report_path
    return out


if __name__ == "__main__":
    res = compare_ccure_vs_sheets(region_filter="APAC", location_city="Pune", export=False, limit_list=20)
    import json as _json
    print(_json.dumps(res, indent=2, default=str))








5)#for Active Employee and ACtive Contractor Comparision Report.

# C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service.py
"""
Compare CCURE active lists (employees & contractors) with uploaded Active sheets (from disk).
Provides compare_ccure_vs_sheets(mode='full', stats_detail='ActiveProfiles', limit_list=200, export=False)

When export=True, writes Excel report to OUTPUT_DIR and returns 'report_path'.
"""

from datetime import datetime
import os
import re
import uuid
import logging
import sys
from pathlib import Path
import pandas as pd

logger = logging.getLogger("data_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Settings fallback (matches app.py pattern)
try:
    from settings import OUTPUT_DIR as SETTINGS_OUTPUT_DIR, DATA_DIR as SETTINGS_DATA_DIR, RAW_UPLOAD_DIR as SETTINGS_RAW_UPLOAD_DIR
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    RAW_UPLOAD_DIR = Path(SETTINGS_RAW_UPLOAD_DIR)
except Exception:
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"
    DATA_DIR = Path(__file__).resolve().parent / "data"
    RAW_UPLOAD_DIR = DATA_DIR / "raw_uploads"

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# ccure client helper (optional)
try:
    import ccure_client
except Exception:
    ccure_client = None
    logger.warning("ccure_client not importable; CCURE calls will return None")

# ---------- small normalizers (kept local) ----------
def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

def _make_w_variant(s):
    if s is None:
        return None
    ss = str(s).strip()
    if ss.upper().startswith('W'):
        return ss
    return 'W' + ss

def _numeric_variants(s):
    out = set()
    if s is None:
        return out
    try:
        s = str(s)
        clean = re.sub(r'\D', '', s)
        if clean:
            out.add(clean)
            out.add(clean.lstrip('0') or clean)
            out.add('W' + clean)
    except Exception:
        pass
    return out

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- CCURE fetch helpers ----------
def _fetch_ccure_list(detail_name):
    """
    Uses ccure_client.fetch_all_stats(detail_name) if available, otherwise tries per-page fetch.
    Returns list of dicts or [].
    """
    if ccure_client is None:
        logger.warning("ccure_client missing - cannot fetch CCURE lists")
        return []
    try:
        if hasattr(ccure_client, "fetch_all_stats"):
            res = ccure_client.fetch_all_stats(detail_name, limit=500)
            return res or []
    except Exception:
        logger.exception("fetch_all_stats failed")
    # fallback to paged fetch if available
    try:
        data = []
        page = 1
        limit = 500
        while True:
            if not hasattr(ccure_client, "fetch_stats_page"):
                break
            page_res = ccure_client.fetch_stats_page(detail_name, page=page, limit=limit)
            if not page_res:
                break
            part = page_res.get("data") or []
            if not part:
                break
            data.extend(part)
            total = int(page_res.get("total") or len(data) or 0)
            if len(data) >= total:
                break
            page += 1
            if page > 1000:
                break
        return data
    except Exception:
        logger.exception("per-page fetch failed for %s", detail_name)
        return []

# ---------- helpers to find/load latest sheet from disk ----------
def _find_latest_file_for_kind(kind: str):
    """
    kind == 'employee' or 'contractor'
    Prefer canonical file in DATA_DIR: active_<kind>.*,
    otherwise pick latest in RAW_UPLOAD_DIR that contains the kind token.
    """
    # 1) canonical in DATA_DIR
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_{kind}{ext}"
        if p.exists():
            return p

    # 2) fallback: find latest raw file with kind token in RAW_UPLOAD_DIR
    try:
        files = [p for p in RAW_UPLOAD_DIR.iterdir() if p.is_file() and kind in p.name.lower() and p.suffix.lower() in (".xlsx", ".xls", ".csv")]
        if not files:
            # last fallback: any active_{kind}.* in RAW_UPLOAD_DIR
            files = [p for p in RAW_UPLOAD_DIR.iterdir() if p.is_file() and f"active_{kind}" in p.name.lower()]
        if not files:
            return None
        files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        return files[0]
    except Exception:
        return None

def _read_table(path: Path):
    try:
        if path.suffix.lower() == ".csv":
            df = pd.read_csv(path, dtype=str)
        else:
            df = pd.read_excel(path, sheet_name=0, dtype=str)
        df.columns = [str(c).strip() for c in df.columns]
        return df.fillna("")
    except Exception:
        logger.exception("Failed to read file %s", path)
        return pd.DataFrame()

def _first_present_from_row(row, candidates):
    for c in candidates:
        if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
            return row[c]
    for k in row.index:
        for c in candidates:
            if k.strip().lower() == c.strip().lower():
                val = row[k]
                if pd.notna(val) and str(val).strip() != "":
                    return val
    return None

# ---------- disk-based loaders to replace DB loaders ----------
def _load_active_employees_disk():
    """
    Return (set of normalized employee_ids, dict mapping id -> row sample, total_rows)
    Reads the latest employee sheet from data/.
    """
    path = _find_latest_file_for_kind("employee")
    if not path:
        return set(), {}, 0
    df = _read_table(path)
    ids = set()
    mapping = {}
    id_cols = ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id','Employee_Id']
    name_cols = ['Full Name','FullName','EmpName','Name','First Name','FirstName','Last Name']
    location_cols = ['Location City','Location','Location Description','City']
    status_cols = ['Current Status','Status','Employee Status','Employee_Status','Status (Current)','CurrentStatus']

    for _, row in df.iterrows():
        raw_row = row.to_dict()
        emp_id = _first_present_from_row(row, id_cols)
        if emp_id is None or str(emp_id).strip() == "":
            continue
        emp_id = str(emp_id).strip()
        ids.add(emp_id)
        full_name = _first_present_from_row(row, name_cols) or ""
        mapping[emp_id] = {
            "employee_id": emp_id,
            "full_name": full_name,
            "location_city": _first_present_from_row(row, location_cols),
            "status": _first_present_from_row(row, status_cols),
            "raw": raw_row
        }
    return ids, mapping, len(df)

def _load_active_contractors_disk():
    """
    Return (set of candidate contractor ids, mapping, total_rows)
    Reads latest contractor sheet from data/.
    """
    path = _find_latest_file_for_kind("contractor")
    if not path:
        return set(), {}, 0
    df = _read_table(path)
    ids = set()
    mapping = {}
    wsid_cols = ['Worker System Id','Worker System ID','Worker ID','WorkerSystemId','WorkerId']
    ipass_cols = ['iPass ID','"W" iPass ID','IPassID','iPassID','Ipass ID','IpassID']
    name_cols = ['Full Name','FullName','Name']
    vendor_cols = ['Vendor Company Name','Vendor']

    for _, row in df.iterrows():
        raw_row = row.to_dict()
        wsid = _first_present_from_row(row, wsid_cols)
        ipass = _first_present_from_row(row, ipass_cols)
        full_name = _first_present_from_row(row, name_cols)
        if wsid:
            wsid = str(wsid).strip()
            ids.add(wsid)
            mapping[wsid] = {"worker_system_id": wsid, "full_name": full_name, "vendor": _first_present_from_row(row, vendor_cols), "raw": raw_row}
        if ipass:
            ipass = str(ipass).strip()
            ids.add(ipass)
            mapping[ipass] = {"ipass_id": ipass, "full_name": full_name, "vendor": _first_present_from_row(row, vendor_cols), "raw": raw_row}
            wvar = _make_w_variant(ipass)
            ids.add(wvar)
            mapping[wvar] = {"ipass_id": ipass, "w_ipass": wvar, "full_name": full_name, "raw": raw_row}
        for cand in (wsid, ipass):
            if cand:
                for v in _numeric_variants(cand):
                    ids.add(v)
                    if v not in mapping:
                        mapping[v] = {"derived_id": v, "full_name": full_name, "raw": raw_row}
    return ids, mapping, len(df)

# ---------- helpers to match ccure -> disk lists (same logic as before) ----------
def _employee_matches_disk(cid, emp_ids_disk, emp_map_disk, ccure_row):
    if cid in emp_ids_disk:
        return True
    for v in _numeric_variants(cid):
        if v in emp_ids_disk:
            return True
    try:
        ci = _safe_int(cid)
        if ci is not None:
            for dbk in emp_ids_disk:
                if _safe_int(dbk) == ci:
                    return True
    except Exception:
        pass
    card_cid = _normalize_card_like(cid)
    if card_cid:
        for dbk in emp_ids_disk:
            if _normalize_card_like(dbk) == card_cid:
                return True
    try:
        name = None
        if isinstance(ccure_row, dict):
            name = ccure_row.get("EmpName") or ccure_row.get("FullName") or ccure_row.get("Name")
        normname = _normalize_name(name)
        if normname:
            for dbk, dbv in emp_map_disk.items():
                if _normalize_name(dbv.get("full_name")) == normname:
                    return True
    except Exception:
        pass
    return False

def _contractor_matches_disk(cid, contr_ids_disk, contr_map_disk, ccure_row):
    if cid in contr_ids_disk:
        return True
    for v in _numeric_variants(cid):
        if v in contr_ids_disk:
            return True
    try:
        ci = _safe_int(cid)
        if ci is not None:
            for dbk in contr_ids_disk:
                if _safe_int(dbk) == ci:
                    return True
    except Exception:
        pass
    card_cid = _normalize_card_like(cid)
    if card_cid:
        for dbk in contr_ids_disk:
            if _normalize_card_like(dbk) == card_cid:
                return True
    try:
        name = None
        if isinstance(ccure_row, dict):
            name = ccure_row.get("EmpName") or ccure_row.get("FullName") or ccure_row.get("Name")
        normname = _normalize_name(name)
        if normname:
            for dbk, dbv in contr_map_disk.items():
                if _normalize_name(dbv.get("full_name")) == normname:
                    return True
    except Exception:
        pass
    return False

# ---------- core compare function ----------
def compare_ccure_vs_sheets(mode="full", stats_detail="ActiveProfiles", limit_list=200, export=False):
    """
    Main public function used by /ccure/compare.
    Reads latest uploaded sheets from disk instead of DB tables.
    """
    result = {
        "ccure_active_employees_count": None,
        "ccure_active_contractors_count": None,
        "active_sheet_employee_count": None,
        "active_sheet_contractor_count": None,
        "missing_employees_count": None,
        "missing_contractors_count": None,
        "missing_employees_sample": [],
        "missing_contractors_sample": [],
        "report_path": None
    }

    # 1) fetch CCURE lists
    ccure_emps = _fetch_ccure_list("ActiveEmployees")
    ccure_contrs = _fetch_ccure_list("ActiveContractors")

    result["ccure_active_employees_count"] = len(ccure_emps)
    result["ccure_active_contractors_count"] = len(ccure_contrs)

    # 2) load uploaded sheets from disk (preferred) â returns (ids_set, mapping, total_rows)
    emp_ids_disk, emp_map_disk, emp_total_rows = _load_active_employees_disk()
    contr_ids_disk, contr_map_disk, contr_total_rows = _load_active_contractors_disk()

    result["active_sheet_employee_count"] = int(emp_total_rows)
    result["active_sheet_contractor_count"] = int(contr_total_rows)

    # 3) build ccure id sets for employees
    ccure_emp_id_set = set()
    ccure_emp_rows_by_id = {}
    for row in ccure_emps:
        try:
            eid = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("EmpID") or row.get("Employee Id"))
            if not eid:
                eid = _normalize_card_like(row.get("CardNumber") or row.get("iPass ID") or row.get("IPassID") or row.get("Card"))
            if not eid:
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    eid = f"name::{fname}"
            if eid:
                ccure_emp_id_set.add(eid)
                ccure_emp_rows_by_id[eid] = row
        except Exception:
            continue

    # 4) employees missing = ccure_emp_id_set - emp_ids_disk (but consider numeric variants, int equality, card-like, name match)
    expanded_emp_disk_ids = set(emp_ids_disk)
    for v in list(emp_ids_disk):
        for nv in _numeric_variants(v):
            expanded_emp_disk_ids.add(nv)

    missing_emp_ids = []
    for cid in ccure_emp_id_set:
        try:
            if str(cid).startswith("name::"):
                name = cid.split("::", 1)[1]
                found = False
                for dbk, dbv in emp_map_disk.items():
                    n = _normalize_name(dbv.get("full_name"))
                    if n and n == name:
                        found = True
                        break
                if not found:
                    missing_emp_ids.append(cid)
                continue

            ccure_row = ccure_emp_rows_by_id.get(cid) or {}
            if _employee_matches_disk(cid, expanded_emp_disk_ids, emp_map_disk, ccure_row):
                continue

            found = False
            for v in _numeric_variants(cid):
                if v in expanded_emp_disk_ids:
                    found = True
                    break
            if not found:
                missing_emp_ids.append(cid)
        except Exception:
            missing_emp_ids.append(cid)

    result["missing_employees_count"] = len(missing_emp_ids)
    samp_emp = []
    for mid in missing_emp_ids[:limit_list]:
        r = ccure_emp_rows_by_id.get(mid) or {}

        # extract manager/profile/status from raw if present
        manager_name = r.get("Manager_Name") or r.get("ManagerName") or r.get("Manager") or r.get("Manager_WU_ID")
        profile_disabled = r.get("Profile_Disabled") if "Profile_Disabled" in r else r.get("profile_disabled") if "profile_disabled" in r else r.get("ProfileDisabled") if "ProfileDisabled" in r else None
        employee_status = r.get("Employee_Status") or r.get("Employee Status") or r.get("Status") or r.get("employee_status")

        # ensure string conversion for boolean-like values
        if isinstance(profile_disabled, bool):
            profile_disabled = str(profile_disabled)

        # vendorCompany doesn't apply to employees â keep blank
        vendor_company = r.get("Vendor Company Name") or r.get("Vendor") or r.get("vendor") or ""

        # build sample row with both old and new key names for compatibility
        samp_emp.append({
            "ccure_key": mid,
            # old keys (kept for compatibility)
            "EmployeeID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "PersonnelType": r.get("PersonnelType"),
            # "VendorCompany": vendor_company,
            "Manager_Name": manager_name,
            "Profile_Disabled": profile_disabled,
            "Employee_Status": employee_status,
            # new/canonical lowerCamel keys requested
            "employee_Id": r.get("EmployeeID") or r.get("employee_id") or r.get("Employee Id"),
            "empName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "personnelType": r.get("PersonnelType"),
            "vendorCompany": vendor_company,
            "managerName": manager_name,
            "profileDisabled": profile_disabled,
            "employeeStatus": employee_status,
            "raw": r
        })
    result["missing_employees_sample"] = samp_emp

    # 5) contractors
    ccure_contr_id_set = set()
    ccure_contr_rows_by_id = {}
    for row in ccure_contrs:
        try:
            cand_ids = []
            e1 = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("Employee Id"))
            if e1:
                cand_ids.append(e1)
            ip = _normalize_employee_key(row.get("IPassID") or row.get("iPass ID") or row.get("iPass") or row.get("IPASSID"))
            if ip:
                cand_ids.append(ip)
                cand_ids.append(_make_w_variant(ip))
            cardlike = _normalize_card_like(row.get("CardNumber") or row.get("card_number") or row.get("Badge") or row.get("BadgeNo"))
            if cardlike:
                cand_ids.append(cardlike)
                cand_ids.extend(list(_numeric_variants(cardlike)))
            if not cand_ids:
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    cand_ids.append(f"name::{fname}")
            for cid in cand_ids:
                if cid:
                    ccure_contr_id_set.add(cid)
                    ccure_contr_rows_by_id[cid] = row
            if not cand_ids:
                key = f"unknown::{uuid.uuid4().hex[:8]}"
                ccure_contr_id_set.add(key)
                ccure_contr_rows_by_id[key] = row
        except Exception:
            continue

    expanded_contr_disk_ids = set(contr_ids_disk)
    for v in list(contr_ids_disk):
        for nv in _numeric_variants(v):
            expanded_contr_disk_ids.add(nv)

    missing_contr_ids = []
    for cid in ccure_contr_id_set:
        try:
            if str(cid).startswith("name::"):
                name = cid.split("::", 1)[1]
                found = False
                for dbk, dbv in contr_map_disk.items():
                    n = _normalize_name(dbv.get("full_name"))
                    if n and n == name:
                        found = True
                        break
                if not found:
                    missing_contr_ids.append(cid)
                continue

            ccure_row = ccure_contr_rows_by_id.get(cid) or {}
            if _contractor_matches_disk(cid, expanded_contr_disk_ids, contr_map_disk, ccure_row):
                continue

            found = False
            for v in _numeric_variants(cid):
                if v in expanded_contr_disk_ids:
                    found = True
                    break
            if not found:
                missing_contr_ids.append(cid)
        except Exception:
            missing_contr_ids.append(cid)

    result["missing_contractors_count"] = len(missing_contr_ids)
    samp_contr = []
    for mid in missing_contr_ids[:limit_list]:
        r = ccure_contr_rows_by_id.get(mid) or {}

        # vendor/company
        vendor_company = r.get("Vendor Company Name") or r.get("Vendor") or r.get("vendor") or ""

        # personnel, manager, profile, status extraction
        personnel_type = r.get("PersonnelType") or r.get("Personnel_Type") or r.get("Personnel Type") or None
        manager_name = r.get("Manager_Name") or r.get("ManagerName") or r.get("Manager") or r.get("Manager_WU_ID")
        profile_disabled = r.get("Profile_Disabled") if "Profile_Disabled" in r else r.get("profile_disabled") if "profile_disabled" in r else None
        employee_status = r.get("Employee_Status") or r.get("Employee Status") or r.get("Status") or r.get("employee_status")

        if isinstance(profile_disabled, bool):
            profile_disabled = str(profile_disabled)

        samp_contr.append({
            "ccure_key": mid,
            # old keys kept
            "Employee_ID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "VendorCompany": vendor_company,
            "PersonnelType": personnel_type,
            "Manager_Name": manager_name,
            "Profile_Disabled": profile_disabled,
            "Employee_Status": employee_status,
            # new requested lowerCamel keys
            "employeeId": r.get("EmployeeID") or r.get("employee_id") or r.get("Employee Id"),
            "empName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "vendorCompany": vendor_company,
            "personnelType": personnel_type,
            "managerName": manager_name,
            "profileDisabled": profile_disabled,
            "employeeStatus": employee_status,
            "raw": r
        })
    result["missing_contractors_sample"] = samp_contr

    # 6) optionally export report
    if export:
        try:
            OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
            ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            fname = f"missing_vs_ccure_{ts}.xlsx"
            fullpath = OUTPUT_DIR / fname

            # Canonical column set requested (lowerCamel)
            columns = ["ccure_key", "employee_Id", "empName", "personnelType", "managerName", "profileDisabled", "employeeStatus"]

            # Build DataFrames and ensure columns order (fill missing with empty string)
            if samp_emp:
                df_emp = pd.DataFrame(samp_emp)
                # reindex to requested columns; missing columns will be added with NaN
                df_emp = df_emp.reindex(columns=columns).fillna("")
            else:
                df_emp = pd.DataFrame(columns=columns)

            if samp_contr:
                df_con = pd.DataFrame(samp_contr)
                df_con = df_con.reindex(columns=columns).fillna("")
            else:
                df_con = pd.DataFrame(columns=columns)

            try:
                with pd.ExcelWriter(fullpath, engine="openpyxl") as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
            except Exception:
                # fallback to default engine
                with pd.ExcelWriter(fullpath) as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
        except Exception:
            logger.exception("Failed to export report")
            result["report_path"] = None

    return result






#export 

def export_uploaded_sheets():
    """
    Create a single xlsx workbook with:
      - Sheet "Employee" -> contents of latest canonical employee sheet (if present)
      - Sheet "Contractor" -> contents of latest canonical contractor sheet (if present)
    Writes file into OUTPUT_DIR and returns the filename (not full path).
    """
    try:
        emp_path = _find_latest_file_for_kind("employee")
        contr_path = _find_latest_file_for_kind("contractor")

        # Ensure OUTPUT_DIR exists
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
        fname = f"uploaded_sheets_{ts}.xlsx"
        fullpath = OUTPUT_DIR / fname

        # Read tables (if present) or create empty dataframe placeholders
        if emp_path and emp_path.exists():
            df_emp = _read_table(emp_path)
        else:
            df_emp = pd.DataFrame()

        if contr_path and contr_path.exists():
            df_con = _read_table(contr_path)
        else:
            df_con = pd.DataFrame()

        # Write to one workbook (try openpyxl if available)
        try:
            with pd.ExcelWriter(fullpath, engine="openpyxl") as writer:
                df_emp.to_excel(writer, sheet_name="Employee", index=False)
                df_con.to_excel(writer, sheet_name="Contractor", index=False)
        except Exception:
            # fallback to default engine
            with pd.ExcelWriter(fullpath) as writer:
                df_emp.to_excel(writer, sheet_name="Employee", index=False)
                df_con.to_excel(writer, sheet_name="Contractor", index=False)

        return fname
    except Exception:
        logger.exception("export_uploaded_sheets failed")
        return None


# Expose public function name expected by app.py
__all__ =  ["compare_ccure_vs_sheets", "compare_ccure_vs_sheets", "export_uploaded_sheets"]  # keep backwards compatibility
# End of data_compare_service.py




6)


# region_clients.py
"""
HTTP helpers for region occupancy endpoints.
Returns:
 - fetch_all_regions() -> list of {"region": <name>, "count": <int>}
 - fetch_all_details(timeout=...) -> list of person-detail dicts (where available)
 - fetch_all_history(timeout=...) -> list of date-or-detail dicts (history endpoints)

Drop-in replacement for your region_clients.py.
"""

import requests
from requests.exceptions import RequestException
from datetime import datetime
import logging
import time
import sys

logger = logging.getLogger("region_clients")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# endpoints (edit if your hosts differ)
endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/live-summary",
    "emea":  "http://10.199.22.57:3007/api/occupancy/live-summary",
    "laca":  "http://10.199.22.57:4000/api/occupancy/live-summary",
    "apac":  "http://10.199.22.57:3008/api/occupancy/live-summary"
}

history_endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/history",
    "emea":  "http://10.199.22.57:3007/api/occupancy/history",
    "apac":  "http://10.199.22.57:3008/api/occupancy/history",
    "laca":  "http://10.199.22.57:4000/api/occupancy/history"
}

DEFAULT_ATTEMPTS = 3
DEFAULT_BACKOFF = 0.6

def _do_get_with_retries(url, timeout, attempts=DEFAULT_ATTEMPTS, backoff=DEFAULT_BACKOFF):
    last_err = None
    for attempt in range(1, attempts + 1):
        try:
            # use connect,read timeouts tuple for clarity
            r = requests.get(url, timeout=(3, max(5, timeout)))
            r.raise_for_status()
            try:
                return r.json()
            except ValueError:
                try:
                    return {"_raw_text": r.text}
                except Exception:
                    return None
        except RequestException as e:
            last_err = e
            logger.warning(f"[region_clients] attempt {attempt}/{attempts} failed for {url}: {e}")
            if attempt < attempts:
                time.sleep(backoff * attempt)
            continue
    logger.warning(f"[region_clients] all {attempts} attempts failed for {url}: {last_err}")
    return None

def fetch_all_regions(timeout=6):
    results = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            realtime = {}
            if isinstance(data, dict):
                realtime = data.get("realtime", {}) or {}
            total = 0
            for site in realtime.values():
                try:
                    total += int(site.get("total", 0))
                except Exception:
                    pass
            if total == 0 and isinstance(data, dict):
                for k, v in data.items():
                    if isinstance(v, dict) and "total" in v:
                        try:
                            total += int(v.get("total", 0))
                        except Exception:
                            pass
            results.append({"region": region, "count": total})
        except Exception as e:
            logger.exception(f"[region_clients] unexpected error fetching live-summary for {region}: {e}")
            results.append({"region": region, "count": None})
    return results

def _extract_list_candidates(data):
    """
    Helper to find lists that may contain detail rows inside a payload.
    """
    if isinstance(data, list):
        return [x for x in data if isinstance(x, dict)]
    if isinstance(data, dict):
        for k in ("details","people","list","items","results","data","entries"):
            if k in data and isinstance(data[k], list):
                return [x for x in data[k] if isinstance(x, dict)]
        # top-level dict might be single row; return empty here (caller will inspect)
        return []
    return []

def fetch_all_details(timeout=6):
    """
    Attempt to fetch person-level detail records from live-summary endpoints.
    Returns a list of dicts (flattened detail rows). Regions that don't provide details will be skipped.
    """
    all_details = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            details = _extract_list_candidates(data)
            # also attempt to find nested partitions that contain details
            if not details and isinstance(data, dict):
                for v in data.values():
                    if isinstance(v, dict):
                        candidates = _extract_list_candidates(v)
                        if candidates:
                            details.extend(candidates)
            for d in details:
                try:
                    d2 = dict(d)
                    d2["_region"] = region
                    d2["_source_url"] = url
                    all_details.append(d2)
                except Exception:
                    continue
        except Exception as e:
            logger.warning(f"[region_clients] warning: cannot fetch details from {region}@{url}: {e}")
            continue

    # try history endpoints as second chance (they sometimes include per-person rows)
    if not all_details:
        for region, url in history_endpoints.items():
            try:
                data = _do_get_with_retries(url, timeout=timeout) or {}
                details = _extract_list_candidates(data)
                for d in details:
                    try:
                        d2 = dict(d)
                        d2["_region"] = region
                        d2["_source_url"] = url
                        all_details.append(d2)
                    except Exception:
                        continue
            except Exception:
                logger.debug(f"[region_clients] history details fetch for {region} failed", exc_info=True)
                continue

    logger.info("[region_clients] fetched %d detail rows across endpoints", len(all_details))
    return all_details

def fetch_history_for_region(region, timeout=6):
    url = history_endpoints.get(region)
    if not url:
        logger.debug(f"[fetch_history_for_region] no history endpoint for {region}")
        return []
    try:
        data = _do_get_with_retries(url, timeout=timeout) or {}
        summary = []
        if isinstance(data, dict):
            # try common key names
            for key in ("summaryByDate","summary","data","entries","results","details","items"):
                if key in data and isinstance(data.get(key), list):
                    for s in data.get(key):
                        if isinstance(s, dict):
                            s2 = dict(s)
                            s2["_region"] = region
                            s2["_source_url"] = url
                            summary.append(s2)
                    break
            else:
                # maybe single dict date-summary
                if "date" in data or "day" in data:
                    s2 = dict(data)
                    s2["_region"] = region
                    s2["_source_url"] = url
                    summary.append(s2)
        elif isinstance(data, list):
            for s in data:
                if isinstance(s, dict):
                    s2 = dict(s)
                    s2["_region"] = region
                    s2["_source_url"] = url
                    summary.append(s2)
        return summary
    except Exception as e:
        logger.warning(f"[region_clients] cannot fetch history for {region}@{url}: {e}")
        return []

def fetch_all_history(timeout=6):
    all_entries = []
    for region in history_endpoints.keys():
        try:
            entries = fetch_history_for_region(region, timeout=timeout)
            if entries:
                all_entries.extend(entries)
        except Exception:
            logger.exception(f"[region_clients] error aggregating history for {region}")
            continue
    logger.info("[region_clients] fetched %d history entries", len(all_entries))
    return all_entries






7)# C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\ingest_excel.py
import pandas as pd
from datetime import datetime
from sqlalchemy.exc import IntegrityError
from db import SessionLocal, engine
from models import Base, ActiveEmployee, ActiveContractor
from settings import UPLOAD_DIR
import uuid, os

# --- database setup: do NOT run create_all at import time ---
def init_db():
    """
    Create DB tables if they do not exist.
    Call this manually only when you want to initialize/repair the DB:
      python -c "from ingest_excel import init_db; init_db()"
    """
    from db import engine
    from models import Base
    Base.metadata.create_all(bind=engine)

def _first_present(row, candidates):
    for c in candidates:
        v = row.get(c)
        if v is not None and str(v).strip() != "":
            return v
    return None

def ingest_employee_excel(path, uploaded_by="system"):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    # robust mapping keys
    with SessionLocal() as db:
        for _, row in df.iterrows():
            emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id'])
            if emp_id:
                emp_id = str(emp_id).strip()
            if not emp_id:
                # skip rows without an employee id
                continue
            full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
            # robust current_status detection
            status_candidates = ['Current Status','Status','Employee Status','Employee_Status','Status (Current)','CurrentStatus']
            current_status = _first_present(row, status_candidates)
            email = _first_present(row, ["Employee's Email",'Email','Email Address'])
            location_city = _first_present(row, ['Location City','Location','Location Description','City'])
            rec = ActiveEmployee(
                employee_id=emp_id,
                full_name=full_name,
                email=email,
                location_city=location_city,
                location_desc=row.get('Location Description'),
                current_status=current_status,
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)  # upsert
                db.commit()
            except IntegrityError:
                db.rollback()
            except Exception:
                db.rollback()

def ingest_contractor_excel(path):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    with SessionLocal() as db:
        for _, row in df.iterrows():
            wsid = _first_present(row, ['Worker System Id','Worker System ID','Worker ID','WorkerSystemId'])
            if wsid:
                wsid = str(wsid).strip()
            if not wsid:
                continue
            ipass = _first_present(row, ['iPass ID','"W" iPass ID','IPassID','iPassID','Ipass ID'])
            full_name = _first_present(row, ['Full Name','FullName','Name'])
            rec = ActiveContractor(
                worker_system_id=wsid,
                ipass_id=ipass,
                full_name=full_name,
                vendor=_first_present(row, ['Vendor Company Name','Vendor']),
                location=_first_present(row, ['Worker Location','Location']),
                status=_first_present(row, ['Status','Current Status']),
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)
                db.commit()
            except IntegrityError:
                db.rollback()
            except Exception:
                db.rollback()

if __name__ == "__main__":
    # ingestion convenience: read all uploaded files
    for f in os.listdir(UPLOAD_DIR):
        p = UPLOAD_DIR / f
        if 'contractor' in f.lower() or 'contractor' in str(p).lower():
            ingest_contractor_excel(p)
        else:
            ingest_employee_excel(p)
    print("Ingestion completed.")











