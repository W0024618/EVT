I have Update above Duration logic afrer updating above logic Nothing work properly.
AVerage calculation got error Average failed fetch alos Duration Reports (with Compliance & Category)
 duration report not displaying anything it continously Loading ..

Check below each file carerfully alos check each API endpoint and their related file and fix the issue , i need Quick result...



Below is Console Details ..

(.venv) PS C:\Users\W0024618\Desktop\global-page\backend> cd attendance-analytics                                 
(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> python -m uvicorn app:app --host 0.0.0.0 --port 8000
INFO:     Started server process [20288]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:724: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  df = pd.read_sql(sql, conn)
INFO:     127.0.0.1:59693 - "GET /ccure/stream HTTP/1.1" 200 OK
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:807: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:724: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  df = pd.read_sql(sql, conn)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:807: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:724: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  df = pd.read_sql(sql, conn)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:807: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:724: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  df = pd.read_sql(sql, conn)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:807: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:724: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  df = pd.read_sql(sql, conn)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:807: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:724: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  df = pd.read_sql(sql, conn)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:807: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:724: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  df = pd.read_sql(sql, conn)
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py:807: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)





If need inform me will share you another all file alos for more details ...



1)C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py

# app.py (top portion) - minor cleanup (remove duplicate import)
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import shutil
import uuid
import json
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
import re
import asyncio
from typing import Optional, Dict, Any

# NEW: pandas import required for /duration serialization
import pandas as pd

# --- DB / models imports (kept for endpoints that still use DB like headcount/attendance fallback) ---
from db import SessionLocal
from models import LiveSwipe, AttendanceSummary

# --- settings (optional override) ---
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"

import sys  # ensure logging stream available
RAW_UPLOADS_DIR = DATA_DIR / "raw_uploads"
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)




# ----------------- GLOBAL TIMEOUTS (UNIFY) -----------------
REGION_TIMEOUT_SECONDS = 20
COMPUTE_WAIT_TIMEOUT_SECONDS = 30
COMPUTE_SYNC_TIMEOUT_SECONDS = 60
# ----------------------------------------------------------

_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

_broadcaster_clients = set()

def broadcast_ccure_update(payload: dict):
    if not _broadcaster_clients:
        return
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = None
    for q in list(_broadcaster_clients):
        try:
            if loop and loop.is_running():
                loop.call_soon_threadsafe(q.put_nowait, payload)
            else:
                q.put_nowait(payload)
        except Exception:
            logger.exception("Failed to push payload to SSE client (will remove client)")
            try:
                _broadcaster_clients.discard(q)
            except Exception:
                pass

async def _sse_event_generator(client_queue: asyncio.Queue):
    try:
        while True:
            payload = await client_queue.get()
            try:
                data = json.dumps(payload, default=str)
            except Exception:
                data = json.dumps({"error": "serialization error", "payload": str(payload)})
            yield f"data: {data}\n\n"
    finally:
        try:
            _broadcaster_clients.discard(client_queue)
        except Exception:
            pass
        return

@app.get("/ccure/stream")
async def ccure_stream():
    q = asyncio.Queue()
    _broadcaster_clients.add(q)
    generator = _sse_event_generator(q)
    headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    return StreamingResponse(generator, media_type="text/event-stream", headers=headers)

def _guess_region_from_text(txt: str) -> str:
    if not txt:
        return "unknown"
    s = str(txt).strip().lower()
    s = re.sub(r"[,\-/()]", " ", s)
    if any(k in s for k in ("pune","quezon city","taguig city","bengaluru","hyderabad","chennai","manila","singapore","hong kong","beijing","shanghai","jakarta","kuala","osaka","tokyo","seoul","bangkok")):
        return "apac"
    if any(k in s for k in ("london","dublin","paris","frankfurt","amsterdam","stockholm","cape town","johannesburg","berlin","brussels","madrid","rome","milan")):
        return "emea"
    if any(k in s for k in ("mexico","bogota","buenos","santiago","sao","salvador","lima","caracas")):
        return "laca"
    if any(k in s for k in ("denver","new york","ny","chicago","toronto","vancouver","los angeles","san francisco","boston","houston","atlanta","miami")):
        return "namer"
    return "unknown"

@app.get("/headcount")
def api_headcount():
    try:
        totals = {"apac": 0, "emea": 0, "laca": 0, "namer": 0, "unknown": 0}
        with SessionLocal() as db:
            try:
                today = date.today()
                rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                if rows:
                    for r in rows:
                        try:
                            partition = None
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                            loc = partition or "unknown"
                            region = _guess_region_from_text(loc)
                            totals[region] = totals.get(region, 0) + 1
                        except Exception:
                            totals["unknown"] += 1
                else:
                    start = datetime.combine(today, datetime.min.time())
                    end = datetime.combine(today, datetime.max.time())
                    swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
                    for s in swipes:
                        loc = s.partition or "unknown"
                        region = _guess_region_from_text(loc)
                        totals[region] = totals.get(region, 0) + 1
            except Exception:
                logger.exception("Failed to compute headcount regions")
        out = {
            "apac": int(totals.get("apac", 0)),
            "emea": int(totals.get("emea", 0)),
            "laca": int(totals.get("laca", 0)),
            "namer": int(totals.get("namer", 0))
        }
        return JSONResponse(out)
    except Exception as exc:
        logger.exception("api_headcount failed")
        raise HTTPException(status_code=500, detail=f"headcount error: {exc}")

# ---------- Helpers retained (normalize / safe conversions) -------------
def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- build_ccure_averages (fallback) ------------------------------
def build_ccure_averages(start_date: Optional[str] = None, end_date: Optional[str] = None):
    """
    Fallback averages computation using AttendanceSummary only.
    Returns a compact shape (live_today, ccure_active, averages, notes).
    """
    try:
        def _parse_date_param(s):
            if not s:
                return None
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                try:
                    return date.fromisoformat(s)
                except Exception:
                    return None

        today = date.today()
        start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
        end_obj = _parse_date_param(end_date) if end_date else today
        if start_obj is None or end_obj is None or start_obj > end_obj:
            start_obj = today - timedelta(days=6)
            end_obj = today

        with SessionLocal() as db:
            try:
                att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
            except Exception:
                logger.exception("Failed to query AttendanceSummary")
                att_rows = []

            live_emp = 0
            live_contr = 0
            unknown_count = 0
            seen_keys = set()

            def classify_from_derived(derived):
                try:
                    if not derived or not isinstance(derived, dict):
                        return "contractor"
                    for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                        v = derived.get(k)
                        if v and "employee" in str(v).strip().lower():
                            return "employee"
                    for k in ("Employee_Status","Employee Status","Status"):
                        v = derived.get(k)
                        if v and "terminated" in str(v).strip().lower():
                            return "employee"
                    return "contractor"
                except Exception:
                    return "contractor"

            if att_rows:
                for a in att_rows:
                    key = None
                    try:
                        key = _normalize_employee_key(a.employee_id)
                    except Exception:
                        key = None
                    if not key:
                        try:
                            key = _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None)
                        except Exception:
                            key = None
                    if not key:
                        unknown_count += 1
                        continue
                    if key in seen_keys:
                        continue
                    seen_keys.add(key)
                    cls = classify_from_derived(a.derived)
                    if cls == "employee":
                        live_emp += 1
                    elif cls == "contractor":
                        live_contr += 1
                    else:
                        unknown_count += 1

                live_total_reported = live_emp + live_contr + unknown_count
                live_total_details = len(att_rows)
            else:
                live_total_reported = 0
                live_total_details = 0
                try:
                    import region_clients
                    details = []
                    try:
                        details = region_clients.fetch_all_details(timeout=REGION_TIMEOUT_SECONDS) or []
                    except Exception:
                        logger.exception("region_clients.fetch_all_details failed in build_ccure_averages()")
                        details = []
                    if details:
                        for d in details:
                            try:
                                cls = "contractor"
                                for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                                    v = d.get(k)
                                    if v and "employee" in str(v).strip().lower():
                                        cls = "employee"
                                        break
                                if cls == "employee":
                                    live_emp += 1
                                else:
                                    live_contr += 1
                                live_total_details += 1
                            except Exception:
                                continue
                        live_total_reported = live_emp + live_contr
                    else:
                        try:
                            regions = region_clients.fetch_all_regions(timeout=REGION_TIMEOUT_SECONDS) or []
                            for r in regions:
                                try:
                                    c = r.get("count")
                                    if isinstance(c, (int, float)):
                                        live_total_reported += int(c)
                                except Exception:
                                    continue
                        except Exception:
                            logger.exception("region_clients.fetch_all_regions failed in build_ccure_averages()")
                except Exception:
                    logger.exception("region_clients not importable in build_ccure_averages()")

            # compute avg range using AttendanceSummary if possible
            avg_range = None
            try:
                q = db.query(AttendanceSummary.date, AttendanceSummary.employee_id, AttendanceSummary.presence_count)\
                      .filter(AttendanceSummary.date >= start_obj, AttendanceSummary.date <= end_obj).all()
                by_date = {}
                for row in q:
                    d = row[0]
                    key = (row[1] or "").strip() if row[1] else None
                    if not key:
                        continue
                    if d not in by_date:
                        by_date[d] = set()
                    try:
                        presence_val = getattr(row, 'presence_count', row[2])
                        if int(presence_val) > 0:
                            by_date[d].add(key)
                    except Exception:
                        by_date[d].add(key)
                days_count = (end_obj - start_obj).days + 1
                daily_counts = [len(by_date.get(start_obj + timedelta(days=i), set())) for i in range(days_count)]
                if days_count and any(daily_counts):
                    avg_range = int(round(sum(daily_counts) / float(days_count)))
                else:
                    avg_range = None
            except Exception:
                logger.exception("Failed computing range average from AttendanceSummary")
                avg_range = None

        # fallback: use region history to compute avg_range if still None
        if avg_range is None:
            try:
                import region_clients
                entries = region_clients.fetch_all_history(timeout=REGION_TIMEOUT_SECONDS) or []
                agg = {}
                for e in entries:
                    try:
                        dstr = e.get("date")
                        if not dstr:
                            continue
                        region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                        emp = None
                        con = None
                        tot = None
                        if region_obj:
                            emp = _safe_int(region_obj.get("Employee"))
                            con = _safe_int(region_obj.get("Contractor"))
                            tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                        else:
                            emp = _safe_int(e.get("Employee"))
                            con = _safe_int(e.get("Contractor"))
                            tot = _safe_int(e.get("total"))
                        if emp is None and con is None:
                            continue
                        if tot is None:
                            tot = (emp or 0) + (con or 0)
                        if dstr not in agg:
                            agg[dstr] = {"total": 0, "count": 0}
                        agg[dstr]["total"] += tot or 0
                        agg[dstr]["count"] += 1
                    except Exception:
                        continue
                per_date_totals = []
                days_count = (end_obj - start_obj).days + 1
                for i in range(days_count):
                    dcheck = (start_obj + timedelta(days=i)).isoformat()
                    if dcheck in agg and agg[dcheck]["count"] > 0:
                        per_day_avg = float(agg[dcheck]["total"]) / float(agg[dcheck]["count"])
                        per_date_totals.append(per_day_avg)
                if per_date_totals:
                    avg_range = int(round(sum(per_date_totals) / float(len(per_date_totals))))
            except Exception:
                logger.exception("Failed computing avg_range from region history in fallback")

        # get ccure stats if available
        ccure_stats = {}
        try:
            import ccure_client
            if hasattr(ccure_client, "get_global_stats"):
                ccure_stats = ccure_client.get_global_stats() or {}
        except Exception:
            logger.debug("ccure_client.get_global_stats not available", exc_info=True)

        cc_active_emps = None
        cc_active_contractors = None
        try:
            if isinstance(ccure_stats, dict):
                a = ccure_stats.get("ActiveEmployees") or ccure_stats.get("active_employees") or None
                b = ccure_stats.get("ActiveContractors") or ccure_stats.get("active_contractors") or None
                if a is not None and str(a).strip() != "":
                    cc_active_emps = int(a)
                if b is not None and str(b).strip() != "":
                    cc_active_contractors = int(b)
        except Exception:
            cc_active_emps = cc_active_contractors = None

        emp_pct = None
        contr_pct = None
        overall_pct = None
        try:
            if isinstance(cc_active_emps, int) and cc_active_emps > 0:
                emp_pct = round((live_emp / float(cc_active_emps)) * 100.0, 2)
            if isinstance(cc_active_contractors, int) and cc_active_contractors > 0:
                contr_pct = round((live_contr / float(cc_active_contractors)) * 100.0, 2)
            if isinstance(cc_active_emps, int) and isinstance(cc_active_contractors, int) and (cc_active_emps + cc_active_contractors) > 0:
                overall_pct = round(((live_emp + live_contr) / float(cc_active_emps + cc_active_contractors)) * 100.0, 2)
        except Exception:
            emp_pct = contr_pct = overall_pct = None

        resp = {
            "date": today.isoformat(),
            "notes": f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}" if (start_date or end_date) else None,
            "live_today": {
                "employee": live_emp,
                "contractor": live_contr,
                "total_reported": live_total_reported,
                "total_from_details": live_total_details
            },
            "ccure_active": {
                "active_employees": cc_active_emps,
                "active_contractors": cc_active_contractors,
                "ccure_active_employees_reported": cc_active_emps,
                "ccure_active_contractors_reported": cc_active_contractors
            },
            "averages": {
                "employee_pct": emp_pct,
                "contractor_pct": contr_pct,
                "overall_pct": overall_pct,
                "avg_headcount_last_7_days": avg_range,
                "head_emp_pct_vs_ccure_today": emp_pct,
                "head_contractor_pct_vs_ccure_today": contr_pct,
                "headcount_overall_pct_vs_ccure_today": overall_pct,
                "history_avg_overall_last_7_days": avg_range
            }
        }

        return resp
    except Exception:
        logger.exception("build_ccure_averages failed")
        raise

# -------------------------
# Upload endpoints (store uploads under data/ and data/raw_uploads/ and rotate)
# -------------------------
ALLOWED_EXT = (".xls", ".xlsx", ".csv")

def _remove_old_files_for_kind(kind: str):
    """
    Remove previous canonical files and previous raw uploads that include kind in filename.
    """
    try:
        # canonical in DATA_DIR
        for p in DATA_DIR.iterdir():
            if p.is_file() and f"active_{kind}" in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old canonical file %s", p)
        # remove raw uploads for same kind to keep only latest raw (user requested)
        for p in RAW_UPLOADS_DIR.iterdir():
            if p.is_file() and kind in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old raw file %s", p)
    except Exception:
        logger.exception("error while removing old files for kind=%s", kind)

def _save_upload_and_rotate(upload_file: UploadFile, kind: str) -> dict:
    """
    Save the uploaded file as:
      - data/active_<kind>.<ext>     (canonical)
      - data/raw_uploads/<timestamp>_<kind>_<origname>  (raw trace)
    Remove previous files for same kind (both canonical & raw).
    Returns metadata dict.
    """
    fname = Path(upload_file.filename).name
    ext = Path(fname).suffix.lower()
    if ext not in ALLOWED_EXT:
        raise HTTPException(status_code=400, detail="Only .xls .xlsx .csv allowed")

    # rotate old files
    _remove_old_files_for_kind(kind)

    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    raw_name = f"{ts}_{kind}_{fname}"
    raw_path = RAW_UPLOADS_DIR / raw_name

    # Save raw
    try:
        with raw_path.open("wb") as out_f:
            shutil.copyfileobj(upload_file.file, out_f)
    finally:
        try:
            upload_file.file.close()
        except Exception:
            pass

    # Write canonical: keep same extension as original (simpler)
    canonical_path = DATA_DIR / f"active_{kind}{ext}"
    try:
        shutil.copy(raw_path, canonical_path)
    except Exception:
        logger.exception("Failed to write canonical copy for %s", canonical_path)

    info = {
        "raw_saved": str(raw_path),
        "canonical_saved": str(canonical_path),
        "original_filename": fname,
        "size_bytes": raw_path.stat().st_size if raw_path.exists() else None
    }
    logger.info("Uploaded %s: %s", kind, info)
    return info





@app.post("/api/upload/active-employees")
async def upload_active_employees_api(file: UploadFile = File(...)):
    return await upload_active_employees(file)

@app.post("/api/upload/active-contractors")
async def upload_active_contractors_api(file: UploadFile = File(...)):
    return await upload_active_contractors(file)



@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    """
    Upload Active Employee sheet:
      - stores raw to data/raw_uploads and canonical to data/active_employee.*
      - removes previous uploaded employee sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="employee")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("employee upload failed")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    """
    Upload Active Contractor sheet:
      - stores raw to data/raw_uploads and canonical to data/active_contractor.*
      - removes previous uploaded contractor sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="contractor")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("contractor upload failed")
        raise HTTPException(status_code=500, detail=str(e))

# ---------- map detailed -> compact (used when compute returns detailed) ----
def _map_detailed_to_resp(detailed: Dict[str, Any]) -> Dict[str, Any]:
    # unchanged mapping from earlier implementation (kept identical to previous)
    live_h = detailed.get("live_headcount", {}) or {}
    head_h = detailed.get("headcount", {}) or {}
    ccure_active_obj = detailed.get("ccure_active", {}) or {}
    averages_obj = detailed.get("averages", {}) or {}

    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    live_employee = int(live_h.get("employee") or head_h.get("employee") or 0)
    live_contractor = int(live_h.get("contractor") or head_h.get("contractor") or 0)
    total_reported = int(
        live_h.get("currently_present_total")
        or head_h.get("total_visited_today")
        or (live_employee + live_contractor)
        or 0
    )
    total_from_details = int(head_h.get("total_visited_today") or 0)

    mapped_headcount = {
        "total_visited_today": int(head_h.get("total_visited_today") or 0),
        "employee": int(head_h.get("employee") or 0),
        "contractor": int(head_h.get("contractor") or 0),
        "by_location": head_h.get("by_location") or {}
    }

    resp = {
        "date": detailed.get("date"),
        "notes": detailed.get("notes"),
        "live_today": {
            "employee": live_employee,
            "contractor": live_contractor,
            "total_reported": total_reported,
            "total_from_details": total_from_details
        },
        "headcount": mapped_headcount,
        "live_headcount": live_h,
        "ccure_active": {
            "active_employees": ccure_active_obj.get("ccure_active_employees_reported")
                             or ccure_active_obj.get("active_employees"),
            "active_contractors": ccure_active_obj.get("ccure_active_contractors_reported")
                               or ccure_active_obj.get("active_contractors"),
            "ccure_active_employees_reported": ccure_active_obj.get("ccure_active_employees_reported"),
            "ccure_active_contractors_reported": ccure_active_obj.get("ccure_active_contractors_reported")
        },
        "averages": averages_obj
    }
    return resp

# ---------- build a verify-compatible summary from mapped payload -----------
def _build_verify_like_summary_from_mapped(mapped: Dict[str, Any], include_raw: bool = False) -> Dict[str, Any]:
    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    cc = mapped.get("ccure_active", {}) or {}
    head = mapped.get("headcount", {}) or {}
    live_head = mapped.get("live_headcount", {}) or {}
    averages = mapped.get("averages", {}) or {}

    cc_emp = to_int(cc.get("ccure_active_employees_reported") or cc.get("active_employees"))
    cc_con = to_int(cc.get("ccure_active_contractors_reported") or cc.get("active_contractors"))

    head_total = to_int(head.get("total_visited_today") or mapped.get("live_today", {}).get("total_from_details"))
    head_emp = to_int(head.get("employee") or mapped.get("live_today", {}).get("employee"))
    head_con = to_int(head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    live_total = to_int(live_head.get("currently_present_total") or mapped.get("live_today", {}).get("total_reported"))
    live_emp = to_int(live_head.get("employee") or mapped.get("live_today", {}).get("employee"))
    live_con = to_int(live_head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    history_emp_avg = averages.get("history_avg_employee_last_7_days")
    history_con_avg = averages.get("history_avg_contractor_last_7_days")
    history_overall_avg = averages.get("history_avg_overall_last_7_days")

    def pct(n, d):
        try:
            if n is None or d is None:
                return None
            if float(d) == 0:
                return None
            return round((float(n) / float(d)) * 100.0, 2)
        except Exception:
            return None

    summary = {
        "date": mapped.get("date"),
        "ccure_reported": {
            "employees": cc_emp,
            "contractors": cc_con,
            "total_reported": (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None
        },
        "headcount_attendance_summary": {
            "total_visited_today": head_total,
            "employee": head_emp,
            "contractor": head_con,
        },
        "live_headcount_region_clients": {
            "currently_present_total": live_total,
            "employee": live_emp,
            "contractor": live_con,
        },
        "percentages_vs_ccure": {
            "head_employee_pct_vs_ccure_today": pct(head_emp, cc_emp),
            "head_contractor_pct_vs_ccure_today": pct(head_con, cc_con),
            "head_overall_pct_vs_ccure_today": pct(head_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "live_employee_pct_vs_ccure_today": pct(live_emp, cc_emp),
            "live_contractor_pct_vs_ccure_today": pct(live_con, cc_con),
            "live_overall_pct_vs_ccure_today": pct(live_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "history_employee_pct_vs_ccure": pct(history_emp_avg, cc_emp),
            "history_contractor_pct_vs_ccure": pct(history_con_avg, cc_con),
            "history_overall_pct_vs_ccure": pct(history_overall_avg, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None)
        },
        "averages": {
            "history_avg_employee_last_7_days": history_emp_avg,
            "history_avg_contractor_last_7_days": history_con_avg,
            "history_avg_overall_last_7_days": history_overall_avg,
            "avg_headcount_last_7_days_db": averages.get("avg_headcount_last_7_days") or averages.get("avg_headcount_last_7_days_db"),
            "avg_headcount_per_site_last_7_days": averages.get("avg_headcount_per_site_last_7_days"),
            "employee_pct": averages.get("employee_pct"),
            "contractor_pct": averages.get("contractor_pct"),
            "overall_pct": averages.get("overall_pct"),
            **({k: v for k, v in averages.items() if k not in (
                "history_avg_employee_last_7_days",
                "history_avg_contractor_last_7_days",
                "history_avg_overall_last_7_days",
                "avg_headcount_last_7_days",
                "avg_headcount_last_7_days_db",
                "avg_headcount_per_site_last_7_days",
                "employee_pct","contractor_pct","overall_pct"
            )})
        },
        "notes": mapped.get("notes")
    }

    summary["headcount_details"] = {
        "total_visited_today": head_total,
        "employee": head_emp,
        "contractor": head_con,
        "by_location": head.get("by_location") if isinstance(head.get("by_location"), dict) else {}
    }
    summary["live_headcount_details"] = {
        "currently_present_total": live_total,
        "employee": live_emp,
        "contractor": live_con,
        "by_location": live_head.get("by_location") if isinstance(live_head.get("by_location"), dict) else {}
    }

    summary["ccure_active"] = {
        "active_employees": cc.get("active_employees") or cc.get("ccure_active_employees_reported"),
        "active_contractors": cc.get("active_contractors") or cc.get("ccure_active_contractors_reported"),
        "ccure_active_employees_reported": cc.get("ccure_active_employees_reported"),
        "ccure_active_contractors_reported": cc.get("ccure_active_contractors_reported")
    }

    return summary

# ---------- /ccure/verify (already present in your original) -----
@app.get("/ccure/verify")
def ccure_verify(
    raw: bool = Query(False, description="if true, include the raw compute payload for debugging"),
    start_date: Optional[str] = Query(None, description="YYYY-MM-DD start date (inclusive)"),
    end_date: Optional[str] = Query(None, description="YYYY-MM-DD end date (inclusive)")
):
    """
    Synchronous verification endpoint. Prefer compute_visit_averages() (synchronous call).
    If compute raises or fails, fall back to build_ccure_averages() so output shape remains consistent.
    """
    try:
        detailed = None
        try:
            from ccure_compare_service import compute_visit_averages
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
        except Exception:
            logger.exception("compute_visit_averages() failed inside /ccure/verify; falling back")
            detailed = None

        if isinstance(detailed, dict):
            mapped = _map_detailed_to_resp(detailed)
            summary = _build_verify_like_summary_from_mapped(mapped, include_raw=raw)
            if raw and isinstance(detailed, dict):
                summary["raw"] = detailed
            return JSONResponse(summary)
        else:
            fallback = build_ccure_averages(start_date, end_date)
            mapped_fallback = {
                "date": fallback.get("date"),
                "notes": fallback.get("notes"),
                "live_today": fallback.get("live_today", {}),
                "headcount": {
                    "total_visited_today": fallback.get("live_today", {}).get("total_from_details") or fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "live_headcount": {
                    "currently_present_total": fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "ccure_active": fallback.get("ccure_active", {}),
                "averages": fallback.get("averages", {})
            }

            summary = _build_verify_like_summary_from_mapped(mapped_fallback, include_raw=raw)
            if raw:
                summary["raw"] = mapped_fallback
       
            return JSONResponse(summary)
    except Exception as e:
        logger.exception("ccure_verify failed")
        raise HTTPException(status_code=500, detail=f"ccure verify error: {e}")

# ---------- /ccure/compare -> uses data_compare_service.compare_ccure_vs_sheets ----------
@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from data_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)




@app.get("/ccure/export_uploaded")
def ccure_export_uploaded():
    """
    Export the currently uploaded canonical sheets into one workbook.
    Returns JSON: { status: "ok", report_path: "<filename>" }
    Downloadable at /ccure/report/{filename}
    """
    try:
        from data_compare_service import export_uploaded_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed for export_uploaded")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    fname = export_uploaded_sheets()
    if not fname:
        raise HTTPException(status_code=500, detail="failed to export uploaded sheets")
    return JSONResponse({"status": "ok", "report_path": fname})





@app.get("/ccure/compare_v2")
def ccure_compare_v2(
    mode: str = Query("full"),
    stats_detail: str = Query("ActiveProfiles"),
    limit_list: int = Query(200, ge=1, le=5000),
    export: bool = Query(False),
    region_filter: Optional[str] = Query(None, description="Region code e.g. APAC"),
    location_city: Optional[str] = Query(None, description="Location city e.g. Pune"),
    location_state: Optional[str] = Query(None, description="Location state/province"),
    location_description: Optional[str] = Query(None, description="Location Description"),
    week_ref_date: Optional[str] = Query(None, description="YYYY-MM-DD for week (Mon-Fri)")
):
    try:
        from data_compare_service_v2 import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service_v2 import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(
        mode=mode,
        stats_detail=stats_detail,
        limit_list=limit_list,
        export=export,
        region_filter=region_filter,
        location_city=location_city,
        location_state=location_state,
        location_description=location_description,
        week_ref_date=week_ref_date
    )
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)




@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = OUTPUT_DIR / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(str(full),
                            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            filename=safe_name)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")
      

# End of app.py original content

# ---------------------------
# NEW: Duration API endpoint
# ---------------------------
# Endpoint: GET /duration
# Query params:
#   date (YYYY-MM-DD) — optional, defaults to today in Asia/Kolkata
#   regions (comma-separated) — optional, default all (apac,emea,laca,namer)
#   outdir — optional, path where CSVs will be written (default: OUTPUT_DIR/duration_reports)
#   sample_rows — optional number of sample rows to return per region



#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py

@app.get("/duration")
async def api_duration(
    date_param: Optional[str] = Query(None, alias="date", description="Target date YYYY-MM-DD. Defaults to today in Asia/Kolkata"),
    start_date: Optional[str] = Query(None, description="Start date for a range (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date for a range (YYYY-MM-DD)"),
    regions: Optional[str] = Query(None, description="Comma-separated list: apac,emea,laca,namer. Default: all"),
    city: Optional[str] = Query(None, description="Optional city/location filter (e.g. Pune). Case-insensitive, matches PartitionName2/PrimaryLocation/Door/EmployeeName"),
    outdir: Optional[str] = Query(None, description="Output directory for CSVs. Defaults to OUTPUT_DIR/duration_reports"),
    sample_rows: int = Query(10, ge=0, le=200, description="How many sample rows to include per region in response"),
    compliance_target: int = Query(3, ge=1, le=7, description="Compliance target days (e.g. 3 or 5). Default 3")
):
    """
    Returns per-region duration aggregates and swipe-level details with weekly compliance & duration category splits.
    """
    try:
        # parse regions / outdir
        if regions:
            regions_list = [r.strip().lower() for r in regions.split(",") if r.strip()]
        else:
            regions_list = ["apac", "emea", "laca", "namer"]

        if outdir:
            outdir_path = Path(outdir)
        else:
            outdir_path = OUTPUT_DIR / "duration_reports"
        outdir_path.mkdir(parents=True, exist_ok=True)

        def _parse_date(s: str) -> date:
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                return date.fromisoformat(s)

        if start_date and end_date:
            start_obj = _parse_date(start_date)
            end_obj = _parse_date(end_date)
            if start_obj > end_obj:
                raise HTTPException(status_code=400, detail="start_date must be <= end_date")
            max_days = 92
            days_count = (end_obj - start_obj).days + 1
            if days_count > max_days:
                raise HTTPException(status_code=400, detail=f"Date range too large (> {max_days} days). Please request a smaller range.")
            date_list = [start_obj + timedelta(days=i) for i in range(days_count)]
            range_mode = True
        else:
            if date_param:
                target_date = _parse_date(date_param)
            else:
                tz = ZoneInfo("Asia/Kolkata")
                target_date = datetime.now(tz).date()
            date_list = [target_date]
            start_obj = end_obj = date_list[0]
            range_mode = False

        try:
            import duration_report
        except Exception as e:
            logger.exception("Failed importing duration_report module")
            raise HTTPException(status_code=500, detail=f"duration module import failed: {e}")

        loop = asyncio.get_running_loop()

        def _to_json_safe(v):
            try:
                if pd.isna(v):
                    return None
            except Exception:
                pass
            if isinstance(v, (datetime, date)):
                return v.isoformat()
            if hasattr(v, "isoformat") and not isinstance(v, str):
                try:
                    return v.isoformat()
                except Exception:
                    pass
            try:
                if isinstance(v, (int, float, bool)):
                    return v
            except Exception:
                pass
            try:
                return str(v)
            except Exception:
                return None

        per_date_results = {}
        for single_date in date_list:
            try:
                task = loop.run_in_executor(None, duration_report.run_for_date, single_date, regions_list, str(outdir_path), city)
                per_date_results[single_date.isoformat()] = await asyncio.wait_for(task, timeout=COMPUTE_WAIT_TIMEOUT_SECONDS)
            except asyncio.TimeoutError:
                raise HTTPException(status_code=504, detail=f"Duration computation timed out for date {single_date.isoformat()}")
            except Exception as e:
                logger.exception("duration run_for_date failed for date %s", single_date)
                raise HTTPException(status_code=500, detail=f"duration run failed for {single_date.isoformat()}: {e}")

        dates_iso = [d.isoformat() for d in date_list]
        date_objs = [d for d in date_list]
        week_starts = sorted({(d - timedelta(days=d.weekday())) for d in date_objs})

        resp: Dict[str, Any] = {
            "start_date": start_obj.isoformat(),
            "end_date": end_obj.isoformat(),
            "regions": {}
        }

        for r in regions_list:
            try:
                employees_map: Dict[str, Dict[str, Any]] = {}
                swipes_by_date: Dict[str, list] = {}
                date_rows = {}

                for iso_d, day_res in per_date_results.items():
                    region_obj = day_res.get(r) if isinstance(day_res, dict) else None
                    durations_df = None
                    swipes_df = None
                    if isinstance(region_obj, dict):
                        swipes_df = region_obj.get("swipes")
                        durations_df = region_obj.get("durations")
                    elif isinstance(region_obj, pd.DataFrame):
                        durations_df = region_obj

                    rows_count = int(len(durations_df)) if isinstance(durations_df, pd.DataFrame) else 0
                    swipe_count = int(len(swipes_df)) if isinstance(swipes_df, pd.DataFrame) else 0
                    date_rows[iso_d] = {"rows": rows_count, "swipe_rows": swipe_count}

                    if isinstance(swipes_df, pd.DataFrame) and not swipes_df.empty:
                        for c in ["LocaleMessageTime", "EmployeeName", "Door", "EmployeeID", "CardNumber", "PartitionName2", "PersonnelTypeName", "CompanyName", "PrimaryLocation", "Direction", "EmployeeIdentity", "MessageType", "Text5"]:
                            if c not in swipes_df.columns:
                                swipes_df[c] = None
                        swipe_records = []
                        for _, srow in swipes_df.iterrows():
                            dt = srow.get("LocaleMessageTime")
                            iso_ts = None
                            if pd.notna(dt):
                                try:
                                    iso_ts = pd.to_datetime(dt).isoformat()
                                except Exception:
                                    try:
                                        iso_ts = datetime.fromtimestamp(float(dt)).isoformat()
                                    except Exception:
                                        iso_ts = str(dt)
                            swipe_records.append({
                                "LocaleMessageTime": iso_ts,
                                "Dateonly": iso_d,
                                "Swipe_Time": (pd.to_datetime(srow.get("LocaleMessageTime")).time().isoformat() if pd.notna(srow.get("LocaleMessageTime")) else None),
                                "EmployeeID": _to_json_safe(srow.get("EmployeeID")),
                                "PersonGUID": _to_json_safe(srow.get("EmployeeIdentity") or srow.get("EmployeeIdentity")),
                                "ObjectName1": _to_json_safe(srow.get("EmployeeName")),
                                "Door": _to_json_safe(srow.get("Door")),
                                "PersonnelType": _to_json_safe(srow.get("PersonnelTypeName") or srow.get("PersonnelType")),
                                "CardNumber": _to_json_safe(srow.get("CardNumber")),
                                "Text5": _to_json_safe(srow.get("PrimaryLocation") or srow.get("Text5")),
                                "PartitionName2": _to_json_safe(srow.get("PartitionName2")),
                                "AdmitCode": _to_json_safe(srow.get("AdmitCode") or srow.get("MessageType")),
                                "Direction": _to_json_safe(srow.get("Direction")),
                                "CompanyName": _to_json_safe(srow.get("CompanyName")),
                                "PrimaryLocation": _to_json_safe(srow.get("PrimaryLocation") or srow.get("Text5")),
                            })
                        swipes_by_date[iso_d] = swipe_records
                    else:
                        swipes_by_date.setdefault(iso_d, [])

                    if isinstance(durations_df, pd.DataFrame) and not durations_df.empty:
                        for col in ["person_uid", "EmployeeID", "EmployeeName", "CardNumber", "Date",
                                    "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor",
                                    "Duration", "DurationSeconds", "PersonnelTypeName", "PartitionName2",
                                    "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"]:
                            if col not in durations_df.columns:
                                durations_df[col] = None

                        for _, drow in durations_df.iterrows():
                            person_uid = drow.get("person_uid")
                            if pd.isna(person_uid) or person_uid is None or str(person_uid).strip() == "":
                                person_uid = f"{_to_json_safe(drow.get('EmployeeID'))}|{_to_json_safe(drow.get('EmployeeName'))}"

                            if person_uid not in employees_map:
                                employees_map[person_uid] = {
                                    "person_uid": person_uid,
                                    "EmployeeID": _to_json_safe(drow.get("EmployeeID")),
                                    "EmployeeName": _to_json_safe(drow.get("EmployeeName")),
                                    "CardNumber": _to_json_safe(drow.get("CardNumber")),
                                    "durations": {d: None for d in dates_iso},
                                    "durations_seconds": {d: None for d in dates_iso},
                                    "total_seconds_present_in_range": 0,
                                    # keep internal First/Last but we'll remove them before returning
                                    "FirstSwipe": None,
                                    "LastSwipe": None,
                                    "FirstDoor": _to_json_safe(drow.get("FirstDoor")),
                                    "LastDoor": _to_json_safe(drow.get("LastDoor")),
                                    "PersonnelType": _to_json_safe(drow.get("PersonnelTypeName") or drow.get("PersonnelType")),
                                    "PartitionName2": _to_json_safe(drow.get("PartitionName2")),
                                    "CompanyName": _to_json_safe(drow.get("CompanyName")),
                                    "PrimaryLocation": _to_json_safe(drow.get("PrimaryLocation")),
                                    "FirstDirection": _to_json_safe(drow.get("FirstDirection")),
                                    "LastDirection": _to_json_safe(drow.get("LastDirection")),
                                }

                            dur_str = None if pd.isna(drow.get("Duration")) else str(drow.get("Duration"))
                            dur_secs = None
                            try:
                                v = drow.get("DurationSeconds")
                                if pd.notna(v):
                                    dur_secs = int(float(v))
                            except Exception:
                                dur_secs = None

                            employees_map[person_uid]["durations"][iso_d] = dur_str
                            employees_map[person_uid]["durations_seconds"][iso_d] = dur_secs
                            if dur_secs is not None:
                                employees_map[person_uid]["total_seconds_present_in_range"] += dur_secs

                            try:
                                fs = drow.get("FirstSwipe")
                                ls = drow.get("LastSwipe")
                                if pd.notna(fs):
                                    fs_dt = pd.to_datetime(fs)
                                    cur_fs = employees_map[person_uid].get("FirstSwipe")
                                    if cur_fs is None:
                                        employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                    else:
                                        if pd.to_datetime(cur_fs) > fs_dt:
                                            employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                if pd.notna(ls):
                                    ls_dt = pd.to_datetime(ls)
                                    cur_ls = employees_map[person_uid].get("LastSwipe")
                                    if cur_ls is None:
                                        employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                                    else:
                                        if pd.to_datetime(cur_ls) < ls_dt:
                                            employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                            except Exception:
                                pass

                emp_list = list(employees_map.values())
                emp_list.sort(key=lambda x: ((x.get("EmployeeName") or "") or "").lower())

                # ------------------- SHIFT-FIX block (correctly indented) -------------------
                # SHIFT-FIX constants
                SHIFT_GAP_SECONDS = 6 * 3600        # gap > 6 hours -> new session
                SHIFT_MIN_FIX_SECONDS = 4 * 3600    # only attempt fix if a day duration < 4h
                SHIFT_MAX_FIX_SECONDS = 20 * 3600   # or if a day duration > 20h

                # helper to parse swipe timestamps safely
                def _parse_swipe_ts(swipe_rec):
                    ts = swipe_rec.get("LocaleMessageTime")
                    if not ts:
                        return None
                    try:
                        return pd.to_datetime(ts)
                    except Exception:
                        try:
                            return datetime.fromisoformat(str(ts))
                        except Exception:
                            return None

                # Build swipes_by_person for quick lookup
                swipes_by_person = {}
                for iso_d in dates_iso:
                    for s in swipes_by_date.get(iso_d, []):
                        pids = []
                        if s.get("PersonGUID"):
                            pids.append(str(s.get("PersonGUID")))
                        if s.get("EmployeeID") is not None:
                            pids.append(str(s.get("EmployeeID")))
                        if s.get("CardNumber") is not None:
                            pids.append(str(s.get("CardNumber")))
                        for pid in pids:
                            swipes_by_person.setdefault(pid, []).append(s)

                # Sort each person's swipe list by timestamp
                for pid, arr in list(swipes_by_person.items()):
                    arr_ts = []
                    for s in arr:
                        ts = _parse_swipe_ts(s)
                        if ts is not None:
                            arr_ts.append((ts, s))
                    arr_ts.sort(key=lambda x: x[0])
                    swipes_by_person[pid] = [s for _, s in arr_ts]

                def _needs_shift_fix(emp):
                    for v in (emp.get("durations_seconds") or {}).values():
                        if v is None:
                            continue
                        if v < SHIFT_MIN_FIX_SECONDS or v > SHIFT_MAX_FIX_SECONDS:
                            return True
                    return False

                # Attempt session reconstruction for employees that appear problematic
                for emp in emp_list:
                    try:
                        # merge all matching swipe lists for this person
                        person_keys = []
                        if emp.get("person_uid"):
                            person_keys.append(str(emp["person_uid"]))
                        if emp.get("EmployeeID") is not None:
                            person_keys.append(str(emp["EmployeeID"]))
                        if emp.get("CardNumber") is not None:
                            person_keys.append(str(emp["CardNumber"]))

                        all_swipes = []
                        for k in person_keys:
                            lst = swipes_by_person.get(k) or []
                            for s in lst:
                                ts = _parse_swipe_ts(s)
                                if ts is not None:
                                    all_swipes.append((ts, s))
                        if not all_swipes:
                            continue
                        all_swipes.sort(key=lambda x: x[0])
                        swipe_times = [ts for ts, _ in all_swipes]

                        # only attempt shift reconstruction if day durations look problematic
                        if not _needs_shift_fix(emp):
                            continue

                        # build sessions: consecutive swipes with gap <= SHIFT_GAP_SECONDS belong to same session
                        sessions = []
                        cur_start = swipe_times[0]
                        cur_last = swipe_times[0]
                        for ts in swipe_times[1:]:
                            gap = (ts - cur_last).total_seconds()
                            if gap > SHIFT_GAP_SECONDS:
                                sessions.append((cur_start, cur_last))
                                cur_start = ts
                                cur_last = ts
                            else:
                                cur_last = ts
                        sessions.append((cur_start, cur_last))

                        # rebuild per-date durations_seconds (accumulate sessions by session start date)
                        new_durations_seconds = {d: None for d in dates_iso}
                        for s_start, s_end in sessions:
                            session_secs = max(0, int((s_end - s_start).total_seconds()))
                            session_date_iso = s_start.date().isoformat()
                            if session_date_iso not in new_durations_seconds:
                                continue
                            prev = new_durations_seconds.get(session_date_iso)
                            if prev is None:
                                new_durations_seconds[session_date_iso] = session_secs
                            else:
                                new_durations_seconds[session_date_iso] = prev + session_secs

                        # preserve any pre-existing per-day aggregated values for days not touched by sessions
                        for d in dates_iso:
                            if new_durations_seconds.get(d) is None and emp.get("durations_seconds", {}).get(d) is not None:
                                new_durations_seconds[d] = emp["durations_seconds"][d]

                        # convert seconds -> duration strings
                        new_durations_str = {}
                        for d, secs in new_durations_seconds.items():
                            if secs is None:
                                new_durations_str[d] = None
                            else:
                                try:
                                    new_durations_str[d] = str(timedelta(seconds=int(secs)))
                                except Exception:
                                    new_durations_str[d] = None

                        # recompute total_seconds_present_in_range
                        total = 0
                        for v in new_durations_seconds.values():
                            if v is not None:
                                total += int(v)

                        # apply updates back to emp
                        emp["durations_seconds"] = new_durations_seconds
                        emp["durations"] = new_durations_str
                        emp["total_seconds_present_in_range"] = total

                    except Exception:
                        logger.exception("shift-fix reconstruction failed for employee %s", emp.get("person_uid") or emp.get("EmployeeID"))
                # ----------------- end SHIFT-FIX block -----------------

                # compute per-employee weekly compliance and categories
                for emp in emp_list:
                    weeks_info = {}
                    weeks_met = 0
                    weeks_total = 0

                    cat_counts = {"0-30m": 0, "30m-2h": 0, "2h-6h": 0, "6h-8h": 0, "8h+": 0}
                    cat_dates = {k: [] for k in cat_counts.keys()}

                    for ws in week_starts:
                        week_start_iso = ws.isoformat()
                        week_dates = [(ws + timedelta(days=i)).isoformat() for i in range(7)]
                        relevant_dates = [d for d in week_dates if d in dates_iso]
                        if not relevant_dates:
                            continue

                        days_present = 0
                        days_ge8 = 0
                        per_date_durations = {}
                        per_date_compliance = {}

                        for d in relevant_dates:
                            secs = emp["durations_seconds"].get(d)
                            per_date_durations[d] = secs
                            if secs is not None and secs > 0:
                                days_present += 1
                            # day compliant = >= 8h
                            is_ge8 = (secs is not None and secs >= 28800)
                            if is_ge8:
                                days_ge8 += 1
                            per_date_compliance[d] = True if is_ge8 else False

                            # COUNT categories only when present (>0)
                            if secs is not None and secs > 0:
                                cat = duration_report.categorize_seconds(secs) if hasattr(duration_report, 'categorize_seconds') else "0-30m"
                                if cat in cat_counts:
                                    cat_counts[cat] += 1
                                    cat_dates[cat].append(d)

                        ct = int(compliance_target or 3)
                        compliant = False
                        if days_present >= ct:
                            if ct == 3:
                                if days_present == 3:
                                    compliant = (days_ge8 == days_present)
                                else:
                                    compliant = False

                        weeks_info[week_start_iso] = {
                            "week_start": week_start_iso,
                            "dates": per_date_durations,
                            "dates_compliance": per_date_compliance,
                            "days_present": days_present,
                            "days_ge8": days_ge8,
                            "compliant": compliant
                        }

                        weeks_total += 1
                        if compliant:
                            weeks_met += 1

                    # dominant category: choose category with highest count (ties resolved by first encountered)
                    dominant_category = None
                    max_count = -1
                    for k, v in cat_counts.items():
                        if v > max_count:
                            max_count = v
                            dominant_category = k

                    # Remove internal swipe fields from returned object
                    for _k in ("FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor"):
                        if _k in emp:
                            try:
                                del emp[_k]
                            except Exception:
                                pass

                    emp["compliance"] = {
                        "weeks": weeks_info,
                        "weeks_met": weeks_met,
                        "weeks_total": weeks_total,
                        "month_summary": f"{weeks_met}/{weeks_total}" if weeks_total > 0 else "0/0",
                        "compliance_target": int(compliance_target or 3)
                    }
                    emp["duration_categories"] = {
                        "counts": cat_counts,
                        "dominant_category": dominant_category,
                        "category_dates": cat_dates,
                        "red_flag": cat_counts.get("2h-6h", 0)
                    }

                durations_sample = emp_list[:sample_rows] if sample_rows and sample_rows > 0 else []

                resp["regions"][r] = {
                    "dates": dates_iso,
                    "employees": emp_list,
                    "durations_sample": durations_sample,
                    "date_rows": date_rows,
                    "swipes_by_date": swipes_by_date
                }
            except Exception:
                logger.exception("Failed to aggregate range results for region %s", r)
                resp["regions"][r] = {"dates": [d.isoformat() for d in date_list], "employees": [], "durations_sample": [], "date_rows": {}, "swipes_by_date": {}}

        return JSONResponse(resp)
    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("api_duration (range) failed")
        raise HTTPException(status_code=500, detail=f"duration api error: {exc}")








2) 



# duration_report.py
"""
duration_report.py

Updated: supports querying multiple monthly ACVSUJournal databases (current + previous N).
Fixes: category counting only for present days (so dominant category is correct).
"""
import argparse
import logging
import os
import re
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd

# Optional: import pyodbc only when connecting (allows importing this module even without driver)
try:
    import pyodbc
except Exception:
    pyodbc = None

ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# (REGION_CONFIG unchanged - omitted here for brevity in inline view; keep your config)
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "partitions": [
            "APAC.Default", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur"
        ]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "database": "ACVSUJournal_00011028",
        "last_n_databases": 2,
        "partitions": [
            "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
            "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
        ]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "partitions": [
            "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
            "PA.Panama City", "PE.Lima"
        ]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# GENERIC_SQL_TEMPLATE and DB helper functions remain unchanged.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
INNER JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
INNER JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
"""

# helper functions (split name, expand DB list, connect master, filter existing dbs)
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# DB connection helper (same as before)
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "CardNumber", "PersonnelTypeName", "EmployeeIdentity",
        "PartitionName2", "LocaleMessageTime", "MessageType", "Direction", "CompanyName", "PrimaryLocation"
    ]
    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = get_connection(region_key)
    try:
        df = pd.read_sql(sql, conn)
    finally:
        conn.close()

    for c in cols:
        if c not in df.columns:
            df[c] = None

    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    return df[cols]

def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    if df["LocaleMessageTime"].dtype == object:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    df = df.drop_duplicates(subset=dedupe_cols, keep="first")

    df["Date"] = df["LocaleMessageTime"].dt.date

    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    df["person_uid"] = df.apply(make_person_uid, axis=1)
    df = df[df["person_uid"].notna()].copy()

    def agg_for_group(g):
        g_sorted = g.sort_values("LocaleMessageTime")
        first = g_sorted.iloc[0]
        last = g_sorted.iloc[-1]
        first_dir = first.get("Direction")
        last_dir = last.get("Direction")

        return pd.Series({
            "person_uid": first["person_uid"],
            "EmployeeIdentity": first.get("EmployeeIdentity"),
            "EmployeeID": first.get("EmployeeID"),
            "EmployeeName": first.get("EmployeeName"),
            "CardNumber": first.get("CardNumber"),
            "Date": first["Date"],
            "FirstSwipe": first["LocaleMessageTime"],
            "LastSwipe": last["LocaleMessageTime"],
            "FirstDoor": first.get("Door"),
            "LastDoor": last.get("Door"),
            "CountSwipes": int(len(g_sorted)),
            "PersonnelTypeName": first.get("PersonnelTypeName"),
            "PartitionName2": first.get("PartitionName2"),
            "CompanyName": first.get("CompanyName"),
            "PrimaryLocation": first.get("PrimaryLocation"),
            "FirstDirection": first_dir,
            "LastDirection": last_dir
        })

    grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    return grouped[out_cols]

def categorize_seconds(s: Optional[int]) -> str:
    """
    Category labels (used when secs present > 0):
      - "0-30m"      -> 0 .. 1800 (inclusive)
      - "30m-2h"     -> 1801 .. 7200
      - "2h-6h"      -> 7201 .. 21600
      - "6h-8h"      -> 21601 .. 28800
      - "8h+"        -> >= 28800
    Note: this helper alone will still return "0-30m" for s None/0, but in counting code we
    only increment when s is present and >0.
    """
    try:
        if s is None or s <= 0:
            return "0-30m"
        s = int(s)
        if s <= 1800:
            return "0-30m"
        if s <= 7200:
            return "30m-2h"
        if s <= 21600:
            return "2h-6h"
        if s < 28800:
            return "6h-8h"
        return "8h+"
    except Exception:
        return "0-30m"

def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        try:
            swipes = fetch_swipes_for_region(r, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", r)
            swipes = pd.DataFrame()

        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                swipes = swipes[combined_mask].copy()
            else:
                logging.debug("City filter provided but no matching columns to filter on for region %s", r)

        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", r)
            durations = pd.DataFrame()

        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
        try:
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", r)
        try:
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", r)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
        results[r] = {"swipes": swipes, "durations": durations}

    return results

# CLI runner (unchanged)
def parse_args():
    p = argparse.ArgumentParser(description="Generate daily duration report from ACVSUJournal swipe logs.")
    p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
    p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
                   default="apac,emea,laca,namer")
    p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
    p.add_argument("--city", help="Optional city/location filter (e.g. Pune)", default=None)
    return p.parse_args()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    args = parse_args()

    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    else:
        tz = ZoneInfo("Asia/Kolkata")
        target_date = datetime.now(tz).date()

    regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
    outdir = args.outdir

    logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
    results = run_for_date(target_date, regions, outdir, city=args.city)

    for r, obj in results.items():
        dur = obj.get("durations")
        logging.info("Region %s: %d persons with computed durations", r, len(dur) if dur is not None else 0)
    logging.info("Completed. CSVs are in %s", Path(outdir).absolute())









3)
#for Active Employee and ACtive Contractor Comparision Report.

# C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service.py
"""
Compare CCURE active lists (employees & contractors) with uploaded Active sheets (from disk).
Provides compare_ccure_vs_sheets(mode='full', stats_detail='ActiveProfiles', limit_list=200, export=False)

When export=True, writes Excel report to OUTPUT_DIR and returns 'report_path'.
"""

from datetime import datetime
import os
import re
import uuid
import logging
import sys
from pathlib import Path
import pandas as pd

logger = logging.getLogger("data_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Settings fallback (matches app.py pattern)
try:
    from settings import OUTPUT_DIR as SETTINGS_OUTPUT_DIR, DATA_DIR as SETTINGS_DATA_DIR, RAW_UPLOAD_DIR as SETTINGS_RAW_UPLOAD_DIR
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    RAW_UPLOAD_DIR = Path(SETTINGS_RAW_UPLOAD_DIR)
except Exception:
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"
    DATA_DIR = Path(__file__).resolve().parent / "data"
    RAW_UPLOAD_DIR = DATA_DIR / "raw_uploads"

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# ccure client helper (optional)
try:
    import ccure_client
except Exception:
    ccure_client = None
    logger.warning("ccure_client not importable; CCURE calls will return None")

# ---------- small normalizers (kept local) ----------
def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

def _make_w_variant(s):
    if s is None:
        return None
    ss = str(s).strip()
    if ss.upper().startswith('W'):
        return ss
    return 'W' + ss

def _numeric_variants(s):
    out = set()
    if s is None:
        return out
    try:
        s = str(s)
        clean = re.sub(r'\D', '', s)
        if clean:
            out.add(clean)
            out.add(clean.lstrip('0') or clean)
            out.add('W' + clean)
    except Exception:
        pass
    return out

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- CCURE fetch helpers ----------
def _fetch_ccure_list(detail_name):
    """
    Uses ccure_client.fetch_all_stats(detail_name) if available, otherwise tries per-page fetch.
    Returns list of dicts or [].
    """
    if ccure_client is None:
        logger.warning("ccure_client missing - cannot fetch CCURE lists")
        return []
    try:
        if hasattr(ccure_client, "fetch_all_stats"):
            res = ccure_client.fetch_all_stats(detail_name, limit=500)
            return res or []
    except Exception:
        logger.exception("fetch_all_stats failed")
    # fallback to paged fetch if available
    try:
        data = []
        page = 1
        limit = 500
        while True:
            if not hasattr(ccure_client, "fetch_stats_page"):
                break
            page_res = ccure_client.fetch_stats_page(detail_name, page=page, limit=limit)
            if not page_res:
                break
            part = page_res.get("data") or []
            if not part:
                break
            data.extend(part)
            total = int(page_res.get("total") or len(data) or 0)
            if len(data) >= total:
                break
            page += 1
            if page > 1000:
                break
        return data
    except Exception:
        logger.exception("per-page fetch failed for %s", detail_name)
        return []

# ---------- helpers to find/load latest sheet from disk ----------
def _find_latest_file_for_kind(kind: str):
    """
    kind == 'employee' or 'contractor'
    Prefer canonical file in DATA_DIR: active_<kind>.*,
    otherwise pick latest in RAW_UPLOAD_DIR that contains the kind token.
    """
    # 1) canonical in DATA_DIR
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_{kind}{ext}"
        if p.exists():
            return p

    # 2) fallback: find latest raw file with kind token in RAW_UPLOAD_DIR
    try:
        files = [p for p in RAW_UPLOAD_DIR.iterdir() if p.is_file() and kind in p.name.lower() and p.suffix.lower() in (".xlsx", ".xls", ".csv")]
        if not files:
            # last fallback: any active_{kind}.* in RAW_UPLOAD_DIR
            files = [p for p in RAW_UPLOAD_DIR.iterdir() if p.is_file() and f"active_{kind}" in p.name.lower()]
        if not files:
            return None
        files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        return files[0]
    except Exception:
        return None

def _read_table(path: Path):
    try:
        if path.suffix.lower() == ".csv":
            df = pd.read_csv(path, dtype=str)
        else:
            df = pd.read_excel(path, sheet_name=0, dtype=str)
        df.columns = [str(c).strip() for c in df.columns]
        return df.fillna("")
    except Exception:
        logger.exception("Failed to read file %s", path)
        return pd.DataFrame()

def _first_present_from_row(row, candidates):
    for c in candidates:
        if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
            return row[c]
    for k in row.index:
        for c in candidates:
            if k.strip().lower() == c.strip().lower():
                val = row[k]
                if pd.notna(val) and str(val).strip() != "":
                    return val
    return None

# ---------- disk-based loaders to replace DB loaders ----------
def _load_active_employees_disk():
    """
    Return (set of normalized employee_ids, dict mapping id -> row sample, total_rows)
    Reads the latest employee sheet from data/.
    """
    path = _find_latest_file_for_kind("employee")
    if not path:
        return set(), {}, 0
    df = _read_table(path)
    ids = set()
    mapping = {}
    id_cols = ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id','Employee_Id']
    name_cols = ['Full Name','FullName','EmpName','Name','First Name','FirstName','Last Name']
    location_cols = ['Location City','Location','Location Description','City']
    status_cols = ['Current Status','Status','Employee Status','Employee_Status','Status (Current)','CurrentStatus']

    for _, row in df.iterrows():
        raw_row = row.to_dict()
        emp_id = _first_present_from_row(row, id_cols)
        if emp_id is None or str(emp_id).strip() == "":
            continue
        emp_id = str(emp_id).strip()
        ids.add(emp_id)
        full_name = _first_present_from_row(row, name_cols) or ""
        mapping[emp_id] = {
            "employee_id": emp_id,
            "full_name": full_name,
            "location_city": _first_present_from_row(row, location_cols),
            "status": _first_present_from_row(row, status_cols),
            "raw": raw_row
        }
    return ids, mapping, len(df)

def _load_active_contractors_disk():
    """
    Return (set of candidate contractor ids, mapping, total_rows)
    Reads latest contractor sheet from data/.
    """
    path = _find_latest_file_for_kind("contractor")
    if not path:
        return set(), {}, 0
    df = _read_table(path)
    ids = set()
    mapping = {}
    wsid_cols = ['Worker System Id','Worker System ID','Worker ID','WorkerSystemId','WorkerId']
    ipass_cols = ['iPass ID','"W" iPass ID','IPassID','iPassID','Ipass ID','IpassID']
    name_cols = ['Full Name','FullName','Name']
    vendor_cols = ['Vendor Company Name','Vendor']

    for _, row in df.iterrows():
        raw_row = row.to_dict()
        wsid = _first_present_from_row(row, wsid_cols)
        ipass = _first_present_from_row(row, ipass_cols)
        full_name = _first_present_from_row(row, name_cols)
        if wsid:
            wsid = str(wsid).strip()
            ids.add(wsid)
            mapping[wsid] = {"worker_system_id": wsid, "full_name": full_name, "vendor": _first_present_from_row(row, vendor_cols), "raw": raw_row}
        if ipass:
            ipass = str(ipass).strip()
            ids.add(ipass)
            mapping[ipass] = {"ipass_id": ipass, "full_name": full_name, "vendor": _first_present_from_row(row, vendor_cols), "raw": raw_row}
            wvar = _make_w_variant(ipass)
            ids.add(wvar)
            mapping[wvar] = {"ipass_id": ipass, "w_ipass": wvar, "full_name": full_name, "raw": raw_row}
        for cand in (wsid, ipass):
            if cand:
                for v in _numeric_variants(cand):
                    ids.add(v)
                    if v not in mapping:
                        mapping[v] = {"derived_id": v, "full_name": full_name, "raw": raw_row}
    return ids, mapping, len(df)

# ---------- helpers to match ccure -> disk lists (same logic as before) ----------
def _employee_matches_disk(cid, emp_ids_disk, emp_map_disk, ccure_row):
    if cid in emp_ids_disk:
        return True
    for v in _numeric_variants(cid):
        if v in emp_ids_disk:
            return True
    try:
        ci = _safe_int(cid)
        if ci is not None:
            for dbk in emp_ids_disk:
                if _safe_int(dbk) == ci:
                    return True
    except Exception:
        pass
    card_cid = _normalize_card_like(cid)
    if card_cid:
        for dbk in emp_ids_disk:
            if _normalize_card_like(dbk) == card_cid:
                return True
    try:
        name = None
        if isinstance(ccure_row, dict):
            name = ccure_row.get("EmpName") or ccure_row.get("FullName") or ccure_row.get("Name")
        normname = _normalize_name(name)
        if normname:
            for dbk, dbv in emp_map_disk.items():
                if _normalize_name(dbv.get("full_name")) == normname:
                    return True
    except Exception:
        pass
    return False

def _contractor_matches_disk(cid, contr_ids_disk, contr_map_disk, ccure_row):
    if cid in contr_ids_disk:
        return True
    for v in _numeric_variants(cid):
        if v in contr_ids_disk:
            return True
    try:
        ci = _safe_int(cid)
        if ci is not None:
            for dbk in contr_ids_disk:
                if _safe_int(dbk) == ci:
                    return True
    except Exception:
        pass
    card_cid = _normalize_card_like(cid)
    if card_cid:
        for dbk in contr_ids_disk:
            if _normalize_card_like(dbk) == card_cid:
                return True
    try:
        name = None
        if isinstance(ccure_row, dict):
            name = ccure_row.get("EmpName") or ccure_row.get("FullName") or ccure_row.get("Name")
        normname = _normalize_name(name)
        if normname:
            for dbk, dbv in contr_map_disk.items():
                if _normalize_name(dbv.get("full_name")) == normname:
                    return True
    except Exception:
        pass
    return False

# ---------- core compare function ----------
def compare_ccure_vs_sheets(mode="full", stats_detail="ActiveProfiles", limit_list=200, export=False):
    """
    Main public function used by /ccure/compare.
    Reads latest uploaded sheets from disk instead of DB tables.
    """
    result = {
        "ccure_active_employees_count": None,
        "ccure_active_contractors_count": None,
        "active_sheet_employee_count": None,
        "active_sheet_contractor_count": None,
        "missing_employees_count": None,
        "missing_contractors_count": None,
        "missing_employees_sample": [],
        "missing_contractors_sample": [],
        "report_path": None
    }

    # 1) fetch CCURE lists
    ccure_emps = _fetch_ccure_list("ActiveEmployees")
    ccure_contrs = _fetch_ccure_list("ActiveContractors")

    result["ccure_active_employees_count"] = len(ccure_emps)
    result["ccure_active_contractors_count"] = len(ccure_contrs)

    # 2) load uploaded sheets from disk (preferred) — returns (ids_set, mapping, total_rows)
    emp_ids_disk, emp_map_disk, emp_total_rows = _load_active_employees_disk()
    contr_ids_disk, contr_map_disk, contr_total_rows = _load_active_contractors_disk()

    result["active_sheet_employee_count"] = int(emp_total_rows)
    result["active_sheet_contractor_count"] = int(contr_total_rows)

    # 3) build ccure id sets for employees
    ccure_emp_id_set = set()
    ccure_emp_rows_by_id = {}
    for row in ccure_emps:
        try:
            eid = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("EmpID") or row.get("Employee Id"))
            if not eid:
                eid = _normalize_card_like(row.get("CardNumber") or row.get("iPass ID") or row.get("IPassID") or row.get("Card"))
            if not eid:
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    eid = f"name::{fname}"
            if eid:
                ccure_emp_id_set.add(eid)
                ccure_emp_rows_by_id[eid] = row
        except Exception:
            continue

    # 4) employees missing = ccure_emp_id_set - emp_ids_disk (but consider numeric variants, int equality, card-like, name match)
    expanded_emp_disk_ids = set(emp_ids_disk)
    for v in list(emp_ids_disk):
        for nv in _numeric_variants(v):
            expanded_emp_disk_ids.add(nv)

    missing_emp_ids = []
    for cid in ccure_emp_id_set:
        try:
            if str(cid).startswith("name::"):
                name = cid.split("::", 1)[1]
                found = False
                for dbk, dbv in emp_map_disk.items():
                    n = _normalize_name(dbv.get("full_name"))
                    if n and n == name:
                        found = True
                        break
                if not found:
                    missing_emp_ids.append(cid)
                continue

            ccure_row = ccure_emp_rows_by_id.get(cid) or {}
            if _employee_matches_disk(cid, expanded_emp_disk_ids, emp_map_disk, ccure_row):
                continue

            found = False
            for v in _numeric_variants(cid):
                if v in expanded_emp_disk_ids:
                    found = True
                    break
            if not found:
                missing_emp_ids.append(cid)
        except Exception:
            missing_emp_ids.append(cid)

    result["missing_employees_count"] = len(missing_emp_ids)
    samp_emp = []
    for mid in missing_emp_ids[:limit_list]:
        r = ccure_emp_rows_by_id.get(mid) or {}

        # extract manager/profile/status from raw if present
        manager_name = r.get("Manager_Name") or r.get("ManagerName") or r.get("Manager") or r.get("Manager_WU_ID")
        profile_disabled = r.get("Profile_Disabled") if "Profile_Disabled" in r else r.get("profile_disabled") if "profile_disabled" in r else r.get("ProfileDisabled") if "ProfileDisabled" in r else None
        employee_status = r.get("Employee_Status") or r.get("Employee Status") or r.get("Status") or r.get("employee_status")

        # ensure string conversion for boolean-like values
        if isinstance(profile_disabled, bool):
            profile_disabled = str(profile_disabled)

        # vendorCompany doesn't apply to employees — keep blank
        vendor_company = r.get("Vendor Company Name") or r.get("Vendor") or r.get("vendor") or ""

        # build sample row with both old and new key names for compatibility
        samp_emp.append({
            "ccure_key": mid,
            # old keys (kept for compatibility)
            "EmployeeID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "PersonnelType": r.get("PersonnelType"),
            # "VendorCompany": vendor_company,
            "Manager_Name": manager_name,
            "Profile_Disabled": profile_disabled,
            "Employee_Status": employee_status,
            # new/canonical lowerCamel keys requested
            "employee_Id": r.get("EmployeeID") or r.get("employee_id") or r.get("Employee Id"),
            "empName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "personnelType": r.get("PersonnelType"),
            "vendorCompany": vendor_company,
            "managerName": manager_name,
            "profileDisabled": profile_disabled,
            "employeeStatus": employee_status,
            "raw": r
        })
    result["missing_employees_sample"] = samp_emp

    # 5) contractors
    ccure_contr_id_set = set()
    ccure_contr_rows_by_id = {}
    for row in ccure_contrs:
        try:
            cand_ids = []
            e1 = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("Employee Id"))
            if e1:
                cand_ids.append(e1)
            ip = _normalize_employee_key(row.get("IPassID") or row.get("iPass ID") or row.get("iPass") or row.get("IPASSID"))
            if ip:
                cand_ids.append(ip)
                cand_ids.append(_make_w_variant(ip))
            cardlike = _normalize_card_like(row.get("CardNumber") or row.get("card_number") or row.get("Badge") or row.get("BadgeNo"))
            if cardlike:
                cand_ids.append(cardlike)
                cand_ids.extend(list(_numeric_variants(cardlike)))
            if not cand_ids:
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    cand_ids.append(f"name::{fname}")
            for cid in cand_ids:
                if cid:
                    ccure_contr_id_set.add(cid)
                    ccure_contr_rows_by_id[cid] = row
            if not cand_ids:
                key = f"unknown::{uuid.uuid4().hex[:8]}"
                ccure_contr_id_set.add(key)
                ccure_contr_rows_by_id[key] = row
        except Exception:
            continue

    expanded_contr_disk_ids = set(contr_ids_disk)
    for v in list(contr_ids_disk):
        for nv in _numeric_variants(v):
            expanded_contr_disk_ids.add(nv)

    missing_contr_ids = []
    for cid in ccure_contr_id_set:
        try:
            if str(cid).startswith("name::"):
                name = cid.split("::", 1)[1]
                found = False
                for dbk, dbv in contr_map_disk.items():
                    n = _normalize_name(dbv.get("full_name"))
                    if n and n == name:
                        found = True
                        break
                if not found:
                    missing_contr_ids.append(cid)
                continue

            ccure_row = ccure_contr_rows_by_id.get(cid) or {}
            if _contractor_matches_disk(cid, expanded_contr_disk_ids, contr_map_disk, ccure_row):
                continue

            found = False
            for v in _numeric_variants(cid):
                if v in expanded_contr_disk_ids:
                    found = True
                    break
            if not found:
                missing_contr_ids.append(cid)
        except Exception:
            missing_contr_ids.append(cid)

    result["missing_contractors_count"] = len(missing_contr_ids)
    samp_contr = []
    for mid in missing_contr_ids[:limit_list]:
        r = ccure_contr_rows_by_id.get(mid) or {}

        # vendor/company
        vendor_company = r.get("Vendor Company Name") or r.get("Vendor") or r.get("vendor") or ""

        # personnel, manager, profile, status extraction
        personnel_type = r.get("PersonnelType") or r.get("Personnel_Type") or r.get("Personnel Type") or None
        manager_name = r.get("Manager_Name") or r.get("ManagerName") or r.get("Manager") or r.get("Manager_WU_ID")
        profile_disabled = r.get("Profile_Disabled") if "Profile_Disabled" in r else r.get("profile_disabled") if "profile_disabled" in r else None
        employee_status = r.get("Employee_Status") or r.get("Employee Status") or r.get("Status") or r.get("employee_status")

        if isinstance(profile_disabled, bool):
            profile_disabled = str(profile_disabled)

        samp_contr.append({
            "ccure_key": mid,
            # old keys kept
            "Employee_ID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "VendorCompany": vendor_company,
            "PersonnelType": personnel_type,
            "Manager_Name": manager_name,
            "Profile_Disabled": profile_disabled,
            "Employee_Status": employee_status,
            # new requested lowerCamel keys
            "employeeId": r.get("EmployeeID") or r.get("employee_id") or r.get("Employee Id"),
            "empName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "vendorCompany": vendor_company,
            "personnelType": personnel_type,
            "managerName": manager_name,
            "profileDisabled": profile_disabled,
            "employeeStatus": employee_status,
            "raw": r
        })
    result["missing_contractors_sample"] = samp_contr

    # 6) optionally export report
    if export:
        try:
            OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
            ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            fname = f"missing_vs_ccure_{ts}.xlsx"
            fullpath = OUTPUT_DIR / fname

            # Canonical column set requested (lowerCamel)
            columns = ["ccure_key", "employee_Id", "empName", "personnelType", "managerName", "profileDisabled", "employeeStatus"]

            # Build DataFrames and ensure columns order (fill missing with empty string)
            if samp_emp:
                df_emp = pd.DataFrame(samp_emp)
                # reindex to requested columns; missing columns will be added with NaN
                df_emp = df_emp.reindex(columns=columns).fillna("")
            else:
                df_emp = pd.DataFrame(columns=columns)

            if samp_contr:
                df_con = pd.DataFrame(samp_contr)
                df_con = df_con.reindex(columns=columns).fillna("")
            else:
                df_con = pd.DataFrame(columns=columns)

            try:
                with pd.ExcelWriter(fullpath, engine="openpyxl") as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
            except Exception:
                # fallback to default engine
                with pd.ExcelWriter(fullpath) as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
        except Exception:
            logger.exception("Failed to export report")
            result["report_path"] = None

    return result






#export 

def export_uploaded_sheets():
    """
    Create a single xlsx workbook with:
      - Sheet "Employee" -> contents of latest canonical employee sheet (if present)
      - Sheet "Contractor" -> contents of latest canonical contractor sheet (if present)
    Writes file into OUTPUT_DIR and returns the filename (not full path).
    """
    try:
        emp_path = _find_latest_file_for_kind("employee")
        contr_path = _find_latest_file_for_kind("contractor")

        # Ensure OUTPUT_DIR exists
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
        fname = f"uploaded_sheets_{ts}.xlsx"
        fullpath = OUTPUT_DIR / fname

        # Read tables (if present) or create empty dataframe placeholders
        if emp_path and emp_path.exists():
            df_emp = _read_table(emp_path)
        else:
            df_emp = pd.DataFrame()

        if contr_path and contr_path.exists():
            df_con = _read_table(contr_path)
        else:
            df_con = pd.DataFrame()

        # Write to one workbook (try openpyxl if available)
        try:
            with pd.ExcelWriter(fullpath, engine="openpyxl") as writer:
                df_emp.to_excel(writer, sheet_name="Employee", index=False)
                df_con.to_excel(writer, sheet_name="Contractor", index=False)
        except Exception:
            # fallback to default engine
            with pd.ExcelWriter(fullpath) as writer:
                df_emp.to_excel(writer, sheet_name="Employee", index=False)
                df_con.to_excel(writer, sheet_name="Contractor", index=False)

        return fname
    except Exception:
        logger.exception("export_uploaded_sheets failed")
        return None


# Expose public function name expected by app.py
__all__ =  ["compare_ccure_vs_sheets", "compare_ccure_vs_sheets", "export_uploaded_sheets"]  # keep backwards compatibility
# End of data_compare_service.py







4)


# data_compare_service_v2.py
"""
Comparison service (v2) with broadened matching heuristics, safer prefetch/cache,
and explicit Sheet vs AttendanceSummary comparison diagnostics.

Drop-in replacement for your existing data_compare_service_v2.py
"""

import sys
import re
import uuid
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
from typing import Optional, Dict, Any, List, Tuple

import pandas as pd

# HTTP client for region histories
try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except Exception:
    requests = None

# DB imports (same as your project)
from db import SessionLocal
from models import AttendanceSummary

# Settings / defaults
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR, REGION_HISTORY_URLS as SETTINGS_REGION_HISTORY_URLS
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
    REGION_HISTORY_URLS = SETTINGS_REGION_HISTORY_URLS
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"
    REGION_HISTORY_URLS = [
        "http://10.199.22.57:3008/api/occupancy/history",  # APAC
        "http://10.199.22.57:3006/api/occupancy/history",  # NAMER
        "http://10.199.22.57:3007/api/occupancy/history",  # EMEA
        "http://10.199.22.57:4000/api/occupancy/history"   # LACA
    ]

DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger("data_compare_service_v2")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# In-memory cache for prefetched region history entries
REGION_HISTORY_CACHE = None
REGION_HISTORY_CACHE_FETCHED_AT = None
REGION_HISTORY_CACHE_TTL_SECONDS = 300  # 5 minutes

# Matching config
ID_FIELD_CANDIDATES = [
    "EmployeeID","employeeId","Employee Id","EmpID","Emp Id","EmpNo","EmployeeNo","Employee_Number",
    "PersonID","PersonId","personId","person_id","employee_id","id","Id","employeeNumber","EmployeeNumber",
    "worker_system_id","wsid","WorkerID","Worker System Id","workerId","WorkerSystemId"
]
CARD_FIELD_CANDIDATES = [
    "CardNumber","Card","cardNumber","card_number","BadgeNumber","BadgeNo","Badge","badgeNumber","badge_no",
    "iPassID","IPassID","iPass","i_pass_id","CardNo","card_no","card","IPASSID","IPass"
]
NAME_FIELD_CANDIDATES = [
    "FullName","Full Name","EmpName","Name","full_name","displayName","personName","PersonName"
]

# ----------------------------
# Utilities
# ----------------------------
def _find_active_employee_file():
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_employee{ext}"
        if p.exists():
            return p
    for p in DATA_DIR.iterdir():
        if p.is_file() and "active_employee" in p.name.lower():
            return p
    return None

def _normalize_key(k):
    if k is None:
        return None
    try:
        s = str(k).strip()
        return s if s != "" else None
    except Exception:
        return None

def _digits_only(s):
    if s is None:
        return ""
    return re.sub(r'\D+', '', str(s))

def _safe_int(v, default=0):
    try:
        if v is None or v == "":
            return default
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return default

def _week_monday_and_friday(ref_date: Optional[date] = None):
    if ref_date is None:
        ref_date = date.today()
    iso = ref_date.isoweekday()
    monday = ref_date - timedelta(days=(iso - 1))
    friday = monday + timedelta(days=4)
    return monday, friday

def _maybe_mark_on_leave(status_str: Optional[str]) -> bool:
    if not status_str:
        return False
    s = str(status_str).strip().lower()
    for tok in ("leave", "vacation", "on leave", "holiday", "sabbatical", "furlough", "loa"):
        if tok in s:
            return True
    return False

# ----------------------------
# Load active sheet
# ----------------------------
def load_active_employees_dataframe() -> pd.DataFrame:
    src = _find_active_employee_file()
    if not src:
        raise FileNotFoundError(f"Active employee canonical file not found in {DATA_DIR}")
    ext = src.suffix.lower()
    if ext in (".xls", ".xlsx"):
        df = pd.read_excel(src, sheet_name=0, dtype=str)
    else:
        df = pd.read_csv(src, dtype=str)
    df.columns = [c.strip() for c in df.columns]

    def _first_present(row, candidates):
        for c in candidates:
            if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
                return row[c]
        return None

    rows = []
    for _, row in df.iterrows():
        emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id','EmployeeNo','Employee No'])
        if emp_id is None:
            continue
        full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or (
            f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
        )
        location_city = _first_present(row, ['Location City','Location','City'])
        location_desc = _first_present(row, ['Location Description','LocationDescription'])
        location_state = _first_present(row, ['Location State / Province','Location State','State','Province'])
        region_code = _first_present(row, ['Region Code','Region','RegionCode'])
        current_status = _first_present(row, ['Current Status','Status','Employee Status'])
        employee_type = _first_present(row, ['Employee Type','Type','Time Type'])
        rows.append({
            "employee_id": _normalize_key(emp_id),
            "full_name": _normalize_key(full_name),
            "location_city": _normalize_key(location_city),
            "location_desc": _normalize_key(location_desc),
            "location_state": _normalize_key(location_state),
            "region_code": (str(region_code).strip() if region_code is not None else None),
            "current_status": _normalize_key(current_status),
            "employee_type": _normalize_key(employee_type),
            "raw_row": row.to_dict()
        })
    ndf = pd.DataFrame(rows)
    for col in ("employee_id","full_name","location_city","location_desc","location_state","region_code","current_status","employee_type","raw_row"):
        if col not in ndf:
            ndf[col] = None
    ndf = ndf[ndf["employee_id"].notna() & (ndf["employee_id"].str.strip() != "")]
    ndf.reset_index(drop=True, inplace=True)
    return ndf

# ----------------------------
# HTTP Session factory (retries/backoff)
# ----------------------------
def _build_requests_session():
    if requests is None:
        return None
    s = requests.Session()
    allowed = frozenset(['GET', 'HEAD'])
    try:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=allowed
        )
    except TypeError:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            method_whitelist=allowed
        )
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

# ----------------------------
# Prefetch / cache helpers
# ----------------------------
def prefetch_region_history(timeout: int = 10, force: bool = False):
    """
    Fetch region history entries (cached). Returns list of raw entries.
    """
    global REGION_HISTORY_CACHE, REGION_HISTORY_CACHE_FETCHED_AT
    try:
        now = datetime.utcnow()
        if not force and REGION_HISTORY_CACHE is not None and REGION_HISTORY_CACHE_FETCHED_AT:
            elapsed = (now - REGION_HISTORY_CACHE_FETCHED_AT).total_seconds()
            if elapsed < REGION_HISTORY_CACHE_TTL_SECONDS:
                logger.info("[region_cache] Using cached region history (age %.1fs)", elapsed)
                return REGION_HISTORY_CACHE

        entries = []
        # Prefer region_clients when available
        try:
            import region_clients
            logger.info("[region_cache] fetching region history via region_clients.fetch_all_history()")
            try:
                got = region_clients.fetch_all_history(timeout=timeout)
            except TypeError:
                got = region_clients.fetch_all_history()
            entries = got or []
        except Exception:
            entries = []

        # If empty, try direct requests to configured URLs
        if not entries and requests is not None:
            logger.info("[region_cache] fetching region history directly from endpoints")
            session = _build_requests_session() or requests
            for url in REGION_HISTORY_URLS:
                if not url:
                    continue
                try:
                    resp = session.get(url, timeout=(3, max(5, timeout)))
                    if not resp or resp.status_code != 200:
                        continue
                    try:
                        payload = resp.json()
                    except Exception:
                        continue
                    # flatten possible lists
                    if isinstance(payload, list):
                        for p in payload:
                            if isinstance(p, dict):
                                p['_source_url'] = url
                                entries.append(p)
                    elif isinstance(payload, dict):
                        found_list = False
                        for k in ("results","summaryByDate","details","data","entries","list","people","items"):
                            if k in payload and isinstance(payload[k], list):
                                for p in payload[k]:
                                    if isinstance(p, dict):
                                        p['_source_url'] = url
                                        entries.append(p)
                                found_list = True
                                break
                        if not found_list:
                            payload['_source_url'] = url
                            entries.append(payload)
                except requests.exceptions.RequestException as e:
                    logger.warning("[region_cache] fetch failed for %s: %s", url, str(e))
                    continue

        REGION_HISTORY_CACHE = entries or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        logger.info("[region_cache] prefetched %d region history entries", len(REGION_HISTORY_CACHE))
        return REGION_HISTORY_CACHE
    except Exception:
        logger.exception("[region_cache] prefetch failed")
        REGION_HISTORY_CACHE = REGION_HISTORY_CACHE or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        return REGION_HISTORY_CACHE

# ----------------------------
# Payload helpers (deep search)
# ----------------------------
def _iter_scalars_in_obj(obj, parent_key=""):
    """
    Yield all scalar key->value pairs inside nested dict/list structures as (key_path, value).
    """
    if isinstance(obj, dict):
        for k, v in obj.items():
            new_key = f"{parent_key}.{k}" if parent_key else str(k)
            if isinstance(v, (dict, list)):
                yield from _iter_scalars_in_obj(v, parent_key=new_key)
            else:
                yield new_key, v
    elif isinstance(obj, list):
        for idx, it in enumerate(obj):
            new_key = f"{parent_key}[{idx}]" if parent_key else f"[{idx}]"
            if isinstance(it, (dict, list)):
                yield from _iter_scalars_in_obj(it, parent_key=new_key)
            else:
                yield new_key, it

def _extract_details_from_payload(payload):
    """
    Normalize to list of dict rows that look like detail records.
    """
    if payload is None:
        return []
    if isinstance(payload, list):
        return [p for p in payload if isinstance(p, dict)]
    if isinstance(payload, dict):
        for k in ("details","results","data","entries","list","people","items"):
            v = payload.get(k)
            if isinstance(v, list):
                return [x for x in v if isinstance(x, dict)]
        # single-record-like payload (date-summary) -> return as single element to allow higher-level scanner to inspect fields
        if any(k in payload for k in ("date","Employee","Contractor","total","day")) and len(payload.keys()) <= 40:
            return [payload]
    return []

# ----------------------------
# ID matching helpers
# ----------------------------
def _match_candidate_to_employees(candidate_raw, orig_ids_set, orig_ids_list, digits_map):
    """
    Try many heuristics to map candidate_raw to one of the orig_ids_list.
    Returns matched_orig_id or None.
    """
    if candidate_raw is None:
        return None
    cand_str = str(candidate_raw).strip()
    if not cand_str:
        return None

    # direct exact match
    if cand_str in orig_ids_set:
        return cand_str

    # direct case-insensitive match
    for o in orig_ids_list:
        if isinstance(o, str) and cand_str.lower() == o.lower():
            return o

    # numeric transformations
    cand_digits = _digits_only(cand_str)
    if cand_digits:
        cand_nolead = cand_digits.lstrip('0') or cand_digits
        # direct digits match to original ids
        for o in orig_ids_list:
            if not isinstance(o, str):
                continue
            if o == cand_digits or o == cand_nolead:
                return o
            od = _digits_only(o)
            if od == cand_digits or od == cand_nolead:
                return o
        # numeric equality by int
        try:
            ci = int(cand_nolead)
            for o in orig_ids_list:
                try:
                    od = _digits_only(o)
                    if not od:
                        continue
                    oi = int(od.lstrip('0') or od)
                    if oi == ci:
                        return o
                except Exception:
                    continue
        except Exception:
            pass

        # last-n digits heuristics (conservative)
        if len(cand_digits) >= 3:
            for n in (6, 4):
                suf = cand_digits[-n:]
                if not suf:
                    continue
                for o in orig_ids_list:
                    od = _digits_only(o)
                    if od and od.endswith(suf):
                        return o

    # strip common prefixes and retry
    up = cand_str.upper()
    for pref in ("W", "IPASS", "IPASSID", "IPass"):
        if up.startswith(pref):
            stripped = cand_str[len(pref):]
            m = _match_candidate_to_employees(stripped, orig_ids_set, orig_ids_list, digits_map)
            if m:
                return m

    # fallback: find numeric substrings inside string and try mapping
    for match in re.finditer(r'(\d{3,})', cand_str):
        ssub = match.group(1)
        m = _match_candidate_to_employees(ssub, orig_ids_set, orig_ids_list, digits_map)
        if m:
            return m

    return None

# ----------------------------
# Region history scanning -> build presence map
# ----------------------------
def _fetch_presence_from_region_histories(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None, preloaded_entries: Optional[List[dict]] = None):
    """
    Scans preloaded_entries (or global cache) and returns presence mapping: {employee_id -> {date: 0/1}}
    """
    presence = {eid: {} for eid in employee_ids}
    if not employee_ids:
        return presence

    if preloaded_entries is None:
        global REGION_HISTORY_CACHE
        preloaded_entries = REGION_HISTORY_CACHE or []

    if not preloaded_entries:
        logger.info("[region_history] no preloaded region history entries to scan")
    else:
        orig_ids_list = [str(e).strip() for e in employee_ids]
        orig_ids_set = set(orig_ids_list)
        digits_map = {e: _digits_only(e) for e in orig_ids_list}

        scanned = 0
        matched = 0
        examples_matched = []

        for entry in preloaded_entries:
            detail_rows = []
            if isinstance(entry, dict):
                # prefer explicit lists
                for key in ("details","people","items","list","results","entries","data"):
                    if key in entry and isinstance(entry.get(key), list):
                        detail_rows = [r for r in entry.get(key) if isinstance(r, dict)]
                        break
                if not detail_rows:
                    # treat entry itself as a candidate detail row if it has plausible fields (timestamp or id-like)
                    candidate_keys = set(ID_FIELD_CANDIDATES + CARD_FIELD_CANDIDATES + ["date","timestamp","time","SwipeDate","LocaleMessageTime","day"])
                    if any(k in entry for k in candidate_keys):
                        detail_rows = [entry]
                    else:
                        # try extract via helper (covers nested payload shapes)
                        detail_rows = _extract_details_from_payload(entry) or []
            else:
                continue

            scanned += len(detail_rows)
            for d in detail_rows:
                try:
                    # find a timestamp / date for row
                    ts = None
                    # common keys first
                    for tkey in ("LocaleMessageTime","LocaleMessageDateTime","SwipeDate","SwipeTime","timestamp","time","DateTime","date","swipeDate","day"):
                        if tkey in d and d.get(tkey):
                            ts = d.get(tkey)
                            break

                    # fallback: scan scalar values in row for iso-like date
                    if ts is None:
                        for k, v in _iter_scalars_in_obj(d):
                            if v is None:
                                continue
                            try:
                                s = str(v)
                            except Exception:
                                continue
                            # quick heuristic
                            if re.search(r'\d{4}-\d{2}-\d{2}', s) or re.search(r'\d{2}/\d{2}/\d{4}', s):
                                ts = s
                                break

                    if ts is None:
                        # skip row (no date info)
                        continue

                    # parse timestamp into date (robust)
                    t = None
                    if isinstance(ts, (int, float)):
                        try:
                            t = datetime.fromtimestamp(int(ts))
                        except Exception:
                            try:
                                t = datetime.utcfromtimestamp(int(ts) / 1000.0)
                            except Exception:
                                t = None
                    elif isinstance(ts, str):
                        s = ts.strip()
                        if s == "":
                            t = None
                        else:
                            parsed = None
                            # try dateutil if available (best)
                            try:
                                from dateutil import parser as _parser
                                parsed = _parser.parse(s)
                            except Exception:
                                parsed = None
                            if parsed:
                                try:
                                    if parsed.tzinfo is not None:
                                        parsed = parsed.astimezone(tz=None).replace(tzinfo=None)
                                except Exception:
                                    pass
                                t = parsed
                            else:
                                # try common formats
                                fmts = [
                                    "%Y-%m-%dT%H:%M:%S.%fZ",
                                    "%Y-%m-%dT%H:%M:%S.%f",
                                    "%Y-%m-%dT%H:%M:%S",
                                    "%Y-%m-%d %H:%M:%S",
                                    "%Y-%m-%d",
                                    "%d/%m/%Y %H:%M:%S",
                                    "%d/%m/%Y"
                                ]
                                for f in fmts:
                                    try:
                                        t = datetime.strptime(s, f)
                                        break
                                    except Exception:
                                        t = None
                    if t is None:
                        continue

                    dt = t.date()
                    if dt < start_date or dt > end_date:
                        continue

                    # partition filter (if provided)
                    if partition_filter:
                        part_values = []
                        for k in ("PartitionNameFriendly","PartitionName","PrimaryLocation","partition","location","Partition","Location","Site","PartitionName1","PartitionName2"):
                            v = d.get(k)
                            if v:
                                part_values.append(str(v))
                        if part_values:
                            ok = any(part_filter_match(part, partition_filter) for part in part_values)
                            if not ok:
                                continue
                        else:
                            # no partition info -> skip when filter exists
                            continue

                    # attempt to match explicit id keys first
                    matched_key = None
                    for k in ID_FIELD_CANDIDATES:
                        if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                            m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                            if m:
                                matched_key = m
                                break

                    # try card fields
                    if not matched_key:
                        for k in CARD_FIELD_CANDIDATES:
                            if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                                m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break

                    # deep-scan scalars for numeric substrings and name fields
                    if not matched_key:
                        for key_path, val in _iter_scalars_in_obj(d):
                            if val is None:
                                continue
                            sval = str(val)
                            # numeric substring preference
                            if re.search(r'\d{3,}', sval):
                                m = _match_candidate_to_employees(sval, orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break
                        # check name fields (disabled by default to avoid false positives)

                    if matched_key:
                        matched += 1
                        # coerce matched_key to exact string from orig list
                        matched_key = next((o for o in orig_ids_list if str(o).strip() == str(matched_key).strip()), str(matched_key).strip())
                        presence.setdefault(matched_key, {})
                        presence[matched_key][dt] = 1
                        if len(examples_matched) < 10:
                            examples_matched.append({"matched": matched_key, "date": dt.isoformat(), "sample_row_keys": list(d.keys())[:8]})
                except Exception:
                    continue

        logger.info("[region_history] scanned %d detail rows from preloaded entries; matched %d presence entries", scanned, matched)
        if examples_matched:
            logger.debug("[region_history] example matches (up to 10): %s", examples_matched)

    # fill zeros for any missing date
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            presence.setdefault(eid, {})
            if cur not in presence[eid]:
                presence[eid][cur] = 0
        cur = cur + timedelta(days=1)

    return presence

def part_filter_match(src_val, partition_filter):
    try:
        if not src_val:
            return False
        return partition_filter.strip().lower() in str(src_val).strip().lower()
    except Exception:
        return False

# ----------------------------
# DB / combined fetch
# ----------------------------
def _fetch_presence_for_employees(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None):
    """
    1) chunked DB IN queries (AttendanceSummary)
    2) fallback broad DB query
    3) fallback region history cache scan (prefetch_region_history)
    Returns mapping {employee_id: {date: 0/1}}
    """
    if not employee_ids:
        return {}

    orig_ids = [str(e).strip() for e in employee_ids]
    norm_id_set = set([s for s in orig_ids if s])
    result = {eid: {} for eid in orig_ids}

    # 1) chunked DB fetch
    rows = []
    chunk_size = 500
    try:
        with SessionLocal() as db:
            for i in range(0, len(orig_ids), chunk_size):
                chunk = orig_ids[i:i+chunk_size]
                try:
                    q = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date,
                        AttendanceSummary.employee_id.in_(chunk)
                    )
                    rows_chunk = q.all()
                    if rows_chunk:
                        rows.extend(rows_chunk)
                except Exception:
                    logger.exception("chunked query failed (continuing)")
                    continue

            # fallback broad query if none found
            if not rows:
                try:
                    rows = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date
                    ).all()
                    logger.info("[presence_fetch] fallback broad DB query returned %d rows for %s -> %s", len(rows), start_date, end_date)
                except Exception:
                    logger.exception("fallback broad DB query failed")
                    rows = []
    except Exception:
        logger.exception("DB session error in _fetch_presence_for_employees")
        rows = []

    # map DB rows to provided employee ids
    for r in rows:
        try:
            raw = r.employee_id
            if raw is None:
                continue
            db_key = str(raw).strip()
            match_key = None
            if db_key in norm_id_set:
                match_key = db_key
            else:
                digits = _digits_only(db_key)
                if digits:
                    cand = digits.lstrip('0') or digits
                    if cand in norm_id_set:
                        match_key = cand
                if match_key is None:
                    for o in orig_ids:
                        if o == db_key or o.lstrip('0') == db_key or db_key.lstrip('0') == o:
                            match_key = o
                            break
            if not match_key:
                continue
            d = r.date
            present = 0
            try:
                present = int(r.presence_count or 0)
            except Exception:
                present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
            result.setdefault(match_key, {})
            prev = result[match_key].get(d, 0)
            result[match_key][d] = 1 if (prev == 1 or present > 0) else 0
        except Exception:
            continue

    # fill zeros
    cur = start_date
    while cur <= end_date:
        for eid in orig_ids:
            result.setdefault(eid, {})
            if cur not in result[eid]:
                result[eid][cur] = 0
        cur = cur + timedelta(days=1)

    db_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] DB-derived presence found for %d/%d employees", db_positive, len(orig_ids))

    # fallback to region details/history if needed
    if db_positive == 0 or db_positive < max(10, int(0.1 * len(orig_ids))):
        try:
            logger.info("[presence_fetch] DB coverage low (%d/%d) - trying region occupancy detail/history fallback", db_positive, len(orig_ids))

            # 1) Try region_clients.fetch_all_details() first (per-person rows are most useful)
            try:
                import region_clients
                details = []
                if hasattr(region_clients, "fetch_all_details"):
                    try:
                        details = region_clients.fetch_all_details(timeout=10)
                    except TypeError:
                        details = region_clients.fetch_all_details()
                details = details or []
                if details:
                    logger.info("[presence_fetch] fetched %d detail rows from region_clients.fetch_all_details()", len(details))
                    # attempt to match details to employee ids and fill presence
                    for d in details:
                        try:
                            # extract candidate id fields (fast checks)
                            cand_fields = []
                            for k in ("EmployeeID","employee_id","EmpID","CardNumber","Card","CardNo","IPassID","iPassID","PersonGUID","PersonId"):
                                v = d.get(k) if isinstance(d, dict) else None
                                if v and str(v).strip():
                                    cand_fields.append(v)
                            # if no explicit candidate id, attempt to deep-scan for numeric substring
                            if not cand_fields:
                                for key_path, val in _iter_scalars_in_obj(d):
                                    if val is None:
                                        continue
                                    sval = str(val)
                                    if re.search(r'\d{3,}', sval):
                                        cand_fields.append(sval)
                            # determine date for this row (fast heuristics)
                            ts = None
                            for tkey in ("LocaleMessageTime","SwipeDate","SwipeTime","timestamp","time","DateTime","date"):
                                if tkey in d and d.get(tkey):
                                    ts = d.get(tkey)
                                    break
                            if ts is None:
                                # quick scalar scan for iso date fragment
                                for key_path, val in _iter_scalars_in_obj(d):
                                    if val is None:
                                        continue
                                    s = str(val)
                                    if re.search(r'\d{4}-\d{2}-\d{2}', s) or re.search(r'\d{2}/\d{2}/\d{4}', s):
                                        ts = s
                                        break
                            if ts is None:
                                continue
                            # parse to date - handle common string formats without heavy dateutil
                            parsed_dt = None
                            if isinstance(ts, (int, float)):
                                try:
                                    parsed_dt = datetime.fromtimestamp(int(ts))
                                except Exception:
                                    try:
                                        parsed_dt = datetime.utcfromtimestamp(int(ts) / 1000.0)
                                    except Exception:
                                        parsed_dt = None
                            elif isinstance(ts, str):
                                s = ts.strip()
                                # try ISO-like fast parse
                                m = re.search(r'(\d{4}-\d{2}-\d{2})', s)
                                if m:
                                    try:
                                        parsed_dt = datetime.fromisoformat(m.group(1))
                                    except Exception:
                                        try:
                                            parsed_dt = datetime.strptime(m.group(1), "%Y-%m-%d")
                                        except Exception:
                                            parsed_dt = None
                                else:
                                    # fallback: try a couple common formats
                                    for f in ("%Y-%m-%dT%H:%M:%S.%fZ","%Y-%m-%dT%H:%M:%S","%d/%m/%Y %H:%M:%S","%d/%m/%Y"):
                                        try:
                                            parsed_dt = datetime.strptime(s, f)
                                            break
                                        except Exception:
                                            parsed_dt = None
                            if parsed_dt is None:
                                continue
                            dt = parsed_dt.date()
                            if dt < start_date or dt > end_date:
                                continue

                            # attempt to map candidate fields to orig_ids using the same matching helper
                            for cand in cand_fields:
                                try:
                                    m = _match_candidate_to_employees(cand, set(orig_ids), orig_ids, {})  # re-use existing matcher
                                    if m:
                                        # coerce matched to orig string
                                        matched_key = next((o for o in orig_ids if str(o).strip() == str(m).strip()), str(m).strip())
                                        result.setdefault(matched_key, {})
                                        prev = result[matched_key].get(dt, 0)
                                        result[matched_key][dt] = 1 if (prev == 1 or 1) else prev
                                        break
                                except Exception:
                                    continue
                        except Exception:
                            continue
                else:
                    logger.info("[presence_fetch] region_clients.fetch_all_details returned no details")
            except Exception:
                logger.exception("region_clients.fetch_all_details fallback failed (continuing)")

            # 2) ensure region history cache is populated and try scanning preloaded history (less precise)
            try:
                prefetch_region_history()
                region_presence = _fetch_presence_from_region_histories(orig_ids, start_date, end_date, partition_filter=partition_filter, preloaded_entries=REGION_HISTORY_CACHE)
                for eid in orig_ids:
                    rp = region_presence.get(eid, {})
                    for d, v in rp.items():
                        if v and result.setdefault(eid, {}).get(d, 0) == 0:
                            result[eid][d] = 1
            except Exception:
                logger.exception("region_history fallback failed")
        except Exception:
            logger.exception("detail/history fallback collapsed")

    final_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] final presence coverage: %d/%d employees have at least one positive day", final_positive, len(orig_ids))
    return result

# ----------------------------
# Sheet vs AttendanceSummary comparison
# ----------------------------
def _compare_sheet_vs_db_summary(sel_df: pd.DataFrame, start_date: date, end_date: date) -> Dict[str, Any]:
    """
    For each employee in sel_df compute DB-derived weekly presence totals and produce mismatch diagnostics.
    Returns dict with:
      - per_employee_summary (employee_id -> {sheet:..., db_total:..., db_by_date: {...}})
      - missing_in_summary (list of employee rows with zero DB presence)
      - mismatches (list where sheet vs DB total differ)
      - extra_summary_ids (IDs present in DB during week but not in sheet selection)
    """
    out = {
        "per_employee_summary": {},
        "missing_in_summary": [],
        "mismatches": [],
        "extra_summary_ids": []
    }
    employee_ids = sel_df["employee_id"].astype(str).str.strip().tolist()
    employee_set = set(employee_ids)

    # fetch attendance rows for the range and for relevant employee ids (DB query)
    try:
        with SessionLocal() as db:
            rows = db.query(AttendanceSummary).filter(
                AttendanceSummary.date >= start_date,
                AttendanceSummary.date <= end_date
            ).all()
    except Exception:
        logger.exception("Failed to query AttendanceSummary for sheet-vs-db comparison; will attempt per-id chunked queries")
        rows = []

    # Map DB rows by normalized employee id
    db_map = {}
    for r in rows:
        try:
            rid = r.employee_id
            if rid is None:
                continue
            rk = str(rid).strip()
            # prefer exact string match to sheet ids
            candidates = [rk]
            digits = _digits_only(rk)
            if digits:
                candidates.append(digits.lstrip('0') or digits)
            # find best matching sheet id
            matched = None
            for c in candidates:
                if c in employee_set:
                    matched = c
                    break
            if not matched:
                # try more expensive matching
                for s in employee_ids:
                    if s == rk or rk.lstrip('0') == s or s.lstrip('0') == rk:
                        matched = s
                        break
            if not matched:
                # add as extra (db-only) under its raw key
                db_map.setdefault(rk, {})
                db_map[rk].setdefault(r.date, 0)
                try:
                    present = int(r.presence_count or 0)
                except Exception:
                    present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
                db_map[rk][r.date] = 1 if (db_map[rk].get(r.date, 0) == 1 or present > 0) else db_map[rk].get(r.date, 0)
            else:
                db_map.setdefault(matched, {})
                db_map[matched].setdefault(r.date, 0)
                try:
                    present = int(r.presence_count or 0)
                except Exception:
                    present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
                db_map[matched][r.date] = 1 if (db_map[matched].get(r.date, 0) == 1 or present > 0) else db_map[matched].get(r.date, 0)
        except Exception:
            continue

    # ensure all sheet employees have entries per day (0 default)
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            db_map.setdefault(eid, {})
            if cur not in db_map[eid]:
                db_map[eid][cur] = 0
        cur = cur + timedelta(days=1)

    # compute per-employee summary and mismatches
    for eid in employee_ids:
        try:
            db_by_date = {d.isoformat(): int(v) for d, v in db_map.get(eid, {}).items()}
            db_total = sum(int(v) for v in db_map.get(eid, {}).values())
            # sheet-side: we don't have presence per-day on the sheet; so sheet's weekly presence is unknown.
            # But we can at least mark employees missing from DB or with low coverage.
            entry = {
                "employee_id": eid,
                "full_name": sel_df[sel_df["employee_id"] == eid]["full_name"].iloc[0] if not sel_df[sel_df["employee_id"] == eid].empty else None,
                "db_total_week_presence": db_total,
                "db_by_date": db_by_date
            }
            out["per_employee_summary"][eid] = entry
            if db_total == 0:
                out["missing_in_summary"].append(entry)
            # If you had a target expected presence from sheet metadata you could compare here; for now, put thresholds:
            # We'll treat employees with db_total < 3 (for regulars) as defaulters -> included in mismatches so they are visible
            out["mismatches"].append(entry) if db_total < 3 else None
        except Exception:
            continue

    # compute extras: DB IDs with presence >0 that are not in sheet employee_ids
    extras = []
    for dbid, bydates in db_map.items():
        try:
            if dbid not in employee_set:
                total = sum(int(v) for v in bydates.values())
                if total > 0:
                    extras.append({"employee_id": dbid, "db_total_week_presence": total, "db_by_date": {d.isoformat(): v for d, v in bydates.items()}})
        except Exception:
            continue
    out["extra_summary_ids"] = extras

    return out

# ----------------------------
# Main compare function (public)
# ----------------------------
def compare_ccure_vs_sheets(
    mode: str = "full",
    stats_detail: str = "ActiveProfiles",
    limit_list: int = 200,
    export: bool = False,
    region_filter: Optional[str] = None,
    location_city: Optional[str] = None,
    location_state: Optional[str] = None,
    location_description: Optional[str] = None,
    week_ref_date: Optional[str] = None
):
    # compute week window
    if week_ref_date:
        monday, friday = _week_monday_and_friday(date.fromisoformat(week_ref_date))
    else:
        monday, friday = _week_monday_and_friday(date.today())

    try:
        df = load_active_employees_dataframe()
    except Exception as e:
        logger.exception("Failed to load active employees sheet")
        return {"error": f"active sheet load failed: {e}"}

    rf = region_filter.strip().lower() if region_filter else None
    lc = location_city.strip().lower() if location_city else None
    ls = location_state.strip().lower() if location_state else None
    ld = location_description.strip().lower() if location_description else None

    sel = df.copy()
    if rf:
        sel = sel[sel["region_code"].fillna("").str.strip().str.lower() == rf]
    if lc:
        sel = sel[sel["location_city"].fillna("").str.strip().str.lower() == lc]
    if ls:
        sel = sel[sel["location_state"].fillna("").str.strip().str.lower() == ls]
    if ld:
        sel = sel[sel["location_desc"].fillna("").str.strip().str.lower() == ld]

    total_active = len(sel)
    employee_ids = sel["employee_id"].astype(str).str.strip().tolist()

    # Ensure region history cache is primed (should be done by caller/app)
    try:
        prefetch_region_history(timeout=10)
    except Exception:
        logger.exception("prefetch_region_history failed (continuing)")

    # presence_map: best-effort using DB + region history fallback
    presence_map = _fetch_presence_for_employees(employee_ids, monday, friday, partition_filter=location_city)

    # compute today count (today inside week may differ; we compute today's presence using presence_map and DB fallback)
    today = date.today()
    today_count = 0
    for eid in employee_ids:
        pm = presence_map.get(eid, {})
        if pm.get(today, 0) > 0:
            today_count += 1
        else:
            # fallback DB check
            try:
                with SessionLocal() as db:
                    row = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == eid, AttendanceSummary.date == today).first()
                    if row and getattr(row, "presence_count", 0) > 0:
                        today_count += 1
                        continue
                    digits = _digits_only(eid)
                    if digits:
                        cand = digits.lstrip('0') or digits
                        row2 = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == cand, AttendanceSummary.date == today).first()
                        if row2 and getattr(row2, "presence_count", 0) > 0:
                            today_count += 1
            except Exception:
                continue

    today_pct = round((today_count / float(total_active)) * 100.0, 2) if total_active > 0 else None

    sel["on_leave"] = sel["current_status"].apply(lambda x: _maybe_mark_on_leave(x))
    leave_count = int(sel["on_leave"].sum())
    sel["employee_type_norm"] = sel["employee_type"].fillna("").str.strip().str.lower()
    type_counts = sel["employee_type_norm"].value_counts().to_dict()

    regular_df = sel[sel["employee_type_norm"].str.contains("regular", na=False)]
    regular_ids = regular_df["employee_id"].astype(str).str.strip().tolist()

    regular_presence = {}
    for eid in regular_ids:
        week_map = presence_map.get(eid, {})
        days_present = sum(1 for d, v in week_map.items() if v and (monday <= d <= friday))
        days_present = int(days_present)
        regular_presence[eid] = {
            "days_present": days_present,
            "on_leave": bool(sel[sel["employee_id"] == eid]["on_leave"].any()),
            "full_name": sel[sel["employee_id"] == eid]["full_name"].iloc[0] if not sel[sel["employee_id"] == eid].empty else None
        }

    present_5_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 5)
    present_3_or_more_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 3)
    present_less_than_3_count = sum(1 for v in regular_presence.values() if v["days_present"] < 3)
    present_only_1_count = sum(1 for v in regular_presence.values() if v["days_present"] == 1)

    present_5_list = []
    present_3_list = []
    defaulters_list = []

    for eid, info in regular_presence.items():
        entry = {
            "employee_id": eid,
            "full_name": info["full_name"],
            "days_present": info["days_present"],
            "on_leave": info["on_leave"]
        }
        if info["days_present"] >= 5:
            present_5_list.append(entry)
        if info["days_present"] >= 3:
            present_3_list.append(entry)
        if info["days_present"] < 3:
            defaulters_list.append(entry)

    present_5_list = sorted(present_5_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    present_3_list = sorted(present_3_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    defaulters_list = sorted(defaulters_list, key=lambda x: (x["days_present"], x["on_leave"], x["full_name"] or ""))[:limit_list]

    # NEW: explicit sheet vs DB summary comparison diagnostics
    try:
        sheet_vs_summary = _compare_sheet_vs_db_summary(sel, monday, friday)
    except Exception:
        logger.exception("sheet vs summary comparison failed")
        sheet_vs_summary = {"error": "comparison failure"}

    summary = {
        "filters": {
            "region": region_filter,
            "location_city": location_city,
            "location_state": location_state,
            "location_description": location_description,
            "week_monday": monday.isoformat(),
            "week_friday": friday.isoformat()
        },
        "counts": {
            "total_active_in_sheet": total_active,
            "today_headcount_from_summary": today_count,
            "today_headcount_pct_vs_sheet": today_pct,
            "on_leave_count_in_sheet": leave_count,
            "employee_type_counts": type_counts
        },
        "regular_attendance_summary": {
            "regular_total": len(regular_ids),
            "present_5_day_count": present_5_count,
            "present_3_or_more_count": present_3_or_more_count,
            "present_less_than_3_count": present_less_than_3_count,
            "present_only_1_day_count": present_only_1_count
        },
        # attach diagnostics
        "sheet_vs_summary": sheet_vs_summary
    }

    details = {
        "present_5_days": present_5_list,
        "present_3_or_more_days": present_3_list,
        "defaulters_less_than_3_days": defaulters_list,
        "sheet_vs_summary_rows": sheet_vs_summary.get("per_employee_summary") if isinstance(sheet_vs_summary, dict) else {}
    }

    report_path = None
    if export:
        try:
            report_name = f"attendance_compare_{(region_filter or 'all')}_{(location_city or 'all')}_{uuid.uuid4().hex[:8]}.xlsx"
            report_file = OUTPUT_DIR / report_name
            with pd.ExcelWriter(report_file, engine="openpyxl") as writer:
                sel_df_for_export = sel.copy()
                sel_df_for_export["raw_row_str"] = sel_df_for_export["raw_row"].apply(lambda r: str(r) if r is not None else "")
                sel_df_for_export.to_excel(writer, sheet_name="active_sheet_selection", index=False)
                pd.DataFrame([summary["counts"]]).to_excel(writer, sheet_name="summary_counts", index=False)
                pd.DataFrame([summary["regular_attendance_summary"]]).to_excel(writer, sheet_name="regular_summary", index=False)
                pd.DataFrame(details["present_5_days"]).to_excel(writer, sheet_name="present_5_days", index=False)
                pd.DataFrame(details["present_3_or_more_days"]).to_excel(writer, sheet_name="present_3_plus", index=False)
                pd.DataFrame(details["defaulters_less_than_3_days"]).to_excel(writer, sheet_name="defaulters_lt3", index=False)
                # sheet vs summary diagnostics
                try:
                    # per_employee_summary -> long table
                    per_emp = sheet_vs_summary.get("per_employee_summary") or {}
                    per_emp_rows = []
                    for k, v in per_emp.items():
                        r = {"employee_id": k, "full_name": v.get("full_name"), "db_total_week_presence": v.get("db_total_week_presence")}
                        # flatten db_by_date
                        for d, val in sorted((v.get("db_by_date") or {}).items()):
                            r[f"day_{d}"] = val
                        per_emp_rows.append(r)
                    if per_emp_rows:
                        pd.DataFrame(per_emp_rows).to_excel(writer, sheet_name="sheet_vs_db_per_employee", index=False)
                    if sheet_vs_summary.get("missing_in_summary"):
                        pd.DataFrame(sheet_vs_summary.get("missing_in_summary")).to_excel(writer, sheet_name="missing_in_summary", index=False)
                    if sheet_vs_summary.get("extra_summary_ids"):
                        pd.DataFrame(sheet_vs_summary.get("extra_summary_ids")).to_excel(writer, sheet_name="extra_summary_ids", index=False)
                except Exception:
                    logger.exception("Failed writing sheet_vs_summary sections to export")
            report_path = str(report_file.name)
        except Exception:
            logger.exception("Failed to write export report")
            report_path = None

    out = {
        "mode": mode,
        "stats_detail": stats_detail,
        "summary": summary,
        "details": details
    }
    if report_path:
        out["report_path"] = report_path
    return out


if __name__ == "__main__":
    res = compare_ccure_vs_sheets(region_filter="APAC", location_city="Pune", export=False, limit_list=20)
    import json as _json
    print(_json.dumps(res, indent=2, default=str))








5)



# region_clients.py
"""
HTTP helpers for region occupancy endpoints.
Returns:
 - fetch_all_regions() -> list of {"region": <name>, "count": <int>}
 - fetch_all_details(timeout=...) -> list of person-detail dicts (where available)
 - fetch_all_history(timeout=...) -> list of date-or-detail dicts (history endpoints)

Drop-in replacement for your region_clients.py.
"""

import requests
from requests.exceptions import RequestException
from datetime import datetime
import logging
import time
import sys

logger = logging.getLogger("region_clients")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# endpoints (edit if your hosts differ)
endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/live-summary",
    "emea":  "http://10.199.22.57:3007/api/occupancy/live-summary",
    "laca":  "http://10.199.22.57:4000/api/occupancy/live-summary",
    "apac":  "http://10.199.22.57:3008/api/occupancy/live-summary"
}

history_endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/history",
    "emea":  "http://10.199.22.57:3007/api/occupancy/history",
    "apac":  "http://10.199.22.57:3008/api/occupancy/history",
    "laca":  "http://10.199.22.57:4000/api/occupancy/history"
}

DEFAULT_ATTEMPTS = 3
DEFAULT_BACKOFF = 0.6

def _do_get_with_retries(url, timeout, attempts=DEFAULT_ATTEMPTS, backoff=DEFAULT_BACKOFF):
    last_err = None
    for attempt in range(1, attempts + 1):
        try:
            # use connect,read timeouts tuple for clarity
            # r = requests.get(url, timeout=(3, max(5, timeout)))

            r = requests.get(url, timeout=(3, max(10, timeout)))

            r.raise_for_status()
            try:
                return r.json()
            except ValueError:
                try:
                    return {"_raw_text": r.text}
                except Exception:
                    return None
        except RequestException as e:
            last_err = e
            logger.warning(f"[region_clients] attempt {attempt}/{attempts} failed for {url}: {e}")
            if attempt < attempts:
                time.sleep(backoff * attempt)
            continue
    logger.warning(f"[region_clients] all {attempts} attempts failed for {url}: {last_err}")
    return None

def fetch_all_regions(timeout=6):
    results = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            realtime = {}
            if isinstance(data, dict):
                realtime = data.get("realtime", {}) or {}
            total = 0
            for site in realtime.values():
                try:
                    total += int(site.get("total", 0))
                except Exception:
                    pass
            if total == 0 and isinstance(data, dict):
                for k, v in data.items():
                    if isinstance(v, dict) and "total" in v:
                        try:
                            total += int(v.get("total", 0))
                        except Exception:
                            pass
            results.append({"region": region, "count": total})
        except Exception as e:
            logger.exception(f"[region_clients] unexpected error fetching live-summary for {region}: {e}")
            results.append({"region": region, "count": None})
    return results

def _extract_list_candidates(data):
    """
    Helper to find lists that may contain detail rows inside a payload.
    """
    if isinstance(data, list):
        return [x for x in data if isinstance(x, dict)]
    if isinstance(data, dict):
        for k in ("details","people","list","items","results","data","entries"):
            if k in data and isinstance(data[k], list):
                return [x for x in data[k] if isinstance(x, dict)]
        # top-level dict might be single row; return empty here (caller will inspect)
        return []
    return []

def fetch_all_details(timeout=6):
    """
    Attempt to fetch person-level detail records from live-summary endpoints.
    Returns a list of dicts (flattened detail rows). Regions that don't provide details will be skipped.
    """
    all_details = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            details = _extract_list_candidates(data)
            # also attempt to find nested partitions that contain details
            if not details and isinstance(data, dict):
                for v in data.values():
                    if isinstance(v, dict):
                        candidates = _extract_list_candidates(v)
                        if candidates:
                            details.extend(candidates)
            for d in details:
                try:
                    d2 = dict(d)
                    d2["_region"] = region
                    d2["_source_url"] = url
                    all_details.append(d2)
                except Exception:
                    continue
        except Exception as e:
            logger.warning(f"[region_clients] warning: cannot fetch details from {region}@{url}: {e}")
            continue

    # try history endpoints as second chance (they sometimes include per-person rows)
    if not all_details:
        for region, url in history_endpoints.items():
            try:
                data = _do_get_with_retries(url, timeout=timeout) or {}
                details = _extract_list_candidates(data)
                for d in details:
                    try:
                        d2 = dict(d)
                        d2["_region"] = region
                        d2["_source_url"] = url
                        all_details.append(d2)
                    except Exception:
                        continue
            except Exception:
                logger.debug(f"[region_clients] history details fetch for {region} failed", exc_info=True)
                continue

    logger.info("[region_clients] fetched %d detail rows across endpoints", len(all_details))
    return all_details

def fetch_history_for_region(region, timeout=6):
    url = history_endpoints.get(region)
    if not url:
        logger.debug(f"[fetch_history_for_region] no history endpoint for {region}")
        return []
    try:
        data = _do_get_with_retries(url, timeout=timeout) or {}
        summary = []
        if isinstance(data, dict):
            # try common key names
            for key in ("summaryByDate","summary","data","entries","results","details","items"):
                if key in data and isinstance(data.get(key), list):
                    for s in data.get(key):
                        if isinstance(s, dict):
                            s2 = dict(s)
                            s2["_region"] = region
                            s2["_source_url"] = url
                            summary.append(s2)
                    break
            else:
                # maybe single dict date-summary
                if "date" in data or "day" in data:
                    s2 = dict(data)
                    s2["_region"] = region
                    s2["_source_url"] = url
                    summary.append(s2)
        elif isinstance(data, list):
            for s in data:
                if isinstance(s, dict):
                    s2 = dict(s)
                    s2["_region"] = region
                    s2["_source_url"] = url
                    summary.append(s2)
        return summary
    except Exception as e:
        logger.warning(f"[region_clients] cannot fetch history for {region}@{url}: {e}")
        return []

def fetch_all_history(timeout=6):
    all_entries = []
    for region in history_endpoints.keys():
        try:
            entries = fetch_history_for_region(region, timeout=timeout)
            if entries:
                all_entries.extend(entries)
        except Exception:
            logger.exception(f"[region_clients] error aggregating history for {region}")
            continue
    logger.info("[region_clients] fetched %d history entries", len(all_entries))
    return all_entries



