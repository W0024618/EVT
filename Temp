check below each file line by line carefully and solve the issue 
only 
to fetch average , ccure comparision , and Upload active Employee , Contractor and their Comparision ,
Location average issue only , keep another logic as it is 

Failed to load CCURE averages we got this error again and again

check console details ..

	PS C:\Users\W0024618\Desktop\global-page> & C:/Users/W0024618/Desktop/global-page/backend/attendance-analytics/.venv/Scripts/Activate.ps1
(.venv) PS C:\Users\W0024618\Desktop\global-page> cd backend
(.venv) PS C:\Users\W0024618\Desktop\global-page\backend> cd attendance-analytics                             
(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> python -m uvicorn app:app --host 0.0.0.0 --port 8000
INFO:     Started server process [26160]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:50856 - "GET /ccure/stream HTTP/1.1" 200 OK
2025-09-09 15:55:55,199 ERROR attendance_app: Failed to aggregate range results for region apac
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py", line 4088, in api_duration
    coerced = _coerce_result_region(region_obj)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py", line 4021, in _coerce_result_region
    out = {"durations": pd.DataFrame(), "swipes": pd.DataFrame()}
                        ^^
NameError: name 'pd' is not defined. Did you mean: 'id'?
ERROR:attendance_app:Failed to aggregate range results for region apac
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py", line 4088, in api_duration
    coerced = _coerce_result_region(region_obj)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py", line 4021, in _coerce_result_region
    out = {"durations": pd.DataFrame(), "swipes": pd.DataFrame()}
                        ^^
NameError: name 'pd' is not defined. Did you mean: 'id'?
INFO:     127.0.0.1:53127 - "GET /duration?start_date=2025-08-25&end_date=2025-08-26&regions=apac&city=APAC.Default HTTP/1.1" 200 OK
INFO:     127.0.0.1:64869 - "GET /ccure/stream HTTP/1.1" 200 OK
[compute_daily_attendance] no swipes for 2025-09-09
2025-09-09 15:56:14,684 INFO region_clients: [region_clients] fetched 1834 detail rows across endpoints
INFO:region_clients:[region_clients] fetched 1834 detail rows across endpoints
2025-09-09 15:56:19,181 ERROR attendance_app: Failed to aggregate range results for region apac
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py", line 4088, in api_duration
    coerced = _coerce_result_region(region_obj)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py", line 4021, in _coerce_result_region
    out = {"durations": pd.DataFrame(), "swipes": pd.DataFrame()}
                        ^^
NameError: name 'pd' is not defined. Did you mean: 'id'?
ERROR:attendance_app:Failed to aggregate range results for region apac
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py", line 4088, in api_duration
    coerced = _coerce_result_region(region_obj)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py", line 4021, in _coerce_result_region
    out = {"durations": pd.DataFrame(), "swipes": pd.DataFrame()}
                        ^^
NameError: name 'pd' is not defined. Did you mean: 'id'?
[compute_daily_attendance] no swipes for 2025-09-09
[compute_daily_attendance] no swipes for 2025-09-09
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=20)")': /api/occupancy/history
2025-09-09 15:56:41,243 INFO region_clients: [region_clients] fetched 1838 detail rows across endpoints
INFO:region_clients:[region_clients] fetched 1838 detail rows across endpoints
2025-09-09 15:56:42,601 INFO region_clients: [region_clients] fetched 1838 detail rows across endpoints
INFO:region_clients:[region_clients] fetched 1838 detail rows across endpoints
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=20)")': /api/occupancy/history
2025-09-09 15:57:38,398 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:3006/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3006): Max retries exceeded with url: /api/occupancy/history (Caused by ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=20)"))
WARNING:region_clients:[region_clients] attempt 1/3 failed for http://10.199.22.57:3006/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=3006): Max retries exceeded with url: /api/occupancy/history (Caused by ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=20)"))
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=20)")': /api/occupancy/history
2025-09-09 15:58:46,515 INFO region_clients: [region_clients] fetched 72 history entries
INFO:region_clients:[region_clients] fetched 72 history entries
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)")': /api/occupancy/history
2025-09-09 15:59:59,764 INFO region_clients: [region_clients] fetched 72 history entries
INFO:region_clients:[region_clients] fetched 72 history entries
INFO:     127.0.0.1:52988 - "GET /ccure/verify?raw=true HTTP/1.1" 200 OK
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)")': /api/occupancy/history
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)")': /api/occupancy/history
2025-09-09 16:01:08,961 INFO region_clients: [region_clients] fetched 72 history entries
INFO:region_clients:[region_clients] fetched 72 history entries
INFO:     127.0.0.1:56721 - "GET /ccure/verify?raw=true HTTP/1.1" 200 OK



# app.py (top portion) - minor cleanup (remove duplicate import)
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.encoders import jsonable_encoder
import shutil
import uuid
import json
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
import re
import asyncio
from typing import Optional, Dict, Any

# --- DB / models imports (kept for endpoints that still use DB like headcount/attendance fallback) ---
from db import SessionLocal
from models import LiveSwipe, AttendanceSummary

# --- settings (optional override) ---
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"

import sys  # ensure logging stream available
RAW_UPLOADS_DIR = DATA_DIR / "raw_uploads"
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)




# ----------------- GLOBAL TIMEOUTS (UNIFY) -----------------
REGION_TIMEOUT_SECONDS = 20
COMPUTE_WAIT_TIMEOUT_SECONDS = 30
COMPUTE_SYNC_TIMEOUT_SECONDS = 60
# ----------------------------------------------------------

_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

_broadcaster_clients = set()

def broadcast_ccure_update(payload: dict):
    if not _broadcaster_clients:
        return
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = None
    for q in list(_broadcaster_clients):
        try:
            if loop and loop.is_running():
                loop.call_soon_threadsafe(q.put_nowait, payload)
            else:
                q.put_nowait(payload)
        except Exception:
            logger.exception("Failed to push payload to SSE client (will remove client)")
            try:
                _broadcaster_clients.discard(q)
            except Exception:
                pass

async def _sse_event_generator(client_queue: asyncio.Queue):
    try:
        while True:
            payload = await client_queue.get()
            try:
                data = json.dumps(payload, default=str)
            except Exception:
                data = json.dumps({"error": "serialization error", "payload": str(payload)})
            yield f"data: {data}\n\n"
    finally:
        try:
            _broadcaster_clients.discard(client_queue)
        except Exception:
            pass
        return

@app.get("/ccure/stream")
async def ccure_stream():
    q = asyncio.Queue()
    _broadcaster_clients.add(q)
    generator = _sse_event_generator(q)
    headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    return StreamingResponse(generator, media_type="text/event-stream", headers=headers)

def _guess_region_from_text(txt: str) -> str:
    if not txt:
        return "unknown"
    s = str(txt).strip().lower()
    s = re.sub(r"[,\-/()]", " ", s)
    if any(k in s for k in ("pune","quezon city","taguig city","bengaluru","hyderabad","chennai","manila","singapore","hong kong","beijing","shanghai","jakarta","kuala","osaka","tokyo","seoul","bangkok")):
        return "apac"
    if any(k in s for k in ("london","dublin","paris","frankfurt","amsterdam","stockholm","cape town","johannesburg","berlin","brussels","madrid","rome","milan")):
        return "emea"
    if any(k in s for k in ("mexico","bogota","buenos","santiago","sao","salvador","lima","caracas")):
        return "laca"
    if any(k in s for k in ("denver","new york","ny","chicago","toronto","vancouver","los angeles","san francisco","boston","houston","atlanta","miami")):
        return "namer"
    return "unknown"

@app.get("/headcount")
def api_headcount():
    try:
        totals = {"apac": 0, "emea": 0, "laca": 0, "namer": 0, "unknown": 0}
        with SessionLocal() as db:
            try:
                today = date.today()
                rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                if rows:
                    for r in rows:
                        try:
                            partition = None
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                            loc = partition or "unknown"
                            region = _guess_region_from_text(loc)
                            totals[region] = totals.get(region, 0) + 1
                        except Exception:
                            totals["unknown"] += 1
                else:
                    start = datetime.combine(today, datetime.min.time())
                    end = datetime.combine(today, datetime.max.time())
                    swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
                    for s in swipes:
                        loc = s.partition or "unknown"
                        region = _guess_region_from_text(loc)
                        totals[region] = totals.get(region, 0) + 1
            except Exception:
                logger.exception("Failed to compute headcount regions")
        out = {
            "apac": int(totals.get("apac", 0)),
            "emea": int(totals.get("emea", 0)),
            "laca": int(totals.get("laca", 0)),
            "namer": int(totals.get("namer", 0))
        }
        return JSONResponse(out)
    except Exception as exc:
        logger.exception("api_headcount failed")
        raise HTTPException(status_code=500, detail=f"headcount error: {exc}")

# ---------- Helpers retained (normalize / safe conversions) -------------
def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- build_ccure_averages (fallback) ------------------------------
def build_ccure_averages(start_date: Optional[str] = None, end_date: Optional[str] = None):
    """
    Fallback averages computation using AttendanceSummary only.
    Returns a compact shape (live_today, ccure_active, averages, notes).
    """
    try:
        def _parse_date_param(s):
            if not s:
                return None
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                try:
                    return date.fromisoformat(s)
                except Exception:
                    return None

        today = date.today()
        start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
        end_obj = _parse_date_param(end_date) if end_date else today
        if start_obj is None or end_obj is None or start_obj > end_obj:
            start_obj = today - timedelta(days=6)
            end_obj = today

        with SessionLocal() as db:
            try:
                att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
            except Exception:
                logger.exception("Failed to query AttendanceSummary")
                att_rows = []

            live_emp = 0
            live_contr = 0
            unknown_count = 0
            seen_keys = set()

            def classify_from_derived(derived):
                try:
                    if not derived or not isinstance(derived, dict):
                        return "contractor"
                    for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                        v = derived.get(k)
                        if v and "employee" in str(v).strip().lower():
                            return "employee"
                    for k in ("Employee_Status","Employee Status","Status"):
                        v = derived.get(k)
                        if v and "terminated" in str(v).strip().lower():
                            return "employee"
                    return "contractor"
                except Exception:
                    return "contractor"

            if att_rows:
                for a in att_rows:
                    key = None
                    try:
                        key = _normalize_employee_key(a.employee_id)
                    except Exception:
                        key = None
                    if not key:
                        try:
                            key = _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None)
                        except Exception:
                            key = None
                    if not key:
                        unknown_count += 1
                        continue
                    if key in seen_keys:
                        continue
                    seen_keys.add(key)
                    cls = classify_from_derived(a.derived)
                    if cls == "employee":
                        live_emp += 1
                    elif cls == "contractor":
                        live_contr += 1
                    else:
                        unknown_count += 1

                live_total_reported = live_emp + live_contr + unknown_count
                live_total_details = len(att_rows)
            else:
                live_total_reported = 0
                live_total_details = 0
                try:
                    import region_clients
                    details = []
                    try:
                        details = region_clients.fetch_all_details(timeout=REGION_TIMEOUT_SECONDS) or []
                    except Exception:
                        logger.exception("region_clients.fetch_all_details failed in build_ccure_averages()")
                        details = []
                    if details:
                        for d in details:
                            try:
                                cls = "contractor"
                                for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                                    v = d.get(k)
                                    if v and "employee" in str(v).strip().lower():
                                        cls = "employee"
                                        break
                                if cls == "employee":
                                    live_emp += 1
                                else:
                                    live_contr += 1
                                live_total_details += 1
                            except Exception:
                                continue
                        live_total_reported = live_emp + live_contr
                    else:
                        try:
                            regions = region_clients.fetch_all_regions(timeout=REGION_TIMEOUT_SECONDS) or []
                            for r in regions:
                                try:
                                    c = r.get("count")
                                    if isinstance(c, (int, float)):
                                        live_total_reported += int(c)
                                except Exception:
                                    continue
                        except Exception:
                            logger.exception("region_clients.fetch_all_regions failed in build_ccure_averages()")
                except Exception:
                    logger.exception("region_clients not importable in build_ccure_averages()")

            # compute avg range using AttendanceSummary if possible
            avg_range = None
            try:
                q = db.query(AttendanceSummary.date, AttendanceSummary.employee_id, AttendanceSummary.presence_count)\
                      .filter(AttendanceSummary.date >= start_obj, AttendanceSummary.date <= end_obj).all()
                by_date = {}
                for row in q:
                    d = row[0]
                    key = (row[1] or "").strip() if row[1] else None
                    if not key:
                        continue
                    if d not in by_date:
                        by_date[d] = set()
                    try:
                        presence_val = getattr(row, 'presence_count', row[2])
                        if int(presence_val) > 0:
                            by_date[d].add(key)
                    except Exception:
                        by_date[d].add(key)
                days_count = (end_obj - start_obj).days + 1
                daily_counts = [len(by_date.get(start_obj + timedelta(days=i), set())) for i in range(days_count)]
                if days_count and any(daily_counts):
                    avg_range = int(round(sum(daily_counts) / float(days_count)))
                else:
                    avg_range = None
            except Exception:
                logger.exception("Failed computing range average from AttendanceSummary")
                avg_range = None

        # fallback: use region history to compute avg_range if still None
        if avg_range is None:
            try:
                import region_clients
                entries = region_clients.fetch_all_history(timeout=REGION_TIMEOUT_SECONDS) or []
                agg = {}
                for e in entries:
                    try:
                        dstr = e.get("date")
                        if not dstr:
                            continue
                        region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                        emp = None
                        con = None
                        tot = None
                        if region_obj:
                            emp = _safe_int(region_obj.get("Employee"))
                            con = _safe_int(region_obj.get("Contractor"))
                            tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                        else:
                            emp = _safe_int(e.get("Employee"))
                            con = _safe_int(e.get("Contractor"))
                            tot = _safe_int(e.get("total"))
                        if emp is None and con is None:
                            continue
                        if tot is None:
                            tot = (emp or 0) + (con or 0)
                        if dstr not in agg:
                            agg[dstr] = {"total": 0, "count": 0}
                        agg[dstr]["total"] += tot or 0
                        agg[dstr]["count"] += 1
                    except Exception:
                        continue
                per_date_totals = []
                days_count = (end_obj - start_obj).days + 1
                for i in range(days_count):
                    dcheck = (start_obj + timedelta(days=i)).isoformat()
                    if dcheck in agg and agg[dcheck]["count"] > 0:
                        per_day_avg = float(agg[dcheck]["total"]) / float(agg[dcheck]["count"])
                        per_date_totals.append(per_day_avg)
                if per_date_totals:
                    avg_range = int(round(sum(per_date_totals) / float(len(per_date_totals))))
            except Exception:
                logger.exception("Failed computing avg_range from region history in fallback")

        # get ccure stats if available
        ccure_stats = {}
        try:
            import ccure_client
            if hasattr(ccure_client, "get_global_stats"):
                ccure_stats = ccure_client.get_global_stats() or {}
        except Exception:
            logger.debug("ccure_client.get_global_stats not available", exc_info=True)

        cc_active_emps = None
        cc_active_contractors = None
        try:
            if isinstance(ccure_stats, dict):
                a = ccure_stats.get("ActiveEmployees") or ccure_stats.get("active_employees") or None
                b = ccure_stats.get("ActiveContractors") or ccure_stats.get("active_contractors") or None
                if a is not None and str(a).strip() != "":
                    cc_active_emps = int(a)
                if b is not None and str(b).strip() != "":
                    cc_active_contractors = int(b)
        except Exception:
            cc_active_emps = cc_active_contractors = None

        emp_pct = None
        contr_pct = None
        overall_pct = None
        try:
            if isinstance(cc_active_emps, int) and cc_active_emps > 0:
                emp_pct = round((live_emp / float(cc_active_emps)) * 100.0, 2)
            if isinstance(cc_active_contractors, int) and cc_active_contractors > 0:
                contr_pct = round((live_contr / float(cc_active_contractors)) * 100.0, 2)
            if isinstance(cc_active_emps, int) and isinstance(cc_active_contractors, int) and (cc_active_emps + cc_active_contractors) > 0:
                overall_pct = round(((live_emp + live_contr) / float(cc_active_emps + cc_active_contractors)) * 100.0, 2)
        except Exception:
            emp_pct = contr_pct = overall_pct = None

        resp = {
            "date": today.isoformat(),
            "notes": f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}" if (start_date or end_date) else None,
            "live_today": {
                "employee": live_emp,
                "contractor": live_contr,
                "total_reported": live_total_reported,
                "total_from_details": live_total_details
            },
            "ccure_active": {
                "active_employees": cc_active_emps,
                "active_contractors": cc_active_contractors,
                "ccure_active_employees_reported": cc_active_emps,
                "ccure_active_contractors_reported": cc_active_contractors
            },
            "averages": {
                "employee_pct": emp_pct,
                "contractor_pct": contr_pct,
                "overall_pct": overall_pct,
                "avg_headcount_last_7_days": avg_range,
                "head_emp_pct_vs_ccure_today": emp_pct,
                "head_contractor_pct_vs_ccure_today": contr_pct,
                "headcount_overall_pct_vs_ccure_today": overall_pct,
                "history_avg_overall_last_7_days": avg_range
            }
        }

        return resp
    except Exception:
        logger.exception("build_ccure_averages failed")
        raise

# -------------------------
# Upload endpoints (store uploads under data/ and data/raw_uploads/ and rotate)
# -------------------------
ALLOWED_EXT = (".xls", ".xlsx", ".csv")

def _remove_old_files_for_kind(kind: str):
    """
    Remove previous canonical files and previous raw uploads that include kind in filename.
    """
    try:
        # canonical in DATA_DIR
        for p in DATA_DIR.iterdir():
            if p.is_file() and f"active_{kind}" in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old canonical file %s", p)
        # remove raw uploads for same kind to keep only latest raw (user requested)
        for p in RAW_UPLOADS_DIR.iterdir():
            if p.is_file() and kind in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old raw file %s", p)
    except Exception:
        logger.exception("error while removing old files for kind=%s", kind)

def _save_upload_and_rotate(upload_file: UploadFile, kind: str) -> dict:
    """
    Save the uploaded file as:
      - data/active_<kind>.<ext>     (canonical)
      - data/raw_uploads/<timestamp>_<kind>_<origname>  (raw trace)
    Remove previous files for same kind (both canonical & raw).
    Returns metadata dict.
    """
    fname = Path(upload_file.filename).name
    ext = Path(fname).suffix.lower()
    if ext not in ALLOWED_EXT:
        raise HTTPException(status_code=400, detail="Only .xls .xlsx .csv allowed")

    # rotate old files
    _remove_old_files_for_kind(kind)

    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    raw_name = f"{ts}_{kind}_{fname}"
    raw_path = RAW_UPLOADS_DIR / raw_name

    # Save raw
    try:
        with raw_path.open("wb") as out_f:
            shutil.copyfileobj(upload_file.file, out_f)
    finally:
        try:
            upload_file.file.close()
        except Exception:
            pass

    # Write canonical: keep same extension as original (simpler)
    canonical_path = DATA_DIR / f"active_{kind}{ext}"
    try:
        shutil.copy(raw_path, canonical_path)
    except Exception:
        logger.exception("Failed to write canonical copy for %s", canonical_path)

    info = {
        "raw_saved": str(raw_path),
        "canonical_saved": str(canonical_path),
        "original_filename": fname,
        "size_bytes": raw_path.stat().st_size if raw_path.exists() else None
    }
    logger.info("Uploaded %s: %s", kind, info)
    return info





@app.post("/api/upload/active-employees")
async def upload_active_employees_api(file: UploadFile = File(...)):
    return await upload_active_employees(file)

@app.post("/api/upload/active-contractors")
async def upload_active_contractors_api(file: UploadFile = File(...)):
    return await upload_active_contractors(file)



@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    """
    Upload Active Employee sheet:
      - stores raw to data/raw_uploads and canonical to data/active_employee.*
      - removes previous uploaded employee sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="employee")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("employee upload failed")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    """
    Upload Active Contractor sheet:
      - stores raw to data/raw_uploads and canonical to data/active_contractor.*
      - removes previous uploaded contractor sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="contractor")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("contractor upload failed")
        raise HTTPException(status_code=500, detail=str(e))

# ---------- map detailed -> compact (used when compute returns detailed) ----
def _map_detailed_to_resp(detailed: Dict[str, Any]) -> Dict[str, Any]:
    # unchanged mapping from earlier implementation (kept identical to previous)
    live_h = detailed.get("live_headcount", {}) or {}
    head_h = detailed.get("headcount", {}) or {}
    ccure_active_obj = detailed.get("ccure_active", {}) or {}
    averages_obj = detailed.get("averages", {}) or {}

    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    live_employee = int(live_h.get("employee") or head_h.get("employee") or 0)
    live_contractor = int(live_h.get("contractor") or head_h.get("contractor") or 0)
    total_reported = int(
        live_h.get("currently_present_total")
        or head_h.get("total_visited_today")
        or (live_employee + live_contractor)
        or 0
    )
    total_from_details = int(head_h.get("total_visited_today") or 0)

    mapped_headcount = {
        "total_visited_today": int(head_h.get("total_visited_today") or 0),
        "employee": int(head_h.get("employee") or 0),
        "contractor": int(head_h.get("contractor") or 0),
        "by_location": head_h.get("by_location") or {}
    }

    resp = {
        "date": detailed.get("date"),
        "notes": detailed.get("notes"),
        "live_today": {
            "employee": live_employee,
            "contractor": live_contractor,
            "total_reported": total_reported,
            "total_from_details": total_from_details
        },
        "headcount": mapped_headcount,
        "live_headcount": live_h,
        "ccure_active": {
            "active_employees": ccure_active_obj.get("ccure_active_employees_reported")
                             or ccure_active_obj.get("active_employees"),
            "active_contractors": ccure_active_obj.get("ccure_active_contractors_reported")
                               or ccure_active_obj.get("active_contractors"),
            "ccure_active_employees_reported": ccure_active_obj.get("ccure_active_employees_reported"),
            "ccure_active_contractors_reported": ccure_active_obj.get("ccure_active_contractors_reported")
        },
        "averages": averages_obj
    }
    return resp

# ---------- build a verify-compatible summary from mapped payload -----------
def _build_verify_like_summary_from_mapped(mapped: Dict[str, Any], include_raw: bool = False) -> Dict[str, Any]:
    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    cc = mapped.get("ccure_active", {}) or {}
    head = mapped.get("headcount", {}) or {}
    live_head = mapped.get("live_headcount", {}) or {}
    averages = mapped.get("averages", {}) or {}

    cc_emp = to_int(cc.get("ccure_active_employees_reported") or cc.get("active_employees"))
    cc_con = to_int(cc.get("ccure_active_contractors_reported") or cc.get("active_contractors"))

    head_total = to_int(head.get("total_visited_today") or mapped.get("live_today", {}).get("total_from_details"))
    head_emp = to_int(head.get("employee") or mapped.get("live_today", {}).get("employee"))
    head_con = to_int(head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    live_total = to_int(live_head.get("currently_present_total") or mapped.get("live_today", {}).get("total_reported"))
    live_emp = to_int(live_head.get("employee") or mapped.get("live_today", {}).get("employee"))
    live_con = to_int(live_head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    history_emp_avg = averages.get("history_avg_employee_last_7_days")
    history_con_avg = averages.get("history_avg_contractor_last_7_days")
    history_overall_avg = averages.get("history_avg_overall_last_7_days")

    def pct(n, d):
        try:
            if n is None or d is None:
                return None
            if float(d) == 0:
                return None
            return round((float(n) / float(d)) * 100.0, 2)
        except Exception:
            return None

    summary = {
        "date": mapped.get("date"),
        "ccure_reported": {
            "employees": cc_emp,
            "contractors": cc_con,
            "total_reported": (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None
        },
        "headcount_attendance_summary": {
            "total_visited_today": head_total,
            "employee": head_emp,
            "contractor": head_con,
        },
        "live_headcount_region_clients": {
            "currently_present_total": live_total,
            "employee": live_emp,
            "contractor": live_con,
        },
        "percentages_vs_ccure": {
            "head_employee_pct_vs_ccure_today": pct(head_emp, cc_emp),
            "head_contractor_pct_vs_ccure_today": pct(head_con, cc_con),
            "head_overall_pct_vs_ccure_today": pct(head_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "live_employee_pct_vs_ccure_today": pct(live_emp, cc_emp),
            "live_contractor_pct_vs_ccure_today": pct(live_con, cc_con),
            "live_overall_pct_vs_ccure_today": pct(live_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "history_employee_pct_vs_ccure": pct(history_emp_avg, cc_emp),
            "history_contractor_pct_vs_ccure": pct(history_con_avg, cc_con),
            "history_overall_pct_vs_ccure": pct(history_overall_avg, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None)
        },
        "averages": {
            "history_avg_employee_last_7_days": history_emp_avg,
            "history_avg_contractor_last_7_days": history_con_avg,
            "history_avg_overall_last_7_days": history_overall_avg,
            "avg_headcount_last_7_days_db": averages.get("avg_headcount_last_7_days") or averages.get("avg_headcount_last_7_days_db"),
            "avg_headcount_per_site_last_7_days": averages.get("avg_headcount_per_site_last_7_days"),
            "employee_pct": averages.get("employee_pct"),
            "contractor_pct": averages.get("contractor_pct"),
            "overall_pct": averages.get("overall_pct"),
            **({k: v for k, v in averages.items() if k not in (
                "history_avg_employee_last_7_days",
                "history_avg_contractor_last_7_days",
                "history_avg_overall_last_7_days",
                "avg_headcount_last_7_days",
                "avg_headcount_last_7_days_db",
                "avg_headcount_per_site_last_7_days",
                "employee_pct","contractor_pct","overall_pct"
            )})
        },
        "notes": mapped.get("notes")
    }

    summary["headcount_details"] = {
        "total_visited_today": head_total,
        "employee": head_emp,
        "contractor": head_con,
        "by_location": head.get("by_location") if isinstance(head.get("by_location"), dict) else {}
    }
    summary["live_headcount_details"] = {
        "currently_present_total": live_total,
        "employee": live_emp,
        "contractor": live_con,
        "by_location": live_head.get("by_location") if isinstance(live_head.get("by_location"), dict) else {}
    }

    summary["ccure_active"] = {
        "active_employees": cc.get("active_employees") or cc.get("ccure_active_employees_reported"),
        "active_contractors": cc.get("active_contractors") or cc.get("ccure_active_contractors_reported"),
        "ccure_active_employees_reported": cc.get("ccure_active_employees_reported"),
        "ccure_active_contractors_reported": cc.get("ccure_active_contractors_reported")
    }

    return summary









# ---------- /ccure/verify (already present in your original) -----
@app.get("/ccure/verify")
def ccure_verify(
    raw: bool = Query(False, description="if true, include the raw compute payload for debugging"),
    start_date: Optional[str] = Query(None, description="YYYY-MM-DD start date (inclusive)"),
    end_date: Optional[str] = Query(None, description="YYYY-MM-DD end date (inclusive)")
):
    """
    Synchronous verification endpoint. Prefer compute_visit_averages() (synchronous call).
    If compute raises or fails, fall back to build_ccure_averages() so output shape remains consistent.
    """
    try:
        detailed = None
        try:
            from ccure_compare_service import compute_visit_averages
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
        except Exception:
            logger.exception("compute_visit_averages() failed inside /ccure/verify; falling back")
            detailed = None

        if isinstance(detailed, dict):
            mapped = _map_detailed_to_resp(detailed)
            summary = _build_verify_like_summary_from_mapped(mapped, include_raw=raw)
            if raw and isinstance(detailed, dict):
                summary["raw"] = detailed
            return JSONResponse(summary)
        else:
            fallback = build_ccure_averages(start_date, end_date)
            mapped_fallback = {
                "date": fallback.get("date"),
                "notes": fallback.get("notes"),
                "live_today": fallback.get("live_today", {}),
                "headcount": {
                    "total_visited_today": fallback.get("live_today", {}).get("total_from_details") or fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "live_headcount": {
                    "currently_present_total": fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "ccure_active": fallback.get("ccure_active", {}),
                "averages": fallback.get("averages", {})
            }

            summary = _build_verify_like_summary_from_mapped(mapped_fallback, include_raw=raw)
            if raw:
                summary["raw"] = mapped_fallback
       
            return JSONResponse(summary)
    except Exception as e:
        logger.exception("ccure_verify failed")
        raise HTTPException(status_code=500, detail=f"ccure verify error: {e}")

# ---------- /ccure/compare -> uses data_compare_service.compare_ccure_vs_sheets ----------
@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from data_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)




@app.get("/ccure/export_uploaded")
def ccure_export_uploaded():
    """
    Export the currently uploaded canonical sheets into one workbook.
    Returns JSON: { status: "ok", report_path: "<filename>" }
    Downloadable at /ccure/report/{filename}
    """
    try:
        from data_compare_service import export_uploaded_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed for export_uploaded")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    fname = export_uploaded_sheets()
    if not fname:
        raise HTTPException(status_code=500, detail="failed to export uploaded sheets")
    return JSONResponse({"status": "ok", "report_path": fname})





@app.get("/ccure/compare_v2")
def ccure_compare_v2(
    mode: str = Query("full"),
    stats_detail: str = Query("ActiveProfiles"),
    limit_list: int = Query(200, ge=1, le=5000),
    export: bool = Query(False),
    region_filter: Optional[str] = Query(None, description="Region code e.g. APAC"),
    location_city: Optional[str] = Query(None, description="Location city e.g. Pune"),
    location_state: Optional[str] = Query(None, description="Location state/province"),
    location_description: Optional[str] = Query(None, description="Location Description"),
    week_ref_date: Optional[str] = Query(None, description="YYYY-MM-DD for week (Mon-Fri)")
):
    try:
        from data_compare_service_v2 import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service_v2 import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(
        mode=mode,
        stats_detail=stats_detail,
        limit_list=limit_list,
        export=export,
        region_filter=region_filter,
        location_city=location_city,
        location_state=location_state,
        location_description=location_description,
        week_ref_date=week_ref_date
    )
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)




@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = OUTPUT_DIR / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(str(full),
                            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            filename=safe_name)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")
      

# End of app.py


@app.get("/duration")
async def api_duration(
    date_param: Optional[str] = Query(None, alias="date", description="Target date YYYY-MM-DD. Defaults to today in Asia/Kolkata"),
    start_date: Optional[str] = Query(None, description="Start date for a range (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date for a range (YYYY-MM-DD)"),
    regions: Optional[str] = Query(None, description="Comma-separated list: apac,emea,laca,namer. Default: all"),
    city: Optional[str] = Query(None, description="Optional city/location filter (e.g. Pune). Case-insensitive"),
    outdir: Optional[str] = Query(None, description="Output directory for CSVs. Defaults to OUTPUT_DIR/duration_reports"),
    sample_rows: int = Query(10, ge=0, le=200, description="How many sample rows to include per region in response"),
    compliance_target: int = Query(3, ge=1, le=7, description="Compliance target days (e.g. 3 or 5). Default 3")
):
    """
    Returns per-region duration aggregates and swipe-level details with weekly compliance & duration category splits.
    This endpoint is defensive: if duration_report or DB is slow/unavailable, returns partial results with diagnostic messages.
    """
    try:
        # parse regions / outdir
        if regions:
            regions_list = [r.strip().lower() for r in regions.split(",") if r.strip()]
        else:
            regions_list = ["apac", "emea", "laca", "namer"]

        if outdir:
            outdir_path = Path(outdir)
        else:
            outdir_path = OUTPUT_DIR / "duration_reports"
        outdir_path.mkdir(parents=True, exist_ok=True)

        def _parse_date(s: str) -> date:
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                return date.fromisoformat(s)

        if start_date and end_date:
            start_obj = _parse_date(start_date)
            end_obj = _parse_date(end_date)
            if start_obj > end_obj:
                raise HTTPException(status_code=400, detail="start_date must be <= end_date")
            max_days = 92
            days_count = (end_obj - start_obj).days + 1
            if days_count > max_days:
                raise HTTPException(status_code=400, detail=f"Date range too large (> {max_days} days). Please request a smaller range.")
            date_list = [start_obj + timedelta(days=i) for i in range(days_count)]
            range_mode = True
        else:
            if date_param:
                target_date = _parse_date(date_param)
            else:
                tz = ZoneInfo("Asia/Kolkata")
                target_date = datetime.now(tz).date()
            date_list = [target_date]
            start_obj = end_obj = date_list[0]
            range_mode = False

        try:
            import duration_report
        except Exception as e:
            logger.exception("Failed importing duration_report module")
            raise HTTPException(status_code=500, detail=f"duration module import failed: {e}")

        loop = asyncio.get_running_loop()

        def _to_json_safe(v):
            try:
                if pd.isna(v):
                    return None
            except Exception:
                pass
            if isinstance(v, (datetime, date)):
                return v.isoformat()
            if hasattr(v, "isoformat") and not isinstance(v, str):
                try:
                    return v.isoformat()
                except Exception:
                    pass
            try:
                if isinstance(v, (int, float, bool)):
                    return v
            except Exception:
                pass
            try:
                return str(v)
            except Exception:
                return None

        per_date_results: Dict[str, Any] = {}
        # run duration_report.run_for_date for each date concurrently but guarded by a per-date timeout
        for single_date in date_list:
            try:
                # schedule on threadpool
                fut = loop.run_in_executor(None, duration_report.run_for_date, single_date, regions_list, str(outdir_path), city)
                result = await asyncio.wait_for(fut, timeout=COMPUTE_WAIT_TIMEOUT_SECONDS)
                # result expected: dict(region -> {"swipes": DataFrame, "durations": DataFrame})
                per_date_results[single_date.isoformat()] = result if isinstance(result, dict) else {}
            except asyncio.TimeoutError:
                logger.exception("Duration computation timed out for date %s", single_date)
                # store diagnostic so front-end can show partial failure
                per_date_results[single_date.isoformat()] = {"__error": f"timeout after {COMPUTE_WAIT_TIMEOUT_SECONDS}s"}
            except Exception as e:
                logger.exception("duration run_for_date failed for date %s: %s", single_date, e)
                per_date_results[single_date.isoformat()] = {"__error": f"{e}"}

        dates_iso = [d.isoformat() for d in date_list]
        date_objs = [d for d in date_list]
        week_starts = sorted({(d - timedelta(days=d.weekday())) for d in date_objs})

        resp: Dict[str, Any] = {
            "start_date": start_obj.isoformat(),
            "end_date": end_obj.isoformat(),
            "regions": {},
            "diagnostics": {"per_date_status": {}}
        }

        # helper: if a returned value is a filesystem path to CSV(s), try reading with pandas
        def _coerce_result_region(region_val):
            # Accept dict with DataFrames or CSV paths (strings). Return dict {'durations': DataFrame, 'swipes': DataFrame}
            out = {"durations": pd.DataFrame(), "swipes": pd.DataFrame()}
            if not region_val:
                return out
            if isinstance(region_val, dict):
                dur = region_val.get("durations")
                sw = region_val.get("swipes")

                # DURATIONS handling
                try:
                    if isinstance(dur, str):
                        p = Path(dur)
                        if p.exists():
                            try:
                                out["durations"] = _read_csv_compat(p, parse_dates=["LocaleMessageTime"], dtype=str)
                            except Exception:
                                logger.exception("Failed to read durations CSV path %s", p)
                                out["durations"] = pd.DataFrame()
                        else:
                            out["durations"] = pd.DataFrame()
                    elif isinstance(dur, pd.DataFrame):
                        out["durations"] = dur.copy()
                except Exception:
                    logger.exception("Failed to coerce durations into DataFrame (dur may be %s)", type(dur))

                # SWIPES handling
                try:
                    if isinstance(sw, str):
                        p = Path(sw)
                        if p.exists():
                            try:
                                out["swipes"] = _read_csv_compat(p, parse_dates=["LocaleMessageTime"], dtype=str)
                            except Exception:
                                logger.exception("Failed to read swipes CSV path %s", p)
                                out["swipes"] = pd.DataFrame()
                        else:
                            out["swipes"] = pd.DataFrame()
                    elif isinstance(sw, pd.DataFrame):
                        out["swipes"] = sw.copy()
                except Exception:
                    logger.exception("Failed to coerce swipes into DataFrame (sw may be %s)", type(sw))

                return out

            # If caller provided a DataFrame directly
            if isinstance(region_val, pd.DataFrame):
                return {"durations": region_val.copy(), "swipes": pd.DataFrame()}

            # unexpected types
            return out

        for r in regions_list:
            try:
                employees_map: Dict[str, Dict[str, Any]] = {}
                swipes_by_date: Dict[str, List[Dict[str, Any]]] = {}
                date_rows = {}

                # aggregate per-date
                for iso_d, day_res in per_date_results.items():
                    # check for error recorded earlier
                    if isinstance(day_res, dict) and "__error" in day_res:
                        resp["diagnostics"]["per_date_status"][iso_d] = day_res["__error"]
                        # mark empty row counts for this date
                        date_rows.setdefault(iso_d, {"rows": 0, "swipe_rows": 0})
                        swipes_by_date.setdefault(iso_d, [])
                        continue

                    region_obj = day_res.get(r) if isinstance(day_res, dict) else None
                    coerced = _coerce_result_region(region_obj)
                    durations_df = coerced.get("durations")
                    swipes_df = coerced.get("swipes")

                    rows_count = int(len(durations_df)) if isinstance(durations_df, pd.DataFrame) else 0
                    swipe_count = int(len(swipes_df)) if isinstance(swipes_df, pd.DataFrame) else 0
                    date_rows[iso_d] = {"rows": rows_count, "swipe_rows": swipe_count}

                    # SWIPES -> convert to list of dicts (safe)
                    if isinstance(swipes_df, pd.DataFrame) and not swipes_df.empty:
                        for c in ["LocaleMessageTime", "EmployeeName", "Door", "EmployeeID", "CardNumber", "PartitionName2", "PersonnelTypeName", "CompanyName", "PrimaryLocation", "Direction", "EmployeeIdentity", "MessageType", "Text5"]:
                            if c not in swipes_df.columns:
                                swipes_df[c] = None
                        swipe_records = []
                        for _, srow in swipes_df.iterrows():
                            dt = srow.get("LocaleMessageTime")
                            iso_ts = None
                            if pd.notna(dt):
                                try:
                                    iso_ts = pd.to_datetime(dt).isoformat()
                                except Exception:
                                    try:
                                        iso_ts = datetime.fromtimestamp(float(dt)).isoformat()
                                    except Exception:
                                        iso_ts = str(dt)
                            swipe_records.append({
                                "LocaleMessageTime": iso_ts,
                                "Dateonly": iso_d,
                                "Swipe_Time": (pd.to_datetime(srow.get("LocaleMessageTime")).time().isoformat() if pd.notna(srow.get("LocaleMessageTime")) else None),
                                "EmployeeID": srow.get("EmployeeID") if srow.get("EmployeeID") is not None else None,
                                "PersonGUID": srow.get("EmployeeIdentity") or srow.get("EmployeeIdentity"),
                                "ObjectName1": srow.get("EmployeeName"),
                                "Door": srow.get("Door"),
                                "PersonnelType": srow.get("PersonnelTypeName") or srow.get("PersonnelType"),
                                "CardNumber": srow.get("CardNumber"),
                                "Text5": srow.get("PrimaryLocation") or srow.get("Text5"),
                                "PartitionName2": srow.get("PartitionName2"),
                                "AdmitCode": srow.get("AdmitCode") or srow.get("MessageType"),
                                "Direction": srow.get("Direction"),
                                "CompanyName": srow.get("CompanyName"),
                                "PrimaryLocation": srow.get("PrimaryLocation") or srow.get("Text5"),
                            })
                        swipes_by_date[iso_d] = swipe_records
                    else:
                        swipes_by_date.setdefault(iso_d, [])

                    # DURATIONS -> iterate rows and aggregate per person
                    if isinstance(durations_df, pd.DataFrame) and not durations_df.empty:
                        # ensure columns exist
                        for col in ["person_uid", "EmployeeID", "EmployeeName", "CardNumber", "Date",
                                    "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor",
                                    "Duration", "DurationSeconds", "PersonnelTypeName", "PartitionName2",
                                    "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"]:
                            if col not in durations_df.columns:
                                durations_df[col] = None

                        for _, drow in durations_df.iterrows():
                            try:
                                person_uid = drow.get("person_uid")
                                if pd.isna(person_uid) or person_uid is None or str(person_uid).strip() == "":
                                    person_uid = f"{_to_json_safe(drow.get('EmployeeID'))}|{_to_json_safe(drow.get('EmployeeName'))}"

                                if person_uid not in employees_map:
                                    employees_map[person_uid] = {
                                        "person_uid": person_uid,
                                        "EmployeeID": _to_json_safe(drow.get("EmployeeID")),
                                        "EmployeeName": _to_json_safe(drow.get("EmployeeName")),
                                        "CardNumber": _to_json_safe(drow.get("CardNumber")),
                                        "durations": {d: None for d in dates_iso},
                                        "durations_seconds": {d: None for d in dates_iso},
                                        "total_seconds_present_in_range": 0,
                                        # keep internal First/Last but we'll remove them before returning
                                        "FirstSwipe": None,
                                        "LastSwipe": None,
                                        "FirstDoor": _to_json_safe(drow.get("FirstDoor")),
                                        "LastDoor": _to_json_safe(drow.get("LastDoor")),
                                        "PersonnelType": _to_json_safe(drow.get("PersonnelTypeName") or drow.get("PersonnelType")),
                                        "PartitionName2": _to_json_safe(drow.get("PartitionName2")),
                                        "CompanyName": _to_json_safe(drow.get("CompanyName")),
                                        "PrimaryLocation": _to_json_safe(drow.get("PrimaryLocation")),
                                        "FirstDirection": _to_json_safe(drow.get("FirstDirection")),
                                        "LastDirection": _to_json_safe(drow.get("LastDirection")),
                                    }

                                dur_str = None if pd.isna(drow.get("Duration")) else str(drow.get("Duration"))
                                dur_secs = None
                                try:
                                    v = drow.get("DurationSeconds")
                                    if pd.notna(v):
                                        dur_secs = int(float(v))
                                except Exception:
                                    dur_secs = None

                                employees_map[person_uid]["durations"][iso_d] = dur_str
                                employees_map[person_uid]["durations_seconds"][iso_d] = dur_secs
                                if dur_secs is not None:
                                    employees_map[person_uid]["total_seconds_present_in_range"] += dur_secs

                                # First/Last swipe times: keep earliest first, latest last
                                try:
                                    fs = drow.get("FirstSwipe")
                                    ls = drow.get("LastSwipe")
                                    if pd.notna(fs):
                                        fs_dt = pd.to_datetime(fs)
                                        cur_fs = employees_map[person_uid].get("FirstSwipe")
                                        if cur_fs is None:
                                            employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                        else:
                                            if pd.to_datetime(cur_fs) > fs_dt:
                                                employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                    if pd.notna(ls):
                                        ls_dt = pd.to_datetime(ls)
                                        cur_ls = employees_map[person_uid].get("LastSwipe")
                                        if cur_ls is None:
                                            employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                                        else:
                                            if pd.to_datetime(cur_ls) < ls_dt:
                                                employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                                except Exception:
                                    pass
                            except Exception:
                                logger.exception("Failed processing duration row for region %s date %s", r, iso_d)

                # Build list sorted by name
                emp_list = list(employees_map.values())
                emp_list.sort(key=lambda x: ((x.get("EmployeeName") or "") or "").lower())

                # SHIFT-FIX: attempt reconstruction when days look anomalous
                SHIFT_GAP_SECONDS = 6 * 3600        # gap > 6 hours -> new session
                SHIFT_MIN_FIX_SECONDS = 4 * 3600    # attempt fix if a day duration < 4h
                SHIFT_MAX_FIX_SECONDS = 20 * 3600   # or a day duration > 20h

                def _parse_swipe_ts(swipe_rec):
                    ts = swipe_rec.get("LocaleMessageTime")
                    if not ts:
                        return None
                    try:
                        return pd.to_datetime(ts)
                    except Exception:
                        try:
                            return datetime.fromisoformat(str(ts))
                        except Exception:
                            return None

                # Build swipes_by_person map
                swipes_by_person = {}
                for iso_d in dates_iso:
                    for s in swipes_by_date.get(iso_d, []):
                        pids = []
                        if s.get("PersonGUID"):
                            pids.append(str(s.get("PersonGUID")))
                        if s.get("EmployeeID") is not None:
                            pids.append(str(s.get("EmployeeID")))
                        if s.get("CardNumber") is not None:
                            pids.append(str(s.get("CardNumber")))
                        for pid in pids:
                            swipes_by_person.setdefault(pid, []).append(s)

                # sort each person's swipes by timestamp
                for pid, arr in list(swipes_by_person.items()):
                    arr_ts = []
                    for s in arr:
                        ts = _parse_swipe_ts(s)
                        if ts is not None:
                            arr_ts.append((ts, s))
                    arr_ts.sort(key=lambda x: x[0])
                    swipes_by_person[pid] = [s for _, s in arr_ts]

                def _needs_shift_fix(emp):
                    for v in (emp.get("durations_seconds") or {}).values():
                        if v is None:
                            continue
                        if v < SHIFT_MIN_FIX_SECONDS or v > SHIFT_MAX_FIX_SECONDS:
                            return True
                    return False

                for emp in emp_list:
                    try:
                        # assemble all swipes for this person from swipes_by_person using multiple identifiers
                        person_keys = []
                        if emp.get("person_uid"):
                            person_keys.append(str(emp["person_uid"]))
                        if emp.get("EmployeeID") is not None:
                            person_keys.append(str(emp["EmployeeID"]))
                        if emp.get("CardNumber") is not None:
                            person_keys.append(str(emp["CardNumber"]))

                        all_swipes = []
                        for k in person_keys:
                            lst = swipes_by_person.get(k) or []
                            for s in lst:
                                ts = _parse_swipe_ts(s)
                                if ts is not None:
                                    all_swipes.append((ts, s))
                        if not all_swipes:
                            continue
                        all_swipes.sort(key=lambda x: x[0])
                        swipe_times = [ts for ts, _ in all_swipes]

                        if not _needs_shift_fix(emp):
                            continue

                        sessions = []
                        cur_start = swipe_times[0]
                        cur_last = swipe_times[0]
                        for ts in swipe_times[1:]:
                            gap = (ts - cur_last).total_seconds()
                            if gap > SHIFT_GAP_SECONDS:
                                sessions.append((cur_start, cur_last))
                                cur_start = ts
                                cur_last = ts
                            else:
                                cur_last = ts
                        sessions.append((cur_start, cur_last))

                        new_durations_seconds = {d: None for d in dates_iso}
                        for s_start, s_end in sessions:
                            session_secs = max(0, int((s_end - s_start).total_seconds()))
                            session_date_iso = s_start.date().isoformat()
                            if session_date_iso not in new_durations_seconds:
                                continue
                            prev = new_durations_seconds.get(session_date_iso)
                            if prev is None:
                                new_durations_seconds[session_date_iso] = session_secs
                            else:
                                new_durations_seconds[session_date_iso] = prev + session_secs

                        # preserve previous per-day values for untouched days
                        for d in dates_iso:
                            if new_durations_seconds.get(d) is None and emp.get("durations_seconds", {}).get(d) is not None:
                                new_durations_seconds[d] = emp["durations_seconds"][d]

                        new_durations_str = {}
                        for d, secs in new_durations_seconds.items():
                            if secs is None:
                                new_durations_str[d] = None
                            else:
                                try:
                                    new_durations_str[d] = str(timedelta(seconds=int(secs)))
                                except Exception:
                                    new_durations_str[d] = None

                        total = 0
                        for v in new_durations_seconds.values():
                            if v is not None:
                                total += int(v)

                        emp["durations_seconds"] = new_durations_seconds
                        emp["durations"] = new_durations_str
                        emp["total_seconds_present_in_range"] = total
                    except Exception:
                        logger.exception("shift-fix reconstruction failed for employee %s", emp.get("person_uid") or emp.get("EmployeeID"))

                # compute per-employee weekly compliance and categories
                for emp in emp_list:
                    try:
                        weeks_info = {}
                        weeks_met = 0
                        weeks_total = 0

                        cat_counts = {"0-30m": 0, "30m-2h": 0, "2h-6h": 0, "6h-8h": 0, "8h+": 0}
                        cat_dates = {k: [] for k in cat_counts.keys()}

                        for ws in week_starts:
                            week_start_iso = ws.isoformat()
                            week_dates = [(ws + timedelta(days=i)).isoformat() for i in range(7)]
                            relevant_dates = [d for d in week_dates if d in dates_iso]
                            if not relevant_dates:
                                continue

                            days_present = 0
                            days_ge8 = 0
                            per_date_durations = {}
                            per_date_compliance = {}

                            for d in relevant_dates:
                                secs = emp["durations_seconds"].get(d)
                                per_date_durations[d] = secs
                                if secs is not None and secs > 0:
                                    days_present += 1
                                is_ge8 = (secs is not None and secs >= 28800)
                                if is_ge8:
                                    days_ge8 += 1
                                per_date_compliance[d] = True if is_ge8 else False

                                if secs is not None and secs > 0:
                                    # use duration_report.categorize_seconds if available
                                    cat = "0-30m"
                                    try:
                                        if hasattr(duration_report, 'categorize_seconds'):
                                            cat = duration_report.categorize_seconds(secs)
                                        else:
                                            # fallback: simple bucket
                                            if secs <= 1800:
                                                cat = "0-30m"
                                            elif secs <= 7200:
                                                cat = "30m-2h"
                                            elif secs <= 21600:
                                                cat = "2h-6h"
                                            elif secs < 28800:
                                                cat = "6h-8h"
                                            else:
                                                cat = "8h+"
                                    except Exception:
                                        cat = "0-30m"
                                    if cat in cat_counts:
                                        cat_counts[cat] += 1
                                        cat_dates[cat].append(d)

                            ct = int(compliance_target or 3)
                            compliant = False
                            # Basic rule: weeks considered only if at least ct days present AND those present days are >=8h
                            if days_present >= ct:
                                if days_present == days_ge8:
                                    compliant = True
                                else:
                                    compliant = False

                            weeks_info[week_start_iso] = {
                                "week_start": week_start_iso,
                                "dates": per_date_durations,
                                "dates_compliance": per_date_compliance,
                                "days_present": days_present,
                                "days_ge8": days_ge8,
                                "compliant": compliant
                            }

                            weeks_total += 1
                            if compliant:
                                weeks_met += 1

                        dominant_category = None
                        max_count = -1
                        for k, v in cat_counts.items():
                            if v > max_count:
                                max_count = v
                                dominant_category = k

                        # cleanup
                        for _k in ("FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor"):
                            if _k in emp:
                                try:
                                    del emp[_k]
                                except Exception:
                                    pass

                        emp["compliance"] = {
                            "weeks": weeks_info,
                            "weeks_met": weeks_met,
                            "weeks_total": weeks_total,
                            "month_summary": f"{weeks_met}/{weeks_total}" if weeks_total > 0 else "0/0",
                            "compliance_target": int(compliance_target or 3)
                        }
                        emp["duration_categories"] = {
                            "counts": cat_counts,
                            "dominant_category": dominant_category,
                            "category_dates": cat_dates,
                            "red_flag": cat_counts.get("2h-6h", 0)
                        }
                    except Exception:
                        logger.exception("Failed computing compliance/categories for employee %s", emp.get("person_uid"))

                durations_sample = emp_list[:sample_rows] if sample_rows and sample_rows > 0 else []

                resp["regions"][r] = {
                    "dates": dates_iso,
                    "employees": emp_list,
                    "durations_sample": durations_sample,
                    "date_rows": date_rows,
                    "swipes_by_date": swipes_by_date
                }
            except Exception:
                logger.exception("Failed to aggregate range results for region %s", r)
                resp["regions"][r] = {"dates": [d.isoformat() for d in date_list], "employees": [], "durations_sample": [], "date_rows": {}, "swipes_by_date": {}}

        # ensure all numpy/pandas types are converted to serializable Python types
        safe_content = jsonable_encoder(resp)
        return JSONResponse(safe_content)

    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("api_duration (range) failed")
        raise HTTPException(status_code=500, detail=f"duration api error: {exc}")





2)


# data_compare_service_v2.py
"""
Comparison service (v2) with broadened matching heuristics, safer prefetch/cache,
and explicit Sheet vs AttendanceSummary comparison diagnostics.

Drop-in replacement for your existing data_compare_service_v2.py
"""

import sys
import re
import uuid
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
from typing import Optional, Dict, Any, List, Tuple

import pandas as pd

# HTTP client for region histories
try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except Exception:
    requests = None

# DB imports (same as your project)
from db import SessionLocal
from models import AttendanceSummary

# Settings / defaults
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR, REGION_HISTORY_URLS as SETTINGS_REGION_HISTORY_URLS
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
    REGION_HISTORY_URLS = SETTINGS_REGION_HISTORY_URLS
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"
    REGION_HISTORY_URLS = [
        "http://10.199.22.57:3008/api/occupancy/history",  # APAC
        "http://10.199.22.57:3006/api/occupancy/history",  # NAMER
        "http://10.199.22.57:3007/api/occupancy/history",  # EMEA
        "http://10.199.22.57:4000/api/occupancy/history"   # LACA
    ]

DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger("data_compare_service_v2")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# In-memory cache for prefetched region history entries
REGION_HISTORY_CACHE = None
REGION_HISTORY_CACHE_FETCHED_AT = None
REGION_HISTORY_CACHE_TTL_SECONDS = 300  # 5 minutes

# Matching config
ID_FIELD_CANDIDATES = [
    "EmployeeID","employeeId","Employee Id","EmpID","Emp Id","EmpNo","EmployeeNo","Employee_Number",
    "PersonID","PersonId","personId","person_id","employee_id","id","Id","employeeNumber","EmployeeNumber",
    "worker_system_id","wsid","WorkerID","Worker System Id","workerId","WorkerSystemId"
]
CARD_FIELD_CANDIDATES = [
    "CardNumber","Card","cardNumber","card_number","BadgeNumber","BadgeNo","Badge","badgeNumber","badge_no",
    "iPassID","IPassID","iPass","i_pass_id","CardNo","card_no","card","IPASSID","IPass"
]
NAME_FIELD_CANDIDATES = [
    "FullName","Full Name","EmpName","Name","full_name","displayName","personName","PersonName"
]

# ----------------------------
# Utilities
# ----------------------------
def _find_active_employee_file():
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_employee{ext}"
        if p.exists():
            return p
    for p in DATA_DIR.iterdir():
        if p.is_file() and "active_employee" in p.name.lower():
            return p
    return None

def _normalize_key(k):
    if k is None:
        return None
    try:
        s = str(k).strip()
        return s if s != "" else None
    except Exception:
        return None

def _digits_only(s):
    if s is None:
        return ""
    return re.sub(r'\D+', '', str(s))

def _safe_int(v, default=0):
    try:
        if v is None or v == "":
            return default
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return default

def _week_monday_and_friday(ref_date: Optional[date] = None):
    if ref_date is None:
        ref_date = date.today()
    iso = ref_date.isoweekday()
    monday = ref_date - timedelta(days=(iso - 1))
    friday = monday + timedelta(days=4)
    return monday, friday

def _maybe_mark_on_leave(status_str: Optional[str]) -> bool:
    if not status_str:
        return False
    s = str(status_str).strip().lower()
    for tok in ("leave", "vacation", "on leave", "holiday", "sabbatical", "furlough", "loa"):
        if tok in s:
            return True
    return False

# ----------------------------
# Load active sheet
# ----------------------------
def load_active_employees_dataframe() -> pd.DataFrame:
    src = _find_active_employee_file()
    if not src:
        raise FileNotFoundError(f"Active employee canonical file not found in {DATA_DIR}")
    ext = src.suffix.lower()
    if ext in (".xls", ".xlsx"):
        df = pd.read_excel(src, sheet_name=0, dtype=str)
    else:
        df = pd.read_csv(src, dtype=str)
    df.columns = [c.strip() for c in df.columns]

    def _first_present(row, candidates):
        for c in candidates:
            if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
                return row[c]
        return None

    rows = []
    for _, row in df.iterrows():
        emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id','EmployeeNo','Employee No'])
        if emp_id is None:
            continue
        full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or (
            f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
        )
        location_city = _first_present(row, ['Location City','Location','City'])
        location_desc = _first_present(row, ['Location Description','LocationDescription'])
        location_state = _first_present(row, ['Location State / Province','Location State','State','Province'])
        region_code = _first_present(row, ['Region Code','Region','RegionCode'])
        current_status = _first_present(row, ['Current Status','Status','Employee Status'])
        employee_type = _first_present(row, ['Employee Type','Type','Time Type'])
        rows.append({
            "employee_id": _normalize_key(emp_id),
            "full_name": _normalize_key(full_name),
            "location_city": _normalize_key(location_city),
            "location_desc": _normalize_key(location_desc),
            "location_state": _normalize_key(location_state),
            "region_code": (str(region_code).strip() if region_code is not None else None),
            "current_status": _normalize_key(current_status),
            "employee_type": _normalize_key(employee_type),
            "raw_row": row.to_dict()
        })
    ndf = pd.DataFrame(rows)
    for col in ("employee_id","full_name","location_city","location_desc","location_state","region_code","current_status","employee_type","raw_row"):
        if col not in ndf:
            ndf[col] = None
    ndf = ndf[ndf["employee_id"].notna() & (ndf["employee_id"].str.strip() != "")]
    ndf.reset_index(drop=True, inplace=True)
    return ndf

# ----------------------------
# HTTP Session factory (retries/backoff)
# ----------------------------
def _build_requests_session():
    if requests is None:
        return None
    s = requests.Session()
    allowed = frozenset(['GET', 'HEAD'])
    try:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=allowed
        )
    except TypeError:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            method_whitelist=allowed
        )
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

# ----------------------------
# Prefetch / cache helpers
# ----------------------------
def prefetch_region_history(timeout: int = 10, force: bool = False):
    """
    Fetch region history entries (cached). Returns list of raw entries.
    """
    global REGION_HISTORY_CACHE, REGION_HISTORY_CACHE_FETCHED_AT
    try:
        now = datetime.utcnow()
        if not force and REGION_HISTORY_CACHE is not None and REGION_HISTORY_CACHE_FETCHED_AT:
            elapsed = (now - REGION_HISTORY_CACHE_FETCHED_AT).total_seconds()
            if elapsed < REGION_HISTORY_CACHE_TTL_SECONDS:
                logger.info("[region_cache] Using cached region history (age %.1fs)", elapsed)
                return REGION_HISTORY_CACHE

        entries = []
        # Prefer region_clients when available
        try:
            import region_clients
            logger.info("[region_cache] fetching region history via region_clients.fetch_all_history()")
            try:
                got = region_clients.fetch_all_history(timeout=timeout)
            except TypeError:
                got = region_clients.fetch_all_history()
            entries = got or []
        except Exception:
            entries = []

        # If empty, try direct requests to configured URLs
        if not entries and requests is not None:
            logger.info("[region_cache] fetching region history directly from endpoints")
            session = _build_requests_session() or requests
            for url in REGION_HISTORY_URLS:
                if not url:
                    continue
                try:
                    resp = session.get(url, timeout=(3, max(5, timeout)))
                    if not resp or resp.status_code != 200:
                        continue
                    try:
                        payload = resp.json()
                    except Exception:
                        continue
                    # flatten possible lists
                    if isinstance(payload, list):
                        for p in payload:
                            if isinstance(p, dict):
                                p['_source_url'] = url
                                entries.append(p)
                    elif isinstance(payload, dict):
                        found_list = False
                        for k in ("results","summaryByDate","details","data","entries","list","people","items"):
                            if k in payload and isinstance(payload[k], list):
                                for p in payload[k]:
                                    if isinstance(p, dict):
                                        p['_source_url'] = url
                                        entries.append(p)
                                found_list = True
                                break
                        if not found_list:
                            payload['_source_url'] = url
                            entries.append(payload)
                except requests.exceptions.RequestException as e:
                    logger.warning("[region_cache] fetch failed for %s: %s", url, str(e))
                    continue

        REGION_HISTORY_CACHE = entries or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        logger.info("[region_cache] prefetched %d region history entries", len(REGION_HISTORY_CACHE))
        return REGION_HISTORY_CACHE
    except Exception:
        logger.exception("[region_cache] prefetch failed")
        REGION_HISTORY_CACHE = REGION_HISTORY_CACHE or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        return REGION_HISTORY_CACHE

# ----------------------------
# Payload helpers (deep search)
# ----------------------------
def _iter_scalars_in_obj(obj, parent_key=""):
    """
    Yield all scalar key->value pairs inside nested dict/list structures as (key_path, value).
    """
    if isinstance(obj, dict):
        for k, v in obj.items():
            new_key = f"{parent_key}.{k}" if parent_key else str(k)
            if isinstance(v, (dict, list)):
                yield from _iter_scalars_in_obj(v, parent_key=new_key)
            else:
                yield new_key, v
    elif isinstance(obj, list):
        for idx, it in enumerate(obj):
            new_key = f"{parent_key}[{idx}]" if parent_key else f"[{idx}]"
            if isinstance(it, (dict, list)):
                yield from _iter_scalars_in_obj(it, parent_key=new_key)
            else:
                yield new_key, it

def _extract_details_from_payload(payload):
    """
    Normalize to list of dict rows that look like detail records.
    """
    if payload is None:
        return []
    if isinstance(payload, list):
        return [p for p in payload if isinstance(p, dict)]
    if isinstance(payload, dict):
        for k in ("details","results","data","entries","list","people","items"):
            v = payload.get(k)
            if isinstance(v, list):
                return [x for x in v if isinstance(x, dict)]
        # single-record-like payload (date-summary) -> return as single element to allow higher-level scanner to inspect fields
        if any(k in payload for k in ("date","Employee","Contractor","total","day")) and len(payload.keys()) <= 40:
            return [payload]
    return []

# ----------------------------
# ID matching helpers
# ----------------------------
def _match_candidate_to_employees(candidate_raw, orig_ids_set, orig_ids_list, digits_map):
    """
    Try many heuristics to map candidate_raw to one of the orig_ids_list.
    Returns matched_orig_id or None.
    """
    if candidate_raw is None:
        return None
    cand_str = str(candidate_raw).strip()
    if not cand_str:
        return None

    # direct exact match
    if cand_str in orig_ids_set:
        return cand_str

    # direct case-insensitive match
    for o in orig_ids_list:
        if isinstance(o, str) and cand_str.lower() == o.lower():
            return o

    # numeric transformations
    cand_digits = _digits_only(cand_str)
    if cand_digits:
        cand_nolead = cand_digits.lstrip('0') or cand_digits
        # direct digits match to original ids
        for o in orig_ids_list:
            if not isinstance(o, str):
                continue
            if o == cand_digits or o == cand_nolead:
                return o
            od = _digits_only(o)
            if od == cand_digits or od == cand_nolead:
                return o
        # numeric equality by int
        try:
            ci = int(cand_nolead)
            for o in orig_ids_list:
                try:
                    od = _digits_only(o)
                    if not od:
                        continue
                    oi = int(od.lstrip('0') or od)
                    if oi == ci:
                        return o
                except Exception:
                    continue
        except Exception:
            pass

        # last-n digits heuristics (conservative)
        if len(cand_digits) >= 3:
            for n in (6, 4):
                suf = cand_digits[-n:]
                if not suf:
                    continue
                for o in orig_ids_list:
                    od = _digits_only(o)
                    if od and od.endswith(suf):
                        return o

    # strip common prefixes and retry
    up = cand_str.upper()
    for pref in ("W", "IPASS", "IPASSID", "IPass"):
        if up.startswith(pref):
            stripped = cand_str[len(pref):]
            m = _match_candidate_to_employees(stripped, orig_ids_set, orig_ids_list, digits_map)
            if m:
                return m

    # fallback: find numeric substrings inside string and try mapping
    for match in re.finditer(r'(\d{3,})', cand_str):
        ssub = match.group(1)
        m = _match_candidate_to_employees(ssub, orig_ids_set, orig_ids_list, digits_map)
        if m:
            return m

    return None

# ----------------------------
# Region history scanning -> build presence map
# ----------------------------
def _fetch_presence_from_region_histories(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None, preloaded_entries: Optional[List[dict]] = None):
    """
    Scans preloaded_entries (or global cache) and returns presence mapping: {employee_id -> {date: 0/1}}
    """
    presence = {eid: {} for eid in employee_ids}
    if not employee_ids:
        return presence

    if preloaded_entries is None:
        global REGION_HISTORY_CACHE
        preloaded_entries = REGION_HISTORY_CACHE or []

    if not preloaded_entries:
        logger.info("[region_history] no preloaded region history entries to scan")
    else:
        orig_ids_list = [str(e).strip() for e in employee_ids]
        orig_ids_set = set(orig_ids_list)
        digits_map = {e: _digits_only(e) for e in orig_ids_list}

        scanned = 0
        matched = 0
        examples_matched = []

        for entry in preloaded_entries:
            detail_rows = []
            if isinstance(entry, dict):
                # prefer explicit lists
                for key in ("details","people","items","list","results","entries","data"):
                    if key in entry and isinstance(entry.get(key), list):
                        detail_rows = [r for r in entry.get(key) if isinstance(r, dict)]
                        break
                if not detail_rows:
                    # treat entry itself as a candidate detail row if it has plausible fields (timestamp or id-like)
                    candidate_keys = set(ID_FIELD_CANDIDATES + CARD_FIELD_CANDIDATES + ["date","timestamp","time","SwipeDate","LocaleMessageTime","day"])
                    if any(k in entry for k in candidate_keys):
                        detail_rows = [entry]
                    else:
                        # try extract via helper (covers nested payload shapes)
                        detail_rows = _extract_details_from_payload(entry) or []
            else:
                continue

            scanned += len(detail_rows)
            for d in detail_rows:
                try:
                    # find a timestamp / date for row
                    ts = None
                    # common keys first
                    for tkey in ("LocaleMessageTime","LocaleMessageDateTime","SwipeDate","SwipeTime","timestamp","time","DateTime","date","swipeDate","day"):
                        if tkey in d and d.get(tkey):
                            ts = d.get(tkey)
                            break

                    # fallback: scan scalar values in row for iso-like date
                    if ts is None:
                        for k, v in _iter_scalars_in_obj(d):
                            if v is None:
                                continue
                            try:
                                s = str(v)
                            except Exception:
                                continue
                            # quick heuristic
                            if re.search(r'\d{4}-\d{2}-\d{2}', s) or re.search(r'\d{2}/\d{2}/\d{4}', s):
                                ts = s
                                break

                    if ts is None:
                        # skip row (no date info)
                        continue

                    # parse timestamp into date (robust)
                    t = None
                    if isinstance(ts, (int, float)):
                        try:
                            t = datetime.fromtimestamp(int(ts))
                        except Exception:
                            try:
                                t = datetime.utcfromtimestamp(int(ts) / 1000.0)
                            except Exception:
                                t = None
                    elif isinstance(ts, str):
                        s = ts.strip()
                        if s == "":
                            t = None
                        else:
                            parsed = None
                            # try dateutil if available (best)
                            try:
                                from dateutil import parser as _parser
                                parsed = _parser.parse(s)
                            except Exception:
                                parsed = None
                            if parsed:
                                try:
                                    if parsed.tzinfo is not None:
                                        parsed = parsed.astimezone(tz=None).replace(tzinfo=None)
                                except Exception:
                                    pass
                                t = parsed
                            else:
                                # try common formats
                                fmts = [
                                    "%Y-%m-%dT%H:%M:%S.%fZ",
                                    "%Y-%m-%dT%H:%M:%S.%f",
                                    "%Y-%m-%dT%H:%M:%S",
                                    "%Y-%m-%d %H:%M:%S",
                                    "%Y-%m-%d",
                                    "%d/%m/%Y %H:%M:%S",
                                    "%d/%m/%Y"
                                ]
                                for f in fmts:
                                    try:
                                        t = datetime.strptime(s, f)
                                        break
                                    except Exception:
                                        t = None
                    if t is None:
                        continue

                    dt = t.date()
                    if dt < start_date or dt > end_date:
                        continue

                    # partition filter (if provided)
                    if partition_filter:
                        part_values = []
                        for k in ("PartitionNameFriendly","PartitionName","PrimaryLocation","partition","location","Partition","Location","Site","PartitionName1","PartitionName2"):
                            v = d.get(k)
                            if v:
                                part_values.append(str(v))
                        if part_values:
                            ok = any(part_filter_match(part, partition_filter) for part in part_values)
                            if not ok:
                                continue
                        else:
                            # no partition info -> skip when filter exists
                            continue

                    # attempt to match explicit id keys first
                    matched_key = None
                    for k in ID_FIELD_CANDIDATES:
                        if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                            m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                            if m:
                                matched_key = m
                                break

                    # try card fields
                    if not matched_key:
                        for k in CARD_FIELD_CANDIDATES:
                            if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                                m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break

                    # deep-scan scalars for numeric substrings and name fields
                    if not matched_key:
                        for key_path, val in _iter_scalars_in_obj(d):
                            if val is None:
                                continue
                            sval = str(val)
                            # numeric substring preference
                            if re.search(r'\d{3,}', sval):
                                m = _match_candidate_to_employees(sval, orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break
                        # check name fields (disabled by default to avoid false positives)

                    if matched_key:
                        matched += 1
                        # coerce matched_key to exact string from orig list
                        matched_key = next((o for o in orig_ids_list if str(o).strip() == str(matched_key).strip()), str(matched_key).strip())
                        presence.setdefault(matched_key, {})
                        presence[matched_key][dt] = 1
                        if len(examples_matched) < 10:
                            examples_matched.append({"matched": matched_key, "date": dt.isoformat(), "sample_row_keys": list(d.keys())[:8]})
                except Exception:
                    continue

        logger.info("[region_history] scanned %d detail rows from preloaded entries; matched %d presence entries", scanned, matched)
        if examples_matched:
            logger.debug("[region_history] example matches (up to 10): %s", examples_matched)

    # fill zeros for any missing date
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            presence.setdefault(eid, {})
            if cur not in presence[eid]:
                presence[eid][cur] = 0
        cur = cur + timedelta(days=1)

    return presence

def part_filter_match(src_val, partition_filter):
    try:
        if not src_val:
            return False
        return partition_filter.strip().lower() in str(src_val).strip().lower()
    except Exception:
        return False

# ----------------------------
# DB / combined fetch
# ----------------------------
def _fetch_presence_for_employees(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None):
    """
    1) chunked DB IN queries (AttendanceSummary)
    2) fallback broad DB query
    3) fallback region history cache scan (prefetch_region_history)
    Returns mapping {employee_id: {date: 0/1}}
    """
    if not employee_ids:
        return {}

    orig_ids = [str(e).strip() for e in employee_ids]
    norm_id_set = set([s for s in orig_ids if s])
    result = {eid: {} for eid in orig_ids}

    # 1) chunked DB fetch
    rows = []
    chunk_size = 500
    try:
        with SessionLocal() as db:
            for i in range(0, len(orig_ids), chunk_size):
                chunk = orig_ids[i:i+chunk_size]
                try:
                    q = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date,
                        AttendanceSummary.employee_id.in_(chunk)
                    )
                    rows_chunk = q.all()
                    if rows_chunk:
                        rows.extend(rows_chunk)
                except Exception:
                    logger.exception("chunked query failed (continuing)")
                    continue

            # fallback broad query if none found
            if not rows:
                try:
                    rows = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date
                    ).all()
                    logger.info("[presence_fetch] fallback broad DB query returned %d rows for %s -> %s", len(rows), start_date, end_date)
                except Exception:
                    logger.exception("fallback broad DB query failed")
                    rows = []
    except Exception:
        logger.exception("DB session error in _fetch_presence_for_employees")
        rows = []

    # map DB rows to provided employee ids
    for r in rows:
        try:
            raw = r.employee_id
            if raw is None:
                continue
            db_key = str(raw).strip()
            match_key = None
            if db_key in norm_id_set:
                match_key = db_key
            else:
                digits = _digits_only(db_key)
                if digits:
                    cand = digits.lstrip('0') or digits
                    if cand in norm_id_set:
                        match_key = cand
                if match_key is None:
                    for o in orig_ids:
                        if o == db_key or o.lstrip('0') == db_key or db_key.lstrip('0') == o:
                            match_key = o
                            break
            if not match_key:
                continue
            d = r.date
            present = 0
            try:
                present = int(r.presence_count or 0)
            except Exception:
                present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
            result.setdefault(match_key, {})
            prev = result[match_key].get(d, 0)
            result[match_key][d] = 1 if (prev == 1 or present > 0) else 0
        except Exception:
            continue

    # fill zeros
    cur = start_date
    while cur <= end_date:
        for eid in orig_ids:
            result.setdefault(eid, {})
            if cur not in result[eid]:
                result[eid][cur] = 0
        cur = cur + timedelta(days=1)

    db_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] DB-derived presence found for %d/%d employees", db_positive, len(orig_ids))

    # fallback to region details/history if needed
    if db_positive == 0 or db_positive < max(10, int(0.1 * len(orig_ids))):
        try:
            logger.info("[presence_fetch] DB coverage low (%d/%d) - trying region occupancy detail/history fallback", db_positive, len(orig_ids))

            # 1) Try region_clients.fetch_all_details() first (per-person rows are most useful)
            try:
                import region_clients
                details = []
                if hasattr(region_clients, "fetch_all_details"):
                    try:
                        details = region_clients.fetch_all_details(timeout=10)
                    except TypeError:
                        details = region_clients.fetch_all_details()
                details = details or []
                if details:
                    logger.info("[presence_fetch] fetched %d detail rows from region_clients.fetch_all_details()", len(details))
                    # attempt to match details to employee ids and fill presence
                    for d in details:
                        try:
                            # extract candidate id fields (fast checks)
                            cand_fields = []
                            for k in ("EmployeeID","employee_id","EmpID","CardNumber","Card","CardNo","IPassID","iPassID","PersonGUID","PersonId"):
                                v = d.get(k) if isinstance(d, dict) else None
                                if v and str(v).strip():
                                    cand_fields.append(v)
                            # if no explicit candidate id, attempt to deep-scan for numeric substring
                            if not cand_fields:
                                for key_path, val in _iter_scalars_in_obj(d):
                                    if val is None:
                                        continue
                                    sval = str(val)
                                    if re.search(r'\d{3,}', sval):
                                        cand_fields.append(sval)
                            # determine date for this row (fast heuristics)
                            ts = None
                            for tkey in ("LocaleMessageTime","SwipeDate","SwipeTime","timestamp","time","DateTime","date"):
                                if tkey in d and d.get(tkey):
                                    ts = d.get(tkey)
                                    break
                            if ts is None:
                                # quick scalar scan for iso date fragment
                                for key_path, val in _iter_scalars_in_obj(d):
                                    if val is None:
                                        continue
                                    s = str(val)
                                    if re.search(r'\d{4}-\d{2}-\d{2}', s) or re.search(r'\d{2}/\d{2}/\d{4}', s):
                                        ts = s
                                        break
                            if ts is None:
                                continue
                            # parse to date - handle common string formats without heavy dateutil
                            parsed_dt = None
                            if isinstance(ts, (int, float)):
                                try:
                                    parsed_dt = datetime.fromtimestamp(int(ts))
                                except Exception:
                                    try:
                                        parsed_dt = datetime.utcfromtimestamp(int(ts) / 1000.0)
                                    except Exception:
                                        parsed_dt = None
                            elif isinstance(ts, str):
                                s = ts.strip()
                                # try ISO-like fast parse
                                m = re.search(r'(\d{4}-\d{2}-\d{2})', s)
                                if m:
                                    try:
                                        parsed_dt = datetime.fromisoformat(m.group(1))
                                    except Exception:
                                        try:
                                            parsed_dt = datetime.strptime(m.group(1), "%Y-%m-%d")
                                        except Exception:
                                            parsed_dt = None
                                else:
                                    # fallback: try a couple common formats
                                    for f in ("%Y-%m-%dT%H:%M:%S.%fZ","%Y-%m-%dT%H:%M:%S","%d/%m/%Y %H:%M:%S","%d/%m/%Y"):
                                        try:
                                            parsed_dt = datetime.strptime(s, f)
                                            break
                                        except Exception:
                                            parsed_dt = None
                            if parsed_dt is None:
                                continue
                            dt = parsed_dt.date()
                            if dt < start_date or dt > end_date:
                                continue

                            # attempt to map candidate fields to orig_ids using the same matching helper
                            for cand in cand_fields:
                                try:
                                    m = _match_candidate_to_employees(cand, set(orig_ids), orig_ids, {})  # re-use existing matcher
                                    if m:
                                        # coerce matched to orig string
                                        matched_key = next((o for o in orig_ids if str(o).strip() == str(m).strip()), str(m).strip())
                                        result.setdefault(matched_key, {})
                                        prev = result[matched_key].get(dt, 0)
                                        result[matched_key][dt] = 1 if (prev == 1 or 1) else prev
                                        break
                                except Exception:
                                    continue
                        except Exception:
                            continue
                else:
                    logger.info("[presence_fetch] region_clients.fetch_all_details returned no details")
            except Exception:
                logger.exception("region_clients.fetch_all_details fallback failed (continuing)")

            # 2) ensure region history cache is populated and try scanning preloaded history (less precise)
            try:
                prefetch_region_history()
                region_presence = _fetch_presence_from_region_histories(orig_ids, start_date, end_date, partition_filter=partition_filter, preloaded_entries=REGION_HISTORY_CACHE)
                for eid in orig_ids:
                    rp = region_presence.get(eid, {})
                    for d, v in rp.items():
                        if v and result.setdefault(eid, {}).get(d, 0) == 0:
                            result[eid][d] = 1
            except Exception:
                logger.exception("region_history fallback failed")
        except Exception:
            logger.exception("detail/history fallback collapsed")

    final_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] final presence coverage: %d/%d employees have at least one positive day", final_positive, len(orig_ids))
    return result

# ----------------------------
# Sheet vs AttendanceSummary comparison
# ----------------------------
def _compare_sheet_vs_db_summary(sel_df: pd.DataFrame, start_date: date, end_date: date) -> Dict[str, Any]:
    """
    For each employee in sel_df compute DB-derived weekly presence totals and produce mismatch diagnostics.
    Returns dict with:
      - per_employee_summary (employee_id -> {sheet:..., db_total:..., db_by_date: {...}})
      - missing_in_summary (list of employee rows with zero DB presence)
      - mismatches (list where sheet vs DB total differ)
      - extra_summary_ids (IDs present in DB during week but not in sheet selection)
    """
    out = {
        "per_employee_summary": {},
        "missing_in_summary": [],
        "mismatches": [],
        "extra_summary_ids": []
    }
    employee_ids = sel_df["employee_id"].astype(str).str.strip().tolist()
    employee_set = set(employee_ids)

    # fetch attendance rows for the range and for relevant employee ids (DB query)
    try:
        with SessionLocal() as db:
            rows = db.query(AttendanceSummary).filter(
                AttendanceSummary.date >= start_date,
                AttendanceSummary.date <= end_date
            ).all()
    except Exception:
        logger.exception("Failed to query AttendanceSummary for sheet-vs-db comparison; will attempt per-id chunked queries")
        rows = []

    # Map DB rows by normalized employee id
    db_map = {}
    for r in rows:
        try:
            rid = r.employee_id
            if rid is None:
                continue
            rk = str(rid).strip()
            # prefer exact string match to sheet ids
            candidates = [rk]
            digits = _digits_only(rk)
            if digits:
                candidates.append(digits.lstrip('0') or digits)
            # find best matching sheet id
            matched = None
            for c in candidates:
                if c in employee_set:
                    matched = c
                    break
            if not matched:
                # try more expensive matching
                for s in employee_ids:
                    if s == rk or rk.lstrip('0') == s or s.lstrip('0') == rk:
                        matched = s
                        break
            if not matched:
                # add as extra (db-only) under its raw key
                db_map.setdefault(rk, {})
                db_map[rk].setdefault(r.date, 0)
                try:
                    present = int(r.presence_count or 0)
                except Exception:
                    present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
                db_map[rk][r.date] = 1 if (db_map[rk].get(r.date, 0) == 1 or present > 0) else db_map[rk].get(r.date, 0)
            else:
                db_map.setdefault(matched, {})
                db_map[matched].setdefault(r.date, 0)
                try:
                    present = int(r.presence_count or 0)
                except Exception:
                    present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
                db_map[matched][r.date] = 1 if (db_map[matched].get(r.date, 0) == 1 or present > 0) else db_map[matched].get(r.date, 0)
        except Exception:
            continue

    # ensure all sheet employees have entries per day (0 default)
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            db_map.setdefault(eid, {})
            if cur not in db_map[eid]:
                db_map[eid][cur] = 0
        cur = cur + timedelta(days=1)

    # compute per-employee summary and mismatches
    for eid in employee_ids:
        try:
            db_by_date = {d.isoformat(): int(v) for d, v in db_map.get(eid, {}).items()}
            db_total = sum(int(v) for v in db_map.get(eid, {}).values())
            # sheet-side: we don't have presence per-day on the sheet; so sheet's weekly presence is unknown.
            # But we can at least mark employees missing from DB or with low coverage.
            entry = {
                "employee_id": eid,
                "full_name": sel_df[sel_df["employee_id"] == eid]["full_name"].iloc[0] if not sel_df[sel_df["employee_id"] == eid].empty else None,
                "db_total_week_presence": db_total,
                "db_by_date": db_by_date
            }
            out["per_employee_summary"][eid] = entry
            if db_total == 0:
                out["missing_in_summary"].append(entry)
            # If you had a target expected presence from sheet metadata you could compare here; for now, put thresholds:
            # We'll treat employees with db_total < 3 (for regulars) as defaulters -> included in mismatches so they are visible
            out["mismatches"].append(entry) if db_total < 3 else None
        except Exception:
            continue

    # compute extras: DB IDs with presence >0 that are not in sheet employee_ids
    extras = []
    for dbid, bydates in db_map.items():
        try:
            if dbid not in employee_set:
                total = sum(int(v) for v in bydates.values())
                if total > 0:
                    extras.append({"employee_id": dbid, "db_total_week_presence": total, "db_by_date": {d.isoformat(): v for d, v in bydates.items()}})
        except Exception:
            continue
    out["extra_summary_ids"] = extras

    return out

# ----------------------------
# Main compare function (public)
# ----------------------------
def compare_ccure_vs_sheets(
    mode: str = "full",
    stats_detail: str = "ActiveProfiles",
    limit_list: int = 200,
    export: bool = False,
    region_filter: Optional[str] = None,
    location_city: Optional[str] = None,
    location_state: Optional[str] = None,
    location_description: Optional[str] = None,
    week_ref_date: Optional[str] = None
):
    # compute week window
    if week_ref_date:
        monday, friday = _week_monday_and_friday(date.fromisoformat(week_ref_date))
    else:
        monday, friday = _week_monday_and_friday(date.today())

    try:
        df = load_active_employees_dataframe()
    except Exception as e:
        logger.exception("Failed to load active employees sheet")
        return {"error": f"active sheet load failed: {e}"}

    rf = region_filter.strip().lower() if region_filter else None
    lc = location_city.strip().lower() if location_city else None
    ls = location_state.strip().lower() if location_state else None
    ld = location_description.strip().lower() if location_description else None

    sel = df.copy()
    if rf:
        sel = sel[sel["region_code"].fillna("").str.strip().str.lower() == rf]
    if lc:
        sel = sel[sel["location_city"].fillna("").str.strip().str.lower() == lc]
    if ls:
        sel = sel[sel["location_state"].fillna("").str.strip().str.lower() == ls]
    if ld:
        sel = sel[sel["location_desc"].fillna("").str.strip().str.lower() == ld]

    total_active = len(sel)
    employee_ids = sel["employee_id"].astype(str).str.strip().tolist()

    # Ensure region history cache is primed (should be done by caller/app)
    try:
        prefetch_region_history(timeout=10)
    except Exception:
        logger.exception("prefetch_region_history failed (continuing)")

    # presence_map: best-effort using DB + region history fallback
    presence_map = _fetch_presence_for_employees(employee_ids, monday, friday, partition_filter=location_city)

    # compute today count (today inside week may differ; we compute today's presence using presence_map and DB fallback)
    today = date.today()
    today_count = 0
    for eid in employee_ids:
        pm = presence_map.get(eid, {})
        if pm.get(today, 0) > 0:
            today_count += 1
        else:
            # fallback DB check
            try:
                with SessionLocal() as db:
                    row = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == eid, AttendanceSummary.date == today).first()
                    if row and getattr(row, "presence_count", 0) > 0:
                        today_count += 1
                        continue
                    digits = _digits_only(eid)
                    if digits:
                        cand = digits.lstrip('0') or digits
                        row2 = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == cand, AttendanceSummary.date == today).first()
                        if row2 and getattr(row2, "presence_count", 0) > 0:
                            today_count += 1
            except Exception:
                continue

    today_pct = round((today_count / float(total_active)) * 100.0, 2) if total_active > 0 else None

    sel["on_leave"] = sel["current_status"].apply(lambda x: _maybe_mark_on_leave(x))
    leave_count = int(sel["on_leave"].sum())
    sel["employee_type_norm"] = sel["employee_type"].fillna("").str.strip().str.lower()
    type_counts = sel["employee_type_norm"].value_counts().to_dict()

    regular_df = sel[sel["employee_type_norm"].str.contains("regular", na=False)]
    regular_ids = regular_df["employee_id"].astype(str).str.strip().tolist()

    regular_presence = {}
    for eid in regular_ids:
        week_map = presence_map.get(eid, {})
        days_present = sum(1 for d, v in week_map.items() if v and (monday <= d <= friday))
        days_present = int(days_present)
        regular_presence[eid] = {
            "days_present": days_present,
            "on_leave": bool(sel[sel["employee_id"] == eid]["on_leave"].any()),
            "full_name": sel[sel["employee_id"] == eid]["full_name"].iloc[0] if not sel[sel["employee_id"] == eid].empty else None
        }

    present_5_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 5)
    present_3_or_more_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 3)
    present_less_than_3_count = sum(1 for v in regular_presence.values() if v["days_present"] < 3)
    present_only_1_count = sum(1 for v in regular_presence.values() if v["days_present"] == 1)

    present_5_list = []
    present_3_list = []
    defaulters_list = []

    for eid, info in regular_presence.items():
        entry = {
            "employee_id": eid,
            "full_name": info["full_name"],
            "days_present": info["days_present"],
            "on_leave": info["on_leave"]
        }
        if info["days_present"] >= 5:
            present_5_list.append(entry)
        if info["days_present"] >= 3:
            present_3_list.append(entry)
        if info["days_present"] < 3:
            defaulters_list.append(entry)

    present_5_list = sorted(present_5_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    present_3_list = sorted(present_3_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    defaulters_list = sorted(defaulters_list, key=lambda x: (x["days_present"], x["on_leave"], x["full_name"] or ""))[:limit_list]

    # NEW: explicit sheet vs DB summary comparison diagnostics
    try:
        sheet_vs_summary = _compare_sheet_vs_db_summary(sel, monday, friday)
    except Exception:
        logger.exception("sheet vs summary comparison failed")
        sheet_vs_summary = {"error": "comparison failure"}

    summary = {
        "filters": {
            "region": region_filter,
            "location_city": location_city,
            "location_state": location_state,
            "location_description": location_description,
            "week_monday": monday.isoformat(),
            "week_friday": friday.isoformat()
        },
        "counts": {
            "total_active_in_sheet": total_active,
            "today_headcount_from_summary": today_count,
            "today_headcount_pct_vs_sheet": today_pct,
            "on_leave_count_in_sheet": leave_count,
            "employee_type_counts": type_counts
        },
        "regular_attendance_summary": {
            "regular_total": len(regular_ids),
            "present_5_day_count": present_5_count,
            "present_3_or_more_count": present_3_or_more_count,
            "present_less_than_3_count": present_less_than_3_count,
            "present_only_1_day_count": present_only_1_count
        },
        # attach diagnostics
        "sheet_vs_summary": sheet_vs_summary
    }

    details = {
        "present_5_days": present_5_list,
        "present_3_or_more_days": present_3_list,
        "defaulters_less_than_3_days": defaulters_list,
        "sheet_vs_summary_rows": sheet_vs_summary.get("per_employee_summary") if isinstance(sheet_vs_summary, dict) else {}
    }

    report_path = None
    if export:
        try:
            report_name = f"attendance_compare_{(region_filter or 'all')}_{(location_city or 'all')}_{uuid.uuid4().hex[:8]}.xlsx"
            report_file = OUTPUT_DIR / report_name
            with pd.ExcelWriter(report_file, engine="openpyxl") as writer:
                sel_df_for_export = sel.copy()
                sel_df_for_export["raw_row_str"] = sel_df_for_export["raw_row"].apply(lambda r: str(r) if r is not None else "")
                sel_df_for_export.to_excel(writer, sheet_name="active_sheet_selection", index=False)
                pd.DataFrame([summary["counts"]]).to_excel(writer, sheet_name="summary_counts", index=False)
                pd.DataFrame([summary["regular_attendance_summary"]]).to_excel(writer, sheet_name="regular_summary", index=False)
                pd.DataFrame(details["present_5_days"]).to_excel(writer, sheet_name="present_5_days", index=False)
                pd.DataFrame(details["present_3_or_more_days"]).to_excel(writer, sheet_name="present_3_plus", index=False)
                pd.DataFrame(details["defaulters_less_than_3_days"]).to_excel(writer, sheet_name="defaulters_lt3", index=False)
                # sheet vs summary diagnostics
                try:
                    # per_employee_summary -> long table
                    per_emp = sheet_vs_summary.get("per_employee_summary") or {}
                    per_emp_rows = []
                    for k, v in per_emp.items():
                        r = {"employee_id": k, "full_name": v.get("full_name"), "db_total_week_presence": v.get("db_total_week_presence")}
                        # flatten db_by_date
                        for d, val in sorted((v.get("db_by_date") or {}).items()):
                            r[f"day_{d}"] = val
                        per_emp_rows.append(r)
                    if per_emp_rows:
                        pd.DataFrame(per_emp_rows).to_excel(writer, sheet_name="sheet_vs_db_per_employee", index=False)
                    if sheet_vs_summary.get("missing_in_summary"):
                        pd.DataFrame(sheet_vs_summary.get("missing_in_summary")).to_excel(writer, sheet_name="missing_in_summary", index=False)
                    if sheet_vs_summary.get("extra_summary_ids"):
                        pd.DataFrame(sheet_vs_summary.get("extra_summary_ids")).to_excel(writer, sheet_name="extra_summary_ids", index=False)
                except Exception:
                    logger.exception("Failed writing sheet_vs_summary sections to export")
            report_path = str(report_file.name)
        except Exception:
            logger.exception("Failed to write export report")
            report_path = None

    out = {
        "mode": mode,
        "stats_detail": stats_detail,
        "summary": summary,
        "details": details
    }
    if report_path:
        out["report_path"] = report_path
    return out


if __name__ == "__main__":
    res = compare_ccure_vs_sheets(region_filter="APAC", location_city="Pune", export=False, limit_list=20)
    import json as _json
    print(_json.dumps(res, indent=2, default=str))









#for Active Employee and ACtive Contractor Comparision Report.

# C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service.py
"""
Compare CCURE active lists (employees & contractors) with uploaded Active sheets (from disk).
Provides compare_ccure_vs_sheets(mode='full', stats_detail='ActiveProfiles', limit_list=200, export=False)

When export=True, writes Excel report to OUTPUT_DIR and returns 'report_path'.
"""

from datetime import datetime
import os
import re
import uuid
import logging
import sys
from pathlib import Path
import pandas as pd

logger = logging.getLogger("data_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Settings fallback (matches app.py pattern)
try:
    from settings import OUTPUT_DIR as SETTINGS_OUTPUT_DIR, DATA_DIR as SETTINGS_DATA_DIR, RAW_UPLOAD_DIR as SETTINGS_RAW_UPLOAD_DIR
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    RAW_UPLOAD_DIR = Path(SETTINGS_RAW_UPLOAD_DIR)
except Exception:
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"
    DATA_DIR = Path(__file__).resolve().parent / "data"
    RAW_UPLOAD_DIR = DATA_DIR / "raw_uploads"

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# ccure client helper (optional)
try:
    import ccure_client
except Exception:
    ccure_client = None
    logger.warning("ccure_client not importable; CCURE calls will return None")

# ---------- small normalizers (kept local) ----------
def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

def _make_w_variant(s):
    if s is None:
        return None
    ss = str(s).strip()
    if ss.upper().startswith('W'):
        return ss
    return 'W' + ss

def _numeric_variants(s):
    out = set()
    if s is None:
        return out
    try:
        s = str(s)
        clean = re.sub(r'\D', '', s)
        if clean:
            out.add(clean)
            out.add(clean.lstrip('0') or clean)
            out.add('W' + clean)
    except Exception:
        pass
    return out

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- CCURE fetch helpers ----------
def _fetch_ccure_list(detail_name):
    """
    Uses ccure_client.fetch_all_stats(detail_name) if available, otherwise tries per-page fetch.
    Returns list of dicts or [].
    """
    if ccure_client is None:
        logger.warning("ccure_client missing - cannot fetch CCURE lists")
        return []
    try:
        if hasattr(ccure_client, "fetch_all_stats"):
            res = ccure_client.fetch_all_stats(detail_name, limit=500)
            return res or []
    except Exception:
        logger.exception("fetch_all_stats failed")
    # fallback to paged fetch if available
    try:
        data = []
        page = 1
        limit = 500
        while True:
            if not hasattr(ccure_client, "fetch_stats_page"):
                break
            page_res = ccure_client.fetch_stats_page(detail_name, page=page, limit=limit)
            if not page_res:
                break
            part = page_res.get("data") or []
            if not part:
                break
            data.extend(part)
            total = int(page_res.get("total") or len(data) or 0)
            if len(data) >= total:
                break
            page += 1
            if page > 1000:
                break
        return data
    except Exception:
        logger.exception("per-page fetch failed for %s", detail_name)
        return []

# ---------- helpers to find/load latest sheet from disk ----------
def _find_latest_file_for_kind(kind: str):
    """
    kind == 'employee' or 'contractor'
    Prefer canonical file in DATA_DIR: active_<kind>.*,
    otherwise pick latest in RAW_UPLOAD_DIR that contains the kind token.
    """
    # 1) canonical in DATA_DIR
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_{kind}{ext}"
        if p.exists():
            return p

    # 2) fallback: find latest raw file with kind token in RAW_UPLOAD_DIR
    try:
        files = [p for p in RAW_UPLOAD_DIR.iterdir() if p.is_file() and kind in p.name.lower() and p.suffix.lower() in (".xlsx", ".xls", ".csv")]
        if not files:
            # last fallback: any active_{kind}.* in RAW_UPLOAD_DIR
            files = [p for p in RAW_UPLOAD_DIR.iterdir() if p.is_file() and f"active_{kind}" in p.name.lower()]
        if not files:
            return None
        files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        return files[0]
    except Exception:
        return None

def _read_table(path: Path):
    try:
        if path.suffix.lower() == ".csv":
            df = pd.read_csv(path, dtype=str)
        else:
            df = pd.read_excel(path, sheet_name=0, dtype=str)
        df.columns = [str(c).strip() for c in df.columns]
        return df.fillna("")
    except Exception:
        logger.exception("Failed to read file %s", path)
        return pd.DataFrame()

def _first_present_from_row(row, candidates):
    for c in candidates:
        if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
            return row[c]
    for k in row.index:
        for c in candidates:
            if k.strip().lower() == c.strip().lower():
                val = row[k]
                if pd.notna(val) and str(val).strip() != "":
                    return val
    return None

# ---------- disk-based loaders to replace DB loaders ----------
def _load_active_employees_disk():
    """
    Return (set of normalized employee_ids, dict mapping id -> row sample, total_rows)
    Reads the latest employee sheet from data/.
    """
    path = _find_latest_file_for_kind("employee")
    if not path:
        return set(), {}, 0
    df = _read_table(path)
    ids = set()
    mapping = {}
    id_cols = ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id','Employee_Id']
    name_cols = ['Full Name','FullName','EmpName','Name','First Name','FirstName','Last Name']
    location_cols = ['Location City','Location','Location Description','City']
    status_cols = ['Current Status','Status','Employee Status','Employee_Status','Status (Current)','CurrentStatus']

    for _, row in df.iterrows():
        raw_row = row.to_dict()
        emp_id = _first_present_from_row(row, id_cols)
        if emp_id is None or str(emp_id).strip() == "":
            continue
        emp_id = str(emp_id).strip()
        ids.add(emp_id)
        full_name = _first_present_from_row(row, name_cols) or ""
        mapping[emp_id] = {
            "employee_id": emp_id,
            "full_name": full_name,
            "location_city": _first_present_from_row(row, location_cols),
            "status": _first_present_from_row(row, status_cols),
            "raw": raw_row
        }
    return ids, mapping, len(df)

def _load_active_contractors_disk():
    """
    Return (set of candidate contractor ids, mapping, total_rows)
    Reads latest contractor sheet from data/.
    """
    path = _find_latest_file_for_kind("contractor")
    if not path:
        return set(), {}, 0
    df = _read_table(path)
    ids = set()
    mapping = {}
    wsid_cols = ['Worker System Id','Worker System ID','Worker ID','WorkerSystemId','WorkerId']
    ipass_cols = ['iPass ID','"W" iPass ID','IPassID','iPassID','Ipass ID','IpassID']
    name_cols = ['Full Name','FullName','Name']
    vendor_cols = ['Vendor Company Name','Vendor']

    for _, row in df.iterrows():
        raw_row = row.to_dict()
        wsid = _first_present_from_row(row, wsid_cols)
        ipass = _first_present_from_row(row, ipass_cols)
        full_name = _first_present_from_row(row, name_cols)
        if wsid:
            wsid = str(wsid).strip()
            ids.add(wsid)
            mapping[wsid] = {"worker_system_id": wsid, "full_name": full_name, "vendor": _first_present_from_row(row, vendor_cols), "raw": raw_row}
        if ipass:
            ipass = str(ipass).strip()
            ids.add(ipass)
            mapping[ipass] = {"ipass_id": ipass, "full_name": full_name, "vendor": _first_present_from_row(row, vendor_cols), "raw": raw_row}
            wvar = _make_w_variant(ipass)
            ids.add(wvar)
            mapping[wvar] = {"ipass_id": ipass, "w_ipass": wvar, "full_name": full_name, "raw": raw_row}
        for cand in (wsid, ipass):
            if cand:
                for v in _numeric_variants(cand):
                    ids.add(v)
                    if v not in mapping:
                        mapping[v] = {"derived_id": v, "full_name": full_name, "raw": raw_row}
    return ids, mapping, len(df)

# ---------- helpers to match ccure -> disk lists (same logic as before) ----------
def _employee_matches_disk(cid, emp_ids_disk, emp_map_disk, ccure_row):
    if cid in emp_ids_disk:
        return True
    for v in _numeric_variants(cid):
        if v in emp_ids_disk:
            return True
    try:
        ci = _safe_int(cid)
        if ci is not None:
            for dbk in emp_ids_disk:
                if _safe_int(dbk) == ci:
                    return True
    except Exception:
        pass
    card_cid = _normalize_card_like(cid)
    if card_cid:
        for dbk in emp_ids_disk:
            if _normalize_card_like(dbk) == card_cid:
                return True
    try:
        name = None
        if isinstance(ccure_row, dict):
            name = ccure_row.get("EmpName") or ccure_row.get("FullName") or ccure_row.get("Name")
        normname = _normalize_name(name)
        if normname:
            for dbk, dbv in emp_map_disk.items():
                if _normalize_name(dbv.get("full_name")) == normname:
                    return True
    except Exception:
        pass
    return False

def _contractor_matches_disk(cid, contr_ids_disk, contr_map_disk, ccure_row):
    if cid in contr_ids_disk:
        return True
    for v in _numeric_variants(cid):
        if v in contr_ids_disk:
            return True
    try:
        ci = _safe_int(cid)
        if ci is not None:
            for dbk in contr_ids_disk:
                if _safe_int(dbk) == ci:
                    return True
    except Exception:
        pass
    card_cid = _normalize_card_like(cid)
    if card_cid:
        for dbk in contr_ids_disk:
            if _normalize_card_like(dbk) == card_cid:
                return True
    try:
        name = None
        if isinstance(ccure_row, dict):
            name = ccure_row.get("EmpName") or ccure_row.get("FullName") or ccure_row.get("Name")
        normname = _normalize_name(name)
        if normname:
            for dbk, dbv in contr_map_disk.items():
                if _normalize_name(dbv.get("full_name")) == normname:
                    return True
    except Exception:
        pass
    return False

# ---------- core compare function ----------
def compare_ccure_vs_sheets(mode="full", stats_detail="ActiveProfiles", limit_list=200, export=False):
    """
    Main public function used by /ccure/compare.
    Reads latest uploaded sheets from disk instead of DB tables.
    """
    result = {
        "ccure_active_employees_count": None,
        "ccure_active_contractors_count": None,
        "active_sheet_employee_count": None,
        "active_sheet_contractor_count": None,
        "missing_employees_count": None,
        "missing_contractors_count": None,
        "missing_employees_sample": [],
        "missing_contractors_sample": [],
        "report_path": None
    }

    # 1) fetch CCURE lists
    ccure_emps = _fetch_ccure_list("ActiveEmployees")
    ccure_contrs = _fetch_ccure_list("ActiveContractors")

    result["ccure_active_employees_count"] = len(ccure_emps)
    result["ccure_active_contractors_count"] = len(ccure_contrs)

    # 2) load uploaded sheets from disk (preferred) — returns (ids_set, mapping, total_rows)
    emp_ids_disk, emp_map_disk, emp_total_rows = _load_active_employees_disk()
    contr_ids_disk, contr_map_disk, contr_total_rows = _load_active_contractors_disk()

    result["active_sheet_employee_count"] = int(emp_total_rows)
    result["active_sheet_contractor_count"] = int(contr_total_rows)

    # 3) build ccure id sets for employees
    ccure_emp_id_set = set()
    ccure_emp_rows_by_id = {}
    for row in ccure_emps:
        try:
            eid = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("EmpID") or row.get("Employee Id"))
            if not eid:
                eid = _normalize_card_like(row.get("CardNumber") or row.get("iPass ID") or row.get("IPassID") or row.get("Card"))
            if not eid:
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    eid = f"name::{fname}"
            if eid:
                ccure_emp_id_set.add(eid)
                ccure_emp_rows_by_id[eid] = row
        except Exception:
            continue

    # 4) employees missing = ccure_emp_id_set - emp_ids_disk (but consider numeric variants, int equality, card-like, name match)
    expanded_emp_disk_ids = set(emp_ids_disk)
    for v in list(emp_ids_disk):
        for nv in _numeric_variants(v):
            expanded_emp_disk_ids.add(nv)

    missing_emp_ids = []
    for cid in ccure_emp_id_set:
        try:
            if str(cid).startswith("name::"):
                name = cid.split("::", 1)[1]
                found = False
                for dbk, dbv in emp_map_disk.items():
                    n = _normalize_name(dbv.get("full_name"))
                    if n and n == name:
                        found = True
                        break
                if not found:
                    missing_emp_ids.append(cid)
                continue

            ccure_row = ccure_emp_rows_by_id.get(cid) or {}
            if _employee_matches_disk(cid, expanded_emp_disk_ids, emp_map_disk, ccure_row):
                continue

            found = False
            for v in _numeric_variants(cid):
                if v in expanded_emp_disk_ids:
                    found = True
                    break
            if not found:
                missing_emp_ids.append(cid)
        except Exception:
            missing_emp_ids.append(cid)

    result["missing_employees_count"] = len(missing_emp_ids)
    samp_emp = []
    for mid in missing_emp_ids[:limit_list]:
        r = ccure_emp_rows_by_id.get(mid) or {}

        # extract manager/profile/status from raw if present
        manager_name = r.get("Manager_Name") or r.get("ManagerName") or r.get("Manager") or r.get("Manager_WU_ID")
        profile_disabled = r.get("Profile_Disabled") if "Profile_Disabled" in r else r.get("profile_disabled") if "profile_disabled" in r else r.get("ProfileDisabled") if "ProfileDisabled" in r else None
        employee_status = r.get("Employee_Status") or r.get("Employee Status") or r.get("Status") or r.get("employee_status")

        # ensure string conversion for boolean-like values
        if isinstance(profile_disabled, bool):
            profile_disabled = str(profile_disabled)

        # vendorCompany doesn't apply to employees — keep blank
        vendor_company = r.get("Vendor Company Name") or r.get("Vendor") or r.get("vendor") or ""

        # build sample row with both old and new key names for compatibility
        samp_emp.append({
            "ccure_key": mid,
            # old keys (kept for compatibility)
            "EmployeeID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "PersonnelType": r.get("PersonnelType"),
            # "VendorCompany": vendor_company,
            "Manager_Name": manager_name,
            "Profile_Disabled": profile_disabled,
            "Employee_Status": employee_status,
            # new/canonical lowerCamel keys requested
            "employee_Id": r.get("EmployeeID") or r.get("employee_id") or r.get("Employee Id"),
            "empName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "personnelType": r.get("PersonnelType"),
            "vendorCompany": vendor_company,
            "managerName": manager_name,
            "profileDisabled": profile_disabled,
            "employeeStatus": employee_status,
            "raw": r
        })
    result["missing_employees_sample"] = samp_emp

    # 5) contractors
    ccure_contr_id_set = set()
    ccure_contr_rows_by_id = {}
    for row in ccure_contrs:
        try:
            cand_ids = []
            e1 = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("Employee Id"))
            if e1:
                cand_ids.append(e1)
            ip = _normalize_employee_key(row.get("IPassID") or row.get("iPass ID") or row.get("iPass") or row.get("IPASSID"))
            if ip:
                cand_ids.append(ip)
                cand_ids.append(_make_w_variant(ip))
            cardlike = _normalize_card_like(row.get("CardNumber") or row.get("card_number") or row.get("Badge") or row.get("BadgeNo"))
            if cardlike:
                cand_ids.append(cardlike)
                cand_ids.extend(list(_numeric_variants(cardlike)))
            if not cand_ids:
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    cand_ids.append(f"name::{fname}")
            for cid in cand_ids:
                if cid:
                    ccure_contr_id_set.add(cid)
                    ccure_contr_rows_by_id[cid] = row
            if not cand_ids:
                key = f"unknown::{uuid.uuid4().hex[:8]}"
                ccure_contr_id_set.add(key)
                ccure_contr_rows_by_id[key] = row
        except Exception:
            continue

    expanded_contr_disk_ids = set(contr_ids_disk)
    for v in list(contr_ids_disk):
        for nv in _numeric_variants(v):
            expanded_contr_disk_ids.add(nv)

    missing_contr_ids = []
    for cid in ccure_contr_id_set:
        try:
            if str(cid).startswith("name::"):
                name = cid.split("::", 1)[1]
                found = False
                for dbk, dbv in contr_map_disk.items():
                    n = _normalize_name(dbv.get("full_name"))
                    if n and n == name:
                        found = True
                        break
                if not found:
                    missing_contr_ids.append(cid)
                continue

            ccure_row = ccure_contr_rows_by_id.get(cid) or {}
            if _contractor_matches_disk(cid, expanded_contr_disk_ids, contr_map_disk, ccure_row):
                continue

            found = False
            for v in _numeric_variants(cid):
                if v in expanded_contr_disk_ids:
                    found = True
                    break
            if not found:
                missing_contr_ids.append(cid)
        except Exception:
            missing_contr_ids.append(cid)

    result["missing_contractors_count"] = len(missing_contr_ids)
    samp_contr = []
    for mid in missing_contr_ids[:limit_list]:
        r = ccure_contr_rows_by_id.get(mid) or {}

        # vendor/company
        vendor_company = r.get("Vendor Company Name") or r.get("Vendor") or r.get("vendor") or ""

        # personnel, manager, profile, status extraction
        personnel_type = r.get("PersonnelType") or r.get("Personnel_Type") or r.get("Personnel Type") or None
        manager_name = r.get("Manager_Name") or r.get("ManagerName") or r.get("Manager") or r.get("Manager_WU_ID")
        profile_disabled = r.get("Profile_Disabled") if "Profile_Disabled" in r else r.get("profile_disabled") if "profile_disabled" in r else None
        employee_status = r.get("Employee_Status") or r.get("Employee Status") or r.get("Status") or r.get("employee_status")

        if isinstance(profile_disabled, bool):
            profile_disabled = str(profile_disabled)

        samp_contr.append({
            "ccure_key": mid,
            # old keys kept
            "Employee_ID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "VendorCompany": vendor_company,
            "PersonnelType": personnel_type,
            "Manager_Name": manager_name,
            "Profile_Disabled": profile_disabled,
            "Employee_Status": employee_status,
            # new requested lowerCamel keys
            "employeeId": r.get("EmployeeID") or r.get("employee_id") or r.get("Employee Id"),
            "empName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "vendorCompany": vendor_company,
            "personnelType": personnel_type,
            "managerName": manager_name,
            "profileDisabled": profile_disabled,
            "employeeStatus": employee_status,
            "raw": r
        })
    result["missing_contractors_sample"] = samp_contr

    # 6) optionally export report
    if export:
        try:
            OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
            ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            fname = f"missing_vs_ccure_{ts}.xlsx"
            fullpath = OUTPUT_DIR / fname

            # Canonical column set requested (lowerCamel)
            columns = ["ccure_key", "employee_Id", "empName", "personnelType", "managerName", "profileDisabled", "employeeStatus"]

            # Build DataFrames and ensure columns order (fill missing with empty string)
            if samp_emp:
                df_emp = pd.DataFrame(samp_emp)
                # reindex to requested columns; missing columns will be added with NaN
                df_emp = df_emp.reindex(columns=columns).fillna("")
            else:
                df_emp = pd.DataFrame(columns=columns)

            if samp_contr:
                df_con = pd.DataFrame(samp_contr)
                df_con = df_con.reindex(columns=columns).fillna("")
            else:
                df_con = pd.DataFrame(columns=columns)

            try:
                with pd.ExcelWriter(fullpath, engine="openpyxl") as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
            except Exception:
                # fallback to default engine
                with pd.ExcelWriter(fullpath) as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
        except Exception:
            logger.exception("Failed to export report")
            result["report_path"] = None

    return result






#export 

def export_uploaded_sheets():
    """
    Create a single xlsx workbook with:
      - Sheet "Employee" -> contents of latest canonical employee sheet (if present)
      - Sheet "Contractor" -> contents of latest canonical contractor sheet (if present)
    Writes file into OUTPUT_DIR and returns the filename (not full path).
    """
    try:
        emp_path = _find_latest_file_for_kind("employee")
        contr_path = _find_latest_file_for_kind("contractor")

        # Ensure OUTPUT_DIR exists
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
        fname = f"uploaded_sheets_{ts}.xlsx"
        fullpath = OUTPUT_DIR / fname

        # Read tables (if present) or create empty dataframe placeholders
        if emp_path and emp_path.exists():
            df_emp = _read_table(emp_path)
        else:
            df_emp = pd.DataFrame()

        if contr_path and contr_path.exists():
            df_con = _read_table(contr_path)
        else:
            df_con = pd.DataFrame()

        # Write to one workbook (try openpyxl if available)
        try:
            with pd.ExcelWriter(fullpath, engine="openpyxl") as writer:
                df_emp.to_excel(writer, sheet_name="Employee", index=False)
                df_con.to_excel(writer, sheet_name="Contractor", index=False)
        except Exception:
            # fallback to default engine
            with pd.ExcelWriter(fullpath) as writer:
                df_emp.to_excel(writer, sheet_name="Employee", index=False)
                df_con.to_excel(writer, sheet_name="Contractor", index=False)

        return fname
    except Exception:
        logger.exception("export_uploaded_sheets failed")
        return None


# Expose public function name expected by app.py
__all__ =  ["compare_ccure_vs_sheets", "compare_ccure_vs_sheets", "export_uploaded_sheets"]  # keep backwards compatibility
# End of data_compare_service.py






4)
# C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\ingest_excel.py
import pandas as pd
from datetime import datetime
from sqlalchemy.exc import IntegrityError
from db import SessionLocal, engine
from models import Base, ActiveEmployee, ActiveContractor
from settings import UPLOAD_DIR
import uuid, os

# --- database setup: do NOT run create_all at import time ---
def init_db():
    """
    Create DB tables if they do not exist.
    Call this manually only when you want to initialize/repair the DB:
      python -c "from ingest_excel import init_db; init_db()"
    """
    from db import engine
    from models import Base
    Base.metadata.create_all(bind=engine)

def _first_present(row, candidates):
    for c in candidates:
        v = row.get(c)
        if v is not None and str(v).strip() != "":
            return v
    return None

def ingest_employee_excel(path, uploaded_by="system"):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    # robust mapping keys
    with SessionLocal() as db:
        for _, row in df.iterrows():
            emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id'])
            if emp_id:
                emp_id = str(emp_id).strip()
            if not emp_id:
                # skip rows without an employee id
                continue
            full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
            # robust current_status detection
            status_candidates = ['Current Status','Status','Employee Status','Employee_Status','Status (Current)','CurrentStatus']
            current_status = _first_present(row, status_candidates)
            email = _first_present(row, ["Employee's Email",'Email','Email Address'])
            location_city = _first_present(row, ['Location City','Location','Location Description','City'])
            rec = ActiveEmployee(
                employee_id=emp_id,
                full_name=full_name,
                email=email,
                location_city=location_city,
                location_desc=row.get('Location Description'),
                current_status=current_status,
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)  # upsert
                db.commit()
            except IntegrityError:
                db.rollback()
            except Exception:
                db.rollback()

def ingest_contractor_excel(path):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    with SessionLocal() as db:
        for _, row in df.iterrows():
            wsid = _first_present(row, ['Worker System Id','Worker System ID','Worker ID','WorkerSystemId'])
            if wsid:
                wsid = str(wsid).strip()
            if not wsid:
                continue
            ipass = _first_present(row, ['iPass ID','"W" iPass ID','IPassID','iPassID','Ipass ID'])
            full_name = _first_present(row, ['Full Name','FullName','Name'])
            rec = ActiveContractor(
                worker_system_id=wsid,
                ipass_id=ipass,
                full_name=full_name,
                vendor=_first_present(row, ['Vendor Company Name','Vendor']),
                location=_first_present(row, ['Worker Location','Location']),
                status=_first_present(row, ['Status','Current Status']),
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)
                db.commit()
            except IntegrityError:
                db.rollback()
            except Exception:
                db.rollback()

if __name__ == "__main__":
    # ingestion convenience: read all uploaded files
    for f in os.listdir(UPLOAD_DIR):
        p = UPLOAD_DIR / f
        if 'contractor' in f.lower() or 'contractor' in str(p).lower():
            ingest_contractor_excel(p)
        else:
            ingest_employee_excel(p)
    print("Ingestion completed.")










5)


# region_clients.py
"""
HTTP helpers for region occupancy endpoints.
Returns:
 - fetch_all_regions() -> list of {"region": <name>, "count": <int>}
 - fetch_all_details(timeout=...) -> list of person-detail dicts (where available)
 - fetch_all_history(timeout=...) -> list of date-or-detail dicts (history endpoints)

Drop-in replacement for your region_clients.py (more defensive retries + per-endpoint cooldown).
"""
import requests
from requests.exceptions import RequestException
from datetime import datetime
import logging
import time
import sys
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

logger = logging.getLogger("region_clients")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# endpoints (edit if your hosts differ)
endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/live-summary",
    "emea":  "http://10.199.22.57:3007/api/occupancy/live-summary",
    "laca":  "http://10.199.22.57:4000/api/occupancy/live-summary",
    "apac":  "http://10.199.22.57:3008/api/occupancy/live-summary"
}

history_endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/history",
    "emea":  "http://10.199.22.57:3007/api/occupancy/history",
    "apac":  "http://10.199.22.57:3008/api/occupancy/history",
    "laca":  "http://10.199.22.57:4000/api/occupancy/history"
}

DEFAULT_ATTEMPTS = 3
DEFAULT_BACKOFF = 0.6
# per-endpoint cooldown in seconds after repeated failure to avoid hammering
ENDPOINT_COOLDOWN_SECONDS = 60
ENDPOINT_FAILURES = {}  # url -> (last_failed_ts, fail_count)

def _build_session():
    s = requests.Session()
    try:
        # Retry for idempotent GETs
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=frozenset(['GET', 'HEAD'])
        )
    except TypeError:
        # older urllib3
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            method_whitelist=frozenset(['GET', 'HEAD'])
        )
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

_SESSION = _build_session()

def _do_get_with_retries(url, timeout, attempts=DEFAULT_ATTEMPTS, backoff=DEFAULT_BACKOFF):
    now = time.time()
    # cooldown check
    last = ENDPOINT_FAILURES.get(url)
    if last:
        last_failed_ts, fail_count = last
        if now - last_failed_ts < ENDPOINT_COOLDOWN_SECONDS:
            logger.debug("[region_clients] skipping %s (in cooldown, last failure %ds ago)", url, int(now - last_failed_ts))
            return None

    last_err = None
    for attempt in range(1, attempts + 1):
        try:
            # use separate connect/read timeouts
            r = _SESSION.get(url, timeout=(3, max(5, timeout)))
            r.raise_for_status()
            try:
                return r.json()
            except ValueError:
                try:
                    return {"_raw_text": r.text}
                except Exception:
                    return None
        except RequestException as e:
            last_err = e
            logger.warning(f"[region_clients] attempt {attempt}/{attempts} failed for {url}: {e}")
            if attempt < attempts:
                time.sleep(backoff * attempt)
            continue
        except Exception as e:
            last_err = e
            logger.warning(f"[region_clients] unexpected error fetching {url}: {e}")
            if attempt < attempts:
                time.sleep(backoff * attempt)
            continue

    # mark failure
    ENDPOINT_FAILURES[url] = (time.time(), (ENDPOINT_FAILURES.get(url, (0,0))[1] or 0) + 1)
    logger.warning(f"[region_clients] all {attempts} attempts failed for {url}: {last_err}")
    return None

def fetch_all_regions(timeout=6):
    results = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            realtime = {}
            if isinstance(data, dict):
                realtime = data.get("realtime", {}) or {}
            total = 0
            for site in realtime.values():
                try:
                    total += int(site.get("total", 0))
                except Exception:
                    pass
            if total == 0 and isinstance(data, dict):
                for k, v in data.items():
                    if isinstance(v, dict) and "total" in v:
                        try:
                            total += int(v.get("total", 0))
                        except Exception:
                            pass
            results.append({"region": region, "count": total})
        except Exception as e:
            logger.exception(f"[region_clients] unexpected error fetching live-summary for {region}: {e}")
            results.append({"region": region, "count": None})
    return results

def _extract_list_candidates(data):
    if isinstance(data, list):
        return [x for x in data if isinstance(x, dict)]
    if isinstance(data, dict):
        for k in ("details","people","list","items","results","data","entries"):
            if k in data and isinstance(data[k], list):
                return [x for x in data[k] if isinstance(x, dict)]
        return []
    return []

def fetch_all_details(timeout=6):
    all_details = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            details = _extract_list_candidates(data)
            if not details and isinstance(data, dict):
                for v in data.values():
                    if isinstance(v, dict):
                        candidates = _extract_list_candidates(v)
                        if candidates:
                            details.extend(candidates)
            for d in details:
                try:
                    d2 = dict(d)
                    d2["_region"] = region
                    d2["_source_url"] = url
                    all_details.append(d2)
                except Exception:
                    continue
        except Exception as e:
            logger.warning(f"[region_clients] warning: cannot fetch details from {region}@{url}: {e}")
            continue

    # try history endpoints as second chance
    if not all_details:
        for region, url in history_endpoints.items():
            try:
                data = _do_get_with_retries(url, timeout=timeout) or {}
                details = _extract_list_candidates(data)
                for d in details:
                    try:
                        d2 = dict(d)
                        d2["_region"] = region
                        d2["_source_url"] = url
                        all_details.append(d2)
                    except Exception:
                        continue
            except Exception:
                logger.debug(f"[region_clients] history details fetch for {region} failed", exc_info=True)
                continue

    logger.info("[region_clients] fetched %d detail rows across endpoints", len(all_details))
    return all_details

def fetch_history_for_region(region, timeout=6):
    url = history_endpoints.get(region)
    if not url:
        logger.debug(f"[fetch_history_for_region] no history endpoint for {region}")
        return []
    try:
        data = _do_get_with_retries(url, timeout=timeout) or {}
        summary = []
        if isinstance(data, dict):
            for key in ("summaryByDate","summary","data","entries","results","details","items"):
                if key in data and isinstance(data.get(key), list):
                    for s in data.get(key):
                        if isinstance(s, dict):
                            s2 = dict(s)
                            s2["_region"] = region
                            s2["_source_url"] = url
                            summary.append(s2)
                    break
            else:
                if "date" in data or "day" in data:
                    s2 = dict(data)
                    s2["_region"] = region
                    s2["_source_url"] = url
                    summary.append(s2)
        elif isinstance(data, list):
            for s in data:
                if isinstance(s, dict):
                    s2 = dict(s)
                    s2["_region"] = region
                    s2["_source_url"] = url
                    summary.append(s2)
        return summary
    except Exception as e:
        logger.warning(f"[region_clients] cannot fetch history for {region}@{url}: {e}")
        # mark failure timestamp so callers don't hammer this endpoint in the same run
        ENDPOINT_FAILURES[url] = (time.time(), (ENDPOINT_FAILURES.get(url, (0,0))[1] or 0) + 1)
        return []

def fetch_all_history(timeout=6):
    all_entries = []
    for region in history_endpoints.keys():
        try:
            entries = fetch_history_for_region(region, timeout=timeout)
            if entries:
                all_entries.extend(entries)
        except Exception:
            logger.exception(f"[region_clients] error aggregating history for {region}")
            continue
    logger.info("[region_clients] fetched %d history entries", len(all_entries))
    return all_entries








6)


// frontend/src/pages/GlobalPage.jsx

import React, { useState, useEffect, useRef } from 'react';
import {
  Box, Typography, CircularProgress, IconButton, Button, Paper, Divider,
  LinearProgress, Snackbar, Alert, List, ListItem, ListItemText,Tooltip
} from '@mui/material';

import HomeIcon from '@mui/icons-material/Home';
import DescriptionIcon from '@mui/icons-material/Description';
import UploadFileIcon from '@mui/icons-material/UploadFile';

import MapChart from '../components/MapChart.jsx';
import api from '../api';
import { useNavigate, Link } from 'react-router-dom';



import TimerIcon from "@mui/icons-material/AccessTime"; // duration icon
import { Link as RouterLink } from "react-router-dom";

/*
  Important:
  - Do NOT mix /api/headcount and /api/ccure/verify.
  - Region cards (APAC/EMEA/LACA/NAMER) come only from /api/headcount.
  - Live vs CCURE Summary now comes from /api/ccure/verify?raw=true.
  - Initial region totals are zero (keeps previous UI behaviour).
  - We implement polling for headcount and SSE for ccure/stream (realtime via SSE).
*/

export default function GlobalPage() {
  const navigate = useNavigate();

  // Region totals (headcount) - default to zeros so UI shows 0 immediately (preserve previous behaviour)
  const [counts, setCounts] = useState({ apac: 0, emea: 0, laca: 0, namer: 0 });
  const [selected, setSelected] = useState('global');

  // Averages/ccure state (left panel)
  const [averages, setAverages] = useState(null);
  const [loadingAverages, setLoadingAverages] = useState(true);
  const [averagesError, setAveragesError] = useState(null);

  // upload state
  const [uploading, setUploading] = useState(false);
  const [uploadResult, setUploadResult] = useState(null);
  const [uploadError, setUploadError] = useState(null);

  // (kept for backwards-compatibility but upload buttons are always shown now)
  const [uploadedEmployee, setUploadedEmployee] = useState(false);
  const [uploadedContractor, setUploadedContractor] = useState(false);

  const [exportReportPath, setExportReportPath] = useState(null);

  // top-row file inputs
  const fileInputEmpRef = useRef();
  const fileInputContrRef = useRef();
  const [snack, setSnack] = useState({ open: false, severity: 'info', message: '' });

  // date-range state for top-right controls
  const [startDate, setStartDate] = useState('');
  const [endDate, setEndDate] = useState('');

  // Polling refs for safe scheduling and backoff
  const headcountRef = useRef({ timerId: null, failureCount: 0, isFetching: false });
  const averagesRef = useRef({ timerId: null, failureCount: 0, isFetching: false });


  const [showCenterUploads, setShowCenterUploads] = useState(false);


  // -----------------------
  // HEADCOUNT POLLING ONLY (unchanged)
  // -----------------------
  useEffect(() => {
    let mounted = true;

    const fetchHeadcount = async () => {
      if (!mounted) return;
      if (headcountRef.current.isFetching) return;
      headcountRef.current.isFetching = true;

      try {
        const res = await api.get('/headcount');
        if (!mounted) return;
        const d = res.data;
        if (d && typeof d === 'object') {
          const newCounts = {
            apac: Number(d.apac || 0),
            emea: Number(d.emea || 0),
            laca: Number(d.laca || 0),
            namer: Number(d.namer || 0),
          };
          setCounts(prev => {
            if (
              prev.apac === newCounts.apac &&
              prev.emea === newCounts.emea &&
              prev.laca === newCounts.laca &&
              prev.namer === newCounts.namer
            ) {
              return prev;
            }
            return newCounts;
          });
        } else {
          console.warn('[headcount] unexpected response shape - ignoring', d);
        }
        headcountRef.current.failureCount = 0;
      } catch (err) {
        headcountRef.current.failureCount = (headcountRef.current.failureCount || 0) + 1;
        console.warn('[headcount] fetch failed:', err?.message || err);
      } finally {
        headcountRef.current.isFetching = false;
        const f = headcountRef.current.failureCount || 0;
        const backoffMs = 15000 * Math.pow(2, Math.min(Math.max(f - 1, 0), 4)); // 15s..240s
        headcountRef.current.timerId = setTimeout(fetchHeadcount, backoffMs);
      }
    };

    fetchHeadcount();

    return () => {
      mounted = false;
      if (headcountRef.current.timerId) clearTimeout(headcountRef.current.timerId);
      headcountRef.current.isFetching = false;
    };
  }, []); // run once

  // AVERAGES: use SSE (direct to Python backend) with fallback initial fetch
  useEffect(() => {
    let stopped = false;
    let es = null;
    let backoff = 1000;

    // Allow override via VITE_PY_BACKEND; otherwise assume python at :8000
    const PY_BACKEND = (import.meta.env.VITE_PY_BACKEND || `${window.location.protocol}//${window.location.hostname}:8000`).replace(/\/$/, '');

    const connect = () => {
      if (stopped) return;
      try {
        es = new EventSource(`${PY_BACKEND}/ccure/stream`);
      } catch (err) {
        console.warn('SSE creation failed', err);
        es = null;
      }

      if (!es) {
        initialFetch();
        return;
      }

      es.onopen = () => {
        console.info('[SSE] connected to', `${PY_BACKEND}/ccure/stream`);
        backoff = 1000;
        setAveragesError(null);
      };

      es.onmessage = (evt) => {
        try {
          const payload = JSON.parse(evt.data);
          setAverages(payload);
          setLoadingAverages(false);
          setAveragesError(null);
        } catch (e) {
          console.warn('Failed to parse SSE message', e);
        }
      };

      es.onerror = (err) => {
        console.warn('[SSE] error/closed, attempting reconnect', err);
        try { es.close(); } catch (e) { }
        es = null;
        if (stopped) return;
        setTimeout(() => {
          backoff = Math.min(backoff * 2, 30000);
          connect();
        }, backoff);
      };
    };

    const initialFetch = async () => {
      setLoadingAverages(true);
      setAveragesError(null);
      try {
        const res = await api.get('/ccure/verify?raw=true');
        setAverages(res.data);
        setLoadingAverages(false);
        setAveragesError(null);
      } catch (err) {
        console.warn('initial /ccure/verify?raw=true fetch failed', err);
        setLoadingAverages(false);
        setAveragesError(err);
      }
    };

    initialFetch();
    connect();

    return () => {
      stopped = true;
      if (es) {
        try { es.close(); } catch (e) { }
        es = null;
      }
    };
  }, []);

  // -----------------------
  // Upload helper (updated to use fetch, kept robust)
  // -----------------------
  const handleUpload = async (file, type) => {
    if (!file) return;
    const endpoint = type === 'employee' ? '/upload/active-employees' : '/upload/active-contractors';

    const PY_BACKEND = (import.meta.env.VITE_PY_BACKEND || `${window.location.protocol}//${window.location.hostname}:8000`).replace(/\/$/, '');
    const url = `${PY_BACKEND}${endpoint}`;

    const fd = new FormData();
    fd.append('file', file, file.name);

    setUploading(true);
    setUploadResult(null);
    setUploadError(null);

    try {
      console.info('Uploading to', url, file.name);
      const resp = await fetch(url, {
        method: 'POST',
        body: fd,
      });

      const rawText = await resp.text();
      let data = null;
      try { data = rawText ? JSON.parse(rawText) : null; } catch (e) { data = { raw: rawText }; }

      console.info('Upload response', resp.status, resp.statusText, data);

      if (!resp.ok) {
        throw new Error(`Upload failed HTTP ${resp.status} ${resp.statusText} - ${JSON.stringify(data)}`);
      }

      setUploadResult(data);
      setSnack({ open: true, severity: 'success', message: `Active Sheet Updated successfully: ${file.name}` });

      // mark canonical presence if backend returned detail.canonical_saved
      const saved = data && data.detail && (data.detail.canonical_saved || data.detail.canonical_saved === "");
      if (type === 'employee') setUploadedEmployee(!!saved);
      if (type === 'contractor') setUploadedContractor(!!saved);

      // best-effort refresh of averages/headcount
      try {
        const r1 = await fetch(`${PY_BACKEND}/ccure/verify?raw=true`);
        if (r1.ok) setAverages(await r1.json());
      } catch (e) { console.warn('refresh verify failed', e); }

      try {
        const r2 = await fetch(`${PY_BACKEND}/headcount`);
        if (r2.ok) {
          const d = await r2.json();
          if (d && typeof d === 'object') {
            setCounts({
              apac: Number(d.apac || 0),
              emea: Number(d.emea || 0),
              laca: Number(d.laca || 0),
              namer: Number(d.namer || 0)
            });
          }
        }
      } catch (e) { console.warn('refresh headcount failed', e); }

    } catch (err) {
      console.error('Upload failed', err);
      setUploadError(err);
      setSnack({ open: true, severity: 'error', message: `Upload failed: ${file.name} — ${err.message}` });
    } finally {
      setUploading(false);
    }
  };

  // --- Helpers: find CCURE arrays in 'averages' and generate CSV ---
  const _escapeCsv = (v) => {
    if (v === null || v === undefined) return '';
    if (typeof v === 'object') {
      try { v = JSON.stringify(v); } catch { v = String(v); }
    }
    const s = String(v).replace(/"/g, '""');
    return `"${s}"`;
  };

  // Find candidate arrays inside the averages object that look like CCURE comparison rows
  const findCcureArrays = (obj) => {
    const found = [];
    if (!obj || typeof obj !== 'object') return found;

    const inspect = (parentKey, val) => {
      if (!val) return;
      if (Array.isArray(val) && val.length > 0 && typeof val[0] === 'object') {
        const sample = val[0];
        // heuristic: many ccure rows have ccure_key or EmployeeID or EmpName fields
        if ('ccure_key' in sample || 'EmployeeID' in sample || 'EmpName' in sample) {
          found.push({ key: parentKey, arr: val });
        }
      } else if (typeof val === 'object') {
        for (const k of Object.keys(val)) {
          inspect(parentKey ? `${parentKey}.${k}` : k, val[k]);
        }
      }
    };

    inspect('', obj);
    return found;
  };

  // Build CSV rows with the requested columns:
  // Employees: ccure_key, EmployeeID, EmpName, PersonnelType, Manager_Name, Profile_Disabled, Employee_Status
  // Contractors: ccure_key, EmployeeID, EmpName, VendorCompany, PersonnelType, Manager_Name, Profile_Disabled, Employee_Status
  const generateCcureCompareCSV = (averagesPayload) => {
    if (!averagesPayload || typeof averagesPayload !== 'object') return null;

    const candidates = findCcureArrays(averagesPayload);
    if (!candidates || candidates.length === 0) return null;

    // choose the first plausible array
    const rows = candidates[0].arr;

    if (!Array.isArray(rows) || rows.length === 0) return null;

    const headers = ['ccure_key', 'EmployeeID', 'EmpName', 'VendorCompany', 'PersonnelType', 'Manager_Name', 'Profile_Disabled', 'Employee_Status'];
    const lines = [headers.map(h => _escapeCsv(h)).join(',')];

    const readField = (r, name) => {
      if (!r) return '';
      if (r[name] !== undefined && r[name] !== null) return r[name];
      if (r.raw && typeof r.raw === 'object' && r.raw[name] !== undefined && r.raw[name] !== null) return r.raw[name];
      // some backends put nested object as string in "raw"
      if (typeof r.raw === 'string') {
        try {
          const parsed = JSON.parse(r.raw);
          if (parsed && parsed[name] !== undefined) return parsed[name];
        } catch { }
      }
      return '';
    };

    for (const r of rows) {
      // derive PersonnelType (sometimes present in row or row.raw)
      const personnelType = readField(r, 'PersonnelType') || readField(r, 'Personnel_Type') || readField(r, 'PersonnelTypeName') || '';

      const ccure_key = readField(r, 'ccure_key') || readField(r, 'CcureKey') || readField(r, 'EmployeeID') || '';
      const employeeId = readField(r, 'EmployeeID') || readField(r, 'Employee_Id') || '';
      const empName = readField(r, 'EmpName') || readField(r, 'EmployeeName') || readField(r, 'Name') || '';
      const vendorCompany = readField(r, 'VendorCompany') || readField(r, 'Vendor_Company') || '';
      const managerName = readField(r, 'Manager_Name') || readField(r, 'ManagerName') || '';
      let profileDisabled = readField(r, 'Profile_Disabled');
      if (profileDisabled === true || profileDisabled === false) profileDisabled = String(profileDisabled);
      const employeeStatus = readField(r, 'Employee_Status') || readField(r, 'Status') || '';

      const rowValues = [
        ccure_key,
        employeeId,
        empName,
        vendorCompany,
        personnelType,
        managerName,
        profileDisabled,
        employeeStatus,
      ];

      lines.push(rowValues.map(v => _escapeCsv(v)).join(','));
    }

    const csvContent = lines.join('\n');
    return csvContent;
  };

  // Export: request server to generate compare report and download the xlsx (binary)
  const exportUploadedSheets = async () => {
    setUploading(true);
    try {
      const PY_BACKEND = (import.meta.env.VITE_PY_BACKEND || `${window.location.protocol}//${window.location.hostname}:8000`).replace(/\/$/, '');

      // attempt server-side generation as before
      let genJson = null;
      try {
        const genRes = await fetch(`${PY_BACKEND}/ccure/compare?export=true`, {
          method: 'GET',
        });

        try {
          genJson = await genRes.json();
        } catch (e) {
          const txt = await genRes.text().catch(() => '');
          throw new Error(`Export generation returned non-JSON response: ${txt}`);
        }

        if (!genRes.ok || !genJson || !genJson.report_path) {
          const msg = (genJson && (genJson.detail || genJson.error)) || JSON.stringify(genJson || {});
          console.warn('Server export generation failed or returned unexpected payload:', msg);
          setSnack({ open: true, severity: 'warning', message: 'Server export generation failed; attempting client CSV export' });
        } else {
          // server returned a report_path — attempt to download it
          const reportPath = genJson.report_path;
          setExportReportPath(reportPath);
          setSnack({ open: true, severity: 'success', message: 'Export created — downloading now' });

          try {
            const dlUrl = `${PY_BACKEND}/ccure/report/${encodeURIComponent(reportPath)}`;
            const fileRes = await fetch(dlUrl, { method: 'GET' });

            if (!fileRes.ok) {
              const txt = await fileRes.text().catch(() => '');
              console.warn('Failed to download server report:', txt);
            } else {
              const blob = await fileRes.blob();
              const blobUrl = window.URL.createObjectURL(blob);
              const a = document.createElement('a');
              a.href = blobUrl;
              a.download = reportPath || 'missing_vs_ccure.xlsx';
              document.body.appendChild(a);
              a.click();
              a.remove();
              window.URL.revokeObjectURL(blobUrl);
            }
          } catch (e) {
            console.warn('Server report download failed:', e);
          }
        }
      } catch (err) {
        // log and fall through to client-side CSV
        console.warn('exportUploadedSheets server call failed:', err);
      }

      // --- Client-side CSV fallback / supplemental export with requested columns ---
      try {
        const csv = generateCcureCompareCSV(averages || {});
        if (csv) {
          const blob = new Blob([csv], { type: 'text/csv;charset=utf-8;' });
          const url = window.URL.createObjectURL(blob);
          const a = document.createElement('a');
          a.href = url;
          // prefer a descriptive filename
          const fname = `missing_vs_ccure_custom_${(new Date()).toISOString().slice(0, 10)}.csv`;
          a.download = fname;
          document.body.appendChild(a);
          a.click();
          a.remove();
          window.URL.revokeObjectURL(url);
          setSnack({ open: true, severity: 'success', message: 'Custom CCURE CSV downloaded' });
        } else {
          // nothing to export client-side
          setSnack(prev => ({ ...prev, open: true, severity: 'info', message: 'No CCURE rows found in current averages payload for CSV export' }));
        }
      } catch (e) {
        console.error('Client CSV generation failed', e);
        setSnack(prev => ({ ...prev, open: true, severity: 'error', message: `CSV generation failed: ${e?.message || e}` }));
      }

    } catch (err) {
      console.error('exportUploadedSheets error', err);
      setSnack({ open: true, severity: 'error', message: `Export failed: ${err.message || err}` });
    } finally {
      setUploading(false);
    }
  };

  const onChooseEmployeeFile = (e) => { const f = e.target.files && e.target.files[0]; if (f) handleUpload(f, 'employee'); e.target.value = null; };
  const onChooseContractorFile = (e) => { const f = e.target.files && e.target.files[0]; if (f) handleUpload(f, 'contractor'); e.target.value = null; };

  // apply date range to re-fetch /ccure/verify
  const applyDateRange = async () => {
    if (!startDate || !endDate) {
      setSnack({ open: true, severity: 'warning', message: 'Please select start and end dates' });
      return;
    }
    setLoadingAverages(true);
    setAveragesError(null);
    try {
      const res = await api.get(`/ccure/verify?raw=true&start_date=${startDate}&end_date=${endDate}`);
      setAverages(res.data);
      setLoadingAverages(false);
      setSnack({ open: true, severity: 'success', message: 'Averages updated' });
    } catch (err) {
      console.warn('applyDateRange failed', err);
      setLoadingAverages(false);
      setAveragesError(err);
      setSnack({ open: true, severity: 'error', message: 'Failed to update averages' });
    }
  };

  // safe helper for nested averages paths
  const safe = (path, fallback = null) => {
    if (!averages) return fallback;
    try {
      return path.split('.').reduce((a, k) => (a && a[k] !== undefined ? a[k] : fallback), averages);
    } catch {
      return fallback;
    }
  };

  // Derived values (unchanged)
  const ccureActiveEmployees = safe('ccure_reported.employees',
    safe('ccure_active.active_employees',
      safe('ccure_active.ccure_active_employees_reported', null)
    )
  );
  const ccureActiveContractors = safe('ccure_reported.contractors',
    safe('ccure_active.active_contractors',
      safe('ccure_active.ccure_active_contractors_reported', null)
    )
  );

  const headTotalVisited = safe('headcount_attendance_summary.total_visited_today',
    safe('headcount_details.total_visited_today', null)
  );
  const headEmployee = safe('headcount_attendance_summary.employee',
    safe('headcount_details.employee', null)
  );
  const headContractor = safe('headcount_attendance_summary.contractor',
    safe('headcount_details.contractor', null)
  );

  const liveCurrentTotal = safe('live_headcount_region_clients.currently_present_total',
    safe('live_headcount_details.currently_present_total',
      null
    )
  );
  const liveEmp = safe('live_headcount_region_clients.employee',
    safe('live_headcount_details.employee', null)
  );
  const liveContr = safe('live_headcount_region_clients.contractor',
    safe('live_headcount_details.contractor', null)
  );

  const empPct = safe('percentages_vs_ccure.head_employee_pct_vs_ccure_today',
    safe('averages.head_emp_pct_vs_ccure_today', null)
  );
  const conPct = safe('percentages_vs_ccure.head_contractor_pct_vs_ccure_today',
    safe('averages.head_contractor_pct_vs_ccure_today', null)
  );
  const overallPct = safe('percentages_vs_ccure.head_overall_pct_vs_ccure_today',
    safe('averages.headcount_overall_pct_vs_ccure_today', null)
  );

  const avg7 = safe('averages.history_avg_overall_last_7_days',
    safe('averages.avg_headcount_last_7_days',
      safe('averages.avg_headcount_last_7_days_db', null)
    )
  );

  const respDate = safe('date', null);

  const locationAvgsObj = safe('averages.history_avg_by_location_last_7_days',
    safe('history_avg_by_location_last_7_days',
      safe('raw.averages.history_avg_by_location_last_7_days', {})
    )
  );

  const locationAvgsList = React.useMemo(() => {
    if (!locationAvgsObj || typeof locationAvgsObj !== 'object') return [];
    const arr = Object.entries(locationAvgsObj).map(([loc, vals]) => {
      return {
        location: loc,
        avg_employee_last_7_days: vals.avg_employee_last_7_days ?? vals.history_avg_employee_last_7_days ?? vals.avg_employee ?? null,
        avg_contractor_last_7_days: vals.avg_contractor_last_7_days ?? vals.history_avg_contractor_last_7_days ?? vals.avg_contractor ?? null,
        avg_overall_last_7_days: vals.avg_overall_last_7_days ?? vals.history_avg_overall_last_7_days ?? vals.avg_overall ?? null,
        history_days_counted: vals.history_days_counted ?? null
      };
    });
    arr.sort((a, b) => (b.avg_overall_last_7_days ?? -Infinity) - (a.avg_overall_last_7_days ?? -Infinity));
    return arr;
  }, [locationAvgsObj]);

  const globalCount = Number((counts.apac || 0)) + Number((counts.emea || 0)) + Number((counts.laca || 0)) + Number((counts.namer || 0));

  const hideScrollbarSx = {
    overflowY: 'auto',
    '&::-webkit-scrollbar': { width: 0, height: 0 },
    scrollbarWidth: 'none',
    msOverflowStyle: 'none',
  };

  // Render
  return (
    <Box sx={{ display: 'flex', flexDirection: 'column', height: '100vh', overflow: 'hidden', bgcolor: 'background.default' }}>
      {/* Header */}
      <Box px={2} py={1} sx={{ backgroundColor: 'black', color: '#fff', borderBottom: '4px solid #FFD700', display: 'flex', alignItems: 'center', justifyContent: 'space-between' }}>
        <Box>
          <IconButton component={Link} to="/" sx={{ color: '#FFC72C' }}><HomeIcon fontSize="medium" /></IconButton>
          <IconButton component={Link} to="/reports" sx={{ color: '#FFC72C', ml: 1 }}><DescriptionIcon fontSize="medium" /></IconButton>
          <IconButton
            component="a"
            href="http://10.138.161.4:3000/dashboard/index.html"

            rel="noopener noreferrer"
            sx={{ color: '#FFC72C', ml: 1 }}
          >
            <i className="fa-solid fa-camera" style={{ fontSize: 20 }} />
          </IconButton>

          <IconButton
            component="a"
            href="http://10.199.22.57:3004/"
            rel="noopener noreferrer"
            sx={{ color: '#FFF', ml: 1 }}
          >
            <i className="bi bi-patch-check"></i>
          </IconButton>



{/* place this after the Associate Verification Tool link */}
<Tooltip title="Duration Reports">
  <IconButton
              component={RouterLink}
              to="/duration"
              size="large"
              aria-label="Duration Reports"
              sx={{ color: '#FFF', ml: 1 }}
            >
              <TimerIcon />
            </IconButton>
          </Tooltip>


        </Box>

        <Box sx={{ flexGrow: 1, display: 'flex', alignItems: 'center', justifyContent: 'center' }}>
          <Box component="img" src="/wu-head-logo.png" alt="WU Logo" sx={{ height: { xs: 30, md: 55 }, mr: 2 }} />
          <Typography variant="h5" sx={{ fontWeight: 'bold', color: 'primary.main' }}>Global Headcount Dashboard</Typography>
        </Box>

        <Box sx={{ width: 120 }} />
      </Box>

      {/* Top row: Uploads | GLOBAL + Region Cards | Date selectors */}
      <Box sx={{ display: 'flex', alignItems: 'center', p: 1, px: 1, gap: 1 }}>
        {/* Left: fixed column with stacked upload/export buttons (aligned left) */}
        <Box sx={{ width: 260, display: 'flex', flexDirection: 'column', gap: 1 }}>
          <input type="file" accept=".xls,.xlsx,.csv" style={{ display: 'none' }} ref={fileInputEmpRef} onChange={onChooseEmployeeFile} />
          <Button
            variant="contained"
            size="small"
            startIcon={<UploadFileIcon />}
            onClick={() => fileInputEmpRef.current && fileInputEmpRef.current.click()}
            sx={{ width: '100%', height: 25, textTransform: 'none', fontWeight: 700 }}
          >
            Upload Active Employee Sheet
          </Button>

          <input type="file" accept=".xls,.xlsx,.csv" style={{ display: 'none' }} ref={fileInputContrRef} onChange={onChooseContractorFile} />
          <Button
            variant="contained"
            size="small"
            startIcon={<UploadFileIcon />}
            onClick={() => fileInputContrRef.current && fileInputContrRef.current.click()}
            sx={{ width: '100%', height: 25, textTransform: 'none', fontWeight: 700 }}
          >
            Upload Active Contractor Sheet
          </Button>

          <Button
            variant="contained"
            size="small"
            startIcon={<DescriptionIcon />}
            onClick={exportUploadedSheets}
            sx={{ width: '100%', height: 25, textTransform: 'none', fontWeight: 700 }}
          >
            Export Comparison → Report
          </Button>
        </Box>

        {/* Center: flexible, keeps region cards exactly centered on screen */}
        <Box sx={{ flex: 1, display: 'flex', justifyContent: 'center' }}>
          <Box sx={{ display: 'flex', gap: 3, alignItems: 'center', justifyContent: 'center', flexWrap: 'wrap' }}>
            {[
              { key: 'global', label: 'GLOBAL', count: globalCount, url: null },
              { key: 'apac', label: 'APAC', count: counts.apac, url: 'http://10.199.22.57:3000/' },
              { key: 'emea', label: 'EMEA', count: counts.emea, url: 'http://10.199.22.57:3001/' },
              { key: 'laca', label: 'LACA', count: counts.laca, url: 'http://10.199.22.57:3003/' },
              { key: 'namer', label: 'NAMER', count: counts.namer, url: 'http://10.199.22.57:3002/' },
            ].map(region => (
              <Box
                key={region.key}
                onClick={() => {
                  if (region.key === 'global') {
                    const el = document.querySelector('[data-global-left-panel]');
                    if (el) el.scrollIntoView({ behavior: 'smooth', block: 'start' });
                    setSelected('global');
                    return;
                  }
                  if (region.url) window.location.href = region.url;
                }}
                sx={{
                  cursor: 'pointer',
                  width: 200,
                  height: 88,
                  display: 'flex',
                  flexDirection: 'column',
                  justifyContent: 'center',
                  alignItems: 'center',
                  border: '4px solid rgba(255, 204, 0, 0.89)',
                  borderRadius: 2,
                  boxShadow: 3,
                  bgcolor: 'transparent',
                  '&:hover': { opacity: 0.95 },
                }}
              >
                <Typography variant="subtitle2" sx={{ fontWeight: 'bold', color: '#FFC72C', fontSize: { xs: '0.95rem', md: '1.2rem' } }}>
                  {region.label}
                </Typography>
                <Typography variant="h4" sx={{ fontWeight: 900, fontSize: { xs: '1.2rem', md: '1.6rem' }, color: '#FFFFFF' }}>
                  {region.count ?? 0}
                </Typography>
              </Box>
            ))}
          </Box>
        </Box>

        {/* Right: fixed column for date selectors (aligned right) */}
        <Box sx={{ width: 360, display: 'flex', flexDirection: 'column', gap: 1, alignItems: 'flex-end' }}>
          <Paper sx={{ p: 1, display: 'flex', gap: 1, alignItems: 'center', boxShadow: 1, width: '100%' }}>
            <Box sx={{ display: 'flex', flexDirection: 'column', width: 160 }}>
              <Typography variant="caption" color="text.secondary">Select Start date</Typography>
              <input
                type="date"
                value={startDate}
                onChange={(e) => setStartDate(e.target.value)}
                style={{ width: '100%', height: 34, borderRadius: 4, border: '1px solid rgba(255,255,255,0.06)', padding: 4, background: 'transparent', color: 'inherit' }}
              />
            </Box>

            <Box sx={{ display: 'flex', flexDirection: 'column', width: 160 }}>
              <Typography variant="caption" color="text.secondary">Select End date</Typography>
              <input
                type="date"
                value={endDate}
                onChange={(e) => setEndDate(e.target.value)}
                style={{ width: '100%', height: 34, borderRadius: 4, border: '1px solid rgba(255,255,255,0.06)', padding: 4, background: 'transparent', color: 'inherit' }}
              />
            </Box>
          </Paper>

          <Box sx={{ width: '100%', display: 'flex', justifyContent: 'flex-start' }}>
            <Button size="small" variant="contained" onClick={applyDateRange} sx={{ height: 36, textTransform: 'none', fontWeight: 700 }}>
              Apply
            </Button>
          </Box>
        </Box>
      </Box>

      {/* Main: left summary | center map | right averages */}
      <Box sx={{ display: 'flex', flex: 1, overflow: 'hidden' }}>
        {/* Left detail panel */}
        <Box
          data-global-left-panel
          sx={{
            width: { xs: 320, md: 360 },
            minWidth: { md: 320 },
            p: 2,
            bgcolor: 'background.paper',
            borderRight: '1px solid rgba(255,255,255,0.06)',
            display: 'flex',
            flexDirection: 'column',
            ...hideScrollbarSx,
            height: '100%',
          }}
        >
          <Typography variant="h6" sx={{ mb: 1, color: 'primary.main' }}>Live vs CCURE Summary</Typography>

          {loadingAverages ? (
            <Box sx={{ py: 2 }}><LinearProgress /></Box>
          ) : averagesError ? (
            <Alert severity="error">Failed to load CCURE averages</Alert>
          ) : averages ? (
            <>
              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.02)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary">CCURE Active (reported)</Typography>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1, alignItems: 'center' }}>
                  <Box>
                    <Typography variant="h4" sx={{ fontWeight: 800 }}>{ccureActiveEmployees ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Active Employees</Typography>
                  </Box>
                  <Box sx={{ textAlign: 'right' }}>
                    <Typography variant="h5" sx={{ fontWeight: 800 }}>{ccureActiveContractors ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Active Contractors</Typography>
                  </Box>
                </Box>
              </Paper>

              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.02)' }} elevation={0}>
                <Box sx={{ display: 'flex', justifyContent: 'space-between' }}>
                  <Typography variant="subtitle2" color="text.secondary">Live Today</Typography>
                  <Typography variant="caption" color="text.secondary">{respDate ?? ''}</Typography>
                </Box>

                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Box>
                    <Typography variant="h5" sx={{ fontWeight: 800 }}>{headEmployee ?? liveEmp ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Employee</Typography>
                  </Box>
                  <Box>
                    <Typography variant="h5" sx={{ fontWeight: 800 }}>{headContractor ?? liveContr ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Contractor</Typography>
                  </Box>
                </Box>

                <Divider sx={{ my: 1 }} />

                <Box>
                  <Typography variant="caption" color="text.secondary">Totals</Typography>
                  <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.75 }}>
                    <Typography variant="body2">Attendance total (today)</Typography>
                    <Typography variant="body2" sx={{ fontWeight: 700 }}>{headTotalVisited ?? '—'}</Typography>
                  </Box>

                  <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                    <Typography variant="body2">Live region total</Typography>
                    <Typography variant="body2" sx={{ fontWeight: 700 }}>{liveCurrentTotal ?? '—'}</Typography>
                  </Box>

                  {safe('headcount_details.total_visited_today', null) != null && (
                    <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                      <Typography variant="body2">Detail rows total</Typography>
                      <Typography variant="body2" sx={{ fontWeight: 700 }}>{safe('headcount_details.total_visited_today', '—')}</Typography>
                    </Box>
                  )}
                </Box>
              </Paper>

              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.02)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary">Percentages vs CCURE</Typography>

                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Typography variant="body2">Employees</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{empPct != null ? `${empPct}%` : '—'}</Typography>
                </Box>
                <Box sx={{ display: 'flex', justifyContent: 'space-between' }}>
                  <Typography variant="body2">Contractors</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{conPct != null ? `${conPct}%` : '—'}</Typography>
                </Box>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                  <Typography variant="body2">Overall</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{overallPct != null ? `${overallPct}%` : '—'}</Typography>
                </Box>

                <Divider sx={{ my: 1 }} />
                <Typography variant="caption" color="text.secondary">Averages</Typography>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Typography variant="body2">7-day avg headcount</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{avg7 ?? '—'}</Typography>
                </Box>
              </Paper>

              {averages.notes && (
                <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.01)' }}>
                  <Typography variant="body2" sx={{ mt: 1 }}>{averages.notes}</Typography>
                </Paper>
              )}
            </>
          ) : (
            <Typography variant="body2" color="text.secondary">No data</Typography>
          )}
        </Box>

        {/* Center: map (flex) */}
        <Box sx={{ flex: 1, minWidth: 0, position: 'relative', display: 'flex', flexDirection: 'column' }}>
          <Box sx={{ flex: 1, minHeight: 0 }}>
            <MapChart selected={selected} onClickSite={r => setSelected(r)} initialZoom={1.8} />
          </Box>
        </Box>

        {/* Right side: Location averages panel */}
        <Box
          sx={{
            width: { xs: 320, md: 360 },
            minWidth: { md: 320 },
            borderLeft: '1px solid rgba(255,255,255,0.06)',
            bgcolor: 'background.paper',
            p: 2,
            display: 'flex',
            flexDirection: 'column',
            ...hideScrollbarSx,
            height: '100%',
          }}
        >
          <Typography variant="h6" sx={{ mb: 1, color: 'primary.main' }}>Location Averages</Typography>

          {loadingAverages ? (
            <Box sx={{ display: 'flex', alignItems: 'center', justifyContent: 'center', py: 4 }}>
              <CircularProgress />
            </Box>
          ) : averagesError ? (
            <Alert severity="error">Failed to load location averages</Alert>
          ) : locationAvgsList.length === 0 ? (
            <Typography variant="body2" color="text.secondary">No location averages available</Typography>
          ) : (
            <List dense disablePadding sx={{ flex: 1 }}>
              {locationAvgsList.map(item => (
                <ListItem key={item.location} sx={{ alignItems: 'flex-start', py: 1.25, borderBottom: '1px solid rgba(255,255,255,0.03)' }}>
                  <ListItemText
                    primary={
                      <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
                        <Typography sx={{ fontWeight: 800 }}>{item.location}</Typography>
                        <Typography variant="body2" sx={{ fontWeight: 800 }}>
                          {item.avg_overall_last_7_days != null ? Math.round(item.avg_overall_last_7_days) : '—'}
                        </Typography>
                      </Box>
                    }
                    secondary={
                      <Box sx={{ display: 'flex', gap: 2, mt: 0.5, flexWrap: 'wrap' }}>
                        <Typography variant="caption" color="text.secondary">Emp: <strong>{item.avg_employee_last_7_days != null ? Math.round(item.avg_employee_last_7_days) : '—'}</strong></Typography>
                        <Typography variant="caption" color="text.secondary">Contr: <strong>{item.avg_contractor_last_7_days != null ? Math.round(item.avg_contractor_last_7_days) : '—'}</strong></Typography>
                        {item.history_days_counted != null && <Typography variant="caption" color="text.secondary">Days: {item.history_days_counted}</Typography>}
                      </Box>
                    }
                    primaryTypographyProps={{ component: 'div' }}
                    secondaryTypographyProps={{ component: 'div' }}
                  />
                </ListItem>
              ))}
            </List>
          )}
        </Box>
      </Box>

      <Snackbar open={snack.open} autoHideDuration={3500} onClose={() => setSnack(prev => ({ ...prev, open: false }))}>
        <Alert severity={snack.severity} onClose={() => setSnack(prev => ({ ...prev, open: false }))}>{snack.message}</Alert>
      </Snackbar>
       
    </Box>
  );
}




