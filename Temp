// frontend/src/pages/GlobalPage.jsx
import React, { useState, useEffect, useRef } from 'react';
import {
  Box, Typography, CircularProgress, IconButton, Button, Paper, Divider,
  LinearProgress, Snackbar, Alert
} from '@mui/material';
import HomeIcon from '@mui/icons-material/Home';
import DescriptionIcon from '@mui/icons-material/Description';
import UploadFileIcon from '@mui/icons-material/UploadFile';
import MapChart from '../components/MapChart.jsx';
import api from '../api';
import { useNavigate, Link } from 'react-router-dom';

const REGION_PARTITIONS = {
  apac: ['Pune','Quezon','Taguig','Tokyo','Kuala'],
  emea: ['Vilnius','Casablanca','Dublin','Moscow','London','Vienna'],
  laca: ['Costa','Mexico','Sao Paulo','Lima','Cordoba','Buenos'],
  namer: ['Denver','NYC','Miami','Austin','New York']
};
function regionFromPartition(partition){
  if(!partition) return null;
  const p = String(partition).toLowerCase();
  if(!p) return null;
  for(const [region, parts] of Object.entries(REGION_PARTITIONS)){
    for(const candidate of parts){
      if(p.includes(candidate.toLowerCase())) return region;
    }
  }
  return null;
}

export default function GlobalPage(){
  const navigate = useNavigate();

  const [counts, setCounts] = useState({ apac:0, emea:0, laca:0, namer:0 });
  const [selected, setSelected] = useState('global');

  const [averages, setAverages] = useState(null);
  const [loadingAverages, setLoadingAverages] = useState(true);
  const [averagesError, setAveragesError] = useState(null);

  const [uploading, setUploading] = useState(false);
  const [uploadResult, setUploadResult] = useState(null);
  const [uploadError, setUploadError] = useState(null);

  const fileInputEmpRef = useRef();
  const fileInputContrRef = useRef();
  const [snack, setSnack] = useState({ open:false, severity:'info', message:'' });

  // polling controls
  const pollingRef = useRef({ running:false, failureCount:0, timerId:null });

  // map averages->region totals (heuristic)
  const mapFromAverages = (averagesObj) => {
    const byLocation = averagesObj?.headcount?.by_location || averagesObj?.live_headcount?.by_location || {};
    const perRegion = { apac:0, emea:0, laca:0, namer:0 };
    for(const [part, stats] of Object.entries(byLocation)){
      const total = Number(stats?.total || 0);
      const region = regionFromPartition(part) || regionFromPartition(part.split('.')[0]);
      if(region) perRegion[region] += total;
      else {
        const pLower = String(part || '').toLowerCase();
        if(pLower.includes('pune')||pLower.includes('philipp')||pLower.includes('taguig')||pLower.includes('tokyo')) perRegion.apac += total;
        else if(pLower.includes('vilnius')||pLower.includes('dublin')||pLower.includes('madrid')||pLower.includes('rome')||pLower.includes('casablanca')) perRegion.emea += total;
        else if(pLower.includes('mexico')||pLower.includes('costa')||pLower.includes('lima')||pLower.includes('buenos')) perRegion.laca += total;
        else perRegion.namer += total;
      }
    }
    return perRegion;
  };

  // single-shot fetchCounts with fallback; uses mounted guard
  const fetchCountsOnce = async (mounted=true) => {
    try {
      const res = await api.get('/headcount');
      if(!mounted) return {success:false, from:'headcount', data:null};
      const d = res.data;
      if(d && typeof d === 'object' && ('apac' in d || 'emea' in d || 'laca' in d || 'namer' in d)){
        return { success:true, from:'headcount', data: {
          apac: Number(d.apac||0), emea: Number(d.emea||0), laca: Number(d.laca||0), namer: Number(d.namer||0)
        }};
      }
      if(d && d.totals && typeof d.totals === 'object'){
        return { success:true, from:'headcount.totals', data: {
          apac: Number(d.totals.apac||0), emea: Number(d.totals.emea||0), laca: Number(d.totals.laca||0), namer: Number(d.totals.namer||0)
        }};
      }
      // unexpected shape -> fallback to averages
      throw new Error('unexpected headcount shape');
    } catch(errHead){
      // fallback to ccure/averages
      try {
        const res2 = await api.get('/ccure/averages');
        if(!mounted) return {success:false, from:'averages', data:null};
        const r = res2.data;
        // set averages state for left panel when this call succeeds
        setAverages(r);
        // derive per-region totals heuristically
        const mapped = mapFromAverages(r);
        return { success:true, from:'averages', data: mapped };
      } catch(errAvg){
        // both failed
        return { success:false, from:'both', data:null, error: errAvg || errHead };
      }
    }
  };

  // controlled polling with backoff
  useEffect(() => {
    let mounted = true;
    pollingRef.current.running = true;
    pollingRef.current.failureCount = 0;

    const doPoll = async () => {
      if(!mounted) return;
      // prevent overlapping runs
      if(pollingRef.current.running === false) return;
      const result = await fetchCountsOnce(mounted);
      if(!mounted) return;
      if(result.success && result.data){
        setCounts(prev => {
          // only update if changed to avoid unnecessary re-renders
          if(prev.apac === result.data.apac && prev.emea === result.data.emea && prev.laca === result.data.laca && prev.namer === result.data.namer) {
            return prev;
          }
          return result.data;
        });
        pollingRef.current.failureCount = 0; // reset on success
        // on success schedule next poll after normal interval
        pollingRef.current.timerId = setTimeout(doPoll, 15000);
      } else {
        // failure -> increment failure count and backoff
        pollingRef.current.failureCount = (pollingRef.current.failureCount || 0) + 1;
        const f = pollingRef.current.failureCount;
        // exponential backoff: 15s * 2^(min(f-1, 4))  => max ~240s
        const wait = 15000 * Math.pow(2, Math.min(Math.max(f-1,0), 4));
        console.warn('[fetchCounts] failure, will retry after backoff (s):', Math.round(wait/1000), result.error || result);
        pollingRef.current.timerId = setTimeout(doPoll, wait);
      }
    };

    // start immediately
    doPoll();

    return () => {
      mounted = false;
      pollingRef.current.running = false;
      if(pollingRef.current.timerId) clearTimeout(pollingRef.current.timerId);
    };
  }, []); // run once when mounted

  // Averages polling (separate, less frequent)
  useEffect(() => {
    let mounted = true;
    let timer = null;
    const pollAverages = async () => {
      try {
        setLoadingAverages(true);
        const res = await api.get('/ccure/averages');
        if(!mounted) return;
        setAverages(res.data);
        setAveragesError(null);
      } catch(err){
        if(!mounted) return;
        setAveragesError(err);
      } finally {
        if(!mounted) return;
        setLoadingAverages(false);
        timer = setTimeout(pollAverages, 30000);
      }
    };
    pollAverages();
    return () => { mounted = false; if(timer) clearTimeout(timer); };
  }, []);

  // upload helper (unchanged)
  const handleUpload = async (file, type) => {
    if(!file) return;
    const endpoint = type === 'employee' ? '/upload/active-employees' : '/upload/active-contractors';
    const fd = new FormData();
    fd.append('file', file, file.name);
    setUploading(true); setUploadResult(null); setUploadError(null);
    try {
      const res = await api.post(endpoint, fd, { headers: {'Content-Type':'multipart/form-data'}, timeout: 120000 });
      setUploadResult(res.data);
      setSnack({ open:true, severity:'success', message:`Upload successful: ${file.name}` });
      // refresh headcount+averages after upload
      try { const r = await api.get('/ccure/averages'); setAverages(r.data); } catch(_) {}
      try { const r2 = await api.get('/headcount'); if(r2?.data) setCounts(prev => ({...prev, ...r2.data})); } catch(_) {}
    } catch(err){
      setUploadError(err);
      setSnack({ open:true, severity:'error', message:`Upload failed: ${file.name}` });
    } finally {
      setUploading(false);
    }
  };

  const onChooseEmployeeFile = (e) => { const f = e.target.files && e.target.files[0]; if(f) handleUpload(f,'employee'); e.target.value=null; };
  const onChooseContractorFile = (e) => { const f = e.target.files && e.target.files[0]; if(f) handleUpload(f,'contractor'); e.target.value=null; };

  const safe = (path, fallback=null) => {
    if(!averages) return fallback;
    try { return path.split('.').reduce((a,k)=> (a && a[k] !== undefined) ? a[k] : fallback, averages); } catch { return fallback; }
  };

  const liveEmployee = safe('live_today.employee', safe('live_headcount.employee', null));
  const liveContractor = safe('live_today.contractor', safe('live_headcount.contractor', null));
  const liveTotalReported = safe('live_today.total_reported', safe('live_headcount.currently_present_total', null));
  const liveTotalDetails = safe('live_today.total_from_details', null);

  const ccureActiveEmployees = safe('ccure_active.active_employees', safe('ccure_active.ccure_active_employees_reported', null));
  const ccureActiveContractors = safe('ccure_active.active_contractors', safe('ccure_active.ccure_active_contractors_reported', null));

  const empPct = safe('averages.employee_pct', safe('averages.head_emp_pct_vs_ccure_today', null));
  const conPct = safe('averages.contractor_pct', safe('averages.head_contractor_pct_vs_ccure_today', null));
  const overallPct = safe('averages.overall_pct', safe('averages.headcount_overall_pct_vs_ccure_today', null));
  const avg7 = safe('averages.avg_headcount_last_7_days', safe('averages.history_avg_overall_last_7_days', null));

  // Render
  if(!counts) {
    // should not happen because we initialize counts; but keep fallback
    return <Box display="flex" justifyContent="center" mt={6}><CircularProgress/></Box>;
  }

  return (
    <Box sx={{ display:'flex', flexDirection:'column', height:'100vh', overflow:'hidden' }}>
      {/* Header */}
      <Box px={2} py={1} sx={{ backgroundColor:'black', color:'#fff', borderBottom:'4px solid #FFD700', display:'flex', alignItems:'center', justifyContent:'space-between' }}>
        <Box>
          <IconButton component={Link} to="/" sx={{ color: '#FFC72C' }}><HomeIcon/></IconButton>
          <IconButton component={Link} to="/reports" sx={{ color: '#FFC72C', ml:1 }}><DescriptionIcon/></IconButton>
        </Box>
        <Box sx={{ flexGrow:1, display:'flex', alignItems:'center', justifyContent:'center' }}>
          <Box component="img" src="/wu-head-logo.png" alt="WU" sx={{ height:{xs:30,md:55}, mr:2 }} />
          <Typography variant="h5" sx={{ fontWeight:'bold', color:'primary.main' }}>Global Headcount Dashboard</Typography>
        </Box>
        <Box sx={{ width:120 }} />
      </Box>

      {/* Region Cards */}
      <Box sx={{ display:'flex', gap:2, p:2, flexWrap:'wrap', justifyContent:'center' }}>
        {[
          { key:'apac', label:'APAC', url:'http://10.199.22.57:3000/', textColor:'#f5650c' },
          { key:'emea', label:'EMEA', url:'http://10.199.22.57:3001/', textColor:'#11e6ed' },
          { key:'laca', label:'LACA', url:'http://10.199.22.57:3003/', textColor:'#FF2DD1' },
          { key:'namer', label:'NAMER', url:'http://10.199.22.57:3002/', textColor:'#a6e61c' },
        ].map(region => (
          <Box key={region.key} onClick={() => { window.location.href = region.url; }} sx={{ cursor:'pointer', width:200, height:80, display:'flex', flexDirection:'column', justifyContent:'center', alignItems:'center', border:'4px solid rgba(255,204,0,0.89)', borderRadius:2, boxShadow:3, color:region.textColor, '&:hover':{opacity:0.9} }}>
            <Typography variant="subtitle1" sx={{ fontWeight:'bold', color:region.textColor, fontSize:{xs:'1.3rem'} }}>{region.label}</Typography>
            <Typography variant="h3" sx={{ fontWeight:700, fontSize:{xs:'1.5rem', sm:'1.8rem'}, color:region.textColor }}>
              { typeof counts[region.key] === 'number' ? counts[region.key] : 0 }
            </Typography>
          </Box>
        ))}
      </Box>

      {/* Main */}
      <Box sx={{ display:'flex', flex:1, overflow:'hidden' }}>
        <Box sx={{ width:360, p:2, bgcolor:'background.paper', borderRight:'1px solid rgba(255,255,255,0.06)', overflowY:'auto' }}>
          <Typography variant="h6" sx={{ mb:1, color:'primary.main' }}>Live vs CCURE Summary</Typography>

          { loadingAverages ? (<Box sx={{ py:2 }}><LinearProgress/></Box>) : averagesError ? (<Alert severity="error">Failed to load CCURE averages</Alert>) : averages ? (
            <>
              <Paper sx={{ p:2, mb:2, bgcolor:'rgba(255,255,255,0.03)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary">CCURE Active (reported)</Typography>
                <Box sx={{ display:'flex', justifyContent:'space-between', mt:1 }}>
                  <Box><Typography variant="h5" sx={{ fontWeight:700 }}>{ccureActiveEmployees ?? '—'}</Typography><Typography variant="caption" color="text.secondary">Active Employees</Typography></Box>
                  <Box sx={{ textAlign:'right' }}><Typography variant="h5" sx={{ fontWeight:700 }}>{ccureActiveContractors ?? '—'}</Typography><Typography variant="caption" color="text.secondary">Active Contractors</Typography></Box>
                </Box>
              </Paper>

              <Paper sx={{ p:2, mb:2, bgcolor:'rgba(255,255,255,0.03)' }} elevation={0}>
                <Box sx={{ display:'flex', justifyContent:'space-between' }}>
                  <Typography variant="subtitle2" color="text.secondary">Live Today</Typography>
                  <Typography variant="caption" color="text.secondary">{averages.date ?? ''}</Typography>
                </Box>
                <Box sx={{ display:'flex', justifyContent:'space-between', mt:1 }}>
                  <Box><Typography variant="h6" sx={{ fontWeight:700 }}>{liveEmployee ?? '—'}</Typography><Typography variant="caption" color="text.secondary">Employee</Typography></Box>
                  <Box><Typography variant="h6" sx={{ fontWeight:700 }}>{liveContractor ?? '—'}</Typography><Typography variant="caption" color="text.secondary">Contractor</Typography></Box>
                </Box>
                <Divider sx={{ my:1 }} />
                <Box>
                  <Typography variant="caption" color="text.secondary">Totals</Typography>
                  <Box sx={{ display:'flex', justifyContent:'space-between', mt:0.5 }}>
                    <Typography variant="body2">Reported total</Typography>
                    <Typography variant="body2" sx={{ fontWeight:700 }}>{liveTotalReported ?? '—'}</Typography>
                  </Box>
                  { liveTotalDetails != null && (<Box sx={{ display:'flex', justifyContent:'space-between', mt:0.5 }}><Typography variant="body2">Detail rows total</Typography><Typography variant="body2" sx={{ fontWeight:700 }}>{liveTotalDetails}</Typography></Box>) }
                </Box>
              </Paper>

              <Paper sx={{ p:2, mb:2, bgcolor:'rgba(255,255,255,0.03)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary">Percentages vs CCURE</Typography>
                <Box sx={{ display:'flex', justifyContent:'space-between', mt:1 }}><Typography variant="body2">Employees</Typography><Typography variant="body2" sx={{ fontWeight:700 }}>{empPct != null ? `${empPct}%` : '—'}</Typography></Box>
                <Box sx={{ display:'flex', justifyContent:'space-between' }}><Typography variant="body2">Contractors</Typography><Typography variant="body2" sx={{ fontWeight:700 }}>{conPct != null ? `${conPct}%` : '—'}</Typography></Box>
                <Box sx={{ display:'flex', justifyContent:'space-between', mt:0.5 }}><Typography variant="body2">Overall</Typography><Typography variant="body2" sx={{ fontWeight:700 }}>{overallPct != null ? `${overallPct}%` : '—'}</Typography></Box>
                <Divider sx={{ my:1 }} />
                <Typography variant="caption" color="text.secondary">Averages</Typography>
                <Box sx={{ display:'flex', justifyContent:'space-between', mt:1 }}><Typography variant="body2">7-day avg headcount</Typography><Typography variant="body2" sx={{ fontWeight:700 }}>{avg7 ?? '—'}</Typography></Box>
              </Paper>

              <Paper sx={{ p:2, mb:2, bgcolor:'rgba(255,255,255,0.03)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary" sx={{ mb:1 }}>Upload Active Sheets</Typography>
                <input type="file" accept=".xls,.xlsx" style={{ display:'none' }} ref={fileInputEmpRef} onChange={onChooseEmployeeFile} />
                <Button variant="contained" startIcon={<UploadFileIcon />} sx={{ mr:1 }} onClick={() => fileInputEmpRef.current && fileInputEmpRef.current.click()} disabled={uploading}>Upload Employees</Button>
                <input type="file" accept=".xls,.xlsx" style={{ display:'none' }} ref={fileInputContrRef} onChange={onChooseContractorFile} />
                <Button variant="outlined" startIcon={<UploadFileIcon />} onClick={() => fileInputContrRef.current && fileInputContrRef.current.click()} disabled={uploading}>Upload Contractors</Button>
                {uploading && <Box sx={{ mt:1 }}><LinearProgress/></Box>}
                {uploadResult && <Typography variant="caption" color="success.main" sx={{ mt:1, display:'block' }}>Upload OK</Typography>}
                {uploadError && <Typography variant="caption" color="error.main" sx={{ mt:1, display:'block' }}>Upload error</Typography>}
              </Paper>

              {averages.notes && <Paper sx={{ p:2, mb:2, bgcolor:'rgba(255,255,255,0.02)' }}><Typography variant="caption" color="text.secondary">Notes</Typography><Typography variant="body2" sx={{ mt:1 }}>{averages.notes}</Typography></Paper>}
            </>
          ) : (<Typography variant="body2" color="text.secondary">No data</Typography>)}
        </Box>

        <Box sx={{ flex:1, height:'100%', position:'relative' }}>
          <MapChart selected={selected} onClickSite={r => setSelected(r)} initialZoom={1.8} />
        </Box>
      </Box>

      <Snackbar open={snack.open} autoHideDuration={3500} onClose={() => setSnack(prev=>({...prev, open:false}))}>
        <Alert severity={snack.severity} onClose={() => setSnack(prev=>({...prev, open:false}))}>{snack.message}</Alert>
      </Snackbar>
    </Box>
  );
}
















Read alos backend File and Give me Fully Updated File carefully ..
So i can easily replace file ..


#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py
from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Query
from fastapi.responses import JSONResponse, FileResponse
import shutil, uuid, json
from settings import UPLOAD_DIR, OUTPUT_DIR
from pathlib import Path
import logging

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from ccure_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("ccure_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")
    # run compare (the function itself is defensive)
    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    # Ensure result is a dict for JSONResponse
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)


# NEW: averages endpoint (calls compute_visit_averages)
@app.get("/ccure/averages")
def ccure_averages(timeout: int = Query(6, description="timeout seconds for live-summary requests")):
    """
    Returns:
    {
      "live_today": { "employee": int, "contractor": int, "total": int },
      "ccure_active": { "active_employees": int|None, "active_contractors": int|None },
      "averages": { "employee_pct": float|None, "contractor_pct": float|None, "overall_pct": float|None },
      "sites_queried": int,
      "notes": null | str
    }
    """
    try:
        from ccure_compare_service import compute_visit_averages
    except Exception as e:
        logger.exception("compute_visit_averages import failed")
        raise HTTPException(status_code=500, detail=f"compute_visit_averages unavailable: {e}")

    try:
        res = compute_visit_averages(timeout=timeout)
    except Exception as e:
        logger.exception("compute_visit_averages execution failed")
        raise HTTPException(status_code=500, detail=f"compute_visit_averages failed: {e}")

    if not isinstance(res, dict):
        return JSONResponse({"error": "compute_visit_averages returned unexpected result"}, status_code=500)
    return JSONResponse(res)


@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = Path(OUTPUT_DIR) / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(
            str(full),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            filename=safe_name
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")


@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = Path(UPLOAD_DIR) / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_employee_excel
    except Exception as e:
        logger.exception("ingest_excel import failed")
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_employee_excel(dest)
    return {"status":"ok", "path": str(dest)}

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = Path(UPLOAD_DIR) / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_contractor_excel
    except Exception as e:
        logger.exception("ingest_excel import failed")
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_contractor_excel(dest)
    return {"status":"ok", "path": str(dest)}

# Keep other endpoints unchanged (ingest/fetch-all, reports/daily)...
# If you want, I can provide the rest verbatim — I left them unchanged to minimize merge issues.




# ccure_compare_service.py
"""
Compare CCURE profiles/stats with local sheets + compute visit averages + compliance.

Key behaviors:
 - If AttendanceSummary for today is empty, attempt to call compute_daily_attendance() to build it from LiveSwipe.
 - Provide headcount (AttendanceSummary) and live_headcount (region_clients) with per-location breakdowns.
 - ccure_active exposes only reported ActiveEmployees and ActiveContractors (no derived fields).
 - Computes averages (last 7 days) and today's percentages vs CCURE reported counts.
 - Compliance (meets_5days_8h, meets_3days_8h, defaulters) computed using AttendanceSummary historical data.
"""

import re
import traceback
from datetime import date, datetime, timedelta
from typing import List, Dict, Any, Optional, Set

import logging

logger = logging.getLogger("ccure_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, AttendanceSummary, LiveSwipe
from settings import OUTPUT_DIR

# ---------- small helpers ----------------------------------------------------

def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

def _sanitize_for_json(value):
    try:
        import numpy as _np
    except Exception:
        _np = None
    if value is None:
        return None
    if isinstance(value, (str, bool, int)):
        return value
    if isinstance(value, float):
        if _np is not None and not _np.isfinite(value):
            return None
        return float(value)
    if _np is not None and isinstance(value, (_np.integer,)):
        return int(value)
    if isinstance(value, dict):
        out = {}
        for k, v in value.items():
            try:
                key = str(k)
            except Exception:
                key = repr(k)
            out[key] = _sanitize_for_json(v)
        return out
    if isinstance(value, (list, tuple, set)):
        return [_sanitize_for_json(v) for v in value]
    try:
        return str(value)
    except Exception:
        return None

# ---------- ccure helpers ---------------------------------------------------

def _fetch_ccure_stats():
    try:
        import ccure_client
        if hasattr(ccure_client, "get_global_stats"):
            return ccure_client.get_global_stats()
    except Exception:
        logger.debug("ccure_client.get_global_stats not available", exc_info=True)
    return None

def _fetch_ccure_profiles():
    try:
        import ccure_client
        for fn in ("fetch_all_employees_full", "fetch_all_employees", "fetch_all_profiles", "fetch_profiles", "fetch_all"):
            if hasattr(ccure_client, fn):
                try:
                    res = getattr(ccure_client, fn)()
                    if isinstance(res, list):
                        return res
                except Exception:
                    continue
    except Exception:
        pass
    return []

def _extract_ccure_locations_from_profiles(profiles: List[dict]) -> Set[str]:
    locs = set()
    for p in profiles:
        if not isinstance(p, dict):
            continue
        for k in ("Partition", "PartitionName", "Location", "Location City", "location_city", "location", "Site", "BaseLocation"):
            v = p.get(k) if isinstance(p, dict) else None
            if v and isinstance(v, str) and v.strip():
                locs.add(v.strip())
    return locs

# ---------- classification & partition helpers ------------------------------

def classify_personnel_from_detail(detail: dict) -> str:
    """Map many CCURE / live-summary personnel strings to 'employee' or 'contractor'."""
    try:
        if not isinstance(detail, dict):
            return "contractor"
        candidate_keys = [
            "PersonnelType", "personnelType", "personnel_type", "Personnel Type",
            "PersonnelTypeName", "Personnel", "Type", "personnel", "PersonType", "personType"
        ]
        val = None
        for k in candidate_keys:
            if k in detail and detail.get(k) is not None:
                val = str(detail.get(k)).strip().lower()
                break
        status_keys = ["Employee_Status", "Employee Status", "Status", "Profile_Disabled"]
        status_val = None
        for k in status_keys:
            if k in detail and detail.get(k) is not None:
                status_val = str(detail.get(k)).strip().lower()
                break

        if status_val is not None and "terminated" in status_val:
            return "employee"
        if val is None or val == "":
            return "contractor"
        if "employee" in val:
            return "employee"
        if "terminated" in val:
            return "employee"
        contractor_terms = ["contractor", "visitor", "property", "property management", "temp", "temp badge", "tempbadge"]
        for t in contractor_terms:
            if t in val:
                return "contractor"
        if "contract" in val or "visitor" in val:
            return "contractor"
        return "contractor"
    except Exception:
        return "contractor"

def pick_partition_from_detail(detail: dict) -> str:
    if not isinstance(detail, dict):
        return "Unknown"
    for k in ("PartitionName2","PartitionName1","Partition","PartitionName","Region","Location","Site","location_city","Location City"):
        if k in detail and detail.get(k):
            try:
                return str(detail.get(k)).strip()
            except Exception:
                continue
    if "__region" in detail and detail.get("__region"):
        return str(detail.get("__region")).strip()
    return "Unknown"

# ---------- WFH detection helper -------------------------------------------

def is_employee_wfh(active_emp_row: ActiveEmployee) -> bool:
    try:
        wfh_keywords = ("work from home", "wfh", "remote", "workfromhome", "home")
        for attr in ("is_wfh", "work_from_home", "wfh", "remote_flag"):
            if hasattr(active_emp_row, attr):
                try:
                    val = getattr(active_emp_row, attr)
                    if isinstance(val, bool) and val:
                        return True
                    if isinstance(val, str) and any(k in val.strip().lower() for k in wfh_keywords):
                        return True
                except Exception:
                    pass
        for attr in ("location_description", "location_desc", "location_description1", "base_location", "location", "location_city"):
            if hasattr(active_emp_row, attr):
                try:
                    v = getattr(active_emp_row, attr)
                    if v and isinstance(v, str):
                        s = v.strip().lower()
                        if any(k in s for k in wfh_keywords):
                            return True
                except Exception:
                    pass
        try:
            rr = getattr(active_emp_row, "raw_row", None)
            if rr and isinstance(rr, dict):
                for k, v in rr.items():
                    try:
                        if v and isinstance(v, str) and any(word in v.strip().lower() for word in wfh_keywords):
                            return True
                    except Exception:
                        continue
        except Exception:
            pass
    except Exception:
        pass
    return False

# ---------- utility: fallback headcount builder from LiveSwipe --------------

def build_headcount_from_liveswipes_for_today(session) -> (int, Dict[str, Dict[str, int]]):
    """
    When AttendanceSummary for today is empty, build headcount by scanning LiveSwipe rows for today
    Deduplicate by key (employee_id or card) and compute per-location counts.
    Returns (total_count, by_location dict)
    """
    start = datetime.combine(date.today(), datetime.min.time())
    end = datetime.combine(date.today(), datetime.max.time())
    swipes = session.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
    if not swipes:
        return 0, {}
    seen_keys = {}
    per_loc = {}
    for s in swipes:
        key = _normalize_employee_key(s.employee_id) or _normalize_card_like(s.card_number)
        if not key:
            key = f"nokey_{s.id}"
        rec = seen_keys.get(key)
        ts = s.timestamp
        if rec is None:
            seen_keys[key] = {"first_seen": ts, "last_seen": ts, "partition": (s.partition or "Unknown"), "class": None, "card": s.card_number, "raw": s.raw}
        else:
            if ts and rec.get("first_seen") and ts < rec["first_seen"]:
                rec["first_seen"] = ts
            if ts and rec.get("last_seen") and ts > rec["last_seen"]:
                rec["last_seen"] = ts
    for k, v in seen_keys.items():
        loc = v.get("partition") or "Unknown"
        if not isinstance(loc, str) or not loc.strip():
            loc = "Unknown"
        if loc not in per_loc:
            per_loc[loc] = {"total": 0, "employee": 0, "contractor": 0}
        per_loc[loc]["total"] += 1
        classified = "contractor"
        raw = v.get("raw")
        if isinstance(raw, dict):
            try:
                classified = classify_personnel_from_detail(raw)
            except Exception:
                classified = "contractor"
        per_loc[loc][classified] += 1
    total = sum(p["total"] for p in per_loc.values())
    return int(total), per_loc

# ---------- main compute function -----------------------------------------




def compute_visit_averages(timeout: int = 6) -> Dict[str, Any]:
    notes = []
    today = date.today()
    week_start = today - timedelta(days=6)  # last 7 days inclusive

    # --- try to get CCURE stats/profiles early for filtering & denominators
    ccure_stats = _fetch_ccure_stats()
    reported_active_emps = _safe_int(ccure_stats.get("ActiveEmployees")) if isinstance(ccure_stats, dict) else None
    reported_active_contractors = _safe_int(ccure_stats.get("ActiveContractors")) if isinstance(ccure_stats, dict) else None

    ccure_profiles = _fetch_ccure_profiles()
    ccure_locations = _extract_ccure_locations_from_profiles(ccure_profiles) if isinstance(ccure_profiles, list) else set()

    # --- HEADCOUNT (AttendanceSummary for today) with fallback
    head_total = 0
    head_per_location: Dict[str, Dict[str, int]] = {}
    try:
        session = SessionLocal()
        att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
        if not att_rows_today:
            # Attempt to build AttendanceSummary from LiveSwipe using compute_daily_attendance (if available)
            built_ok = False
            try:
                # import local compare_service.compute_daily_attendance if available
                from compare_service import compute_daily_attendance as _compute_daily_attendance
                try:
                    built = _compute_daily_attendance(today)
                    # If compute_daily_attendance returns rows, requery AttendanceSummary
                    if isinstance(built, list) and len(built) > 0:
                        att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                        built_ok = True
                        notes.append("AttendanceSummary was missing; built from LiveSwipe via compute_daily_attendance().")
                except Exception:
                    # fall through to fallback builder
                    logger.exception("compute_daily_attendance execution failed; falling back")
            except Exception:
                # compare_service not importable -> fallback
                logger.debug("compare_service.compute_daily_attendance not importable; falling back", exc_info=True)

            if not att_rows_today:
                # fallback: build headcount from LiveSwipe directly (non-persistent)
                built_total, built_per_loc = build_headcount_from_liveswipes_for_today(session)
                head_total = built_total
                head_per_location = built_per_loc
                if head_total > 0:
                    notes.append("AttendanceSummary for today empty; built headcount from LiveSwipe rows (non-persistent fallback).")
        if att_rows_today:
            # classify using ActiveEmployee / ActiveContractor sets
            act_emps = session.query(ActiveEmployee).all()
            act_contrs = session.query(ActiveContractor).all()
            emp_id_set = set()
            contr_id_set = set()
            card_to_emp = {}
            for e in act_emps:
                v = _normalize_employee_key(getattr(e, "employee_id", None))
                if v:
                    emp_id_set.add(v)
                try:
                    rr = getattr(e, "raw_row", None)
                    if rr and isinstance(rr, dict):
                        for ck in ("CardNumber","card_number","Card","Card No","CardNo","Badge","BadgeNo","IPassID","iPass ID","IPASSID"):
                            if ck in rr and rr.get(ck):
                                cn = _normalize_card_like(rr.get(ck))
                                if cn:
                                    card_to_emp[cn] = v
                except Exception:
                    pass
            for c in act_contrs:
                wid = _normalize_employee_key(getattr(c, "worker_system_id", None))
                ip = _normalize_employee_key(getattr(c, "ipass_id", None))
                primary = wid or ip
                if primary:
                    contr_id_set.add(primary)

            for a in att_rows_today:
                key = _normalize_employee_key(a.employee_id)
                partition = None
                try:
                    if a.derived and isinstance(a.derived, dict):
                        partition = a.derived.get("partition")
                except Exception:
                    partition = None
                loc = partition or "Unknown"
                if not isinstance(loc, str) or not loc.strip():
                    loc = "Unknown"
                if loc not in head_per_location:
                    head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                if (a.presence_count or 0) > 0:
                    head_total += 1
                    head_per_location[loc]["total"] += 1
                    cls = "contractor"
                    if key and key in emp_id_set:
                        cls = "employee"
                    elif key and key in contr_id_set:
                        cls = "contractor"
                    else:
                        try:
                            card = (a.derived.get("card_number") if (a.derived and isinstance(a.derived, dict)) else None)
                        except Exception:
                            card = None
                        cnorm = _normalize_card_like(card)
                        if cnorm and cnorm in card_to_emp:
                            cls = "employee" if card_to_emp.get(cnorm) in emp_id_set else "contractor"
                        else:
                            cls = "contractor"
                    head_per_location[loc][cls] += 1
        session.expunge_all()
    except Exception:
        logger.exception("Error computing HeadCount")
        notes.append("Failed to compute HeadCount from DB; see server logs.")
    finally:
        try:
            session.close()
        except Exception:
            pass

    # --- LIVE HEADCOUNT via region_clients (as before)
    live_total = 0
    live_per_location: Dict[str, Dict[str, int]] = {}
    sites_queried = 0
    details = []  # ensure defined for later fallbacks
    try:
        import region_clients
        regions_info = []
        try:
            if hasattr(region_clients, "fetch_all_regions"):
                regions_info = region_clients.fetch_all_regions(timeout=timeout) or []
        except Exception:
            logger.exception("region_clients.fetch_all_regions failed")
        try:
            if hasattr(region_clients, "fetch_all_details"):
                details = region_clients.fetch_all_details(timeout=timeout) or []
        except Exception:
            logger.exception("region_clients.fetch_all_details failed")
        sites_queried = len(regions_info) if isinstance(regions_info, list) else 0
        if regions_info:
            for r in regions_info:
                try:
                    c = r.get("count") if isinstance(r, dict) else None
                    ci = _safe_int(c)
                    if ci is not None:
                        live_total += int(ci)
                except Exception:
                    continue
        derived_detail_sum = 0
        if details and isinstance(details, list):
            for d in details:
                try:
                    loc = pick_partition_from_detail(d) or "Unknown"
                    if not isinstance(loc, str) or not loc.strip():
                        loc = "Unknown"
                    pclass = classify_personnel_from_detail(d)
                    if loc not in live_per_location:
                        live_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                    live_per_location[loc]["total"] += 1
                    live_per_location[loc][pclass] += 1
                    derived_detail_sum += 1
                except Exception:
                    continue
            if live_total == 0 and derived_detail_sum > 0:
                live_total = derived_detail_sum
            else:
                if live_total != derived_detail_sum:
                    notes.append(f"Region totals ({live_total}) differ from detail rows ({derived_detail_sum}); using region totals for overall and details for breakdown.")
        else:
            notes.append("No per-person details available from region_clients; live breakdown unavailable.")
    except Exception:
        logger.exception("Error computing Live HeadCount")
        notes.append("Failed to compute Live HeadCount; see logs.")
        live_total = live_total or 0

    # ---------- NEW FALLBACK: if head_total still zero, build from region_clients details ----------
    if (head_total == 0) and details:
        try:
            seen_keys = set()
            for d in details:
                try:
                    # Prefer EmployeeID or CardNumber or PersonGUID as dedupe key
                    key = _normalize_employee_key(d.get("EmployeeID")) or _normalize_card_like(d.get("CardNumber")) or (d.get("PersonGUID") if d.get("PersonGUID") else None)
                    if not key:
                        # try other possible id-like fields
                        key = _normalize_employee_key(d.get("employee_id")) or _normalize_card_like(d.get("Card")) or None
                    if not key:
                        continue
                    key = str(key)
                    if key in seen_keys:
                        continue
                    seen_keys.add(key)
                    loc = pick_partition_from_detail(d) or "Unknown"
                    pclass = classify_personnel_from_detail(d)
                    if loc not in head_per_location:
                        head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                    head_per_location[loc]["total"] += 1
                    head_per_location[loc][pclass] += 1
                    head_total += 1
                except Exception:
                    continue
            if head_total > 0:
                notes.append("AttendanceSummary and LiveSwipe empty; built headcount from region_clients live-summary details (fallback).")
        except Exception:
            logger.exception("Error building headcount from region details fallback")

    # --- CCURE active: exposed only as reported (not derived)
    # reported_active_emps, reported_active_contractors already from ccure_stats above

    # --- Compliance: compute using AttendanceSummary last 7 days (DB)
    compliance = {
        "meets_5days_8h": {"count": 0, "percent_of_ccure_employees": None, "by_location": {}},
        "meets_3days_8h": {"count": 0, "percent_of_ccure_employees": None, "by_location": {}},
        "defaulters": {"count": 0, "percent_of_ccure_employees": None, "by_location": {}, "sample": []}
    }

    try:
        session = SessionLocal()
        active_emps = session.query(ActiveEmployee).all()
        emp_map = {}
        card_to_emp = {}
        for e in active_emps:
            eid = _normalize_employee_key(getattr(e, "employee_id", None))
            emp_map[eid] = e
            try:
                rr = getattr(e, "raw_row", None)
                if rr and isinstance(rr, dict):
                    for ck in ("CardNumber","card_number","Card","Card No","CardNo","Badge","BadgeNo","IPassID","iPass ID","IPASSID"):
                        if ck in rr and rr.get(ck):
                            cn = _normalize_card_like(rr.get(ck))
                            if cn:
                                card_to_emp[cn] = eid
            except Exception:
                pass

        att_rows_range = session.query(AttendanceSummary).filter(AttendanceSummary.date >= week_start, AttendanceSummary.date <= today).all()
        rows_by_key = {}
        for r in att_rows_range:
            key = _normalize_employee_key(r.employee_id)
            if key not in rows_by_key:
                rows_by_key[key] = []
            rows_by_key[key].append(r)

        meets_5 = []
        meets_3 = []
        defaulters_list = []
        for eid, e in emp_map.items():
            candidate_rows = []
            if eid and eid in rows_by_key:
                candidate_rows.extend(rows_by_key[eid])
            for k in list(rows_by_key.keys()):
                if not k:
                    continue
                k_norm = _normalize_card_like(k)
                if k_norm and k_norm in card_to_emp and card_to_emp[k_norm] == eid:
                    candidate_rows.extend(rows_by_key[k])
            by_date = {}
            for r in candidate_rows:
                try:
                    d = r.date
                    if d not in by_date:
                        by_date[d] = r
                    else:
                        if (r.presence_count or 0) > (by_date[d].presence_count or 0):
                            by_date[d] = r
                except Exception:
                    continue
            days_with_8h = 0
            for d, row in by_date.items():
                if (row.presence_count or 0) > 0:
                    try:
                        if row.first_seen and row.last_seen:
                            dur = (row.last_seen - row.first_seen).total_seconds() / 3600.0
                            if dur >= 8.0:
                                days_with_8h += 1
                    except Exception:
                        pass
            meets5 = (days_with_8h >= 5)
            meets3 = (days_with_8h >= 3)
            wfh_flag = is_employee_wfh(e)
            location = None
            for loc_attr in ("location_city", "location", "base_location", "location_desc", "location_description"):
                if hasattr(e, loc_attr):
                    v = getattr(e, loc_attr)
                    if v and isinstance(v, str) and v.strip():
                        location = v.strip()
                        break
            if not location:
                try:
                    rr = getattr(e, "raw_row", None)
                    if rr and isinstance(rr, dict):
                        for ck in ("Partition","PartitionName","Location","Site","location_city","Location City"):
                            if ck in rr and rr.get(ck):
                                location = str(rr.get(ck)).strip()
                                break
                except Exception:
                    pass
            if not location:
                location = "Unknown"

            if meets5:
                meets_5.append((eid, e, location))
            if meets3:
                meets_3.append((eid, e, location))
            if (not meets5) and (not meets3):
                if not wfh_flag:
                    defaulters_list.append((eid, e, location))

        def _build_location_counts(list_of_tuples):
            loc_map = {}
            for (_id, e_obj, loc) in list_of_tuples:
                if not loc:
                    loc = "Unknown"
                if ccure_locations:
                    if loc not in ccure_locations:
                        continue
                if loc not in loc_map:
                    loc_map[loc] = {"count": 0}
                loc_map[loc]["count"] += 1
            return loc_map

        meets_5_count = len(meets_5)
        meets_3_count = len(meets_3)
        defaulter_count = len(defaulters_list)

        compliance["meets_5days_8h"]["count"] = int(meets_5_count)
        compliance["meets_5days_8h"]["by_location"] = {k: {"count": int(v["count"])} for k, v in _build_location_counts(meets_5).items()}
        compliance["meets_3days_8h"]["count"] = int(meets_3_count)
        compliance["meets_3days_8h"]["by_location"] = {k: {"count": int(v["count"])} for k, v in _build_location_counts(meets_3).items()}
        compliance["defaulters"]["count"] = int(defaulter_count)
        compliance["defaulters"]["by_location"] = {k: {"count": int(v["count"])} for k, v in _build_location_counts(defaulters_list).items()}

        denom_emp = reported_active_emps if reported_active_emps is not None else None
        if isinstance(denom_emp, int) and denom_emp > 0:
            compliance["meets_5days_8h"]["percent_of_ccure_employees"] = round((meets_5_count / denom_emp) * 100.0, 2)
            compliance["meets_3days_8h"]["percent_of_ccure_employees"] = round((meets_3_count / denom_emp) * 100.0, 2)
            compliance["defaulters"]["percent_of_ccure_employees"] = round((defaulter_count / denom_emp) * 100.0, 2)
        else:
            compliance["meets_5days_8h"]["percent_of_ccure_employees"] = None
            compliance["meets_3days_8h"]["percent_of_ccure_employees"] = None
            compliance["defaulters"]["percent_of_ccure_employees"] = None

        sample = []
        for (eid, e_obj, loc) in defaulters_list[:50]:
            try:
                sample.append({
                    "employee_id": _sanitize_for_json(eid),
                    "full_name": _sanitize_for_json(getattr(e_obj, "full_name", None)),
                    "location": _sanitize_for_json(loc),
                    "wfh_flag": bool(is_employee_wfh(e_obj))
                })
            except Exception:
                continue
        compliance["defaulters"]["sample"] = sample

        session.expunge_all()
        session.close()
    except Exception:
        logger.exception("Error computing compliance section")
        notes.append("Failed to compute compliance metrics; check server logs for trace.")
    finally:
        try:
            session.close()
        except Exception:
            pass

    # --- Averages: compute last 7 days headcount averages from AttendanceSummary (DB)
    avg_headcount_last_7_days = None
    avg_headcount_per_site_last_7_days = None
    # Per-location DB aggregates (new)
    avg_by_location_last_7_days: Dict[str, Dict[str, Any]] = {}

    try:
        session = SessionLocal()

        # Build ActiveEmployee/ActiveContractor maps for classification during per-day scans
        act_emps = session.query(ActiveEmployee).all()
        act_contrs = session.query(ActiveContractor).all()
        emp_id_set = set()
        contr_id_set = set()
        card_to_emp = {}
        for e in act_emps:
            eid = _normalize_employee_key(getattr(e, "employee_id", None))
            if eid:
                emp_id_set.add(eid)
            try:
                rr = getattr(e, "raw_row", None) or {}
                if isinstance(rr, dict):
                    for ck in ("CardNumber","card_number","Card","Card No","CardNo","IPassID","iPass ID","IPASSID","Badge","BadgeNo"):
                        if ck in rr and rr.get(ck):
                            cn = _normalize_card_like(rr.get(ck))
                            if cn:
                                card_to_emp[cn] = eid
            except Exception:
                pass
        for c in act_contrs:
            wid = _normalize_employee_key(getattr(c, "worker_system_id", None))
            ip = _normalize_employee_key(getattr(c, "ipass_id", None))
            primary = wid or ip
            if primary:
                contr_id_set.add(primary)
            try:
                rr = getattr(c, "raw_row", None) or {}
                if isinstance(rr, dict):
                    for ck in ("Worker System Id","Worker System ID","iPass ID","IPASSID","CardNumber","card_number"):
                        if ck in rr and rr.get(ck):
                            cn = _normalize_card_like(rr.get(ck))
                            if cn:
                                # map to contractor primary
                                card_to_emp[cn] = primary
            except Exception:
                pass

        # Prepare per-location day lists
        loc_day_vals: Dict[str, Dict[str, List[int]]] = {}
        days = []
        for i in range(0, 7):
            d = today - timedelta(days=i)
            days.append(d)
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            # compute per-location counts for that day
            per_loc_counts: Dict[str, Dict[str, int]] = {}
            if rows:
                for r in rows:
                    try:
                        if (r.presence_count or 0) <= 0:
                            continue
                        partition = None
                        try:
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                        except Exception:
                            partition = None
                        loc = partition or "Unknown"
                        if not isinstance(loc, str) or not loc.strip():
                            loc = "Unknown"
                        if loc not in per_loc_counts:
                            per_loc_counts[loc] = {"employee": 0, "contractor": 0, "total": 0}
                        # classify row
                        key = _normalize_employee_key(r.employee_id)
                        cls = "contractor"
                        if key and key in emp_id_set:
                            cls = "employee"
                        elif key and key in contr_id_set:
                            cls = "contractor"
                        else:
                            # try derived card number
                            try:
                                card = (r.derived.get("card_number") if (r.derived and isinstance(r.derived, dict)) else None)
                            except Exception:
                                card = None
                            cnorm = _normalize_card_like(card)
                            if cnorm and cnorm in card_to_emp and card_to_emp.get(cnorm) in emp_id_set:
                                cls = "employee"
                            elif cnorm and cnorm in card_to_emp and card_to_emp.get(cnorm) in contr_id_set:
                                cls = "contractor"
                            else:
                                # fallback: try to classify by looking at presence of explicit PersonnelType in derived/raw (rare for AttendanceSummary)
                                cls = "contractor"
                        per_loc_counts[loc][cls] += 1
                        per_loc_counts[loc]["total"] += 1
                    except Exception:
                        continue
            # for each location seen on that day, append day's counts
            for loc, counts in per_loc_counts.items():
                if loc not in loc_day_vals:
                    loc_day_vals[loc] = {"employee": [], "contractor": [], "total": []}
                loc_day_vals[loc]["employee"].append(counts.get("employee", 0))
                loc_day_vals[loc]["contractor"].append(counts.get("contractor", 0))
                loc_day_vals[loc]["total"].append(counts.get("total", 0))

        # compute per-location averages
        for loc, lists in loc_day_vals.items():
            emp_list = lists.get("employee", [])
            con_list = lists.get("contractor", [])
            tot_list = lists.get("total", [])
            days_counted = len(tot_list)
            avg_emp = round(sum(emp_list) / float(days_counted), 2) if days_counted and sum(emp_list) is not None else 0.0
            avg_con = round(sum(con_list) / float(days_counted), 2) if days_counted and sum(con_list) is not None else 0.0
            avg_tot = round(sum(tot_list) / float(days_counted), 2) if days_counted and sum(tot_list) is not None else 0.0
            avg_by_location_last_7_days[loc] = {
                "history_days_counted": int(days_counted),
                "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
            }

        # compute DB overall avg_headcount_last_7_days (previous behavior)
        days_totals = []
        for d in days:
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            day_total = 0
            if rows:
                for r in rows:
                    if (r.presence_count or 0) > 0:
                        day_total += 1
            days_totals.append(day_total)
        if days_totals:
            avg_headcount_last_7_days = round(sum(days_totals) / float(len(days_totals)), 2)
            if sites_queried and sites_queried > 0:
                avg_headcount_per_site_last_7_days = round((sum(days_totals) / float(len(days_totals))) / float(sites_queried), 2)

        session.close()
    except Exception:
        logger.exception("Error computing averages from AttendanceSummary")
        notes.append("Failed to compute historical averages from AttendanceSummary; partial results only.")

    # --- HISTORY AVERAGES: use region_clients history endpoints (new)
    history_emp_avg = None
    history_contractor_avg = None
    history_overall_avg = None
    history_days = 0
    # per-partition history averages (new)
    history_avg_by_location_last_7_days: Dict[str, Dict[str, Any]] = {}

    try:
        import region_clients
        if hasattr(region_clients, "fetch_all_history"):
            entries = region_clients.fetch_all_history(timeout=timeout) or []
            # aggregate by date across regions
            agg_by_date = {}  # date_str -> {"employee": int, "contractor": int, "total": int}
            # Also aggregate partitions per date
            agg_partitions_by_date: Dict[str, Dict[str, Dict[str, int]]] = {}  # date -> partition -> {employee, contractor, total}
            for e in entries:
                try:
                    dstr = e.get("date")
                    if not dstr:
                        continue
                    region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                    emp = None
                    con = None
                    tot = None
                    if region_obj and isinstance(region_obj, dict):
                        emp = _safe_int(region_obj.get("Employee"))
                        con = _safe_int(region_obj.get("Contractor"))
                        tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                    else:
                        emp = _safe_int(e.get("Employee") or (e.get("region") and e.get("region").get("Employee") if isinstance(e.get("region"), dict) else None))
                        con = _safe_int(e.get("Contractor") or (e.get("region") and e.get("region").get("Contractor") if isinstance(e.get("region"), dict) else None))
                        tot = _safe_int(e.get("total") or ((emp or 0) + (con or 0)))
                    if emp is None and con is None and tot is None:
                        try:
                            robj = e.get("region") or {}
                            if isinstance(robj, dict):
                                emp = _safe_int(robj.get("Employee"))
                                con = _safe_int(robj.get("Contractor"))
                                tot = _safe_int(robj.get("total"))
                        except Exception:
                            pass
                    if emp is None and con is None:
                        continue
                    if tot is None:
                        tot = (emp or 0) + (con or 0)
                    if dstr not in agg_by_date:
                        agg_by_date[dstr] = {"employee": 0, "contractor": 0, "total": 0, "counted_regions": 0}
                    agg_by_date[dstr]["employee"] += (emp or 0)
                    agg_by_date[dstr]["contractor"] += (con or 0)
                    agg_by_date[dstr]["total"] += (tot or 0)
                    agg_by_date[dstr]["counted_regions"] += 1

                    # partitions
                    parts = e.get("partitions") if isinstance(e.get("partitions"), dict) else {}
                    if dstr not in agg_partitions_by_date:
                        agg_partitions_by_date[dstr] = {}
                    for pname, pstat in parts.items():
                        try:
                            p_emp = _safe_int(pstat.get("Employee"))
                            p_con = _safe_int(pstat.get("Contractor"))
                            p_tot = _safe_int(pstat.get("total")) or ((p_emp or 0) + (p_con or 0))
                            if pname not in agg_partitions_by_date[dstr]:
                                agg_partitions_by_date[dstr][pname] = {"employee": 0, "contractor": 0, "total": 0}
                            agg_partitions_by_date[dstr][pname]["employee"] += (p_emp or 0)
                            agg_partitions_by_date[dstr][pname]["contractor"] += (p_con or 0)
                            agg_partitions_by_date[dstr][pname]["total"] += (p_tot or 0)
                        except Exception:
                            continue
                except Exception:
                    continue

            # Now compute per-date lists for last 7 days
            day_vals_emp = []
            day_vals_con = []
            day_vals_tot = []
            for i in range(0, 7):
                d_iso = (today - timedelta(days=i)).isoformat()
                entry = agg_by_date.get(d_iso)
                if entry:
                    day_vals_emp.append(entry.get("employee", 0))
                    day_vals_con.append(entry.get("contractor", 0))
                    day_vals_tot.append(entry.get("total", 0))
            if day_vals_emp:
                history_emp_avg = round(sum(day_vals_emp) / float(len(day_vals_emp)), 2)
            if day_vals_con:
                history_contractor_avg = round(sum(day_vals_con) / float(len(day_vals_con)), 2)
            if day_vals_tot:
                history_overall_avg = round(sum(day_vals_tot) / float(len(day_vals_tot)), 2)
            history_days = len(day_vals_tot)
            if history_days == 0:
                notes.append("History endpoints returned no usable last-7-day rows; history averages not available.")

            # Compute per-partition averages across last 7 days
            # Build partition -> lists
            partition_day_values: Dict[str, Dict[str, List[int]]] = {}
            for i in range(0, 7):
                d_iso = (today - timedelta(days=i)).isoformat()
                per_parts = agg_partitions_by_date.get(d_iso, {})
                for pname, pvals in per_parts.items():
                    if pname not in partition_day_values:
                        partition_day_values[pname] = {"employee": [], "contractor": [], "total": []}
                    partition_day_values[pname]["employee"].append(pvals.get("employee", 0))
                    partition_day_values[pname]["contractor"].append(pvals.get("contractor", 0))
                    partition_day_values[pname]["total"].append(pvals.get("total", 0))
            # finalize per-partition averages
            for pname, lists in partition_day_values.items():
                emp_list = lists.get("employee", [])
                con_list = lists.get("contractor", [])
                tot_list = lists.get("total", [])
                days_counted = len(tot_list)
                if days_counted == 0:
                    continue
                avg_emp = round(sum(emp_list) / float(days_counted), 2)
                avg_con = round(sum(con_list) / float(days_counted), 2)
                avg_tot = round(sum(tot_list) / float(days_counted), 2)
                history_avg_by_location_last_7_days[pname] = {
                    "history_days_counted": int(days_counted),
                    "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                    "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                    "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
                }

    except Exception:
        logger.exception("Error fetching/processing history endpoints")
        notes.append("Failed to compute history averages from region history endpoints; partial results.")

    # ---------- NEW: if DB-based 7-day avg empty, fallback to history_overall_avg ----------
    if (not avg_headcount_last_7_days or avg_headcount_last_7_days == 0) and history_overall_avg:
        try:
            avg_headcount_last_7_days = history_overall_avg
            avg_headcount_per_site_last_7_days = round(history_overall_avg / float(sites_queried), 2) if sites_queried and sites_queried > 0 else None
            notes.append("avg_headcount_last_7_days derived from region history endpoints due to missing AttendanceSummary historical data.")
        except Exception:
            pass

    # --- compute percentages (head/live vs CCURE reported)
    def safe_pct(n, denom):
        try:
            if n is None or denom is None:
                return None
            d = float(denom)
            if d == 0.0:
                return None
            return round((float(n) / d) * 100.0, 2)
        except Exception:
            return None

    cc_emp_denom = reported_active_emps
    cc_con_denom = reported_active_contractors
    cc_total_denom = None
    if isinstance(cc_emp_denom, int) and isinstance(cc_con_denom, int):
        cc_total_denom = cc_emp_denom + cc_con_denom

    head_emp_total = sum(v.get("employee", 0) for v in head_per_location.values())
    head_con_total = sum(v.get("contractor", 0) for v in head_per_location.values())
    live_emp_total = sum(v.get("employee", 0) for v in live_per_location.values())
    live_con_total = sum(v.get("contractor", 0) for v in live_per_location.values())

    # percent of CCURE employees/contractors present today (headcount basis)
    head_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_emp_total, cc_emp_denom))
    head_con_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_con_total, cc_con_denom))
    head_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_total, cc_total_denom))

    # Provide historical key name expected elsewhere (fix for NameError)
    head_contractor_pct_vs_ccure_today = head_con_pct_vs_ccure_today

    live_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_emp_total, cc_emp_denom))
    live_con_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_con_total, cc_con_denom))
    live_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_total, cc_total_denom))

    # history percentages vs CCURE (if denominators exist)
    history_emp_pct_vs_ccure = _sanitize_for_json(safe_pct(history_emp_avg, cc_emp_denom))
    history_con_pct_vs_ccure = _sanitize_for_json(safe_pct(history_contractor_avg, cc_con_denom))
    history_overall_pct_vs_ccure = _sanitize_for_json(safe_pct(history_overall_avg, cc_total_denom))

    result = {
        "date": today.isoformat(),
        "headcount": {
            "total_visited_today": int(head_total),
            "employee": int(head_emp_total),
            "contractor": int(head_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in head_per_location.items() }
        },
        "live_headcount": {
            "currently_present_total": int(live_total),
            "employee": int(live_emp_total),
            "contractor": int(live_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in live_per_location.items() }
        },
        "ccure_active": {
            "ccure_active_employees_reported": _safe_int(reported_active_emps),
            "ccure_active_contractors_reported": _safe_int(reported_active_contractors)
        },
        "averages": {
            # existing AttendanceSummary averages
            "head_emp_pct_vs_ccure_today": head_emp_pct_vs_ccure_today,
            "head_contractor_pct_vs_ccure_today": head_contractor_pct_vs_ccure_today,
            "headcount_overall_pct_vs_ccure_today": head_overall_pct_vs_ccure_today,
            "live_employee_pct_vs_ccure": live_emp_pct_vs_ccure_today,
            "live_contractor_pct_vs_ccure": live_con_pct_vs_ccure_today if False else live_con_pct_vs_ccure_today if 'live_con_pct_vs_ccure_today' in locals() else _sanitize_for_json(safe_pct(live_con_total, cc_con_denom)),
            "live_overall_pct_vs_ccure": live_overall_pct_vs_ccure_today,
            "avg_headcount_last_7_days": _sanitize_for_json(avg_headcount_last_7_days),
            "avg_headcount_per_site_last_7_days": _sanitize_for_json(avg_headcount_per_site_last_7_days),
            "avg_live_per_site": _sanitize_for_json(round(live_total / sites_queried, 2) if sites_queried and sites_queried > 0 else None),

            # NEW: history endpoint averages (region-provided)
            "history_avg_employee_last_7_days": _sanitize_for_json(history_emp_avg),
            "history_avg_contractor_last_7_days": _sanitize_for_json(history_contractor_avg),
            "history_avg_overall_last_7_days": _sanitize_for_json(history_overall_avg),
            "history_days_counted": int(history_days) if history_days is not None else None,
            "history_employee_pct_vs_ccure": history_emp_pct_vs_ccure,
            "history_contractor_pct_vs_ccure": history_con_pct_vs_ccure,
            "history_overall_pct_vs_ccure": history_overall_pct_vs_ccure,

            # NEW per-location aggregates:
            "avg_by_location_last_7_days": _sanitize_for_json(avg_by_location_last_7_days),
            "history_avg_by_location_last_7_days": _sanitize_for_json(history_avg_by_location_last_7_days)
        },
        "compliance": _sanitize_for_json(compliance),
        "sites_queried": int(sites_queried),
        "notes": " | ".join(notes) if notes else None
    }

    # sanitize and return
    return _sanitize_for_json(result)





