now only think in console 

INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097186628 Email=Vinit.Kumar@wu.com for candidate=314009
INFO:werkzeug:127.0.0.1 - - [20/Nov/2025 15:24:53] "GET /record?employee_id=314009 HTTP/1.1" 200 -
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:root:Connected to ACVSCore on SRVWUPNQ0986V using SQL auth (region apac).
INFO:root:get_personnel_info: found ObjectID=2097207981 Email=OmSatyam.Raut@westernunion.com for candidate=327447
INFO:werkzeug:127.0.0.1 - - [20/Nov/2025 15:25:05] "GET /record?employee_id=327447 HTTP/1.1" 200 -



we got data But in Frontend we dont get Image 

Display No image

Email
—

Email section is blank Where is Mismatch i think we need to check trend Eunner.py file 
alos once check EMployeeImage and Trendrunner.py file and fix the issue ...




# backend/employeeimage.py

from __future__ import annotations
import logging
import os
import base64
from pathlib import Path
from typing import Optional, Dict, Any

# optional imports (handled gracefully)
try:
    import pyodbc
except Exception:
    pyodbc = None

# optional region configuration (if available)
try:
    from duration_report import REGION_CONFIG
except Exception:
    REGION_CONFIG = {}

ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")
DEFAULT_OUTDIR = Path(__file__).resolve().parent / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)

# uuid-like regex (optional)
try:
    import re

    _uuid_like_re = re.compile(
        r'^[0-9A-Fa-f]{8}\-[0-9A-Fa-f]{4}\-[0-9A-Fa-f]{4}\-[0-9A-Fa-f]{4}\-[0-9A-Fa-f]{12}$'
    )
except Exception:
    _uuid_like_re = None


def _looks_like_guid(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        if _uuid_like_re:
            if _uuid_like_re.match(st):
                return True
        # support legacy prefixes used in some pipelines
        lower = st.lower()
        if lower.startswith("emp:") or lower.startswith("uid:") or lower.startswith("name:"):
            return True
        return False
    except Exception:
        return False


def _strip_person_uid_prefix(token: object) -> Optional[str]:
    if token is None:
        return None
    try:
        s = str(token).strip()
        if not s:
            return None
        if ":" in s:
            prefix, rest = s.split(":", 1)
            if prefix.lower() in ("emp", "uid", "name"):
                rest = rest.strip()
                if rest:
                    return rest
        return s
    except Exception:
        return None


# ACVSCore connection backoff state (module-level)
_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20


def _get_acvscore_conn(timeout: int = 5):
    """
    Try to connect to ACVSCore using REGION_CONFIG. Returns pyodbc.Connection or None.
    Respects a short backoff to avoid repeated noisy attempts.
    """
    try:
        import time
        now = time.time()
        last = _acvscore_backoff.get("ts")
        if last and _acvscore_backoff.get("failed") and (now - last) < _ACVSCORE_BACKOFF_SECONDS:
            logging.debug("ACVSCore backoff active.")
            return None
    except Exception:
        # if something goes wrong in time/backoff handling, continue attempting connection
        pass

    if pyodbc is None:
        logging.debug("pyodbc not installed - ACVSCore lookup unavailable.")
        return None

    tried = []
    for rkey, rc in (REGION_CONFIG or {}).items():
        server = rc.get("server")
        user = rc.get("user")
        pwd = rc.get("password")
        if not server:
            continue

        # 1) SQL auth (if credentials present)
        if user and pwd:
            tried.append(f"{rkey}@{server}(sql)")
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={server};DATABASE=ACVSCore;UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;"
            )
            try:
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=timeout)
                logging.info("Connected to ACVSCore on %s using SQL auth (region %s).", server, rkey)
                _acvscore_backoff["failed"] = False
                _acvscore_backoff["ts"] = None
                return conn
            except Exception:
                logging.debug("SQL auth to %s failed", server)

        # 2) Trusted connection fallback
        tried.append(f"{rkey}@{server}(trusted)")
        conn_str_trusted = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={server};DATABASE=ACVSCore;Trusted_Connection=yes;"
            "TrustServerCertificate=Yes;"
        )
        try:
            conn = pyodbc.connect(conn_str_trusted, autocommit=True, timeout=timeout)
            logging.info("Connected to ACVSCore on %s using trusted connection (region %s).", server, rkey)
            _acvscore_backoff["failed"] = False
            _acvscore_backoff["ts"] = None
            return conn
        except Exception:
            logging.debug("Trusted connection to %s failed", server)
            continue

    # mark failure & set backoff timestamp
    try:
        import time
        _acvscore_backoff["ts"] = time.time()
        _acvscore_backoff["failed"] = True
    except Exception:
        pass

    logging.error("Failed ACVSCore connection attempts: %s", tried)
    return None


def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    """
    Lookup personnel in ACVSCore.Access.Personnel. Returns dict or {} on failure/unavailable.
    Keys commonly returned: ObjectID, GUID, Name, EmailAddress, EmployeeEmail, ManagerEmail
    """
    out: Dict[str, Any] = {}
    if candidate_identifier is None:
        return out

    conn = _get_acvscore_conn()
    if conn is None:
        logging.debug("get_personnel_info: ACVSCore unavailable.")
        return out

    cur = None
    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        cand_guid = cand if _looks_like_guid(cand) else None
        params = (cand, cand_guid, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()

        if row:
            try:
                out["ObjectID"] = row[0] if len(row) > 0 else None
                out["GUID"] = row[1] if len(row) > 1 else None
                out["Name"] = row[2] if len(row) > 2 else None

                # try a few column positions/names for email; map to consistent keys
                email_val = None
                # row[3] used before — keep it if present
                if len(row) > 3:
                    email_val = row[3]
                # if email is still falsy try alternative columns if present (some DBs have different columns)
                if not email_val and len(row) > 4:
                    email_val = row[4]  # maybe EMail or WorkEmail in some schemas

                # normalise empty -> None
                if email_val is None or (isinstance(email_val, str) and email_val.strip() == ""):
                    email_val = None

                # set multiple aliases so callers can find an email under various names
                out["EmailAddress"] = email_val or None
                out["EmployeeEmail"] = email_val or None
                out["Email"] = email_val or None
                out["WorkEmail"] = email_val or None

                # ManagerEmail if available (row[4] often used)
                out["ManagerEmail"] = row[4] if len(row) > 4 else None

                logging.info(
                    "get_personnel_info: found ObjectID=%s Email=%s for candidate=%s",
                    out.get("ObjectID"),
                    out.get("Email"),
                    candidate_identifier,
                )
            except Exception:
                logging.exception("get_personnel_info: failed parsing DB row")
    except Exception:
        logging.exception("get_personnel_info: DB query failed for %s", candidate_identifier)
    finally:
        try:
            if cur is not None:
                cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out


def get_person_image_bytes(parent_id) -> Optional[bytes]:
    """
    Retrieve an employee image by parent_id:
      1) DB (ACVSCore.Access.Images) - if available
      2) Filesystem under outputs/images, outputs, repo/images, repo root, cwd
      3) Base64 text files (.b64/.txt)
    Returns bytes or None.

    NOTE: This function will attempt to resolve the incoming identifier to Personnel.ObjectID/GUID
    via get_personnel_info() and try those values as ParentId in Images as a fallback.
    """
    if parent_id is None:
        return None

    # Attempt DB lookup first (best-effort)
    try:
        conn = _get_acvscore_conn()
        if conn is not None:
            cur = None
            try:
                cur = conn.cursor()
                sql = """
                    SELECT TOP 1 AI.Image
                    FROM ACVSCore.Access.Images AI
                    WHERE AI.ParentId = ?
                      AND DATALENGTH(AI.Image) > 0
                    ORDER BY AI.ObjectID DESC
                """
                raw_pid = str(parent_id).strip()
                cand_parent_ids = [raw_pid]
                stripped = _strip_person_uid_prefix(raw_pid)
                if stripped and stripped != raw_pid:
                    cand_parent_ids.append(stripped)
                # integer/float fallback forms
                try:
                    if stripped and "." in stripped:
                        f = float(stripped)
                        if f.is_integer():
                            cand_parent_ids.append(str(int(f)))
                except Exception:
                    pass
                try:
                    if stripped and stripped.isdigit():
                        cand_parent_ids.append(f"emp_{stripped}")
                except Exception:
                    pass

                # --- NEW: attempt to resolve candidate -> Personnel.ObjectID / GUID and append ---
                try:
                    pinfo = get_personnel_info(raw_pid) or {}
                    if pinfo:
                        obj = pinfo.get("ObjectID")
                        guid = pinfo.get("GUID")
                        # add objectid/guid as strings (preserve order: prefer ObjectID first)
                        if obj is not None:
                            obj_s = str(obj).strip()
                            if obj_s:
                                cand_parent_ids.append(obj_s)
                        if guid:
                            guid_s = str(guid).strip()
                            if guid_s:
                                cand_parent_ids.append(guid_s)
                except Exception:
                    logging.debug("get_person_image_bytes: personnel resolution failed for %s", raw_pid)

                # dedupe preserving order
                seen = set()
                cand_parent_ids = [x for x in cand_parent_ids if x and (not (x in seen or seen.add(x)))]

                for pid_try in cand_parent_ids:
                    try:
                        cur.execute(sql, (str(pid_try),))
                        row = cur.fetchone()
                        if row and row[0] is not None:
                            try:
                                b = bytes(row[0])
                                logging.info(
                                    "get_person_image_bytes: DB image found for ParentId=%s (len=%d)",
                                    pid_try,
                                    len(b),
                                )
                                return b
                            except Exception:
                                logging.exception(
                                    "get_person_image_bytes: convert DB image to bytes failed for %s", pid_try
                                )
                                return row[0]
                    except Exception:
                        logging.debug("get_person_image_bytes: DB fetch failed for ParentId=%s", pid_try)
                        continue
            except Exception:
                logging.exception("get_person_image_bytes: DB query failed")
            finally:
                try:
                    if cur is not None:
                        cur.close()
                except Exception:
                    pass
                try:
                    conn.close()
                except Exception:
                    pass
    except Exception:
        logging.exception("get_person_image_bytes: unexpected DB path error")

    # Filesystem + base64 fallback
    try:
        pid_raw = str(parent_id).strip()
        cand_ids = [pid_raw]
        try:
            f = float(pid_raw)
            if f.is_integer():
                int_form = str(int(f))
                if int_form not in cand_ids:
                    cand_ids.append(int_form)
        except Exception:
            pass
        try:
            if pid_raw.isdigit():
                cand_ids.append(f"emp_{pid_raw}")
                if len(pid_raw) < 8:
                    cand_ids.append(pid_raw.zfill(8))
        except Exception:
            pass

        # unique preserve order
        cand_ids = list(dict.fromkeys(cand_ids))

        candidate_dirs = [
            (Path(DEFAULT_OUTDIR) / "images"),
            Path(DEFAULT_OUTDIR),
            Path(__file__).resolve().parent / "images",
            Path(__file__).resolve().parent,
            Path.cwd(),
        ]

        exts = (".jpg", ".jpeg", ".png", ".bmp", ".gif", ".webp")
        b64_exts = (".b64", ".txt")

        for c in cand_ids:
            for folder in candidate_dirs:
                try:
                    if not folder.exists():
                        continue
                except Exception:
                    continue

                # exact filename match with normal image extensions
                for ext in exts:
                    fp = folder / f"{c}{ext}"
                    try:
                        if fp.exists() and fp.is_file():
                            try:
                                return fp.read_bytes()
                            except Exception:
                                logging.exception("get_person_image_bytes: failed reading %s", fp)
                                continue
                    except Exception:
                        continue

                # base64 text file containers
                for ext in b64_exts:
                    fp = folder / f"{c}{ext}"
                    try:
                        if fp.exists() and fp.is_file():
                            try:
                                txt = fp.read_text(encoding="utf-8", errors="ignore").strip()
                                if txt:
                                    if "," in txt and txt.startswith("data:"):
                                        try:
                                            b64 = txt.split(",", 1)[1]
                                            return base64.b64decode(b64)
                                        except Exception:
                                            pass
                                    try:
                                        return base64.b64decode(txt)
                                    except Exception:
                                        logging.exception(
                                            "get_person_image_bytes: base64 decode failed for %s", fp
                                        )
                                        continue
                            except Exception:
                                logging.exception("get_person_image_bytes: failed reading base64 file %s", fp)
                                continue
                    except Exception:
                        continue

                # glob fallback: any file containing token c
                try:
                    for fp in folder.glob(f"*{c}*"):
                        if fp.is_file():
                            try:
                                b = fp.read_bytes()
                                if b:
                                    logging.info("get_person_image_bytes: loaded image via glob %s", fp)
                                    return b
                            except Exception:
                                logging.exception("get_person_image_bytes: failed reading glob file %s", fp)
                                continue
                except Exception:
                    logging.exception(
                        "get_person_image_bytes: glob check failed in %s for token %s", folder, c
                    )
                    continue
    except Exception:
        logging.exception("get_person_image_bytes: filesystem search failed for ParentId=%s", parent_id)

    logging.debug("get_person_image_bytes: no image found for ParentId=%s", parent_id)
    return None


# exported symbols
__all__ = ["get_person_image_bytes", "get_personnel_info", "_get_acvscore_conn"]






\



# backend/trend_runner.py
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re
import os
import calendar
import json
from collections import defaultdict
from datetime import datetime as _datetime
from datetime import timedelta as _timedelta
from typing import Optional, List



# ------------------ personnel enrichment (lazy) ------------------
def _get_personnel_funcs_lazy():
    """
    Try to import personnel helpers from app.py at call time (avoids circular imports).
    Returns tuple (get_personnel_info_fn_or_None, get_person_image_bytes_fn_or_None).
    """
    try:
        import importlib
        appmod = importlib.import_module('app')  # adjust if your app module name differs
        gpi = getattr(appmod, 'get_personnel_info', None)
        gpib = getattr(appmod, 'get_person_image_bytes', None)
        return gpi, gpib
    except Exception:
        return None, None

def _normalize_for_lookup(val):
    if val is None:
        return None
    s = str(val).strip()
    if not s:
        return None
    if len(s) > 36 and '-' in s:
        return s
    return s

def _enrich_with_personnel_info(df, image_endpoint_template="/employee/{}/image"):
    if df is None or df.empty:
        return df
    get_personnel_info, get_person_image_bytes = _get_personnel_funcs_lazy()
    emails = []
    image_urls = []
    for _, row in df.iterrows():
        email = None
        image_url = None
        cand_empid = None
        for tok in ('EmployeeID', 'EmployeeID_feat', 'EmployeeID_dur', 'Int1', 'Text12'):
            if tok in row and row.get(tok) not in (None, '', float('nan')):
                cand_empid = _normalize_for_lookup(row.get(tok))
                break
        cand_uid = row.get('EmployeeIdentity') or row.get('person_uid') or None
        if get_personnel_info:
            try:
                lookup = cand_empid or cand_uid
                pi = get_personnel_info(lookup) if lookup else None
                if pi and isinstance(pi, dict):
                    email = pi.get('EmployeeEmail') or pi.get('EmailAddress') or None
                    parent = pi.get('ObjectID') or pi.get('GUID') or cand_empid or cand_uid
                    if parent:
                        image_url = image_endpoint_template.format(str(parent))
            except Exception:
                pass
        if email is None:
            for fld in ('EmployeeEmail', 'Email', 'EmailAddress', 'ManagerEmail'):
                if fld in row and row.get(fld) not in (None, '', float('nan')):
                    email = row.get(fld)
                    break
        if image_url is None:
            if cand_empid:
                image_url = image_endpoint_template.format(cand_empid)
            elif cand_uid:
                image_url = image_endpoint_template.format(cand_uid)
            else:
                image_url = None
        emails.append(email)
        image_urls.append(image_url)
    out = df.copy()
    out['EmployeeEmail'] = emails
    out['imageUrl'] = image_urls
    return out

# ---------------------------------------------------------------------------

# ------------------ duration_report imports (robust) ------------------
try:
    from duration_report import run_for_date, compute_daily_durations, REGION_CONFIG
except Exception:
    try:
        from duration_report import run_for_date, compute_daily_durations
        REGION_CONFIG = {}
    except Exception:
        try:
            from duration_report import run_for_date
            compute_daily_durations = None
            REGION_CONFIG = {}
        except Exception:
            run_for_date = None
            compute_daily_durations = None
            REGION_CONFIG = {}

# ------------------ optional config door_zone mapping ------------------
try:
    from config.door_zone import map_door_to_zone as config_map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    config_map_door_to_zone = None
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

# History profile (optional)
CANDIDATE_HISTORY = [
    Path(__file__).parent / "config" / "current_analysis.csv",
    Path(__file__).parent.parent / "config" / "current_analysis.csv",
    Path.cwd() / "current_analysis.csv",
    Path(__file__).parent / "current_analysis.csv"
]
HIST_PATH = None
for p in CANDIDATE_HISTORY:
    if p.exists():
        HIST_PATH = p
        break

if HIST_PATH:
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception:
        logging.warning("Failed to load historical profile from %s", HIST_PATH)
        HIST_DF = pd.DataFrame()
else:
    HIST_DF = pd.DataFrame()

# Outdir
OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)

# Small helpers
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')


# ---------------------------------------------------------------------------
# small time formatting helper used for raw_swipes_all
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return "-"
        s = int(seconds)
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return "-"


def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _normalize_id_val(v):
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s

def _looks_like_guid(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        if _looks_like_guid(st):
            return False
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None

# Short card xml extractor
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None

# door -> zone mapping fallback
try:
    _BREAK_ZONES = BREAK_ZONES
    _OUT_OF_OFFICE_ZONE = OUT_OF_OFFICE_ZONE
except Exception:
    _BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    _OUT_OF_OFFICE_ZONE = "Out of office"

def map_door_to_zone(door: object, direction: object = None) -> str:
    try:
        if config_map_door_to_zone is not None:
            return config_map_door_to_zone(door, direction)
    except Exception:
        pass
    try:
        if door is None:
            return None
        s = str(door).strip()
        if not s:
            return None
        s_l = s.lower()
        if direction and isinstance(direction, str):
            d = direction.strip().lower()
            if "out" in d:
                return _OUT_OF_OFFICE_ZONE
            if "in" in d:
                return "Reception Area"
        if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
            return _OUT_OF_OFFICE_ZONE
        return "Working Area"
    except Exception:
        return None

# ----- Config and scenarios -----
VIOLATION_WINDOW_DAYS = 90
RISK_THRESHOLDS = [
    (0.5, "Low"),
    (1.5, "Low Medium"),
    (2.5, "Medium"),
    (4.0, "Medium High"),
    (float("inf"), "High"),
]

def map_score_to_label(score: float) -> (int, str):
    try:
        if score is None:
            score = 0.0
        s = float(score)
    except Exception:
        s = 0.0
    bucket = 1
    label = "Low"
    for i, (threshold, lbl) in enumerate(RISK_THRESHOLDS, start=1):
        if s <= threshold:
            bucket = i
            label = lbl
            break
    return bucket, label

# scenario functions (kept from your improved version)
def scenario_long_gap(row):
    try:
        gap = int(row.get('MaxSwipeGapSeconds') or 0)
        return gap >= int(4.5 * 3600)
    except Exception:
        return False

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

# def scenario_zero_swipes(row):
#     return int(row.get('CountSwipes', 0)) == 0


def scenario_zero_swipes(row):
    """
    Return True if CountSwipes is zero (or effectively zero/empty).
    Be defensive: handle None, NaN, numeric strings, floats etc.
    """
    try:
        v = row.get('CountSwipes', 0)
        # handle pandas NaN
        try:
            import pandas as _pd
            if _pd.isna(v):
                v = 0
        except Exception:
            pass
        # convert strings/numeric-like to float -> int
        if v is None:
            return True  # treat missing as zero-swipes for scenario detection
        try:
            # float handles "0.0", "0", "0.00" etc
            num = float(v)
            return int(num) == 0
        except Exception:
            # non-numeric values - be conservative: treat as not zero (avoid false positives)
            return False
    except Exception:
        return False


def scenario_unusually_high_swipes(row):
    cur = int(row.get('CountSwipes') or 0)
    dur = float(row.get('DurationMinutes') or 0.0)
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = rec.get('TotalSwipes_median', np.nan)
            if pd.notna(median) and median > 0:
                return (cur > 3 * float(median)) and (dur < 60)
    except Exception:
        pass
    try:
        if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
            global_med = HIST_DF['TotalSwipes_median'].median()
            if pd.notna(global_med) and global_med > 0:
                return (cur > 3 * float(global_med)) and (dur < 60)
    except Exception:
        pass
    return (cur > 50) and (dur < 60)

def scenario_high_swipes_benign(row):
    cur = int(row.get('CountSwipes') or 0)
    dur = float(row.get('DurationMinutes') or 0.0)
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = rec.get('TotalSwipes_median', np.nan)
            if pd.notna(median) and median > 0:
                return (cur > 3 * float(median)) and (dur >= 60)
    except Exception:
        pass
    try:
        if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
            global_med = HIST_DF['TotalSwipes_median'].median()
            if pd.notna(global_med) and global_med > 0:
                return (cur > 3 * float(global_med)) and (dur >= 60)
    except Exception:
        pass
    return (cur > 50) and (dur >= 60)

def scenario_behaviour_shift(row, hist_df=None, minutes_threshold=180):
    try:
        if pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None:
            return False
        first_ts = pd.to_datetime(row.get('FirstSwipe'))
        today_minutes = first_ts.hour * 60 + first_ts.minute
        empid = row.get('EmployeeID')
        hist = hist_df if hist_df is not None else (HIST_DF if (HIST_DF is not None and not HIST_DF.empty) else None)
        if hist is None or hist.empty or empid is None:
            return False
        try:
            rec = hist[hist['EmployeeID'] == empid]
            if rec.empty:
                return False
            if 'FirstSwipeMinutes_median' in rec.columns:
                median_min = rec.iloc[0].get('FirstSwipeMinutes_median')
            else:
                median_min = rec.iloc[0].get('AvgFirstSwipeMins_median', None)
            if pd.isna(median_min) or median_min is None:
                return False
            diff = abs(today_minutes - float(median_min))
            return diff >= int(minutes_threshold)
        except Exception:
            return False
    except Exception:
        return False

def scenario_repeated_short_breaks(row):
    try:
        break_count = int(row.get('BreakCount') or 0)
        total_break_mins = float(row.get('TotalBreakMinutes') or 0.0)
        long_break_count = int(row.get('LongBreakCount') or 0)
        short_gap_count = int(row.get('ShortGapCount') or 0)
        if break_count >= 2:
            return True
        if short_gap_count >= 5:
            return True
        if total_break_mins >= 180 and short_gap_count >= 2:
            return True
        return False
    except Exception:
        return False

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map

def scenario_shortstay_longout_repeat(row):
    return bool(row.get('PatternShortLongRepeat', False))

SCENARIOS = [
    ("long_gap_>=4.5h", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap),
    ("high_swipes_benign", scenario_high_swipes_benign),
    ("behaviour_shift", scenario_behaviour_shift),
    ("shortstay_longout_repeat", scenario_shortstay_longout_repeat)
]

# --- improved human-readable scenario explanations (use hours for duration/gaps) ---
def _hrs_from_minutes(mins):
    try:
        m = float(mins or 0.0)
        return round(m / 60.0, 1)
    except Exception:
        return None

def _hrs_from_seconds(sec):
    try:
        s = float(sec or 0.0)
        return round(s / 3600.0, 1)
    except Exception:
        return None

SCENARIO_EXPLANATIONS = {
    "long_gap_>=4.5h": lambda r: (
        (lambda h: f"Long gap between swipes (~{h} h)." if h is not None else "Long gap between swipes.")
        (_hrs_from_seconds(r.get('MaxSwipeGapSeconds')))
    ),
    "short_duration_<4h": lambda r: (
        # if duration is zero but we only saw only_in/only_out, be explicit
        "Only 'IN' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyIn', 0)) == 1 else
        "Only 'OUT' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyOut', 0)) == 1 else
        (lambda h: f"Short total presence (~{h} h)." if h is not None else "Short total presence.")(_hrs_from_minutes(r.get('DurationMinutes')))
    ),
    "coffee_badging": lambda r: "Multiple quick swipes in short time.",
    "low_swipe_count_<=2": lambda r: "Very few swipes on day.",
    "single_door": lambda r: "Only a single door used during the day.",
    "only_in": lambda r: "Only 'IN' events recorded.",
    "only_out": lambda r: "Only 'OUT' events recorded.",
    "overtime_>=10h": lambda r: "Overtime detected (>=10 hours).",
    "very_long_duration_>=16h": lambda r: "Very long presence (>=16 hours).",
    "zero_swipes": lambda r: "No swipes recorded on this day.",
    "unusually_high_swipes": lambda r: "Unusually high number of swipes compared to peers/history.",
    "repeated_short_breaks": lambda r: "Many short gaps between swipes.",
    "multiple_location_same_day": lambda r: "Multiple locations/partitions used in same day.",
    "weekend_activity": lambda r: "Activity recorded on weekend day.",
    "repeated_rejection_count": lambda r: "Multiple rejection events recorded.",
    "badge_sharing_suspected": lambda r: "Same card used by multiple users on same day — possible badge sharing.",
    "early_arrival_before_06": lambda r: "First swipe earlier than 06:00.",
    "late_exit_after_22": lambda r: "Last swipe after 22:00.",
    "shift_inconsistency": lambda r: "Duration deviates from historical shift patterns.",
    "trending_decline": lambda r: "Employee shows trending decline in presence.",
    "consecutive_absent_days": lambda r: "Consecutive absent days observed historically.",
    "high_variance_duration": lambda r: "High variance in daily durations historically.",
    "short_duration_on_high_presence_days": lambda r: "Short duration despite normally high presence days.",
    "swipe_overlap": lambda r: "Overlap in swipe times with other persons on same door.",
    "behaviour_shift": lambda r: "Significant change in arrival time compared to historical baseline.",
    "shortstay_longout_repeat": lambda r: "Repeated pattern: short in → long out → short return."
}


def _explain_scenarios_detected(row, detected_list):
    pieces = []
    # derive a human display label consisting of Name and EmployeeID where possible
    try:
        empid = None
        # prefer explicit EmployeeID (non-GUID) from various tokens
        for tok in ('EmployeeID', 'EmployeeID_feat', 'EmployeeID_dur', 'Int1', 'Text12'):
            if tok in row and row.get(tok) not in (None, '', 'nan'):
                val = _normalize_id_val(row.get(tok))
                if val and not _looks_like_guid(val):
                    empid = str(val)
                    break
        # if empid still None, allow non-GUID EmployeeIdentity
        if not empid and row.get('EmployeeIdentity') not in (None, '', 'nan'):
            tmp = _normalize_id_val(row.get('EmployeeIdentity'))
            if tmp and not _looks_like_guid(tmp):
                empid = str(tmp)

        name = None
        try:
            nm = row.get('EmployeeName')
            if nm and _looks_like_name(nm) and not _is_placeholder_str(nm):
                name = str(nm).strip()
            else:
                # fallback: pick first non-guid textual name from common tokens
                for cand in ('EmployeeName', 'EmployeeName_feat', 'EmployeeName_dur', 'ObjectName1'):
                    if cand in row and row.get(cand) not in (None, '', 'nan'):
                        v = row.get(cand)
                        if v and not _looks_like_guid(v) and _looks_like_name(v):
                            name = str(v).strip()
                            break
                if not name:
                    # try to strip person_uid prefixes if present
                    pu = row.get('person_uid')
                    if isinstance(pu, str) and (pu.startswith('emp:') or pu.startswith('uid:') or pu.startswith('name:')):
                        try:
                            stripped = _strip_uid_prefix(pu)
                            if stripped and not _looks_like_guid(stripped):
                                name = str(stripped)
                        except Exception:
                            pass
        except Exception:
            name = None

        # build prefix
        prefix = ""
        if name and empid:
            prefix = f"{name} ({empid}) - "
        elif name:
            prefix = f"{name} - "
        elif empid:
            prefix = f"{empid} - "
        else:
            prefix = ""
    except Exception:
        prefix = ""

    for sc in detected_list:
        sc = sc.strip()
        fn = SCENARIO_EXPLANATIONS.get(sc)
        try:
            if fn:
                pieces.append(fn(row))
            else:
                pieces.append(sc.replace("_", " ").replace(">=", "≥"))
        except Exception:
            pieces.append(sc)
    if not pieces:
        return None
    explanation = " ".join([p if p.endswith('.') else p + '.' for p in pieces])

    # Replace any GUID that accidentally remained inside explanation with the chosen human identifier (without duplicating parentheses)
    try:
        GUID_IN_TEXT_RE = re.compile(r'[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}')
        if prefix and isinstance(explanation, str) and GUID_IN_TEXT_RE.search(explanation):
            # replace GUIDs inside with the prefix label (strip trailing ' - ' from prefix)
            label = prefix.rstrip(' - ')
            explanation = GUID_IN_TEXT_RE.sub(str(label), explanation)
    except Exception:
        pass

    return prefix + explanation


# ---------------- compute_features (robust merged version) ----------------
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:

    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)

    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
    else:
        if 'Date' in sw.columns:
            sw['LocaleMessageTime'] = None
            try:
                sw['LocaleMessageTime'] = pd.to_datetime(sw['Date'], errors='coerce')
            except Exception:
                sw['LocaleMessageTime'] = None

    # By default Date comes from LocaleMessageTime (local, human timestamps).
    # However if an AdjustedMessageTime column exists (the Pune 2AM boundary) prefer that
    # for date assignment so trend grouping matches compute_daily_durations().
    if 'AdjustedMessageTime' in sw.columns and sw['AdjustedMessageTime'].notna().any():
        try:
            sw['AdjustedMessageTime'] = pd.to_datetime(sw['AdjustedMessageTime'], errors='coerce')
            # Prefer adjusted date for rows where it exists (this mirrors duration_report logic).
            mask_adj = sw['AdjustedMessageTime'].notna()
            # Ensure LocaleMessageTime parsed for those not adjusted
            sw.loc[~mask_adj, 'Date'] = pd.to_datetime(sw.loc[~mask_adj, 'LocaleMessageTime'], errors='coerce').dt.date
            sw.loc[mask_adj, 'Date']  = sw.loc[mask_adj,  'AdjustedMessageTime'].dt.date
        except Exception:
            # fallback to LocaleMessageTime date
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
    else:
        # normal path
        sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date

    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    try:
        if dir_col and dir_col in sw.columns:
            sw['Direction'] = sw[dir_col]
        if door_col and door_col in sw.columns:
            sw['Door'] = sw[door_col]
        if empid_col and empid_col in sw.columns:
            sw['EmployeeID'] = sw[empid_col]
        if name_col and name_col in sw.columns:
            sw['EmployeeName'] = sw[name_col]
        if card_col and card_col in sw.columns:
            sw['CardNumber'] = sw[card_col]
        if 'LocaleMessageTime' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
        elif 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
    except Exception:
        logging.exception("Normalization of swipe columns failed.")

    # PersonnelType filtering (tolerant) - avoid dropping if column absent
    if 'PersonnelTypeName' in sw.columns:
        sw['PersonnelTypeName'] = sw['PersonnelTypeName'].astype(str).str.strip()
        mask = sw['PersonnelTypeName'].str.lower().str.contains(r'employee|terminated', na=False)
        logging.info("PersonnelTypeName values example: %s", list(sw['PersonnelTypeName'].dropna().unique()[:6]))
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelTypeName filter applied: before=%d after=%d", before, len(sw))
    elif 'PersonnelType' in sw.columns:
        sw['PersonnelType'] = sw['PersonnelType'].astype(str).str.strip()
        mask = sw['PersonnelType'].str.lower().str.contains(r'employee|terminated', na=False)
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelType filter applied: before=%d after=%d", before, len(sw))

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # person_uid canonical
    if 'person_uid' not in sw.columns:
        def make_person_uid_local(r):
            empid_val = None
            if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
                empid_val = r.get(empid_col)
            elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
                empid_val = r.get('EmployeeID')
            empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
            name_val = None
            if name_col and name_col in r:
                name_val = r.get(name_col)
            elif 'EmployeeName' in r:
                name_val = r.get('EmployeeName')
            elif 'ObjectName1' in r:
                name_val = r.get('ObjectName1')
            return _canonical_person_uid({
                'EmployeeID': empid_val,
                'EmployeeIdentity': empident_val,
                'EmployeeName': name_val
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
                    'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
                    'EmployeeIdentity'])
    if name_col: sel_cols.add(name_col)
    if empid_col: sel_cols.add(empid_col)
    if card_col: sel_cols.add(card_col)
    if door_col: sel_cols.add(door_col)
    if dir_col: sel_cols.add(dir_col)
    sel_cols = [c for c in sel_cols if c in sw.columns]

    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0

        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

        # card extraction
        card_numbers = []
        if card_col and card_col in g.columns:
            card_numbers = list(pd.unique(g[card_col].dropna()))
        if not card_numbers and 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna()))
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
                    try:
                        vals = list(pd.unique(g[c].dropna()))
                        if vals:
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'xml' in cl:
                    for raw in g[c].dropna().astype(str):
                        extracted = _extract_card_from_xml(raw)
                        if extracted:
                            card_numbers.append(extracted)
        card_numbers = list(dict.fromkeys(card_numbers))
        card_number = None
        for c in card_numbers:
            n = _normalize_id_val(c)
            if n and not _looks_like_guid(n):
                card_number = n
                break

        # stable id/name
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None

        if empid_col and empid_col in g.columns:
            vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        elif 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized

        if (not employee_id) and 'PersonnelType' in g.columns:
            try:
                pvals = g['PersonnelType'].dropna().astype(str)
                if not pvals.empty:
                    p0 = pvals.iloc[0]
                    if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
                        for c in g.columns:
                            if c.lower() == 'text12':
                                vals = g[c].dropna().astype(str).map(lambda x: x.strip())
                                employee_id = _pick_first_non_guid_value(vals)
                                if employee_id:
                                    break
            except Exception:
                pass

        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]

        candidate_name_vals = None
        if name_col and name_col in g.columns:
            candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
        elif 'EmployeeName' in g.columns:
            candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
        elif 'ObjectName1' in g.columns:
            candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

        if candidate_name_vals is not None and not candidate_name_vals.empty:
            employee_name = _pick_first_non_guid_value(candidate_name_vals)
            if employee_name is None:
                for v in candidate_name_vals:
                    if _looks_like_name(v) and not _is_placeholder_str(v):
                        employee_name = str(v).strip()
                        break

        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        first_swipe = None
        last_swipe = None
        if times:
            first_swipe = times[0]
            last_swipe = times[-1]

        # timeline & segments with mapping to zones
        timeline = []
        for _, row in g.sort_values('LocaleMessageTime').iterrows():
            t = row.get('LocaleMessageTime')
            dname = None
            if door_col and door_col in row and pd.notna(row.get(door_col)):
                dname = row.get(door_col)
            elif 'Door' in row and pd.notna(row.get('Door')):
                dname = row.get('Door')
            direction = None
            if dir_col and dir_col in row and pd.notna(row.get(dir_col)):
                direction = row.get(dir_col)
            elif 'Direction' in row and pd.notna(row.get('Direction')):
                direction = row.get('Direction')
            zone = map_door_to_zone(dname, direction)
            timeline.append((t, dname, direction, zone))

        segments = []
        if timeline:
            cur_zone = None
            seg_start = timeline[0][0]
            seg_label = None
            for (t, dname, direction, zone) in timeline:
                if zone in _BREAK_ZONES:
                    lbl = 'break'
                elif zone == _OUT_OF_OFFICE_ZONE:
                    lbl = 'out_of_office'
                else:
                    lbl = 'work'
                if cur_zone is None:
                    cur_zone = zone
                    seg_label = lbl
                    seg_start = t
                else:
                    if lbl != seg_label:
                        segments.append({
                            'label': seg_label,
                            'start': seg_start,
                            'end': t,
                            'start_zone': cur_zone
                        })
                        seg_start = t
                        seg_label = lbl
                        cur_zone = zone
                    else:
                        cur_zone = cur_zone or zone
            if seg_label is not None:
                segments.append({
                    'label': seg_label,
                    'start': seg_start,
                    'end': timeline[-1][0],
                    'start_zone': cur_zone
                })

        break_count = 0
        long_break_count = 0
        total_break_minutes = 0.0

        BREAK_MINUTES_THRESHOLD = 60
        OUT_OFFICE_COUNT_MINUTES = 180
        LONG_BREAK_FLAG_MINUTES = 120

        for i, s in enumerate(segments):
            lbl = s.get('label')
            start = s.get('start')
            end = s.get('end')
            dur_mins = ((end - start).total_seconds() / 60.0) if (start and end) else 0.0
            if lbl == 'break':
                if dur_mins >= BREAK_MINUTES_THRESHOLD:
                    break_count += 1
                    total_break_minutes += dur_mins
                    if dur_mins >= LONG_BREAK_FLAG_MINUTES:
                        long_break_count += 1
            elif lbl == 'out_of_office':
                prev_lbl = segments[i-1]['label'] if i > 0 else None
                next_lbl = segments[i+1]['label'] if i < len(segments)-1 else None
                if prev_lbl == 'work' and next_lbl == 'work' and dur_mins >= OUT_OFFICE_COUNT_MINUTES:
                    break_count += 1
                    total_break_minutes += dur_mins
                    if dur_mins >= LONG_BREAK_FLAG_MINUTES:
                        long_break_count += 1

        pattern_flag = False
        pattern_sequence_readable = None
        try:
            seq = []
            for s in segments:
                dur_mins = (s['end'] - s['start']).total_seconds() / 60.0 if (s['end'] and s['start']) else 0
                seq.append((s['label'], int(round(dur_mins))))
            for i in range(len(seq)-2):
                a = seq[i]
                b = seq[i+1]
                c = seq[i+2]
                if (a[0] == 'work' and a[1] < 60) and \
                   (b[0] in ('out_of_office','break') and b[1] >= LONG_BREAK_FLAG_MINUTES) and \
                   (c[0] == 'work' and c[1] < 60):
                    pattern_flag = True
                    seq_fragment = [a, b, c]
                    pattern_sequence_readable = " -> ".join([f"{lbl} ({mins}m)" for lbl, mins in seq_fragment])
                    break
        except Exception:
            pattern_flag = False
            pattern_sequence_readable = None

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type,
            'FirstSwipe': first_swipe,
            'LastSwipe': last_swipe,
            'BreakCount': int(break_count),
            'LongBreakCount': int(long_break_count),
            'TotalBreakMinutes': float(round(total_break_minutes,1)),
            'PatternShortLongRepeat': bool(pattern_flag),
            'PatternSequenceReadable': pattern_sequence_readable,
            'PatternSequence': None
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])[sel_cols]
    grouped = grouped.apply(agg_swipe_group).reset_index()

    # POST-PROCESS: merge early-morning fragments into previous day (heuristic)
    try:
        grouped['FirstSwipe_dt'] = pd.to_datetime(grouped['FirstSwipe'], errors='coerce')
        grouped['LastSwipe_dt']  = pd.to_datetime(grouped['LastSwipe'],  errors='coerce')
        rows_to_drop = set()
        MERGE_GAP_SECONDS = int(4 * 3600)
        for pid, sub in grouped.sort_values(['person_uid','Date']).groupby('person_uid'):
            prev_idx = None
            for idx, r in sub.reset_index().iterrows():
                real_idx = int(r['index']) if 'index' in r else r.name
                cur_first = pd.to_datetime(grouped.at[real_idx, 'FirstSwipe_dt'])
                if prev_idx is not None:
                    prev_last = pd.to_datetime(grouped.at[prev_idx, 'LastSwipe_dt'])
                    if (not pd.isna(cur_first)) and (not pd.isna(prev_last)):
                        gap = (cur_first - prev_last).total_seconds()
                        if 0 <= gap <= MERGE_GAP_SECONDS and cur_first.time().hour < 2:
                            try:
                                grouped.at[prev_idx, 'CountSwipes'] = int(grouped.at[prev_idx, 'CountSwipes']) + int(grouped.at[real_idx, 'CountSwipes'])
                                grouped.at[prev_idx, 'MaxSwipeGapSeconds'] = max(int(grouped.at[prev_idx, 'MaxSwipeGapSeconds'] or 0), int(grouped.at[real_idx, 'MaxSwipeGapSeconds'] or 0), int(gap))
                                if not pd.isna(grouped.at[real_idx, 'LastSwipe_dt']):
                                    if pd.isna(grouped.at[prev_idx, 'LastSwipe_dt']) or grouped.at[real_idx, 'LastSwipe_dt'] > grouped.at[prev_idx, 'LastSwipe_dt']:
                                        grouped.at[prev_idx, 'LastSwipe_dt'] = grouped.at[real_idx, 'LastSwipe_dt']
                                        grouped.at[prev_idx, 'LastSwipe'] = grouped.at[real_idx, 'LastSwipe']
                                if not grouped.at[prev_idx, 'CardNumber']:
                                    grouped.at[prev_idx, 'CardNumber'] = grouped.at[real_idx, 'CardNumber']
                                grouped.at[prev_idx, 'UniqueDoors'] = int(max(int(grouped.at[prev_idx].get('UniqueDoors') or 0), int(grouped.at[real_idx].get('UniqueDoors') or 0)))
                                grouped.at[prev_idx, 'UniqueLocations'] = int(max(int(grouped.at[prev_idx].get('UniqueLocations') or 0), int(grouped.at[real_idx].get('UniqueLocations') or 0)))
                                rows_to_drop.add(real_idx)
                                continue
                            except Exception:
                                pass
                prev_idx = real_idx
        if rows_to_drop:
            grouped = grouped.drop(index=list(rows_to_drop)).reset_index(drop=True)
    except Exception:
        logging.exception("Failed merge-early-morning fragments (non-fatal).")

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # coalesce duplicated columns (_x/_y) produced by merge
    def _coalesce_merge_columns(df, bases):
        for base in bases:
            x = base + "_x"
            y = base + "_y"
            try:
                has_base = base in df.columns
                base_all_null = False
                if has_base:
                    base_all_null = df[base].isnull().all()
            except Exception:
                has_base = base in df.columns
                base_all_null = True
            if (not has_base) or base_all_null:
                if x in df.columns and y in df.columns:
                    try:
                        df[base] = df[x].combine_first(df[y])
                    except Exception:
                        try:
                            df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
                        except Exception:
                            if x in df.columns:
                                df[base] = df[x]
                            elif y in df.columns:
                                df[base] = df[y]
                elif x in df.columns:
                    df[base] = df[x]
                elif y in df.columns:
                    df[base] = df[y]
    _coalesce_merge_columns(merged, [
        "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
    ])
    drop_cols = [c for c in merged.columns if c.endswith("_x") or c.endswith("_y")]
    if drop_cols:
        try:
            merged.drop(columns=drop_cols, inplace=True)
        except Exception:
            for c in drop_cols:
                if c in merged.columns:
                    try:
                        merged.drop(columns=[c], inplace=True)
                    except Exception:
                        pass

    # ensure columns exist and normalized
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)
    ensure_col(merged, 'BreakCount', 0)
    ensure_col(merged, 'LongBreakCount', 0)
    ensure_col(merged, 'TotalBreakMinutes', 0.0)
    ensure_col(merged, 'PatternShortLongRepeat', False)
    ensure_col(merged, 'PatternSequenceReadable', None)
    ensure_col(merged, 'PatternSequence', None)

    if 'EmployeeName' in merged.columns:
        def choose_best_name(row):
            gname = row.get('EmployeeName')
            dname = None
            for cand in ('EmployeeName', 'employee_name', 'objectname1', 'ObjectName1'):
                if cand in row and row.get(cand) is not None:
                    dname = row.get(cand)
                    break
            if _looks_like_name(gname):
                return str(gname).strip()
            if _looks_like_name(dname):
                return str(dname).strip()
            if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
                return str(gname).strip()
            if dname and not _is_placeholder_str(dname):
                return str(dname).strip()
            return None
        merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
    else:
        if not dur.empty:
            def fill_name_from_dur(row):
                gname = row.get('EmployeeName')
                if _looks_like_name(gname) and not _is_placeholder_str(gname):
                    return gname
                for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
                    if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
                        return row[cand]
                return None
            merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

    def normalize_empid(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            try:
                if '.' in s:
                    f = float(s)
                    if math.isfinite(f) and f.is_integer():
                        return str(int(f))
            except Exception:
                pass
            return s
        except Exception:
            return None

    merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

    def normalize_card(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            return s
        except Exception:
            return None

    merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

    if 'DurationSeconds' not in merged.columns or merged['DurationSeconds'].isnull().all():
        try:
            merged['DurationSeconds'] = (pd.to_datetime(merged['LastSwipe']) - pd.to_datetime(merged['FirstSwipe'])).dt.total_seconds().clip(lower=0).fillna(0)
        except Exception:
            merged['DurationSeconds'] = merged.get('DurationSeconds', 0)

    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)
    merged['BreakCount'] = merged['BreakCount'].fillna(0).astype(int)
    merged['LongBreakCount'] = merged['LongBreakCount'].fillna(0).astype(int)
    merged['TotalBreakMinutes'] = merged['TotalBreakMinutes'].fillna(0.0).astype(float)
    merged['PatternShortLongRepeat'] = merged['PatternShortLongRepeat'].fillna(False).astype(bool)

    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
        if c in merged.columns:
            def _clean_str_val(v):
                if v is None:
                    return None
                try:
                    s = str(v).strip()
                    if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                        return None
                    return s
                except Exception:
                    return None
            merged[c] = merged[c].apply(_clean_str_val)

    if 'EmployeeName' in merged.columns:
        merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

    return merged

# ---------------- SCENARIO WEIGHTS ----------------
WEIGHTS = {
    "long_gap_>=4.5h": 0.3,
    "short_duration_<4h": 1.0,
    "coffee_badging": 1.0,
    "low_swipe_count_<=2": 0.5,
    "single_door": 0.25,
    "only_in": 0.8,
    "only_out": 0.8,
    "overtime_>=10h": 0.2,
    "very_long_duration_>=16h": 1.5,
    "zero_swipes": 0.4,
    "unusually_high_swipes": 1.5,
    "repeated_short_breaks": 0.5,
    "multiple_location_same_day": 0.6,
    "weekend_activity": 0.6,
    "repeated_rejection_count": 0.8,
    "badge_sharing_suspected": 2.0,
    "early_arrival_before_06": 0.4,
    "late_exit_after_22": 0.4,
    "shift_inconsistency": 1.2,
    "trending_decline": 0.7,
    "consecutive_absent_days": 1.2,
    "high_variance_duration": 0.8,
    "short_duration_on_high_presence_days": 1.1,
    "swipe_overlap": 2.0,
    "high_swipes_benign": 0.1,
    "shortstay_longout_repeat": 2.0
}
ANOMALY_THRESHOLD = 1.5








# def _read_past_trend_csvs(outdir: str, window_days: int, target_date: date):
#     p = Path(outdir)
#     csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
#     if not csvs:
#         return pd.DataFrame()
#     dfs = []
#     cutoff = target_date - timedelta(days=window_days)
#     for fp in csvs:
#         try:
#             df = pd.read_csv(fp, parse_dates=['Date'])
#             if 'Date' in df.columns:
#                 try:
#                     df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
#                 except Exception:
#                     pass
#                 # include target_date in the window (cutoff <= Date <= target_date)
#                 def _date_in_window(d):
#                     try:
#                         return d is not None and (d >= cutoff and d <= target_date)
#                     except Exception:
#                         return False
#                 df = df[df['Date'].apply(_date_in_window)]
#             dfs.append(df)
#         except Exception:
#             try:
#                 df = pd.read_csv(fp, dtype=str)
#                 if 'Date' in df.columns:
#                     try:
#                         df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
#                         # include target_date in the window (cutoff <= Date <= target_date)
#                         def _date_in_window(d):
#                             try:
#                                 return d is not None and (d >= cutoff and d <= target_date)
#                             except Exception:
#                                 return False
#                         df = df[df['Date'].apply(_date_in_window)]
#                     except Exception:
#                         pass
#                 dfs.append(df)
#             except Exception:
#                 continue
#     if not dfs:
#         return pd.DataFrame()
#     try:
#         out = pd.concat(dfs, ignore_index=True)
#         return out
#     except Exception:
#         return pd.DataFrame()


def _read_past_trend_csvs(outdir: str, window_days: int, target_date: date):
    p = Path(outdir)
    # Previously this was hard-coded to "trend_pune_*.csv" which misses other city files.
    # Use a permissive pattern to capture trend_<city>_YYYYMMDD.csv for all cities.
    csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return pd.DataFrame()
    dfs = []
    cutoff = target_date - timedelta(days=window_days)
    for fp in csvs:
        try:
            df = pd.read_csv(fp, parse_dates=['Date'])
            if 'Date' in df.columns:
                try:
                    df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                except Exception:
                    pass
                # include target_date in the window (cutoff <= Date <= target_date)
                def _date_in_window(d):
                    try:
                        return d is not None and (d >= cutoff and d <= target_date)
                    except Exception:
                        return False
                df = df[df['Date'].apply(_date_in_window)]
            dfs.append(df)
        except Exception:
            try:
                df = pd.read_csv(fp, dtype=str)
                if 'Date' in df.columns:
                    try:
                        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                        def _date_in_window(d):
                            try:
                                return d is not None and (d >= cutoff and d <= target_date)
                            except Exception:
                                return False
                        df = df[df['Date'].apply(_date_in_window)]
                    except Exception:
                        pass
                dfs.append(df)
            except Exception:
                continue
    if not dfs:
        return pd.DataFrame()
    try:
        out = pd.concat(dfs, ignore_index=True)
        return out
    except Exception:
        return pd.DataFrame()



def _read_scenario_counts_by_person(outdir: str, window_days: int, target_date: date, scenario_col: str):
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty or scenario_col not in df.columns:
        return {}
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass
    id_cols = [c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in df.columns]
    out = defaultdict(int)
    q = df[df[scenario_col] == True] if df[scenario_col].dtype == bool else df[df[scenario_col].astype(str).str.lower() == 'true']
    for _, r in q.iterrows():
        for col in id_cols:
            try:
                raw = r.get(col)
                if raw in (None, '', float('nan')):
                    continue
                norm = _normalize_id_val(raw)
                if norm:
                    out[str(norm)] += 1
                    stripped = _strip_uid_prefix(str(norm))
                    if stripped != str(norm):
                        out[str(stripped)] += 1
            except Exception:
                continue
        for fallback in ('Int1', 'Text12'):
            if fallback in r and r.get(fallback) not in (None, '', 'nan'):
                try:
                    norm = _normalize_id_val(r.get(fallback))
                    if norm:
                        out[str(norm)] += 1
                except Exception:
                    continue
    return dict(out)

def _compute_weeks_with_threshold(past_df: pd.DataFrame,
                                  person_col: str = 'person_uid',
                                  date_col: str = 'Date',
                                  scenario_col: str = 'short_duration_<4h',
                                  threshold_days: int = 3) -> dict:
    if past_df is None or past_df.empty:
        return {}
    df = past_df.copy()
    if date_col not in df.columns:
        return {}
    try:
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.date
    except Exception:
        pass
    if scenario_col not in df.columns:
        return {}
    try:
        if df[scenario_col].dtype == bool:
            df['__scenario_flag__'] = df[scenario_col].astype(bool)
        else:
            df['__scenario_flag__'] = df[scenario_col].astype(str).str.strip().str.lower().isin({'true', '1', 'yes', 'y', 't'})
    except Exception:
        df['__scenario_flag__'] = df[scenario_col].apply(lambda v: str(v).strip().lower() in ('true','1','yes','y','t') if v is not None else False)
    df = df[df['__scenario_flag__'] == True].copy()
    if df.empty:
        return {}
    if person_col not in df.columns:
        fallback = next((c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in past_df.columns), None)
        if fallback is None:
            return {}
        person_col = fallback
    def _week_monday(d):
        try:
            if d is None or (isinstance(d, float) and np.isnan(d)):
                return None
            iso = d.isocalendar()
            return date.fromisocalendar(iso[0], iso[1], 1)
        except Exception:
            return None
    df['__week_monday__'] = df[date_col].apply(_week_monday)
    df = df.dropna(subset=['__week_monday__', person_col])
    if df.empty:
        return {}
    week_counts = (df.groupby([person_col, '__week_monday__'])
                     .size()
                     .reset_index(name='days_flagged'))
    valid_weeks = week_counts[week_counts['days_flagged'] >= int(threshold_days)].copy()
    if valid_weeks.empty:
        return {}
    person_weeks = {}
    for person, grp in valid_weeks.groupby(person_col):
        wlist = sorted(pd.to_datetime(grp['__week_monday__']).dt.date.unique(), reverse=True)
        person_weeks[str(person)] = wlist
    def _consecutive_week_count(week_dates_desc):
        if not week_dates_desc:
            return 0
        count = 1
        prev = week_dates_desc[0]
        for cur in week_dates_desc[1:]:
            try:
                if (prev - cur).days == 7:
                    count += 1
                    prev = cur
                else:
                    break
            except Exception:
                break
        return count
    out = {}
    for pid, weeks in person_weeks.items():
        c = _consecutive_week_count(weeks)
        out[str(pid)] = int(c)
        try:
            stripped = _strip_uid_prefix(str(pid))
            if stripped and stripped != str(pid):
                out[str(stripped)] = int(c)
        except Exception:
            pass
    return out

def _strip_uid_prefix(s):
    try:
        if s is None:
            return s
        st = str(s)
        for p in ('emp:', 'uid:', 'name:'):
            if st.startswith(p):
                return st[len(p):]
        return st
    except Exception:
        return s

def compute_violation_days_map(outdir: str, window_days: int, target_date: date):
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty:
        return {}
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass
    id_cols = []
    for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber'):
        if c in df.columns:
            id_cols.append(c)
    if 'IsFlagged' not in df.columns:
        if 'AnomalyScore' in df.columns:
            df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: float(s) >= ANOMALY_THRESHOLD if not pd.isna(s) else False)
        else:
            df['IsFlagged'] = False
    ident_dates = defaultdict(set)
    try:
        flagged = df[df['IsFlagged'] == True]
        for _, r in flagged.iterrows():
            d = r.get('Date')
            if d is None:
                continue
            for col in id_cols:
                try:
                    raw = r.get(col)
                    if raw is None:
                        continue
                    norm = _normalize_id_val(raw)
                    if norm:
                        ident_dates[str(norm)].add(d)
                        stripped = _strip_uid_prefix(str(norm))
                        if stripped != str(norm):
                            ident_dates[str(stripped)].add(d)
                except Exception:
                    continue
            for fallback in ('Int1', 'Text12'):
                if fallback in r and r.get(fallback) not in (None, '', 'nan'):
                    try:
                        norm = _normalize_id_val(r.get(fallback))
                        if norm:
                            ident_dates[str(norm)].add(d)
                            stripped = _strip_uid_prefix(str(norm))
                            if stripped != str(norm):
                                ident_dates[str(stripped)].add(d)
                    except Exception:
                        continue
    except Exception:
        logging.exception("Error building violation days map from history.")
    out = {k: int(len(v)) for k, v in ident_dates.items()}
    return out



def score_trends_from_durations(combined_df: pd.DataFrame, swipes_df: Optional[pd.DataFrame] = None, outdir: Optional[str] = None, target_date: Optional[date] = None) -> pd.DataFrame:
    """
    Take combined durations DataFrame and optional swipes DataFrame and compute:
      - scenario boolean columns
      - Reasons (semicolon-separated scenario keys)
      - ViolationExplanation (human text)
      - AnomalyScore (weighted sum)
      - IsFlagged (AnomalyScore >= ANOMALY_THRESHOLD)
      - ViolationDaysLast90 (from history)
      - historical bumps, weekly bump, MonitorFlag, etc.
    """
    if combined_df is None or combined_df.empty:
        return pd.DataFrame()

    df = combined_df.copy()
    # Ensure person_uid exists
    if 'person_uid' not in df.columns:
        df['person_uid'] = df.apply(lambda r: _canonical_person_uid(r), axis=1)

    # Ensure key columns exist
    for c in ['FirstSwipe','LastSwipe','CountSwipes','DurationMinutes','MaxSwipeGapSeconds','EmployeeID','CardNumber','person_uid','Date']:
        if c not in df.columns:
            df[c] = None

    # reconcile zero CountSwipes with raw swipes (if provided)
    if swipes_df is not None and not swipes_df.empty and 'person_uid' in swipes_df.columns:
        tsw = swipes_df.copy()
        # ensure LocaleMessageTime parsed
        if 'LocaleMessageTime' in tsw.columns:
            tsw['LocaleMessageTime'] = pd.to_datetime(tsw['LocaleMessageTime'], errors='coerce')
        else:
            for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                if cand in tsw.columns:
                    tsw['LocaleMessageTime'] = pd.to_datetime(tsw[cand], errors='coerce')
                    break
        if 'Date' not in tsw.columns:
            if 'LocaleMessageTime' in tsw.columns:
                tsw['Date'] = tsw['LocaleMessageTime'].dt.date
            else:
                for cand in ('date','Date'):
                    if cand in tsw.columns:
                        try:
                            tsw['Date'] = pd.to_datetime(tsw[cand], errors='coerce').dt.date
                        except Exception:
                            tsw['Date'] = None
                        break

        try:
            grp = tsw.dropna(subset=['person_uid', 'Date']).groupby(['person_uid', 'Date'])
            counts = grp.size().to_dict()
            firsts = grp['LocaleMessageTime'].min().to_dict()
            lasts = grp['LocaleMessageTime'].max().to_dict()
        except Exception:
            counts = {}
            firsts = {}
            lasts = {}

        def _fix_row_by_raw(idx, row):
            key = (row.get('person_uid'), row.get('Date'))
            if key in counts and (int(row.get('CountSwipes') or 0) == 0 or pd.isna(row.get('CountSwipes'))):
                try:
                    c = int(counts.get(key, 0))
                    df.at[idx, 'CountSwipes'] = c
                    f = firsts.get(key)
                    l = lasts.get(key)
                    if pd.notna(f) and (pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None):
                        df.at[idx, 'FirstSwipe'] = pd.to_datetime(f)
                    if pd.notna(l) and (pd.isna(row.get('LastSwipe')) or row.get('LastSwipe') is None):
                        df.at[idx, 'LastSwipe'] = pd.to_datetime(l)
                    try:
                        fs = df.at[idx, 'FirstSwipe']
                        ls = df.at[idx, 'LastSwipe']
                        if pd.notna(fs) and pd.notna(ls):
                            dursec = (pd.to_datetime(ls) - pd.to_datetime(fs)).total_seconds()
                            dursec = max(0, dursec)
                            df.at[idx, 'DurationSeconds'] = float(dursec)
                            df.at[idx, 'DurationMinutes'] = float(dursec / 60.0)
                    except Exception:
                        pass
                except Exception:
                    pass

        for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
            try:
                _fix_row_by_raw(ix, r)
            except Exception:
                logging.debug("Failed to reconcile row %s with raw swipes", ix)

    # Build badge map and swipe overlap maps for higher-severity scenarios
    badge_map = {}
    swipe_overlap_map = {}
    if swipes_df is not None and not swipes_df.empty:
        try:
            tmp = swipes_df[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
            if not tmp.empty:
                grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
                badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}
        except Exception:
            badge_map = {}

        overlap_window_seconds = 2
        if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes_df.columns):
            try:
                tmp2 = swipes_df[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
                if not tmp2.empty:
                    tmp2 = tmp2.sort_values(['Door', 'LocaleMessageTime'])
                    for (d, door), g in tmp2.groupby(['Date', 'Door']):
                        items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                        n = len(items)
                        for i in range(n):
                            t_i, uid_i = items[i]
                            j = i+1
                            while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                                uid_j = items[j][1]
                                if uid_i != uid_j:
                                    swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                                    swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                                j += 1
            except Exception:
                swipe_overlap_map = {}

    # Evaluate scenarios (use weighting to compute anomaly score)
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            df[name] = df.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
        elif name == "swipe_overlap":
            df[name] = df.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map=swipe_overlap_map), axis=1)
        else:
            df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)

    def compute_score(r):
        score = 0.0
        detected = []
        for name, _ in SCENARIOS:
            val = bool(r.get(name))
            w = WEIGHTS.get(name, 0.0)
            if val and w > 0:
                score += float(w)
                detected.append(name)
        return score, detected

    scores = df.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
    df['AnomalyScore'] = scores['AnomalyScore'].astype(float)
    df['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
    df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

    # PresentToday flag, ViolationDays from history, and weekly adjustments
    try:
        df['PresentToday'] = df['CountSwipes'].fillna(0).astype(int) > 0

        # historical scenario counts (for escalation)
        hist_pattern_counts = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today(), 'shortstay_longout_repeat')
        hist_rep_breaks = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today(), 'repeated_short_breaks')
        hist_short_duration = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today(), 'short_duration_<4h')

        def get_hist_count_for_row(row, hist_map):
            for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
                if k in row and row.get(k) not in (None, '', float('nan')):
                    try:
                        norm = _normalize_id_val(row.get(k))
                        if norm and str(norm) in hist_map:
                            return int(hist_map.get(str(norm), 0))
                        stripped = _strip_uid_prefix(str(norm)) if norm else None
                        if stripped and str(stripped) in hist_map:
                            return int(hist_map.get(str(stripped), 0))
                    except Exception:
                        continue
            return 0

        df['HistPatternShortLongCount90'] = df.apply(lambda r: get_hist_count_for_row(r, hist_pattern_counts), axis=1)
        df['HistRepeatedShortBreakCount90'] = df.apply(lambda r: get_hist_count_for_row(r, hist_rep_breaks), axis=1)
        df['HistShortDurationCount90'] = df.apply(lambda r: get_hist_count_for_row(r, hist_short_duration), axis=1)

        pat_mask = df['HistPatternShortLongCount90'].fillna(0).astype(int) >= 3
        if pat_mask.any():
            df.loc[pat_mask, 'AnomalyScore'] = df.loc[pat_mask, 'AnomalyScore'].astype(float)  # keep value but escalate risk below
            df.loc[pat_mask, 'RiskScore'] = 5
            df.loc[pat_mask, 'RiskLevel'] = 'High'
            df.loc[pat_mask, 'IsFlagged'] = True

        rep_mask = df['HistRepeatedShortBreakCount90'].fillna(0).astype(int) >= 5
        if rep_mask.any():
            df.loc[rep_mask, 'AnomalyScore'] = df.loc[rep_mask, 'AnomalyScore'].astype(float)
            df.loc[rep_mask, 'RiskScore'] = 5
            df.loc[rep_mask, 'RiskLevel'] = 'High'
            df.loc[rep_mask, 'IsFlagged'] = True

        # ViolationDaysLast90
        vmap = compute_violation_days_map(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today())
        def lookup_violation_days(row):
            try:
                candidates = []
                for k in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
                    v = row.get(k)
                    if v not in (None, '', float('nan')):
                        candidates.append(_normalize_id_val(v))
                for c in candidates:
                    if c is None:
                        continue
                    if c in vmap:
                        return int(vmap.get(c, 0))
                    stripped = _strip_uid_prefix(c)
                    if stripped != c and stripped in vmap:
                        return int(vmap.get(stripped, 0))
                return 0
            except Exception:
                return 0
        df['ViolationDaysLast90'] = df.apply(lookup_violation_days, axis=1)

        # Append monitoring note for persons who have past violations and are present today.
        def _append_monitor_note(idx, row):
            try:
                vd = int(row.get('ViolationDaysLast90') or 0)
            except Exception:
                vd = 0
            if vd <= 0:
                return row.get('ViolationExplanation') or row.get('Explanation')
            if not row.get('PresentToday', False):
                return row.get('ViolationExplanation') or row.get('Explanation')
            note = f"Note: Previously flagged {vd} time{'s' if vd!=1 else ''} in the last {VIOLATION_WINDOW_DAYS} days — monitor when present today."
            ex = row.get('ViolationExplanation') or row.get('Explanation') or ''
            if ex and not ex.strip().endswith('.'):
                ex = ex.strip() + '.'
            if note in ex:
                return ex
            return (ex + ' ' + note).strip()
        df['ViolationExplanation'] = df.apply(lambda r: _append_monitor_note(r.name, r), axis=1)

        df['MonitorFlag'] = df.apply(lambda r: (int(r.get('ViolationDaysLast90') or 0) > 0) and bool(r.get('PresentToday')), axis=1)

        # Now compute consecutive-week short-duration runs (post scoring)
        past_df = _read_past_trend_csvs(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today())
        week_runs = _compute_weeks_with_threshold(past_df, person_col='person_uid', date_col='Date', scenario_col='short_duration_<4h', threshold_days=3)

        def _get_week_run_for_row(r):
            for k in ('person_uid', 'EmployeeID'):
                if k in r and r.get(k):
                    key = str(r.get(k))
                    if key in week_runs:
                        return int(week_runs[key])
                    stripped = _strip_uid_prefix(key)
                    if stripped in week_runs:
                        return int(week_runs[stripped])
            return 0

        df['ConsecWeeksShort4hrs'] = df.apply(_get_week_run_for_row, axis=1)

        # Apply anomaly score bumps now that AnomalyScore exists
        df['AnomalyScore'] = df['AnomalyScore'].astype(float).fillna(0.0)

        mask1 = df['ConsecWeeksShort4hrs'].fillna(0).astype(int) >= 1
        mask2 = df['ConsecWeeksShort4hrs'].fillna(0).astype(int) >= 2

        if mask1.any():
            df.loc[mask1, 'AnomalyScore'] = df.loc[mask1, 'AnomalyScore'].astype(float) + 0.5
        if mask2.any():
            df.loc[mask2, 'AnomalyScore'] = df.loc[mask2, 'AnomalyScore'].astype(float) + 1.0

        # Recompute IsFlagged and RiskLevel after bumping AnomalyScore
        df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

        def _map_risk_after_bump(r):
            score = r.get('AnomalyScore') or 0.0
            bucket, label = map_score_to_label(score)
            return int(bucket), label
        rs2 = df.apply(lambda r: pd.Series(_map_risk_after_bump(r), index=['RiskScore', 'RiskLevel']), axis=1)
        df['RiskScore'] = rs2['RiskScore']
        df['RiskLevel'] = rs2['RiskLevel']

        # OVERRIDE: force High risk when ViolationDaysLast90 >= 4
        try:
            high_violation_mask = df['ViolationDaysLast90'] >= 4
            if high_violation_mask.any():
                df.loc[high_violation_mask, 'RiskScore'] = 5
                df.loc[high_violation_mask, 'RiskLevel'] = 'High'
        except Exception:
            pass

    except Exception:
        logging.exception("Failed post-scoring weekly-run / monitoring augmentation.")

    # Build textual Reasons and Explanation (if not already)
    def reasons_for_row(r):
        if not bool(r.get('IsFlagged')):
            return None, None
        ds_raw = r.get('DetectedScenarios')
        if ds_raw:
            ds = [s.strip() for s in ds_raw.split(";") if s and s.strip()]
            explanation = _explain_scenarios_detected(r, ds)
            reasons_codes = "; ".join(ds) if ds else None
            return reasons_codes, explanation
        return None, None

    reason_tuples = df.apply(lambda r: pd.Series(reasons_for_row(r), index=['Reasons', 'ViolationExplanation']), axis=1)
    def _sanitize_reason_val(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == "" or _is_placeholder_str(s):
                return None
            return s
        except Exception:
            return None

    df['Reasons'] = reason_tuples['Reasons'].apply(_sanitize_reason_val)
    df['ViolationExplanation'] = reason_tuples['ViolationExplanation'].apply(lambda v: None if _is_placeholder_str(v) else (str(v).strip() if v is not None else None))

    # If flagged but no Reasons, ensure fallback
    def _ensure_reason_for_flagged(row):
        if bool(row.get('IsFlagged')) and (row.get('Reasons') is None or row.get('Reasons') == ''):
            ds = row.get('DetectedScenarios')
            if ds and not _is_placeholder_str(ds):
                parts = [p.strip() for p in re.split(r'[;,\|]', str(ds)) if p and not _is_placeholder_str(p)]
                if parts:
                    return "; ".join(parts)
            if int(row.get('ConsecWeeksShort4hrs') or 0) >= 1:
                return "consecutive_short_weeks"
            if int(row.get('ViolationDaysLast90') or 0) > 0:
                return "historical_monitoring"
            return None
        return row.get('Reasons')

    if 'IsFlagged' in df.columns:
        df['Reasons'] = df.apply(_ensure_reason_for_flagged, axis=1)
    else:
        df['Reasons'] = df['Reasons'].apply(_sanitize_reason_val)

    if 'OverlapWith' not in df.columns:
        def overlap_with_fn(r):
            d = r.get('Date')
            uid = r.get('person_uid')
            if (d, uid) in swipe_overlap_map:
                return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
            return None
        df['OverlapWith'] = df.apply(overlap_with_fn, axis=1)

    # ensure booleans native
    for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
        if col in df.columns:
            df[col] = df[col].astype(bool)

    # return DataFrame
    return df


# ---------------- run_trend_for_date ----------------
def _slug_city(city: str) -> str:
    if not city:
        return "pune"
    return str(city).strip().lower().replace(" ", "_")


def run_trend_for_date(target_date: date,
                       regions: Optional[List[str]] = None,
                       outdir: str = None,
                       city: str = "pune",
                       as_dict: bool = False) -> pd.DataFrame:
    city_slug = _slug_city(city)
    if regions is None:
        try:
            regions = list(REGION_CONFIG.keys()) if isinstance(REGION_CONFIG, dict) and REGION_CONFIG else []
        except Exception:
            regions = []
    regions = [r.lower() for r in regions if r]
    outdir_path = Path(outdir) if outdir else OUTDIR
    if run_for_date is None:
        raise RuntimeError("duration_report.run_for_date is not available in this environment.")
    # call run_for_date defensively
    try:
        results = run_for_date(target_date, regions, str(outdir_path), city)
    except TypeError:
        try:
            results = run_for_date(target_date, regions, str(outdir_path))
        except Exception:
            try:
                results = run_for_date(target_date)
            except Exception as e:
                logging.exception("run_for_date failed entirely.")
                raise
    # combine durations and swipes across regions
    dur_list = []
    swipe_list = []
    try:
        for rkey, rr in (results or {}).items():
            try:
                dfdur = rr.get('durations')
                if dfdur is not None and not dfdur.empty:
                    dfdur = dfdur.copy()
                    dfdur['region'] = rkey
                    dur_list.append(dfdur)
            except Exception:
                pass
            try:
                dfsw = rr.get('swipes')
                if dfsw is not None and not dfsw.empty:
                    dfcopy = dfsw.copy()
                    dfcopy['region'] = rkey
                    swipe_list.append(dfcopy)
            except Exception:
                pass
    except Exception:
        logging.exception("Failed to iterate results returned by run_for_date.")
    combined = pd.concat(dur_list, ignore_index=True) if dur_list else pd.DataFrame()
    sw_combined = pd.concat(swipe_list, ignore_index=True) if swipe_list else pd.DataFrame()

    # Decide Pune 2AM boundary
    use_pune_2am_boundary = False
    try:
        if city and isinstance(city, str) and 'pun' in city.strip().lower():
            use_pune_2am_boundary = True
        else:
            if os.getenv("PUNE_2AM_BOUNDARY", "0") == "1":
                use_pune_2am_boundary = True
    except Exception:
        use_pune_2am_boundary = False


 # Prepare for features; possibly shift times for Pune 02:00 grouping
    sw_for_features = sw_combined.copy() if sw_combined is not None else pd.DataFrame()
    durations_for_features = combined.copy() if combined is not None else pd.DataFrame()

    if use_pune_2am_boundary and (sw_for_features is not None) and (not sw_for_features.empty):
        try:
            if 'LocaleMessageTime' in sw_for_features.columns:
                sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features['LocaleMessageTime'], errors='coerce')
            else:
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                    if cand in sw_for_features.columns:
                        sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features[cand], errors='coerce')
                        break
            sw_for_features['OriginalLocaleMessageTime'] = sw_for_features['LocaleMessageTime']
            sw_for_features['LocaleMessageTime'] = sw_for_features['LocaleMessageTime'] - pd.Timedelta(hours=2)
            # recompute durations if compute_daily_durations is available
            if callable(compute_daily_durations):
                try:
                    durations_for_features = compute_daily_durations(sw_for_features)
                except Exception:
                    logging.exception("compute_daily_durations failed for shifted swipes; falling back to original durations.")
                    durations_for_features = combined.copy()
            # save shifted raw optionally
            try:
                sw_shifted_out = Path(outdir_path) / f"swipes_{city_slug}_{target_date.strftime('%Y%m%d')}_shifted.csv"
                cols_keep = [c for c in sw_for_features.columns if c in ('person_uid','EmployeeID','CardNumber','LocaleMessageTime','OriginalLocaleMessageTime','Door','Direction','PartitionName2')]
                sw_for_features[cols_keep].to_csv(sw_shifted_out, index=False)
            except Exception:
                logging.debug("Could not write shifted swipes file.")
        except Exception:
            logging.exception("Failed to prepare shifted swipes for Pune 2AM logic.")
            sw_for_features = sw_combined.copy()
            durations_for_features = combined.copy()


    # compute features once (use possibly-shifted data so grouping uses 02:00 boundary for Pune)
    features = compute_features(sw_for_features, durations_for_features)
    if features is None:
        features = pd.DataFrame()
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        if as_dict:
            return {'rows': 0, 'flagged_rows': 0, 'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': 0}
        return pd.DataFrame()
    # restore FirstSwipe/LastSwipe to original timeline if shifted (only once)
    if use_pune_2am_boundary:
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in features.columns:
                try:
                    features[dtcol] = pd.to_datetime(features[dtcol], errors='coerce') + pd.Timedelta(hours=2)
                except Exception:
                    pass

    # Save raw swipes for evidence
    try:
        if sw_combined is not None and not sw_combined.empty:
            sw_out = Path(outdir_path) / f"swipes_{city_slug}_{target_date.strftime('%Y%m%d')}.csv"
            sw_combined.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception:
        logging.warning("Failed to save raw swipes")

    # Recompute per-row metrics from raw swipes and merge into features
    try:
        if sw_combined is not None and not sw_combined.empty:
            if 'LocaleMessageTime' not in sw_combined.columns:
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                    if cand in sw_combined.columns:
                        sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined[cand], errors='coerce')
                        break
            else:
                sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined['LocaleMessageTime'], errors='coerce')
            if use_pune_2am_boundary:
                sw_combined['DisplayDateKey'] = (sw_combined['LocaleMessageTime'] - pd.Timedelta(hours=2)).dt.date
            else:
                sw_combined['DisplayDateKey'] = sw_combined['LocaleMessageTime'].dt.date

            def _agg_metrics(g):
                times_sorted = sorted(list(pd.to_datetime(g['LocaleMessageTime'].dropna())))
                count_swipes = len(times_sorted)
                max_gap = 0
                short_gap_count = 0
                if len(times_sorted) >= 2:
                    gaps = []
                    for i in range(1, len(times_sorted)):
                        s = (times_sorted[i] - times_sorted[i-1]).total_seconds()
                        gaps.append(s)
                        if s <= 5*60:
                            short_gap_count += 1
                    max_gap = int(max(gaps)) if gaps else 0
                first_ts = times_sorted[0] if times_sorted else None
                last_ts = times_sorted[-1] if times_sorted else None
                unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
                unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
                def _pick_non_guid(colname):
                    if colname in g.columns:
                        for v in pd.unique(g[colname].dropna().astype(str).map(lambda x: x.strip())):
                            if v and (not _GUID_RE.match(v)) and v.lower() not in _PLACEHOLDER_STRS:
                                return v
                    return None
                card = _pick_non_guid('CardNumber')
                empid = _pick_non_guid('EmployeeID') or _pick_non_guid('Int1') or _pick_non_guid('Text12')
                empname = _pick_non_guid('EmployeeName') or _pick_non_guid('ObjectName1')
                duration_sec = 0.0
                if first_ts is not None and last_ts is not None:
                    try:
                        duration_sec = float((pd.to_datetime(last_ts) - pd.to_datetime(first_ts)).total_seconds())
                        if duration_sec < 0:
                            duration_sec = 0.0
                    except Exception:
                        duration_sec = 0.0
                return pd.Series({
                    'FirstSwipe_raw': first_ts,
                    'LastSwipe_raw': last_ts,
                    'CountSwipes_raw': int(count_swipes),
                    'DurationSeconds_raw': float(duration_sec),
                    'DurationMinutes_raw': float(duration_sec/60.0),
                    'MaxSwipeGapSeconds_raw': int(max_gap),
                    'ShortGapCount_raw': int(short_gap_count),
                    'UniqueDoors_raw': int(unique_doors),
                    'UniqueLocations_raw': int(unique_locations),
                    'CardNumber_raw': card,
                    'EmployeeID_raw': empid,
                    'EmployeeName_raw': empname
                })

            grouped_raw = sw_combined.dropna(subset=['person_uid', 'DisplayDateKey'], how='any').groupby(['person_uid', 'DisplayDateKey'])
            if not grouped_raw.ngroups:
                raw_metrics_df = pd.DataFrame(columns=[
                    'person_uid','DisplayDate','FirstSwipe_raw','LastSwipe_raw','CountSwipes_raw','DurationSeconds_raw',
                    'DurationMinutes_raw','MaxSwipeGapSeconds_raw','ShortGapCount_raw','UniqueDoors_raw','UniqueLocations_raw',
                    'CardNumber_raw','EmployeeID_raw','EmployeeName_raw'
                ])
            else:
                raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
                raw_metrics_df.rename(columns={'DisplayDateKey':'DisplayDate'}, inplace=True)

            # --- robust creation of merge keys for DisplayDate ---
            # We used to assume 'DisplayDate' exists; sometimes it doesn't which caused KeyError/AttributeError.
            # Create two helper columns that are safe for joining: a normalized Timestamp and a safe string.
            try:
                if 'DisplayDate' in features.columns:
                    try:
                        features['_DisplayDate_for_merge'] = pd.to_datetime(features['DisplayDate'], errors='coerce').dt.normalize()
                    except Exception:
                        features['_DisplayDate_for_merge'] = pd.NaT
                    try:
                        features['_DisplayDate_for_merge_str'] = pd.to_datetime(features['DisplayDate'], errors='coerce').astype(str).fillna('')
                    except Exception:
                        # fallback to stringification of the original series
                        try:
                            features['_DisplayDate_for_merge_str'] = features['DisplayDate'].astype(str).fillna('')
                        except Exception:
                            features['_DisplayDate_for_merge_str'] = ''
                else:
                    features['_DisplayDate_for_merge'] = pd.NaT
                    features['_DisplayDate_for_merge_str'] = ''
            except Exception:
                logging.exception("Failed to build feature merge keys for DisplayDate; proceeding without them")
                features['_DisplayDate_for_merge'] = pd.NaT
                features['_DisplayDate_for_merge_str'] = ''

            try:
                if 'DisplayDate' in raw_metrics_df.columns:
                    raw_metrics_df['_DisplayDate_for_merge'] = pd.to_datetime(raw_metrics_df['DisplayDate'], errors='coerce').dt.normalize()
                    raw_metrics_df['_DisplayDate_for_merge_str'] = pd.to_datetime(raw_metrics_df['DisplayDate'], errors='coerce').astype(str).fillna('')
                else:
                    raw_metrics_df['_DisplayDate_for_merge'] = pd.NaT
                    raw_metrics_df['_DisplayDate_for_merge_str'] = ''
            except Exception:
                logging.exception("Failed to build raw_metrics merge keys; falling back to string keys")
                raw_metrics_df['_DisplayDate_for_merge'] = pd.NaT
                raw_metrics_df['_DisplayDate_for_merge_str'] = ''

            # Prefer the datetime normalized join if available, else fall back to string join
            merged_metrics = None
            try:
                merged_metrics = pd.merge(features, raw_metrics_df, how='left',
                                          left_on=['person_uid', '_DisplayDate_for_merge'],
                                          right_on=['person_uid', '_DisplayDate_for_merge'],
                                          suffixes=('','_rawagg'))
            except Exception:
                try:
                    merged_metrics = pd.merge(features, raw_metrics_df, how='left',
                                              left_on=['person_uid', '_DisplayDate_for_merge_str'],
                                              right_on=['person_uid', '_DisplayDate_for_merge_str'],
                                              suffixes=('','_rawagg'))
                except Exception:
                    logging.exception("Both merge attempts failed; continuing without raw-agg merge")
                    merged_metrics = features.copy()

            # coalesce raw columns back into features (if present)
            try:
                for base_col, raw_col in [
                    ('FirstSwipe','FirstSwipe_raw'),
                    ('LastSwipe','LastSwipe_raw'),
                    ('CountSwipes','CountSwipes_raw'),
                    ('DurationSeconds','DurationSeconds_raw'),
                    ('DurationMinutes','DurationMinutes_raw'),
                    ('MaxSwipeGapSeconds','MaxSwipeGapSeconds_raw'),
                    ('ShortGapCount','ShortGapCount_raw'),
                    ('UniqueDoors','UniqueDoors_raw'),
                    ('UniqueLocations','UniqueLocations_raw'),
                    ('CardNumber','CardNumber_raw'),
                    ('EmployeeID','EmployeeID_raw'),
                    ('EmployeeName','EmployeeName_raw')
                ]:
                    if raw_col in merged_metrics.columns:
                        try:
                            merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
                        except Exception:
                            # best-effort: if combine_first fails, keep original
                            pass
                feature_cols = list(features.columns)
                if all(c in merged_metrics.columns for c in feature_cols):
                    features = merged_metrics[feature_cols].copy()
                else:
                    features = merged_metrics.copy()
                for helper_col in ['_DisplayDate_for_merge', '_DisplayDate_for_merge_str']:
                    if helper_col in features.columns:
                        try:
                            features.drop(columns=[helper_col], inplace=True)
                        except Exception:
                            pass
            except Exception:
                logging.exception("Post-merge coalescing failed; leaving features as-is.")



    except Exception:
        logging.exception("Failed recomputing raw metrics (non-fatal)")









    # If we used shifted timeline restore displayed times (safety)
    if use_pune_2am_boundary:
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in features.columns:
                try:
                    features[dtcol] = pd.to_datetime(features[dtcol], errors='coerce') + pd.Timedelta(hours=2)
                except Exception:
                    pass

    # Merge features with durations (prefer features)
    try:
        merged = pd.merge(features, combined, how='left', on=['person_uid', 'Date'], suffixes=('_feat', '_dur'))
    except Exception:
        merged = features

    trend_df = score_trends_from_durations(merged, swipes_df=sw_combined, outdir=str(outdir_path), target_date=target_date)

    # write csv (use city_slug, not hard-coded 'pune')
    try:
        write_df = trend_df.copy()
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        write_df = write_df.where(pd.notnull(write_df), None)
        # IMPORTANT: write with city_slug so app.py can find the file
        out_csv = Path(outdir_path) / f"trend_{city_slug}_{target_date.strftime('%Y%m%d')}.csv"
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception:
        logging.exception("Failed to write trend CSV")

    # Format DisplayDate
    try:
        if 'DisplayDate' in trend_df.columns:
            trend_df['DisplayDate'] = pd.to_datetime(trend_df['DisplayDate'], errors='coerce').dt.date
            trend_df['DisplayDate'] = trend_df['DisplayDate'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
    except Exception:
        pass

    if as_dict:
        # ------------------ Ensure sample/aggregated rows contain enrichment (email/image) ------------------
        try:
            # make a copy we will return
            rec_df = trend_df.copy()

            # add friendly string times for First/Last for JSON output
            for dtcol in ('FirstSwipe', 'LastSwipe'):
                if dtcol in rec_df.columns:
                    rec_df[dtcol] = pd.to_datetime(rec_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')

            # add Date ISO strings
            if 'Date' in rec_df.columns:
                try:
                    rec_df['Date'] = pd.to_datetime(rec_df['Date'], errors='coerce').dt.date
                    rec_df['Date'] = rec_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
                except Exception:
                    pass

            # Enrich with personnel info (email, imageUrl) using the helper already defined
            try:
                # use endpoint template that the frontend expects (it will call /employee/<id>/image)
                rec_df = _enrich_with_personnel_info(rec_df, image_endpoint_template="/employee/{}/image")
            except Exception:
                # non-fatal - if enrichment fails, continue without email/image
                logging.exception("Personnel enrichment failed (non-fatal).")

            # Build files list (raw swipe files written earlier)
            files_list = []
            try:
                # looks for swipes file for this city/date naming conventions saved earlier
                # collect any swipes_*.csv in OUTDIR for this run date
                globp = list(Path(outdir_path).glob("swipes_*_*.csv"))
                files_list = [p.name for p in globp]
            except Exception:
                files_list = []

            # Optionally build a raw_swipes map from sw_combined (if large, this can be trimmed later)
            raw_swipes_all = []
            try:
                if sw_combined is not None and not sw_combined.empty:
                    # ensure LocaleMessageTime parsed
                    if 'LocaleMessageTime' in sw_combined.columns:
                        sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined['LocaleMessageTime'], errors='coerce')
                    else:
                        # try common candidates
                        for cand in ('MessageUTC','MessageTime','Timestamp','timestamp','Date'):
                            if cand in sw_combined.columns:
                                sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined[cand], errors='coerce')
                                break
                    # minimal projection fields used by frontend screenshot timeline
                    proj_cols = []
                    for c in ('EmployeeName','EmployeeID','person_uid','CardNumber','Door','Direction','Zone','PartitionName2'):
                        if c in sw_combined.columns:
                            proj_cols.append(c)
                    # add a safe time/date/time-string, and compute gap per door/person grouping if possible
                    tmp = sw_combined.copy()
                    tmp['Date'] = tmp['LocaleMessageTime'].dt.date.astype(str)
                    tmp['Time'] = tmp['LocaleMessageTime'].dt.time.astype(str)
                    # sort so gap calc is consistent
                    tmp = tmp.sort_values(['person_uid','LocaleMessageTime'])
                    tmp['SwipeGapSeconds'] = tmp.groupby(['person_uid','Date'])['LocaleMessageTime'].diff().dt.total_seconds().fillna(0).astype(int)
                    tmp['SwipeGap'] = tmp['SwipeGapSeconds'].apply(lambda s: format_seconds_to_hms(s) if s is not None else "-")
                    # include Zone column if map_door_to_zone produced it earlier - otherwise attempt mapping by door/direction
                    if 'Zone' not in tmp.columns and 'Door' in tmp.columns:
                        tmp['Zone'] = tmp.apply(lambda r: map_door_to_zone(r.get('Door'), r.get('Direction')), axis=1)
                    raw_swipes_all = tmp.to_dict(orient='records')
            except Exception:
                logging.exception("Building raw_swipes list failed (non-fatal).")
                raw_swipes_all = []

            # Build reason counts and risk counts (existing logic)
            reasons_count = {}
            risk_counts = {}
            try:
                if 'Reasons' in rec_df.columns:
                    for v in rec_df['Reasons'].dropna().astype(str):
                        for part in re.split(r'[;,\|]', v):
                            key = part.strip()
                            if key:
                                reasons_count[key] = reasons_count.get(key, 0) + 1
                if 'RiskLevel' in rec_df.columns:
                    for v in rec_df['RiskLevel'].fillna('').astype(str):
                        if v:
                            risk_counts[v] = risk_counts.get(v, 0) + 1
            except Exception:
                pass

            # sample records (top 20)
            sample_records = rec_df.head(20).to_dict(orient='records') if not rec_df.empty else []

            return {
                'rows': int(len(rec_df)),
                'flagged_rows': int(rec_df['IsFlagged'].sum()) if 'IsFlagged' in rec_df.columns else 0,
                'aggregated_unique_persons': int(len(rec_df)),
                'sample': sample_records,
                'reasons_count': reasons_count,
                'risk_counts': risk_counts,
                'files': files_list,
                # convenience additions used by the frontend record endpoint for quick lookup
                'raw_swipes_all': raw_swipes_all
            }
        except Exception:
            logging.exception("Failed to build as_dict output for run_trend_for_date")
            # fallback minimal structure
            return {'rows': len(trend_df), 'flagged_rows': int(trend_df['IsFlagged'].sum() if 'IsFlagged' in trend_df.columns else 0),
                    'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': len(trend_df), 'files': []}

    return trend_df




# ---------------- helper wrappers ----------------
from datetime import datetime as _datetime
from datetime import timedelta as _timedelta
def _ensure_date_obj(d):
    if d is None:
        return None
    if isinstance(d, date):
        return d
    if isinstance(d, _datetime):
        return d.date()
    if isinstance(d, str):
        try:
            return _datetime.strptime(d, "%Y-%m-%d").date()
        except Exception:
            try:
                return _datetime.fromisoformat(d).date()
            except Exception:
                raise ValueError(f"Unsupported date string: {d}")
    raise ValueError(f"Unsupported date type: {type(d)}")

def build_monthly_training(start_date=None, end_date=None, outdir: str = None, city: str = 'Pune', as_dict: bool = False):
    od = Path(outdir) if outdir else OUTDIR
    od.mkdir(parents=True, exist_ok=True)
    if start_date is None and end_date is None:
        today = date.today()
        first = date(today.year, today.month, 1)
        last = date(today.year, today.month, calendar.monthrange(today.year, today.month)[1])
    else:
        if start_date is None:
            raise ValueError("start_date must be provided when end_date is provided")
        first = _ensure_date_obj(start_date)
        if end_date is None:
            last = date(first.year, first.month, calendar.monthrange(first.year, first.month)[1])
        else:
            last = _ensure_date_obj(end_date)
    if last < first:
        raise ValueError("end_date must be >= start_date")
    cur = first
    ran = []
    errors = {}
    total_flagged = 0
    total_rows = 0
    while cur <= last:
        try:
            logging.info("build_monthly_training: running for %s (city=%s)", cur.isoformat(), city)
            res = run_trend_for_date(cur, outdir=str(od), city=city, as_dict=as_dict)
            ran.append({'date': cur.isoformat(), 'result': res})
            if isinstance(res, dict):
                total_flagged += int(res.get('flagged_rows', 0) or 0)
                total_rows += int(res.get('rows', 0) or 0)
        except KeyboardInterrupt:
            raise
        except Exception as e:
            logging.exception("build_monthly_training: failed for %s", cur)
            errors[cur.isoformat()] = str(e)
        cur = cur + _timedelta(days=1)
    summary = {
        'start_date': first.isoformat(),
        'end_date': last.isoformat(),
        'dates_attempted': (last - first).days + 1,
        'dates_succeeded': len([r for r in ran if r.get('result') is not None]),
        'dates_failed': len(errors),
        'errors': errors,
        'total_rows': total_rows,
        'total_flagged': total_flagged
    }
    if as_dict:
        return summary
    return ran





def read_90day_cache(outdir: str = None):
    od = Path(outdir) if outdir else OUTDIR
    fp = od / "90day_cache.json"
    if not fp.exists():
        return {}
    try:
        with fp.open("r", encoding="utf8") as fh:
            return json.load(fh)
    except Exception:
        logging.exception("read_90day_cache: failed to read %s", str(fp))
        return {}

if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today, as_dict=False)
    print("Completed; rows:", len(df) if df is not None else 0)










<!doctype html>
<html>

<head>
  <meta charset="utf-8" />
  <title>Behaviour Analysis — Dashboard</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <!-- React + ReactDOM + Babel (quick prototyping) -->
  <script crossorigin src="https://unpkg.com/react@18/umd/react.development.js"></script>
  <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.development.js"></script>
  <script crossorigin src="https://unpkg.com/babel-standalone@6.26.0/babel.min.js"></script>

  <!-- Chart.js -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

  <!-- Flatpickr -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
  <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.1/font/bootstrap-icons.css">
  <link rel="stylesheet" href="style.css">
</head>

<body>
  <div id="root"></div>

  <script type="text/babel">
    (function () {
      const { useState, useEffect, useRef } = React;

      // CHANGE IF YOUR API HOST DIFFERS
      const API_BASE = "http://localhost:8002";

      function resolveApiImageUrl(imgUrl) {
        if (!imgUrl) return null;
        try {
          if (imgUrl.startsWith('http://') || imgUrl.startsWith('https://')) return imgUrl;
          // ensure API_BASE has no trailing slash
          var base = API_BASE.replace(/\/$/, '');
          if (imgUrl.startsWith('/')) return base + imgUrl;
          return base + '/' + imgUrl;
        } catch (e) {
          return imgUrl;
        }
      }


      // --- Region / Location mapping copied from backend/duration_report.REGION_CONFIG (friendly / UI names) ---
      // Keep the keys lowercase to match backend region keys.
      const REGION_OPTIONS = {
        "apac": {
          label: "APAC",
          // Friendly names used by backend normalisation (duration_report) for APAC
          partitions: ["Pune", "Quezon City", "Taguig City", "MY.Kuala Lumpur", "IN.HYD", "SG.Singapore"]
        },
        "emea": {
          label: "EMEA",
          partitions: ["LT.Vilnius", "IT.Rome", "UK.London", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
        },
        "laca": {
          label: "LACA",
          partitions: ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "PA.Panama City", "PE.Lima", "MX.Mexico City"]
        },
        "namer": {
          label: "NAMER",
          // show friendly names to the user, but we map them to backend partition tokens before sending
          partitions: ["Denver", "Austin Texas", "Miami", "New York"]
        }
      };

      // Map from UI-friendly location label -> backend search token (used for &city=)
      // For APAC, friendly labels match backend PartitionName2 normalised values so they map to themselves.
      // For NAMER, backend normalisation sets PartitionName2 to tokens like "US.CO.OBS", "USA/Canada Default" etc.
      const LOCATION_QUERY_VALUE = {
        "apac": {
          "Pune": "Pune",
          "Quezon City": "Quezon City",
          "Taguig City": "Taguig City",
          "MY.Kuala Lumpur": "MY.Kuala Lumpur",
          "IN.HYD": "IN.HYD",
          "SG.Singapore": "SG.Singapore"
        },
        "namer": {
          // friendly->backend tokens (this matches the backend duration_report normalisation)
          "Denver": "US.CO.OBS",
          "Austin Texas": "USA/Canada Default",
          "Miami": "US.FL.Miami",
          "New York": "US.NYC"
        },
        // default passthrough for other regions (if needed)
        "emea": {},
        "laca": {}
      };

      // Map risk text/colors (same as backend map_score_to_label buckets)
      const RISK_COLORS = {
        "Low": "#10b981",
        "Low Medium": "#86efac",
        "Medium": "#facc15",
        "Medium High": "#fb923c",
        "High": "#ef4444"
      };
      const RISK_LABELS = ["Low", "Low Medium", "Medium", "Medium High", "High"];

      // (rest unchanged) Explanations...
      const SCENARIO_EXPLANATIONS = {
        "long_gap_>=4.5h": "Long gap between swipes (>=4.5 hours).",
        "short_duration_<4h": "Short total presence (<4 hours).",
        "coffee_badging": "Multiple short swipes — possible coffee badging.",
        "low_swipe_count_<=2": "Very few swipes recorded for day (<=2).",
        "single_door": "Single door used during day.",
        "only_in": "Only IN events present.",
        "only_out": "Only OUT events present.",
        "overtime_>=10h": "Overtime (>=10 hours).",
        "very_long_duration_>=16h": "Very long presence (>=16 hours).",
        "unusually_high_swipes": "Unusually high number of swipes vs history.",
        "repeated_short_breaks": "Multiple short breaks in day.",
        "multiple_location_same_day": "Multiple locations used same day.",
        "weekend_activity": "Activity on weekend.",
        "repeated_rejection_count": "Multiple rejections.",
        "badge_sharing_suspected": "Same card used by multiple persons on same day.",
        "early_arrival_before_06": "First swipe earlier than 06:00.",
        "late_exit_after_22": "Last swipe after 22:00.",
        "shift_inconsistency": "Duration inconsistent with historical shift.",
        "trending_decline": "Historical trending decline.",
        "consecutive_absent_days": "Consecutive absent days historically.",
        "high_variance_duration": "High variance in durations historically.",
        "short_duration_on_high_presence_days": "Short duration despite high typical presence.",
        "swipe_overlap": "Swipes overlapping other users (possible tailgating).",
        "shortstay_longout_repeat": "Short in -> long out -> short return pattern."
      };

      // small utilities
      function pad(n) { return n.toString().padStart(2, '0'); }
      function formatDateISO(d) {
        if (!d) return "";
        const dt = (d instanceof Date) ? d : new Date(d);
        return dt.getFullYear() + "-" + pad(dt.getMonth() + 1) + "-" + pad(dt.getDate());
      }
      function safeDateDisplay(val) {
        if (!val && val !== 0) return "";
        try {
          const d = (val instanceof Date) ? val : new Date(val);
          if (isNaN(d.getTime())) return String(val);
          return d.toLocaleString();
        } catch (e) {
          return String(val);
        }
      }

      function sanitizeName(row) {
        if (!row) return "";
        // prefer feature/duration versions if present
        return row.EmployeeName_feat || row.EmployeeName_dur || row.EmployeeName || row.ObjectName1 || row.objectname1 || row.employee_name || row.person_uid || "";
      }


      function downloadCSV(rows, filename) {
        if (!rows || !rows.length) { alert("No rows to export"); return; }
        var cols = Object.keys(rows[0]);
        var lines = [cols.join(",")];
        rows.forEach(function (r) {
          var row = cols.map(function (c) {
            var v = (r[c] === undefined || r[c] === null) ? "" : String(r[c]).replace(/\n/g, ' ');
            return JSON.stringify(v);
          }).join(",");
          lines.push(row);
        });
        var blob = new Blob([lines.join("\n")], { type: 'text/csv' });
        var url = URL.createObjectURL(blob);
        var a = document.createElement('a'); a.href = url; a.download = filename || 'export.csv'; a.click(); URL.revokeObjectURL(url);
      }

      // duration formatting helper
      function formatSecondsToHmsJS(seconds) {
        if (seconds === null || seconds === undefined || seconds === '') return "-";
        const n = Number(seconds);
        if (isNaN(n) || !isFinite(n)) return "-";
        const s = Math.max(0, Math.floor(n));
        const hh = Math.floor(s / 3600);
        const mm = Math.floor((s % 3600) / 60);
        const ss = s % 60;
        return pad(hh) + ":" + pad(mm) + ":" + pad(ss);
      }



      // duration formatting helper (HH:MM) — used for Duration fields (strict HH:MM)
      function formatSecondsToHmJS(seconds) {
        if (seconds === null || seconds === undefined || seconds === '') return "-";
        const n = Number(seconds);
        if (isNaN(n) || !isFinite(n)) return "-";
        const s = Math.max(0, Math.floor(n));
        const hh = Math.floor(s / 3600);
        const mm = Math.floor((s % 3600) / 60);
        // return HH:MM (hours may be >23)
        return String(hh) + ":" + String(mm).padStart(2, '0');
      }


      // ----- Day-boundary helpers -----
      // Backend assigns Date using LocaleMessageTime.date() (no 2AM shift).
      // Keep frontend day-boundary at 0 so logical dates match backend.
      const DAY_BOUNDARY_HOUR = 0;

      function logicalDateForTs(dt, boundaryHour = DAY_BOUNDARY_HOUR) {
        if (!dt || !(dt instanceof Date) || isNaN(dt.getTime())) return null;
        const hour = dt.getHours();
        const year = dt.getFullYear();
        const month = dt.getMonth();
        const day = dt.getDate();
        const out = new Date(year, month, day, 0, 0, 0, 0);
        // with boundaryHour = 0, this never subtracts a day -> matches backend date assignment
        if (hour < boundaryHour) {
          out.setDate(out.getDate() - 1);
        }
        const y = out.getFullYear();
        const m = String(out.getMonth() + 1).padStart(2, '0');
        const d = String(out.getDate()).padStart(2, '0');
        return `${y}-${m}-${d}`;
      }

      function makeLocalDateFromRow(r) {
        try {
          if (!r) return null;

          // backend usually includes LocaleMessageTime (ISO string). Prefer that.
          if (r.LocaleMessageTime) {
            const t = new Date(r.LocaleMessageTime);
            if (!isNaN(t.getTime())) return t;
          }

          function toInt(v, fallback = 0) {
            const n = Number(v);
            return Number.isFinite(n) ? n : fallback;
          }

          // Backend also supplies DateOnly + Time for frontend convenience — use those if present.
          if (r.DateOnly && r.Time) {
            try {
              // DateOnly might be a Date object or 'YYYY-MM-DD' string.
              const dateStr = String(r.DateOnly).slice(0, 10).replace(/\//g, '-');
              const dateParts = dateStr.split('-').map(p => toInt(p, NaN));
              if (dateParts.length === 3 && !isNaN(dateParts[0])) {
                const year = dateParts[0];
                const month = dateParts[1];
                const day = dateParts[2];

                const timeRaw = String(r.Time).split(/[.+Z ]/)[0];
                const timeParts = timeRaw.split(':').map(p => toInt(p, 0));
                const hh = timeParts[0] || 0;
                const mm = timeParts[1] || 0;
                const ss = timeParts[2] || 0;

                return new Date(year, month - 1, day, hh, mm, ss, 0);
              }
            } catch (e) { /* fallthrough */ }
          }

          // fallback: if Date and Time fields exist (older API formats)
          if (r.Date && r.Time) {
            try {
              const dateStr = String(r.Date).slice(0, 10).replace(/\//g, '-');
              const dateParts = dateStr.split('-').map(p => toInt(p, NaN));
              if (dateParts.length === 3 && !isNaN(dateParts[0])) {
                const year = dateParts[0];
                const month = dateParts[1];
                const day = dateParts[2];

                const timeRaw = String(r.Time).split(/[.+Z ]/)[0];
                const timeParts = timeRaw.split(':').map(p => toInt(p, 0));
                const hh = timeParts[0] || 0;
                const mm = timeParts[1] || 0;
                const ss = timeParts[2] || 0;

                return new Date(year, month - 1, day, hh, mm, ss, 0);
              }
            } catch (e) { /* fallthrough */ }
          }

          // if only DateOnly present, return midnight of that date
          if (r.DateOnly) {
            try {
              const parts = String(r.DateOnly).slice(0, 10).replace(/\//g, '-').split('-');
              if (parts.length === 3) {
                const y = toInt(parts[0], NaN);
                const m = toInt(parts[1], NaN);
                const d = toInt(parts[2], NaN);
                if (!isNaN(y)) return new Date(y, m - 1, d, 0, 0, 0, 0);
              }
            } catch (e) { /* fallthrough */ }
          }

          // if only Date present, use that
          if (r.Date) {
            try {
              const parts = String(r.Date).slice(0, 10).replace(/\//g, '-').split('-');
              if (parts.length === 3) {
                const y = toInt(parts[0], NaN);
                const m = toInt(parts[1], NaN);
                const d = toInt(parts[2], NaN);
                if (!isNaN(y)) return new Date(y, m - 1, d, 0, 0, 0, 0);
              }
            } catch (e) { /* fallthrough */ }
          }

        } catch (e) { }
        return null;
      }

      // App component
      function App() {
        var yesterday = new Date();
        yesterday.setDate(yesterday.getDate() - 1);

        const [dateFrom, setDateFrom] = useState(formatDateISO(yesterday));
        const [dateTo, setDateTo] = useState(formatDateISO(new Date()));
        const [loading, setLoading] = useState(false);
        const [summary, setSummary] = useState({ rows: 0, flagged_rows: 0, files: [], end_date: null });
        const [rows, setRows] = useState([]);
        const [reasonsCount, setReasonsCount] = useState({});
        const [riskCounts, setRiskCounts] = useState({ "Low": 0, "Low Medium": 0, "Medium": 0, "Medium High": 0, "High": 0 });
        const [filterText, setFilterText] = useState("");
        const [page, setPage] = useState(1);
        const [selectedReason, setSelectedReason] = useState("");
        const [reasonFilterText, setReasonFilterText] = useState("");
        const [modalRow, setModalRow] = useState(null);
        const [modalDetails, setModalDetails] = useState(null);
        const [modalLoading, setModalLoading] = useState(false);
        const [selectedRiskFilter, setSelectedRiskFilter] = useState("");

        // New: region & location
        const [selectedRegion, setSelectedRegion] = useState("apac"); // default APAC
        const [selectedLocation, setSelectedLocation] = useState("All locations"); // default no city filter

        const pageSize = 25;
        const chartRef = useRef(null);
        const chartInst = useRef(null);

        const fromRef = useRef(null);
        const toRef = useRef(null);
        const fromFp = useRef(null);
        const toFp = useRef(null);

        // Chat state
        const [chatOpen, setChatOpen] = useState(false);
        const [chatMessages, setChatMessages] = useState([]);
        const [chatInput, setChatInput] = useState("");
        const [chatLoading, setChatLoading] = useState(false);

        useEffect(function () {
          if (window.flatpickr && fromRef.current && toRef.current) {
            try { if (fromFp.current) fromFp.current.destroy(); } catch (e) { }
            try { if (toFp.current) toFp.current.destroy(); } catch (e) { }
            fromFp.current = window.flatpickr(fromRef.current, {
              dateFormat: "Y-m-d",
              defaultDate: dateFrom,
              allowInput: true,
              onChange: function (selectedDates, str) {
                if (selectedDates && selectedDates.length) {
                  const iso = formatDateISO(selectedDates[0]);
                  setDateFrom(iso);
                  try { if (toFp.current) toFp.current.set('minDate', iso); } catch (e) { }
                  if (dateTo && new Date(iso) > new Date(dateTo)) {
                    setDateTo(iso);
                    try { if (toFp.current) toFp.current.setDate(iso, true); } catch (e) { }
                  }
                }
              }
            });
            toFp.current = window.flatpickr(toRef.current, {
              dateFormat: "Y-m-d",
              defaultDate: dateTo,
              allowInput: true,
              onChange: function (selectedDates, str) {
                if (selectedDates && selectedDates.length) {
                  const iso = formatDateISO(selectedDates[0]);
                  setDateTo(iso);
                  try { if (fromFp.current) fromFp.current.set('maxDate', iso); } catch (e) { }
                  if (dateFrom && new Date(iso) < new Date(dateFrom)) {
                    setDateFrom(iso);
                    try { if (fromFp.current) fromFp.current.setDate(iso, true); } catch (e) { }
                  }
                }
              }
            });
            try { if (fromFp.current) fromFp.current.set('maxDate', dateTo); if (toFp.current) toFp.current.set('minDate', dateFrom); } catch (e) { }
          }
          loadLatest();
          return function () { try { if (fromFp.current) fromFp.current.destroy(); } catch (e) { } try { if (toFp.current) toFp.current.destroy(); } catch (e) { } };
          // eslint-disable-next-line
        }, []);

        useEffect(function () {
          try { if (fromFp.current && dateFrom) fromFp.current.setDate(dateFrom, false); } catch (e) { }
          try { if (toFp.current && dateTo) toFp.current.setDate(dateTo, false); } catch (e) { }
          try { if (fromFp.current) fromFp.current.set('maxDate', dateTo); } catch (e) { }
          try { if (toFp.current) toFp.current.set('minDate', dateFrom); } catch (e) { }
        }, [dateFrom, dateTo]);

        // When region changes, reset location to "All locations"
        useEffect(() => {
          setSelectedLocation("All locations");
        }, [selectedRegion]);

        async function runForRange() {
          setLoading(true);
          setRows([]);
          setSummary({ rows: 0, flagged_rows: 0, files: [], end_date: null });
          setReasonsCount({});
          setRiskCounts({ "Low": 0, "Low Medium": 0, "Medium": 0, "Medium High": 0, "High": 0 });
          try {
            const start = encodeURIComponent(dateFrom);
            const end = encodeURIComponent(dateTo);
            // include selected region & city if provided
            let url = API_BASE + "/run?start=" + start + "&end=" + end + "&full=true";
            if (selectedRegion) {
              url += "&region=" + encodeURIComponent(selectedRegion);
            }
            if (selectedLocation && selectedLocation !== "All locations") {
              // send backend-aware partition token (use mapping)
              const mapForRegion = LOCATION_QUERY_VALUE[selectedRegion] || {};
              const queryCity = mapForRegion[selectedLocation] || selectedLocation;
              url += "&city=" + encodeURIComponent(queryCity);
            }
            let r = await fetch(url, { method: 'GET' });
            if (!r.ok) { const txt = await r.text(); throw new Error("API returned " + r.status + ": " + txt); }
            let js = await r.json();

            const totalRows = (typeof js.aggregated_unique_persons === 'number') ? js.aggregated_unique_persons
              : (typeof js.rows === 'number') ? js.rows : 0;
            const totalFlagged = (typeof js.flagged_rows === 'number') ? js.flagged_rows : 0;
            const files = js.files || [];

            const sample = Array.isArray(js.flagged_persons) && js.flagged_persons.length ? js.flagged_persons
              : (Array.isArray(js.sample) ? js.sample : []);
            setRows(sample);

            setSummary({ rows: totalRows, flagged_rows: totalFlagged, files: files, end_date: formatDateISO(new Date(dateTo)) });

            if (js.reasons_count && Object.keys(js.reasons_count).length > 0) {
              setReasonsCount(js.reasons_count);
            } else {
              computeReasonsAndRisks(sample);
            }
            if (js.risk_counts && Object.keys(js.risk_counts).length > 0) {
              const all = { "Low": 0, "Low Medium": 0, "Medium": 0, "Medium High": 0, "High": 0 };
              Object.keys(js.risk_counts).forEach(k => { all[k] = js.risk_counts[k]; });
              setRiskCounts(all);
            } else {
              computeReasonsAndRisks(sample);
            }
            setPage(1);
          } catch (err) {
            alert("Error: " + err.message);
            console.error(err);
          } finally {
            setLoading(false);
          }
        }

        function pushChatMessage(msg) {
          setChatMessages(prev => [...prev, msg]);
          setTimeout(() => {
            const el = document.querySelector('.chat-body');
            if (el) el.scrollTop = el.scrollHeight;
          }, 50);
        }

        function computeReasonsAndRisks(dataRows) {
          var counts = {};
          var rcounts = { "Low": 0, "Low Medium": 0, "Medium": 0, "Medium High": 0, "High": 0 };
          (dataRows || []).forEach(function (r) {
            const reasonsField = r.Reasons || r.DetectedScenarios || r.Detected || null;
            if (reasonsField) {
              var parts = String(reasonsField).split(";").map(function (s) { return s.trim(); }).filter(Boolean);
              parts.forEach(function (p) { counts[p] = (counts[p] || 0) + 1; });
            }
            var rl = getRiskLabelForRow(r);
            if (rl && rcounts[rl] !== undefined) {
              rcounts[rl] += 1;
            } else if (rl) {
              rcounts[rl] = (rcounts[rl] || 0) + 1;
            } else {
              rcounts["Low"] += 1;
            }
          });
          setReasonsCount(counts);
          setRiskCounts(rcounts);
        }

        async function loadLatest() {
          setLoading(true);
          try {
            // run for yesterday (to match backend's default behaviour)
            var d = new Date();
            d.setDate(d.getDate() - 1);
            var yesterday = formatDateISO(d);
            setDateFrom(yesterday);
            setDateTo(yesterday);

            const start = encodeURIComponent(yesterday);
            const end = encodeURIComponent(yesterday);
            let url = API_BASE + "/run?start=" + start + "&end=" + end + "&full=true";
            if (selectedRegion) {
              url += "&region=" + encodeURIComponent(selectedRegion);
            }
            if (selectedLocation && selectedLocation !== "All locations") {
              const mapForRegion = LOCATION_QUERY_VALUE[selectedRegion] || {};
              const queryCity = mapForRegion[selectedLocation] || selectedLocation;
              url += "&city=" + encodeURIComponent(queryCity);
            }
            let r = await fetch(url, { method: 'GET' });
            if (!r.ok) { const txt = await r.text(); throw new Error("API returned " + r.status + ": " + txt); }
            let js = await r.json();

            const totalRows = (typeof js.aggregated_unique_persons === 'number') ? js.aggregated_unique_persons
              : (typeof js.rows === 'number') ? js.rows : 0;
            const totalFlagged = (typeof js.flagged_rows === 'number') ? js.flagged_rows : 0;
            const files = js.files || [];

            const sample = Array.isArray(js.sample) ? js.sample : (Array.isArray(js.flagged_persons) ? js.flagged_persons : []);
            setRows(sample);
            setSummary({ rows: totalRows, flagged_rows: totalFlagged, files: files, end_date: yesterday });

            if (js.reasons_count && Object.keys(js.reasons_count).length > 0) {
              setReasonsCount(js.reasons_count);
            } else {
              computeReasonsAndRisks(sample);
            }
            if (js.risk_counts && Object.keys(js.risk_counts).length > 0) {
              const all = { "Low": 0, "Low Medium": 0, "Medium": 0, "Medium High": 0, "High": 0 };
              Object.keys(js.risk_counts).forEach(k => { all[k] = js.risk_counts[k]; });
              setRiskCounts(all);
            } else {
              computeReasonsAndRisks(sample);
            }
            setPage(1);
          } catch (err) {
            alert("Error: " + err.message);
            console.error(err);
          } finally {
            setLoading(false);
          }
        }

        function getRiskLabelForRow(r) {
          if (!r) return null;
          var rl = r.RiskLevel || r.Risk || null;
          if (rl) return String(rl);
          if (r.RiskScore !== undefined && r.RiskScore !== null) {
            const mapNum = { 1: "Low", 2: "Low Medium", 3: "Medium", 4: "Medium High", 5: "High" };
            return mapNum[String(r.RiskScore)] || null;
          }
          if (r.AnomalyScore !== undefined && r.AnomalyScore !== null) {
            if (r.AnomalyScore >= 5) return "High";
            if (r.AnomalyScore >= 4) return "Medium High";
            if (r.AnomalyScore >= 3) return "Medium";
            if (r.AnomalyScore >= 2) return "Low Medium";
            return "Low";
          }
          return null;
        }

        function buildChart(rcounts) {
          var labels = RISK_LABELS;
          var values = labels.map(l => rcounts && rcounts[l] ? rcounts[l] : 0);
          var colors = labels.map(l => {
            if (selectedRiskFilter) {
              return (l === selectedRiskFilter) ? RISK_COLORS[l] : '#e6edf3';
            } else {
              return RISK_COLORS[l] || '#cccccc';
            }
          });

          var ctx = chartRef.current && chartRef.current.getContext ? chartRef.current.getContext('2d') : null;
          if (!ctx) return;
          try { if (chartInst.current) chartInst.current.destroy(); } catch (e) { }

          chartInst.current = new Chart(ctx, {
            type: 'line',
            data: {
              labels: labels,
              datasets: [{
                label: 'Flagged by Risk Level',
                data: values,
                borderColor: '#2563eb',
                backgroundColor: 'rgba(37,99,235,0.2)',
                fill: true,
                tension: 0.3,
                pointBackgroundColor: colors,
                pointRadius: 5,
                pointHoverRadius: 7
              }]
            },
            options: {
              responsive: true,
              maintainAspectRatio: false,
              plugins: {
                legend: { display: false },
                tooltip: {
                  callbacks: {
                    label: function (context) {
                      return context.parsed.y + ' cases';
                    }
                  }
                }
              },
              onClick: function (evt, elements) {
                if (elements && elements.length > 0) {
                  var idx = elements[0].index;
                  var label = this.data.labels[idx];
                  handleRiskBarClick(label);
                }
              },
              scales: {
                y: { beginAtZero: true, ticks: { precision: 0 } }
              }
            }
          });

        }

        useEffect(function () {
          buildChart(riskCounts);
        }, [riskCounts, selectedRiskFilter]);

        // Filtering & pagination
        var filtered = (rows || []).filter(function (r) {
          var hay = (sanitizeName(r) + " " + (r.EmployeeID || "") + " " + (r.CardNumber || "") + " " + (r.Reasons || r.DetectedScenarios || "")).toLowerCase();
          var textOk = !filterText || hay.indexOf(filterText.toLowerCase()) !== -1;
          var reasonOk = !selectedReason || (r.Reasons && ((";" + String(r.Reasons) + ";").indexOf(selectedReason) !== -1)) || (r.DetectedScenarios && ((";" + String(r.DetectedScenarios) + ";").indexOf(selectedReason) !== -1));
          var riskOk = true;
          if (selectedRiskFilter) {
            var rl = getRiskLabelForRow(r);
            if (!rl) { riskOk = false; }
            else riskOk = (String(rl) === String(selectedRiskFilter));
          }
          return textOk && reasonOk && riskOk;
        })
          .sort(function (a, b) {
            var va = Number(a.ViolationDaysLast90 || a.ViolationDaysLast_90 || 0);
            var vb = Number(b.ViolationDaysLast90 || b.ViolationDaysLast_90 || 0);
            if (isNaN(va)) va = 0;
            if (isNaN(vb)) vb = 0;
            if (vb !== va) return vb - va;
            return (sanitizeName(a) || "").localeCompare(sanitizeName(b) || "");
          });

        var totalPages = Math.max(1, Math.ceil(filtered.length / pageSize));
        var pageRows = filtered.slice((page - 1) * pageSize, page * pageSize);

        function exportFiltered() { downloadCSV(filtered, "trend_filtered_export.csv"); }

        function onReasonClick(reason) {
          if (!reason) { setSelectedReason(""); return; }
          if (selectedReason === reason) setSelectedReason(""); else setSelectedReason(reason);
          setPage(1);
        }

        async function openEvidence(row) {
          setModalRow(row);
          setModalDetails(null);
          setModalLoading(true);
          try {
            const q = encodeURIComponent(row.EmployeeID || row.person_uid || "");
            const resp = await fetch(API_BASE + "/record?employee_id=" + q);
            if (!resp.ok) { const txt = await resp.text(); throw new Error("record failed: " + resp.status + " - " + txt); }
            const js = await resp.json();


            // const details = { aggregated_rows: js.aggregated_rows || [], raw_swipe_files: js.raw_swipe_files || [], raw_swipes: js.raw_swipes || [] };
            // setModalDetails(details);

            const details = { aggregated_rows: js.aggregated_rows || [], raw_swipe_files: js.raw_swipe_files || [], raw_swipes: js.raw_swipes || [] };
            setModalDetails(details);

            // If backend returned contact info, merge it into the modalRow so the UI shows the email reliably
            try {
              const first = (details.aggregated_rows && details.aggregated_rows[0]) ? details.aggregated_rows[0] : null;
              const newEmail = first && (first.EmployeeEmail || first.Email || first.EmailAddress || first.WorkEmail || first.EMail);
              if (newEmail) {
                setModalRow(prev => Object.assign({}, prev || {}, { EmployeeEmail: newEmail, Email: newEmail }));
              } else {
                // also attempt to set from personnel keys if present
                const fallback = first && (first.ManagerEmail || first.EmailAddress || null);
                if (fallback) {
                  setModalRow(prev => Object.assign({}, prev || {}, { EmployeeEmail: fallback, Email: fallback }));
                }
              }
            } catch (err) {
              // swallow — UI will keep showing whatever we had
            }


          } catch (e) {
            alert("Failed loading details: " + e.message);
            console.error(e);
          } finally { setModalLoading(false); }
        }

        function closeModal() { setModalRow(null); setModalDetails(null); }

        // helper to render overlap
        function renderOverlapCell(r) {
          var ov = r.OverlapWith || r.swipe_overlap || r.overlap_with || null;
          if (ov && typeof ov === 'string') {
            var parts = ov.split(";").map(function (s) { return s.trim(); }).filter(Boolean);
            if (parts.length === 0) return <span className="muted">—</span>;
            return <span className="pill" title={ov}>{parts.length} overlap</span>;
          }
          return <span className="muted">—</span>;
        }

        function renderReasonChips(reasonText) {
          if (!reasonText) return <span className="muted">—</span>;
          const parts = String(reasonText).split(";").map(s => s.trim()).filter(Boolean);
          return parts.map((p, idx) => (<span key={idx} className="pill" title={SCENARIO_EXPLANATIONS[p] || p}>{p}</span>));
        }

        function renderReasonExplanations(reasonText) {
          if (!reasonText) return <div className="muted">No flags</div>;
          const parts = String(reasonText).split(";").map(s => s.trim()).filter(Boolean);
          return (
            <div>
              {parts.map((p, idx) => (
                <div key={idx} className="why-item" style={{ marginBottom: 8 }}>
                  <b>{p}</b>
                  <div className="small">{SCENARIO_EXPLANATIONS[p] || "No explanation available."}</div>
                </div>
              ))}
            </div>
          );
        }

        async function sendChat(qText, opts = { top_k: 5 }) {
          if (!qText || !qText.toString().trim()) return;
          const text = qText.toString().trim();
          pushChatMessage({ who: 'user', text });
          setChatInput("");
          setChatLoading(true);
          try {
            const payload = Object.assign({ q: text }, opts);
            const resp = await fetch(API_BASE + "/chatbot/query", {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify(payload)
            });
            if (!resp.ok) {
              const t = await resp.text().catch(() => '');
              throw new Error("Server: " + resp.status + " " + t);
            }
            const js = await resp.json();
            const answer = js.answer || js.answer_text || js.result || "No answer returned.";
            const evidence = Array.isArray(js.evidence) ? js.evidence : (js.evidence ? [js.evidence] : []);
            pushChatMessage({ who: 'bot', text: answer, evidence });
          } catch (err) {
            pushChatMessage({ who: 'bot', text: "Error: " + err.message, evidence: [] });
            console.error("chat error", err);
          } finally {
            setChatLoading(false);
            setTimeout(() => {
              const el = document.querySelector('.chat-body');
              if (el) el.scrollTop = el.scrollHeight;
            }, 80);
          }
        }

        const QUICK_PROMPTS = [
          "Who is high risk today",
          "Who is low risk today",
          "Show me 320172 last 90 days",
          "Trend details for today — top reasons",
          "Explain repeated_short_breaks"
        ];
        function useQuickPrompt(q) {
          setChatOpen(true);
          sendChat(q);
        }

        // Swipe timeline rendering uses DAY_BOUNDARY_HOUR = 0 to match backend date assignment
        function renderSwipeTimeline(details, modalRow) {
          if (!details || !details.raw_swipes || details.raw_swipes.length === 0) {
            return <div className="muted">No raw swipe evidence available (person not flagged or raw file missing).</div>;
          }

          const all = details.raw_swipes.map(r => {
            const obj = Object.assign({}, r);
            try { obj.__ts = makeLocalDateFromRow(obj); } catch (e) { obj.__ts = null; }

            let gap = null;
            if (obj.SwipeGapSeconds !== undefined && obj.SwipeGapSeconds !== null) {
              gap = Number(obj.SwipeGapSeconds);
              if (isNaN(gap)) gap = null;
            } else if (obj.SwipeGap) {
              try {
                const parts = String(obj.SwipeGap).split(':').map(p => Number(p));
                if (parts.length === 3 && parts.every(p => !isNaN(p))) gap = parts[0] * 3600 + parts[1] * 60 + parts[2];
              } catch (e) { gap = null; }
            }
            obj.__gap = gap;
            obj.__zone_l = String((obj.Zone || '')).toLowerCase();

            // Prefer backend-provided date fields (DateOnly) or computed timestamp
            if (obj.__ts) {
              obj.__logical_date = logicalDateForTs(obj.__ts, DAY_BOUNDARY_HOUR);
            } else if (obj.DateOnly) {
              // DateOnly may be a string or object; coerce to YYYY-MM-DD
              obj.__logical_date = String(obj.DateOnly).slice(0, 10);
            } else if (obj.Date) {
              obj.__logical_date = String(obj.Date).slice(0, 10);
            } else {
              obj.__logical_date = null;
            }
            return obj;
          }).sort((a, b) => {
            // Primary sort: parsed timestamp if present
            if (a.__ts && b.__ts) return a.__ts - b.__ts;
            if (a.__ts) return -1;
            if (b.__ts) return 1;
            // Fallback: use DateOnly + Time or Date+Time strings
            const ka = (a.DateOnly || a.Date || '') + ' ' + (a.Time || '');
            const kb = (b.DateOnly || b.Date || '') + ' ' + (b.Time || '');
            return ka.localeCompare(kb);
          });

          // flags: dayStart for first row OR when logical date changes between rows
          const flags = new Array(all.length).fill(null).map(() => ({ dayStart: false, outReturn: false }));
          for (let i = 0; i < all.length; i++) {
            const cur = all[i];
            const prev = all[i - 1];
            const curDate = cur.__logical_date || (cur.DateOnly ? String(cur.DateOnly).slice(0, 10) : (cur.Date ? String(cur.Date).slice(0, 10) : null));
            const prevDate = prev ? (prev.__logical_date || (prev.DateOnly ? String(prev.DateOnly).slice(0, 10) : (prev.Date ? String(prev.Date).slice(0, 10) : null))) : null;
            if (!prev || prevDate !== curDate) {
              flags[i].dayStart = true;
            }
          }

          const OUT_RETURN_GAP_SECONDS = 60 * 60;
          for (let i = 0; i < all.length - 1; i++) {
            const a = all[i], b = all[i + 1];
            const aZone = a.__zone_l || ''; const bZone = b.__zone_l || ''; const bGap = b.__gap || 0;
            if (aZone.includes('out of office') || aZone.includes('out_of_office') || aZone.includes('out of')) {
              if (!bZone.includes('out of office') && (bGap >= OUT_RETURN_GAP_SECONDS || (bGap === null && aZone.includes('out')))) {
                flags[i].outReturn = true; flags[i + 1].outReturn = true;
              }
            }
          }

          for (let i = 0; i < all.length; i++) {
            if (flags[i].dayStart) {
              all[i].__gap = 0;
            }
          }

          return (
            <div className="table-scroll">
              <table className="evidence-table" role="table" aria-label="Swipe timeline">
                <thead>
                  <tr>
                    <th>Employee Name</th>
                    <th>Employee ID</th>
                    <th>Card</th>
                    <th>Date</th>
                    <th>Time</th>
                    <th>SwipeGap</th>
                    <th>Door</th>
                    <th>Direction</th>
                    <th>Zone</th>
                    <th>Note</th>
                  </tr>
                </thead>
                <tbody>
                  {all.map((rObj, idx) => {
                    const r = rObj || {};
                    const g = (r.__gap !== undefined && r.__gap !== null) ? Number(r.__gap) : null;
                    const isDayStart = flags[idx].dayStart;
                    const gapFormatted = (isDayStart)
                      ? formatSecondsToHmsJS(0)
                      : (
                        (r.SwipeGap && String(r.SwipeGap).trim())
                          ? String(r.SwipeGap)
                          : (g !== null && g !== undefined)
                            ? formatSecondsToHmsJS(g)
                            : "-"
                      );

                    // display date: prefer logical (backend date), then DateOnly, then Date
                    const displayDate = r.__logical_date || (r.DateOnly ? String(r.DateOnly).slice(0, 10) : (r.Date ? String(r.Date).slice(0, 10) : '-'));
                    const displayTime = r.Time || (r.__ts ? r.__ts.toTimeString().slice(0, 8) : '-');

                    const cls = [];
                    if (isDayStart) cls.push('row-day-start');
                    if (flags[idx].outReturn) cls.push('row-out-return');
                    const rowStyle = isDayStart ? { background: '#e6ffed' } : {};
                    let extraNote = "";
                    try {
                      const originalDate = r.Date ? String(r.Date).slice(0, 10) : null;
                      const logical = r.__logical_date || null;
                      if (originalDate && logical && originalDate !== logical) {
                        extraNote = `Orig: ${originalDate}`;
                        if ((String(r.Direction || '').toLowerCase().indexOf('out') !== -1)) {
                          extraNote += " — Out";
                        }
                      }
                    } catch (e) { extraNote = ""; }

                    return (
                      <tr key={idx} className={cls.join(' ')} style={rowStyle}>
                        <td className="small">{r.EmployeeName || '-'}</td>
                        <td className="small">{r.EmployeeID || '-'}</td>
                        <td className="small">{r.CardNumber || r.Card || '-'}</td>
                        <td className="small">{displayDate}</td>
                        <td className="small">{displayTime}</td>
                        <td className="small">{gapFormatted}</td>
                        <td className="small" style={{ minWidth: 160 }}>{r.Door || '-'}</td>
                        <td className="small">{r.Direction || '-'}</td>
                        <td className="small">{r.Zone || '-'}</td>
                        <td className="small">{r.Note || '-'}{r._source ? <span className="muted"> ({r._source})</span> : null}
                          {extraNote ? <div className="muted" style={{ fontSize: 11, marginTop: 4 }}>{extraNote}</div> : null}
                        </td>
                      </tr>
                    );
                  })}
                </tbody>
              </table>
            </div>
          );
        }

        function handleRiskBarClick(label) {
          if (!label) return;
          if (selectedRiskFilter === label) {
            setSelectedRiskFilter("");
          } else {
            setSelectedRiskFilter(label);
          }
          setPage(1);
        }

        function clearRiskFilter() {
          setSelectedRiskFilter("");
        }

        var rowsCount = (summary && typeof summary.rows === 'number') ? summary.rows : (rows ? rows.length : 0);
        var flaggedCount = (summary && typeof summary.flagged_rows === 'number') ? summary.flagged_rows : (rows ? rows.filter(function (r) { return !!(r.Reasons || r.DetectedScenarios); }).length : 0);
        var flaggedPct = rowsCount ? Math.round((flaggedCount * 100) / (rowsCount || 1)) : 0;

        // helper to get display label for current region
        function regionDisplayLabel(key) {
          if (!key) return '';
          return (REGION_OPTIONS[key] && REGION_OPTIONS[key].label) ? REGION_OPTIONS[key].label : key.toUpperCase();
        }

        return (
          <div className="container" aria-live="polite">
            {loading && (
              <div className="spinner-overlay" role="status" aria-label="Loading">
                <div className="spinner-box">
                  <div className="spinner" />
                  <div style={{ fontWeight: 700 }}>Loading…</div>
                </div>
              </div>
            )}

            <div className="topbar" role="banner">
              <div className="wu-brand" aria-hidden={false}>
                <div className="wu-logo">WU</div>
                <div className="title-block">
                  <h1>Western Union — Trend Analysis</h1>
                  <p style={{ margin: 0, fontSize: 13 }}>
                    {regionDisplayLabel(selectedRegion)} {selectedLocation && selectedLocation !== "All locations" ? "— " + selectedLocation : ""}
                  </p>
                </div>
              </div>

              <div className="header-actions" role="region" aria-label="controls">
                <div className="control">
                  <label className="small" htmlFor="fromDate">From</label>
                  <input id="fromDate" ref={fromRef} className="date-input" type="text" placeholder="YYYY-MM-DD" />
                </div>

                <div className="control">
                  <label className="small" htmlFor="toDate">To</label>
                  <input id="toDate" ref={toRef} className="date-input" type="text" placeholder="YYYY-MM-DD" />
                </div>

                <button className="btn-primary" onClick={runForRange} disabled={loading}>Run</button>
                <button className="btn-ghost" onClick={loadLatest} disabled={loading}>Load latest</button>
              </div>
            </div>

            <div className="card-shell">
              <div className="cards" aria-hidden={loading}>
                <div className="card" title="Rows analysed">
                  <div className="card-content">
                    <div className="card-text">
                      <h3>{(rowsCount !== undefined && rowsCount !== null) ? rowsCount.toLocaleString() : 0}</h3>
                      <p>Rows analysed</p>
                    </div>
                  </div>
                </div>
                <div className="card card-flagged" title="Flagged rows">
                  <div className="card-content">
                    <div className="card-text">
                      <h3>{(flaggedCount !== undefined && flaggedCount !== null) ? flaggedCount.toLocaleString() : 0}</h3>
                      <p>Flagged rows</p>
                    </div>
                  </div>
                </div>
                <div className="card card-rate" title="Flagged rate">
                  <div className="card-content">
                    <div className="card-text">
                      <h3>{flaggedPct}%</h3>
                      <p>Flagged rate</p>
                    </div>
                  </div>
                </div>
              </div>

              <div className="main">
                <div className="left">
                  <div className="chart-wrap" aria-label="Risk level chart">
                    <canvas ref={chartRef}></canvas>
                  </div>

                  <div style={{ display: 'flex', alignItems: 'center', gap: 8, marginTop: 6 }}>
                    <input placeholder="Search name, employee id, card or reason..." value={filterText} onChange={function (e) { setFilterText(e.target.value); setPage(1); }} style={{ flex: 1, padding: 10, borderRadius: 6, border: '1px solid #e6edf3' }} />
                    <div className="muted">Showing {filtered.length} / {rows.length} rows</div>
                    <button className="small-button" onClick={exportFiltered}>Export filtered</button>
                    {selectedRiskFilter ? <button className="small-button" onClick={clearRiskFilter}>Clear risk filter</button> : null}
                  </div>

                  <div style={{ marginTop: 10 }} className="table-scroll" role="region" aria-label="results table">
                    <table>
                      <thead>
                        <tr>
                          <th>Employee</th>
                          <th className="small">ID</th>
                          <th className="small">Card</th>
                          <th className="small">Date</th>
                          <th className="small">Duration</th>
                          <th className="small">ViolationDaysLast90</th>
                          <th className="small">Reasons</th>
                          <th className="small">Evidence</th>
                        </tr>
                      </thead>
                      <tbody>
                        {pageRows.map(function (r, idx) {
                          var empName = sanitizeName(r);
                          var displayDate = safeDateDisplay(r.DisplayDate || r.Date || r.DateOnly || r.FirstSwipe || r.LastSwipe);
                          // var durText = r.Duration || (r.DurationMinutes ? Math.round(r.DurationMinutes) + " min" : (r.DurationSeconds ? formatSecondsToHmsJS(r.DurationSeconds) : ""));

                          var durText = r.Duration
                            || (r.DurationSeconds ? formatSecondsToHmJS(Number(r.DurationSeconds))
                              : (r.DurationMinutes ? formatSecondsToHmJS(Number(r.DurationMinutes) * 60) : ""));


                          var flagged = r.Reasons && String(r.Reasons).trim();
                          return (
                            <tr key={idx} className={flagged ? "flagged-row" : ""}>
                              <td className="row-click" onClick={function () { openEvidence(r); }}>{empName || <span className="muted">—</span>}</td>
                              <td className="small">{r.EmployeeID || ""}</td>
                              <td className="small">{r.CardNumber || ""}</td>
                              <td className="small">{displayDate}</td>
                              <td className="small">{durText}</td>
                              <td className="small">
                                {(r.ViolationDaysLast90 !== undefined && r.ViolationDaysLast90 !== null && r.ViolationDaysLast90 !== "")
                                  ? (Number(r.ViolationDaysLast90).toString())
                                  : ((r.ViolationDaysLast_90 !== undefined && r.ViolationDaysLast_90 !== null && r.ViolationDaysLast_90 !== "")
                                    ? String(r.ViolationDaysLast_90)
                                    : ((r.ViolationDays !== undefined && r.ViolationDays !== null) ? String(r.ViolationDays) : "")
                                  )
                                }
                              </td>
                              <td className="small">{renderReasonChips(r.Reasons || r.DetectedScenarios)}</td>
                              <td className="small">
                                <button className="evidence-btn" onClick={function () { openEvidence(r); }}>Evidence</button>
                              </td>
                            </tr>
                          );
                        })}
                      </tbody>
                    </table>
                  </div>

                  <div style={{ display: 'flex', gap: 8, alignItems: 'center', marginTop: 10 }}>
                    <button onClick={function () { setPage(function (p) { return Math.max(1, p - 1); }); }} disabled={page <= 1}>Prev</button>
                    <div className="muted">Page {page} / {totalPages}</div>
                    <button onClick={function () { setPage(function (p) { return Math.min(totalPages, p + 1); }); }} disabled={page >= totalPages}>Next</button>
                  </div>
                </div>

                <aside className="right" aria-label="side panel">

                  {/* NEW: Region & Location controls */}
                  <div className="sidebar-section" style={{ marginBottom: 12 }}>
                    <strong>Risk filters</strong>
                    <div className="small muted" style={{ marginTop: 6 }}>Select Region and Location to scope the run.</div>

                    <div style={{ display: 'flex', gap: 8, marginTop: 8, alignItems: 'center' }}>
                      <div style={{ flex: 1 }}>
                        <label className="small">Region</label>
                        <select
                          value={selectedRegion}
                          onChange={(e) => { setSelectedRegion(e.target.value); setPage(1); }}
                          style={{ width: '100%', padding: '6px 8px', borderRadius: 6, border: '1px solid #e2e8f0' }}
                        >
                          {Object.keys(REGION_OPTIONS).map(k => (
                            <option key={k} value={k}>{REGION_OPTIONS[k].label}</option>
                          ))}
                        </select>
                      </div>

                      <div style={{ flex: 1 }}>
                        <label className="small">Location</label>
                        <select
                          value={selectedLocation}
                          onChange={(e) => { setSelectedLocation(e.target.value); setPage(1); }}
                          style={{ width: '100%', padding: '6px 8px', borderRadius: 6, border: '1px solid #e2e8f0' }}
                        >
                          <option key="__all" value="All locations">All locations</option>
                          {(REGION_OPTIONS[selectedRegion] && REGION_OPTIONS[selectedRegion].partitions || []).map(loc => (
                            <option key={loc} value={loc}>{loc}</option>
                          ))}
                        </select>
                      </div>
                    </div>
                  </div>

                  {/* existing risk chips */}
                  <div className="sidebar-section">
                    <div className="risk-filter-list" style={{ marginTop: 8 }}>
                      {RISK_LABELS.map((lab) => {
                        const cnt = (riskCounts && riskCounts[lab]) ? riskCounts[lab] : 0;
                        const active = selectedRiskFilter === lab;
                        return (
                          <div key={lab} role="button" tabIndex={0} aria-pressed={active} className={"risk-chip " + (active ? "active" : "")} onClick={function () { handleRiskBarClick(lab); }} onKeyDown={function (e) { if (e.key === 'Enter' || e.key === ' ') { handleRiskBarClick(lab); } }}>
                            <div style={{ width: 10, height: 10, borderRadius: 999, background: RISK_COLORS[lab], boxShadow: '0 2px 6px rgba(0,0,0,0.08)' }}></div>
                            <div style={{ fontSize: 13 }}>{lab} <span className="muted" style={{ marginLeft: 6 }}>({cnt})</span></div>
                          </div>
                        );
                      })}
                    </div>

                    <div style={{ marginTop: 8 }}>
                      <button className="small-button" onClick={clearRiskFilter}>Clear risk filter</button>
                    </div>
                  </div>

                  <div className="sidebar-section" style={{ marginTop: 12 }}>
                    <strong>Top reasons summary</strong>
                    <div className="small muted" style={{ marginTop: 6 }}>Click a reason to filter the table by that reason. Click again to clear.</div>

                    <div style={{ marginTop: 8, display: 'flex', gap: 8 }}>
                      <input placeholder="Filter reason list..." value={reasonFilterText} onChange={function (e) { setReasonFilterText(e.target.value); }} style={{ flex: 1, padding: '6px 8px', borderRadius: 6, border: '1px solid #e2e8f0' }} />
                      <button className="small-button" onClick={function () { setSelectedReason(''); setReasonFilterText(''); }}>Clear</button>
                    </div>

                    <div style={{ marginTop: 8, maxHeight: 320, overflow: 'auto' }}>
                      {Object.keys(reasonsCount).length === 0 && <div className="muted">No flags found</div>}
                      {Object.entries(reasonsCount).sort(function (a, b) { return b[1] - a[1]; }).filter(function (kv) {
                        var name = kv[0];
                        if (!reasonFilterText) return true;
                        return name.toLowerCase().indexOf(reasonFilterText.toLowerCase()) !== -1;
                      }).slice(0, 50).map(function (kv) {
                        var name = kv[0], count = kv[1];
                        var active = selectedReason === name;
                        return (
                          <div key={name} style={{ display: 'flex', alignItems: 'center', justifyContent: 'space-between', gap: 8, marginBottom: 6 }}>
                            <button className={"chip " + (active ? "active" : "")} style={{ textAlign: 'left', flex: 1 }} onClick={function () { onReasonClick(name); }}>
                              {name}
                            </button>
                            <div style={{ minWidth: 48, textAlign: 'right' }} className="small"><b>{count}</b></div>
                          </div>
                        );
                      })}
                    </div>
                  </div>
                </aside>
              </div>
            </div>

            {modalRow &&
              <div className="modal" onClick={closeModal}>
                <div className="modal-inner" onClick={function (e) { e.stopPropagation(); }}>
                  <div className="modal-header">
                    <div className="header-content">
                      <div className="header-icon">
                        <i className="bi bi-clipboard2-data-fill"></i>
                      </div>
                      <div className="header-text">
                        <h3>Details — Evidence</h3>
                        <div className="header-subtitle small">Evidence & explanation for selected row</div>
                      </div>
                    </div>
                    <button className="close-btn" onClick={closeModal}>
                      <i className="bi bi-x-lg"></i>
                      Close
                    </button>
                  </div>
                  <div className="modal-body">
                    {modalLoading && (
                      <div className="loading-state">
                        <div className="loading-spinner"></div>
                        <span>Loading evidence…</span>
                      </div>
                    )}
                    <div className="modal-top" role="region" aria-label="evidence summary">
                      <div className="image-section">
                        <div className="image-container">
                          <div className="multi-color-border">
                            <div className="color-ring color-1"></div>
                            <div className="color-ring color-2"></div>
                            <div className="color-ring color-3"></div>
                            <div className="color-ring color-4"></div>
                            <div className="image-content">




                              {/* Improved modal image block — prefer ObjectID/GUID and try more fallbacks */}

{(modalDetails && modalDetails.aggregated_rows && modalDetails.aggregated_rows.length > 0) ? (
  (() => {
    const md = (modalDetails && modalDetails.aggregated_rows && modalDetails.aggregated_rows[0]) ? modalDetails.aggregated_rows[0] : {};
    // common keys that might contain an image path/url or a filename token
    const candidateImageKeys = ['imageUrl','image_url','ImageUrl','image','Image','img','imgUrl','ImagePath','Photo','PhotoUrl','EmployeePhoto'];
    let imgPath = candidateImageKeys.map(k => (md && md[k]) ? md[k] : null).find(Boolean) || null;

    // build a prioritized list of identifier tokens to try (prefer ObjectID / GUID)
    const idCandidates = [];
    if (md && md.ObjectID) idCandidates.push(String(md.ObjectID));
    if (md && md.ObjectId) idCandidates.push(String(md.ObjectId));
    if (md && md.GUID) idCandidates.push(String(md.GUID));
    if (md && md.GUID0) idCandidates.push(String(md.GUID0));
    // fallback to classic fields
    if (md && md.EmployeeID) idCandidates.push(String(md.EmployeeID));
    if (md && md.person_uid) idCandidates.push(String(md.person_uid));
    if (modalRow && modalRow.EmployeeID) idCandidates.push(String(modalRow.EmployeeID));
    if (modalRow && modalRow.person_uid) idCandidates.push(String(modalRow.person_uid));

    // Deduplicate preserving order
    const uniqIds = idCandidates.filter((v, i) => v && idCandidates.indexOf(v) === i);

    // If we didn't get an explicit image path, build one from the top id candidate
    if (!imgPath && uniqIds.length) {
      imgPath = `/employee/${encodeURIComponent(uniqIds[0])}/image`;
    }

    if (imgPath) {
      const imgSrc = resolveApiImageUrl(imgPath) || imgPath;
      return (
        <img
          className="modal-image"
          src={imgSrc}
          alt={sanitizeName(modalRow) || "Employee image"}
          onLoad={(e) => { try { console.info("employee image loaded:", e.target.src); } catch (err) {} }}
          
          
          
          
          onError={async (e) => {
  try {
    e.target.onerror = null;
    console.warn("image load failed for:", e.target.src);

    // Try a single cache-busted retry for the same URL, then try the API variants once each.
    const tryUrls = [];
    const original = e.target.src;
    tryUrls.push(original + (original.indexOf('?') === -1 ? '?cb=' + Date.now() : '&cb=' + Date.now()));

    // try both API variants for the top id candidates (constructed earlier as uniqIds)
    uniqIds.forEach(id => {
      if (!id) return;
      const a = resolveApiImageUrl(`/api/employees/${encodeURIComponent(id)}/image`);
      const b = resolveApiImageUrl(`/employee/${encodeURIComponent(id)}/image`);
      if (a && tryUrls.indexOf(a) === -1) tryUrls.push(a);
      if (b && tryUrls.indexOf(b) === -1) tryUrls.push(b);
    });

    // Try each URL (GET) until one returns image content-type and ok
    let found = null;
    for (const url of tryUrls) {
      try {
        const getr = await fetch(url, { method: 'GET', cache: 'no-store' });
        if (getr && getr.ok) {
          const ct = (getr.headers.get('content-type') || '').toLowerCase();
          if (ct.startsWith('image')) { found = url; break; }
          // if server returned a 200 but not an image, continue
        }
      } catch (err) {
        // fetch can fail due to CORS; continue to next candidate
      }
    }

    if (found) {
      e.target.src = found + (found.indexOf('?') === -1 ? ('?cb=' + Date.now()) : ('&cb=' + Date.now()));
      return;
    }

    // Final fallback: inline SVG placeholder
    const svg = '<svg xmlns="http://www.w3.org/2000/svg" width="160" height="160"><rect fill="#eef2f7" width="100%" height="100%"/><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle" fill="#64748b" font-size="18">No image</text></svg>';
    e.target.src = 'data:image/svg+xml;utf8,' + encodeURIComponent(svg);
  } catch (err) {
    try { e.target.style.display = 'none'; } catch (err2) {}
    console.error("image fallback error", err);
  }
}}

        />
      );
    } else {
      return <div className="modal-image-placeholder">No image</div>;
    }
  })()
) : (
  <div className="modal-image-placeholder">
    <i className="bi bi-person-square"></i>
    <span>No image</span>
  </div>
)}

                            </div>
                          </div>
                        </div>
                      </div>

                      <div className="modal-details">
                        <div className="details-header">
                          <div className="emp-info">
                            <div className="emp-name">
                              {sanitizeName(modalRow) || "—"}
                              <span
                                className="risk-badge"
                                style={{
                                  marginLeft: "12px",
                                  background:
                                    RISK_COLORS[modalRow.RiskLevel] ||
                                    RISK_COLORS[getRiskLabelForRow(modalRow)] ||
                                    RISK_COLORS["Low"],
                                }}
                              >
                                {modalRow.RiskLevel ||
                                  (modalRow.RiskScore ? "Score " + modalRow.RiskScore : "Low")}
                              </span>
                            </div>
                            <div className="emp-badge">
                              <i className="bi bi-person-badge"></i>
                              ID: {modalRow.EmployeeID || "—"}
                            </div>
                          </div>
                        </div>
                        <div className="details-grid">
                          <div className="detail-item">
                            <div className="detail-icon">
                              <i className="bi bi-credit-card"></i>
                            </div>
                            <div className="detail-content">
                              <label>Card Number</label>
                              <span>{modalRow.CardNumber || "—"}</span>
                            </div>
                          </div>
                          <div className="detail-item">
                            <div className="detail-icon">
                              <i className="bi bi-envelope"></i>
                            </div>
                            <div className="detail-content">
                              <label>Email</label>
                              <span>
                                {(
                                  (modalDetails && modalDetails.aggregated_rows && modalDetails.aggregated_rows[0] && (modalDetails.aggregated_rows[0].EmployeeEmail || modalDetails.aggregated_rows[0].Email))
                                  || modalRow.EmployeeEmail
                                  || modalRow.Email
                                  || (modalDetails && modalDetails.aggregated_rows && modalDetails.aggregated_rows[0] && (modalDetails.aggregated_rows[0].WorkEmail || modalDetails.aggregated_rows[0].EMail))
                                )
                                  ? (
                                    (modalDetails && modalDetails.aggregated_rows && modalDetails.aggregated_rows[0] && (modalDetails.aggregated_rows[0].EmployeeEmail || modalDetails.aggregated_rows[0].Email))
                                    || modalRow.EmployeeEmail
                                    || modalRow.Email
                                    || (modalDetails && modalDetails.aggregated_rows && modalDetails.aggregated_rows[0] && (modalDetails.aggregated_rows[0].WorkEmail || modalDetails.aggregated_rows[0].EMail))
                                  )
                                  : <span className="muted">—</span>
                                }
                              </span>

                            </div>
                          </div>
                          <div className="detail-item">
                            <div className="detail-icon">
                              <i className="bi bi-calendar-date"></i>
                            </div>
                            <div className="detail-content">
                              <label>Date</label>
                              <span>{safeDateDisplay(modalRow.DisplayDate || modalRow.Date || modalRow.DateOnly || modalRow.FirstSwipe)}</span>
                            </div>
                          </div>
                          <div className="detail-item">
                            <div className="detail-icon">
                              <i className="bi bi-clock"></i>
                            </div>
                            <div className="detail-content">
                              <label>Duration</label>

                              <span className="duration-badge">
                                {modalRow.Duration
                                  || (modalRow.DurationSeconds ? formatSecondsToHmJS(Number(modalRow.DurationSeconds))
                                    : (modalRow.DurationMinutes ? formatSecondsToHmJS(Number(modalRow.DurationMinutes) * 60) : "—"))}
                              </span>


                            </div>
                            <div style={{ marginTop: 8, textAlign: 'right' }}>
                              <div className="muted">Violation days (90d)</div>
                              <div style={{ fontWeight: 700 }}>
                                {(modalRow.ViolationDaysLast90 !== undefined && modalRow.ViolationDaysLast90 !== null)
                                  ? modalRow.ViolationDaysLast90
                                  : 0}
                              </div>
                            </div>
                          </div>
                        </div>
                      </div>

                      <div className="modal-reasons">
                        <div className="explanation-section" style={{ marginTop: 12 }}>
                          <div style={{ fontWeight: 700 }}>Explanation</div>
                          <div style={{
                            marginTop: 8,
                            maxHeight: 160,
                            overflow: 'auto',
                            background: '#fff',
                            border: '1px solid #eef2f7',
                            padding: 8,
                            borderRadius: 6
                          }}>
                            {(modalRow.Explanation || modalRow.ViolationExplanation)
                              ? <div style={{ whiteSpace: 'pre-wrap' }}>{modalRow.Explanation || modalRow.ViolationExplanation}</div>
                              : <div className="muted">No explanation provided.</div>}

                          </div>
                        </div>
                        <div className="reasons-section">
                          <div className="section-title">
                            <i className="bi bi-list-check"></i>
                            Reasons Flagged
                          </div>
                          <div className="reasons-list">
                            {renderReasonChips(modalRow.Reasons || modalRow.DetectedScenarios)}
                          </div>
                        </div>
                      </div>
                    </div>

                    <div className="evidence-section">
                      <div className="section-header">
                        <i className="bi bi-folder2-open"></i>
                        <h4>Available Evidence Files</h4>
                      </div>
                      <div className="files-container">
                        {modalDetails && modalDetails.raw_swipe_files && modalDetails.raw_swipe_files.length > 0 ? (
                          <div className="files-list">
                            {modalDetails.raw_swipe_files.map((f, i) => (
                              <div key={i} className="file-item">
                                <i className="bi bi-file-earmark-text"></i>
                                <span className="file-name">{f}</span>
                                <button
                                  className="download-btn"
                                  onClick={function () { window.location = API_BASE + "/swipes/" + encodeURIComponent(f); }}
                                >
                                  <i className="bi bi-download"></i>
                                  Download
                                </button>
                              </div>
                            ))}
                          </div>
                        ) : (
                          <div className="no-files">
                            <i className="bi bi-folder-x"></i>
                            <span>No raw swipe files found for this person/date.</span>
                          </div>
                        )}
                      </div>
                    </div>

                    <div className="timeline-section">
                      <div className="section-header">
                        <i className="bi bi-clock-history"></i>
                        <h4>Swipe Timeline</h4>
                        <span className="subtitle">Filtered for this person/date</span>
                      </div>
                      <div className="timeline-content">
                        {modalDetails ? renderSwipeTimeline(modalDetails, modalRow) : (
                          <div className="loading-timeline">
                            <i className="bi bi-hourglass-split"></i>
                            <span>Evidence not loaded yet.</span>
                          </div>
                        )}
                      </div>
                    </div>

                    <div className="raw-json-section">
                      <label className="toggle-label">
                        <input
                          type="checkbox"
                          id="showraw"
                          onChange={function (e) {
                            const el = document.getElementById('rawpayload');
                            if (el) el.style.display = e.target.checked ? 'block' : 'none';
                          }}
                        />
                        <span className="toggle-slider"></span>
                        <span className="toggle-text">
                          <i className="bi bi-code-slash"></i>
                          Show raw aggregated JSON
                        </span>
                      </label>
                      <div id="rawpayload" className="raw-json" style={{ display: 'none' }}>
                        <pre>{JSON.stringify(modalRow, null, 2)}</pre>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            }

            <button className="chat-fab" title="Ask Trend Details (Ask Me )" onClick={() => setChatOpen(true)} aria-label="Open chat">
              <span className="meta-icon"><img src="chat-bot.png" alt="" /></span>
            </button>


            {chatOpen && (
              <div className="chat-modal" role="dialog" aria-modal="true" aria-label="Trend Chatbot">
                <div className="chat-header">
                  <div style={{ display: 'flex', alignItems: 'center', gap: 8 }}>
                    <div style={{ width: 36, height: 36, borderRadius: 8, background: '#', display: 'flex', alignItems: 'center', justifyContent: 'center', color: '#2563eb', fontWeight: 800 }}><img src="chat-bot.png" alt="" style={{ width: 36, height: 36, }} /></div>
                    <div>
                      <div className="title">Ask me — Trend Details</div>
                      <div style={{ fontSize: 12, opacity: 0.85 }}>Ask trend & risk questions</div>
                    </div>
                  </div>
                  <div style={{ marginLeft: 'auto' }}>
                    <button className="small-button bot-close" onClick={() => { setChatOpen(false); }}>Close</button>
                  </div>
                </div>

                <div className="chat-body">
                  {chatMessages.length === 0 && (
                    <div style={{ color: '#64748b', fontSize: 13 }}>
                      Hi — ask about trends (e.g. "Who is high risk today"). Use the quick prompts below.
                    </div>
                  )}
                  {chatMessages.map((m, i) => (
                    <div key={i} style={{ display: 'block' }}>
                      <div className={"chat-bubble " + (m.who === 'user' ? 'user' : 'bot')}>
                        {m.text}
                        {m.who === 'bot' && m.evidence && m.evidence.length > 0 && (
                          <div className="chat-evidence">
                            <strong>Evidence</strong>
                            <div style={{ marginTop: 6 }}>{m.evidence.slice(0, 5).map((e, j) => (<div key={j}>{typeof e === 'string' ? e : JSON.stringify(e)}</div>))}</div>
                          </div>
                        )}
                      </div>
                    </div>
                  ))}

                  {chatLoading && <div className="chat-loading" style={{ marginTop: 6 }}>Thinking…</div>}
                  <div style={{ marginTop: 8 }} className="quick-prompts" aria-hidden={chatLoading}>
                    {QUICK_PROMPTS.map((q, idx) => (
                      <button key={idx} onClick={() => useQuickPrompt(q)} disabled={chatLoading}>{q}</button>
                    ))}
                  </div>
                </div>

                <div className="chat-input-row">
                  <input
                    className="chat-input"
                    placeholder="Type a question, e.g. 'Who is high risk today'…"
                    value={chatInput}
                    onChange={(e) => setChatInput(e.target.value)}
                    onKeyDown={(e) => { if (e.key === 'Enter' && !e.shiftKey) { e.preventDefault(); sendChat(chatInput); } }}
                  />
                  <button className="chat-send-btn" onClick={() => sendChat(chatInput)} disabled={chatLoading}>Send</button>
                </div>
              </div>
            )}

          </div>
        );
      }

      const root = ReactDOM.createRoot(document.getElementById('root'));
      root.render(React.createElement(App));
    })();
  </script>
</body>

</html>
