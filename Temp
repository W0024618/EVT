When I run API we got got error

http://localhost:8000/duration

  "detail": "duration module import failed: (unicode error) 'unicodeescape' codec can't decode bytes in position 522-523: truncated \\UXXXXXXXX escape (duration_report.py, line 303)"

SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 522-523: truncated \UXXXXXXXX escape
INFO:     127.0.0.1:52593 - "GET /duration HTTP/1.1" 500 Internal Server Error
2025-09-05 12:46:14,415 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:4000/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=10)       
2025-09-05 12:46:25,031 WARNING region_clients: [region_clients] attempt 2/3 failed for http://10.199.22.57:4000/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=10)       
2025-09-05 12:46:36,247 WARNING region_clients: [region_clients] attempt 3/3 failed for http://10.199.22.57:4000/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=10)       
2025-09-05 12:46:36,247 WARNING region_clients: [region_clients] all 3 attempts failed for http://10.199.22.57:4000/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=10)    
2025-09-05 12:46:36,248 INFO region_clients: [region_clients] fetched 16 history entries
2025-09-05 12:46:36,248 INFO data_compare_service_v2: [region_cache] prefetched 16 region history entries
2025-09-05 12:47:02,297 WARNING region_clients: [region_clients] attempt 1/3 failed for http://10.199.22.57:4000/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)       
2025-09-05 12:47:22,915 WARNING region_clients: [region_clients] attempt 2/3 failed for http://10.199.22.57:4000/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)       
2025-09-05 12:47:36,275 INFO region_clients: [region_clients] fetched 68 history entries
INFO:     127.0.0.1:58455 - "GET /ccure/veri





Check app.py also ..



# app.py (top portion) - minor cleanup (remove duplicate import)
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import shutil
import uuid
import json
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
import re
import asyncio
from typing import Optional, Dict, Any

# --- DB / models imports (kept for endpoints that still use DB like headcount/attendance fallback) ---
from db import SessionLocal
from models import LiveSwipe, AttendanceSummary

# --- settings (optional override) ---
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"

import sys  # ensure logging stream available
RAW_UPLOADS_DIR = DATA_DIR / "raw_uploads"
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)




# ----------------- GLOBAL TIMEOUTS (UNIFY) -----------------
REGION_TIMEOUT_SECONDS = 20
COMPUTE_WAIT_TIMEOUT_SECONDS = 30
COMPUTE_SYNC_TIMEOUT_SECONDS = 60
# ----------------------------------------------------------

_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

_broadcaster_clients = set()

def broadcast_ccure_update(payload: dict):
    if not _broadcaster_clients:
        return
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = None
    for q in list(_broadcaster_clients):
        try:
            if loop and loop.is_running():
                loop.call_soon_threadsafe(q.put_nowait, payload)
            else:
                q.put_nowait(payload)
        except Exception:
            logger.exception("Failed to push payload to SSE client (will remove client)")
            try:
                _broadcaster_clients.discard(q)
            except Exception:
                pass

async def _sse_event_generator(client_queue: asyncio.Queue):
    try:
        while True:
            payload = await client_queue.get()
            try:
                data = json.dumps(payload, default=str)
            except Exception:
                data = json.dumps({"error": "serialization error", "payload": str(payload)})
            yield f"data: {data}\n\n"
    finally:
        try:
            _broadcaster_clients.discard(client_queue)
        except Exception:
            pass
        return

@app.get("/ccure/stream")
async def ccure_stream():
    q = asyncio.Queue()
    _broadcaster_clients.add(q)
    generator = _sse_event_generator(q)
    headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    return StreamingResponse(generator, media_type="text/event-stream", headers=headers)

def _guess_region_from_text(txt: str) -> str:
    if not txt:
        return "unknown"
    s = str(txt).strip().lower()
    s = re.sub(r"[,\-/()]", " ", s)
    if any(k in s for k in ("pune","quezon city","taguig city","bengaluru","hyderabad","chennai","manila","singapore","hong kong","beijing","shanghai","jakarta","kuala","osaka","tokyo","seoul","bangkok")):
        return "apac"
    if any(k in s for k in ("london","dublin","paris","frankfurt","amsterdam","stockholm","cape town","johannesburg","berlin","brussels","madrid","rome","milan")):
        return "emea"
    if any(k in s for k in ("mexico","bogota","buenos","santiago","sao","salvador","lima","caracas")):
        return "laca"
    if any(k in s for k in ("denver","new york","ny","chicago","toronto","vancouver","los angeles","san francisco","boston","houston","atlanta","miami")):
        return "namer"
    return "unknown"

@app.get("/headcount")
def api_headcount():
    try:
        totals = {"apac": 0, "emea": 0, "laca": 0, "namer": 0, "unknown": 0}
        with SessionLocal() as db:
            try:
                today = date.today()
                rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                if rows:
                    for r in rows:
                        try:
                            partition = None
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                            loc = partition or "unknown"
                            region = _guess_region_from_text(loc)
                            totals[region] = totals.get(region, 0) + 1
                        except Exception:
                            totals["unknown"] += 1
                else:
                    start = datetime.combine(today, datetime.min.time())
                    end = datetime.combine(today, datetime.max.time())
                    swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
                    for s in swipes:
                        loc = s.partition or "unknown"
                        region = _guess_region_from_text(loc)
                        totals[region] = totals.get(region, 0) + 1
            except Exception:
                logger.exception("Failed to compute headcount regions")
        out = {
            "apac": int(totals.get("apac", 0)),
            "emea": int(totals.get("emea", 0)),
            "laca": int(totals.get("laca", 0)),
            "namer": int(totals.get("namer", 0))
        }
        return JSONResponse(out)
    except Exception as exc:
        logger.exception("api_headcount failed")
        raise HTTPException(status_code=500, detail=f"headcount error: {exc}")

# ---------- Helpers retained (normalize / safe conversions) -------------
def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- build_ccure_averages (fallback) ------------------------------
def build_ccure_averages(start_date: Optional[str] = None, end_date: Optional[str] = None):
    """
    Fallback averages computation using AttendanceSummary only.
    Returns a compact shape (live_today, ccure_active, averages, notes).
    """
    try:
        def _parse_date_param(s):
            if not s:
                return None
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                try:
                    return date.fromisoformat(s)
                except Exception:
                    return None

        today = date.today()
        start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
        end_obj = _parse_date_param(end_date) if end_date else today
        if start_obj is None or end_obj is None or start_obj > end_obj:
            start_obj = today - timedelta(days=6)
            end_obj = today

        with SessionLocal() as db:
            try:
                att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
            except Exception:
                logger.exception("Failed to query AttendanceSummary")
                att_rows = []

            live_emp = 0
            live_contr = 0
            unknown_count = 0
            seen_keys = set()

            def classify_from_derived(derived):
                try:
                    if not derived or not isinstance(derived, dict):
                        return "contractor"
                    for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                        v = derived.get(k)
                        if v and "employee" in str(v).strip().lower():
                            return "employee"
                    for k in ("Employee_Status","Employee Status","Status"):
                        v = derived.get(k)
                        if v and "terminated" in str(v).strip().lower():
                            return "employee"
                    return "contractor"
                except Exception:
                    return "contractor"

            if att_rows:
                for a in att_rows:
                    key = None
                    try:
                        key = _normalize_employee_key(a.employee_id)
                    except Exception:
                        key = None
                    if not key:
                        try:
                            key = _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None)
                        except Exception:
                            key = None
                    if not key:
                        unknown_count += 1
                        continue
                    if key in seen_keys:
                        continue
                    seen_keys.add(key)
                    cls = classify_from_derived(a.derived)
                    if cls == "employee":
                        live_emp += 1
                    elif cls == "contractor":
                        live_contr += 1
                    else:
                        unknown_count += 1

                live_total_reported = live_emp + live_contr + unknown_count
                live_total_details = len(att_rows)
            else:
                live_total_reported = 0
                live_total_details = 0
                try:
                    import region_clients
                    details = []
                    try:
                        details = region_clients.fetch_all_details(timeout=REGION_TIMEOUT_SECONDS) or []
                    except Exception:
                        logger.exception("region_clients.fetch_all_details failed in build_ccure_averages()")
                        details = []
                    if details:
                        for d in details:
                            try:
                                cls = "contractor"
                                for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                                    v = d.get(k)
                                    if v and "employee" in str(v).strip().lower():
                                        cls = "employee"
                                        break
                                if cls == "employee":
                                    live_emp += 1
                                else:
                                    live_contr += 1
                                live_total_details += 1
                            except Exception:
                                continue
                        live_total_reported = live_emp + live_contr
                    else:
                        try:
                            regions = region_clients.fetch_all_regions(timeout=REGION_TIMEOUT_SECONDS) or []
                            for r in regions:
                                try:
                                    c = r.get("count")
                                    if isinstance(c, (int, float)):
                                        live_total_reported += int(c)
                                except Exception:
                                    continue
                        except Exception:
                            logger.exception("region_clients.fetch_all_regions failed in build_ccure_averages()")
                except Exception:
                    logger.exception("region_clients not importable in build_ccure_averages()")

            # compute avg range using AttendanceSummary if possible
            avg_range = None
            try:
                q = db.query(AttendanceSummary.date, AttendanceSummary.employee_id, AttendanceSummary.presence_count)\
                      .filter(AttendanceSummary.date >= start_obj, AttendanceSummary.date <= end_obj).all()
                by_date = {}
                for row in q:
                    d = row[0]
                    key = (row[1] or "").strip() if row[1] else None
                    if not key:
                        continue
                    if d not in by_date:
                        by_date[d] = set()
                    try:
                        presence_val = getattr(row, 'presence_count', row[2])
                        if int(presence_val) > 0:
                            by_date[d].add(key)
                    except Exception:
                        by_date[d].add(key)
                days_count = (end_obj - start_obj).days + 1
                daily_counts = [len(by_date.get(start_obj + timedelta(days=i), set())) for i in range(days_count)]
                if days_count and any(daily_counts):
                    avg_range = int(round(sum(daily_counts) / float(days_count)))
                else:
                    avg_range = None
            except Exception:
                logger.exception("Failed computing range average from AttendanceSummary")
                avg_range = None

        # fallback: use region history to compute avg_range if still None
        if avg_range is None:
            try:
                import region_clients
                entries = region_clients.fetch_all_history(timeout=REGION_TIMEOUT_SECONDS) or []
                agg = {}
                for e in entries:
                    try:
                        dstr = e.get("date")
                        if not dstr:
                            continue
                        region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                        emp = None
                        con = None
                        tot = None
                        if region_obj:
                            emp = _safe_int(region_obj.get("Employee"))
                            con = _safe_int(region_obj.get("Contractor"))
                            tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                        else:
                            emp = _safe_int(e.get("Employee"))
                            con = _safe_int(e.get("Contractor"))
                            tot = _safe_int(e.get("total"))
                        if emp is None and con is None:
                            continue
                        if tot is None:
                            tot = (emp or 0) + (con or 0)
                        if dstr not in agg:
                            agg[dstr] = {"total": 0, "count": 0}
                        agg[dstr]["total"] += tot or 0
                        agg[dstr]["count"] += 1
                    except Exception:
                        continue
                per_date_totals = []
                days_count = (end_obj - start_obj).days + 1
                for i in range(days_count):
                    dcheck = (start_obj + timedelta(days=i)).isoformat()
                    if dcheck in agg and agg[dcheck]["count"] > 0:
                        per_day_avg = float(agg[dcheck]["total"]) / float(agg[dcheck]["count"])
                        per_date_totals.append(per_day_avg)
                if per_date_totals:
                    avg_range = int(round(sum(per_date_totals) / float(len(per_date_totals))))
            except Exception:
                logger.exception("Failed computing avg_range from region history in fallback")

        # get ccure stats if available
        ccure_stats = {}
        try:
            import ccure_client
            if hasattr(ccure_client, "get_global_stats"):
                ccure_stats = ccure_client.get_global_stats() or {}
        except Exception:
            logger.debug("ccure_client.get_global_stats not available", exc_info=True)

        cc_active_emps = None
        cc_active_contractors = None
        try:
            if isinstance(ccure_stats, dict):
                a = ccure_stats.get("ActiveEmployees") or ccure_stats.get("active_employees") or None
                b = ccure_stats.get("ActiveContractors") or ccure_stats.get("active_contractors") or None
                if a is not None and str(a).strip() != "":
                    cc_active_emps = int(a)
                if b is not None and str(b).strip() != "":
                    cc_active_contractors = int(b)
        except Exception:
            cc_active_emps = cc_active_contractors = None

        emp_pct = None
        contr_pct = None
        overall_pct = None
        try:
            if isinstance(cc_active_emps, int) and cc_active_emps > 0:
                emp_pct = round((live_emp / float(cc_active_emps)) * 100.0, 2)
            if isinstance(cc_active_contractors, int) and cc_active_contractors > 0:
                contr_pct = round((live_contr / float(cc_active_contractors)) * 100.0, 2)
            if isinstance(cc_active_emps, int) and isinstance(cc_active_contractors, int) and (cc_active_emps + cc_active_contractors) > 0:
                overall_pct = round(((live_emp + live_contr) / float(cc_active_emps + cc_active_contractors)) * 100.0, 2)
        except Exception:
            emp_pct = contr_pct = overall_pct = None

        resp = {
            "date": today.isoformat(),
            "notes": f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}" if (start_date or end_date) else None,
            "live_today": {
                "employee": live_emp,
                "contractor": live_contr,
                "total_reported": live_total_reported,
                "total_from_details": live_total_details
            },
            "ccure_active": {
                "active_employees": cc_active_emps,
                "active_contractors": cc_active_contractors,
                "ccure_active_employees_reported": cc_active_emps,
                "ccure_active_contractors_reported": cc_active_contractors
            },
            "averages": {
                "employee_pct": emp_pct,
                "contractor_pct": contr_pct,
                "overall_pct": overall_pct,
                "avg_headcount_last_7_days": avg_range,
                "head_emp_pct_vs_ccure_today": emp_pct,
                "head_contractor_pct_vs_ccure_today": contr_pct,
                "headcount_overall_pct_vs_ccure_today": overall_pct,
                "history_avg_overall_last_7_days": avg_range
            }
        }

        return resp
    except Exception:
        logger.exception("build_ccure_averages failed")
        raise

# -------------------------
# Upload endpoints (store uploads under data/ and data/raw_uploads/ and rotate)
# -------------------------
ALLOWED_EXT = (".xls", ".xlsx", ".csv")

def _remove_old_files_for_kind(kind: str):
    """
    Remove previous canonical files and previous raw uploads that include kind in filename.
    """
    try:
        # canonical in DATA_DIR
        for p in DATA_DIR.iterdir():
            if p.is_file() and f"active_{kind}" in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old canonical file %s", p)
        # remove raw uploads for same kind to keep only latest raw (user requested)
        for p in RAW_UPLOADS_DIR.iterdir():
            if p.is_file() and kind in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old raw file %s", p)
    except Exception:
        logger.exception("error while removing old files for kind=%s", kind)

def _save_upload_and_rotate(upload_file: UploadFile, kind: str) -> dict:
    """
    Save the uploaded file as:
      - data/active_<kind>.<ext>     (canonical)
      - data/raw_uploads/<timestamp>_<kind>_<origname>  (raw trace)
    Remove previous files for same kind (both canonical & raw).
    Returns metadata dict.
    """
    fname = Path(upload_file.filename).name
    ext = Path(fname).suffix.lower()
    if ext not in ALLOWED_EXT:
        raise HTTPException(status_code=400, detail="Only .xls .xlsx .csv allowed")

    # rotate old files
    _remove_old_files_for_kind(kind)

    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    raw_name = f"{ts}_{kind}_{fname}"
    raw_path = RAW_UPLOADS_DIR / raw_name

    # Save raw
    try:
        with raw_path.open("wb") as out_f:
            shutil.copyfileobj(upload_file.file, out_f)
    finally:
        try:
            upload_file.file.close()
        except Exception:
            pass

    # Write canonical: keep same extension as original (simpler)
    canonical_path = DATA_DIR / f"active_{kind}{ext}"
    try:
        shutil.copy(raw_path, canonical_path)
    except Exception:
        logger.exception("Failed to write canonical copy for %s", canonical_path)

    info = {
        "raw_saved": str(raw_path),
        "canonical_saved": str(canonical_path),
        "original_filename": fname,
        "size_bytes": raw_path.stat().st_size if raw_path.exists() else None
    }
    logger.info("Uploaded %s: %s", kind, info)
    return info





@app.post("/api/upload/active-employees")
async def upload_active_employees_api(file: UploadFile = File(...)):
    return await upload_active_employees(file)

@app.post("/api/upload/active-contractors")
async def upload_active_contractors_api(file: UploadFile = File(...)):
    return await upload_active_contractors(file)



@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    """
    Upload Active Employee sheet:
      - stores raw to data/raw_uploads and canonical to data/active_employee.*
      - removes previous uploaded employee sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="employee")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("employee upload failed")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    """
    Upload Active Contractor sheet:
      - stores raw to data/raw_uploads and canonical to data/active_contractor.*
      - removes previous uploaded contractor sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="contractor")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("contractor upload failed")
        raise HTTPException(status_code=500, detail=str(e))

# ---------- map detailed -> resp (unchanged) ------------
def _map_detailed_to_resp(detailed: Dict[str, Any]) -> Dict[str, Any]:
    # unchanged mapping from earlier implementation (kept identical to previous)
    live_h = detailed.get("live_headcount", {}) or {}
    head_h = detailed.get("headcount", {}) or {}
    ccure_active_obj = detailed.get("ccure_active", {}) or {}
    averages_obj = detailed.get("averages", {}) or {}

    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    live_employee = int(live_h.get("employee") or head_h.get("employee") or 0)
    live_contractor = int(live_h.get("contractor") or head_h.get("contractor") or 0)
    total_reported = int(
        live_h.get("currently_present_total")
        or head_h.get("total_visited_today")
        or (live_employee + live_contractor)
        or 0
    )
    total_from_details = int(head_h.get("total_visited_today") or 0)

    mapped_headcount = {
        "total_visited_today": int(head_h.get("total_visited_today") or 0),
        "employee": int(head_h.get("employee") or 0),
        "contractor": int(head_h.get("contractor") or 0),
        "by_location": head_h.get("by_location") or {}
    }

    resp = {
        "date": detailed.get("date"),
        "notes": detailed.get("notes"),
        "live_today": {
            "employee": live_employee,
            "contractor": live_contractor,
            "total_reported": total_reported,
            "total_from_details": total_from_details
        },
        "headcount": mapped_headcount,
        "live_headcount": live_h,
        "ccure_active": {
            "active_employees": ccure_active_obj.get("ccure_active_employees_reported")
                             or ccure_active_obj.get("active_employees"),
            "active_contractors": ccure_active_obj.get("ccure_active_contractors_reported")
                               or ccure_active_obj.get("active_contractors"),
            "ccure_active_employees_reported": ccure_active_obj.get("ccure_active_employees_reported"),
            "ccure_active_contractors_reported": ccure_active_obj.get("ccure_active_contractors_reported")
        },
        "averages": averages_obj
    }
    return resp

# ---------- build a verify-compatible summary from mapped payload -----------
def _build_verify_like_summary_from_mapped(mapped: Dict[str, Any], include_raw: bool = False) -> Dict[str, Any]:
    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    cc = mapped.get("ccure_active", {}) or {}
    head = mapped.get("headcount", {}) or {}
    live_head = mapped.get("live_headcount", {}) or {}
    averages = mapped.get("averages", {}) or {}

    cc_emp = to_int(cc.get("ccure_active_employees_reported") or cc.get("active_employees"))
    cc_con = to_int(cc.get("ccure_active_contractors_reported") or cc.get("active_contractors"))

    head_total = to_int(head.get("total_visited_today") or mapped.get("live_today", {}).get("total_from_details"))
    head_emp = to_int(head.get("employee") or mapped.get("live_today", {}).get("employee"))
    head_con = to_int(head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    live_total = to_int(live_head.get("currently_present_total") or mapped.get("live_today", {}).get("total_reported"))
    live_emp = to_int(live_head.get("employee") or mapped.get("live_today", {}).get("employee"))
    live_con = to_int(live_head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    history_emp_avg = averages.get("history_avg_employee_last_7_days")
    history_con_avg = averages.get("history_avg_contractor_last_7_days")
    history_overall_avg = averages.get("history_avg_overall_last_7_days")

    def pct(n, d):
        try:
            if n is None or d is None:
                return None
            if float(d) == 0:
                return None
            return round((float(n) / float(d)) * 100.0, 2)
        except Exception:
            return None

    summary = {
        "date": mapped.get("date"),
        "ccure_reported": {
            "employees": cc_emp,
            "contractors": cc_con,
            "total_reported": (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None
        },
        "headcount_attendance_summary": {
            "total_visited_today": head_total,
            "employee": head_emp,
            "contractor": head_con,
        },
        "live_headcount_region_clients": {
            "currently_present_total": live_total,
            "employee": live_emp,
            "contractor": live_con,
        },
        "percentages_vs_ccure": {
            "head_employee_pct_vs_ccure_today": pct(head_emp, cc_emp),
            "head_contractor_pct_vs_ccure_today": pct(head_con, cc_con),
            "head_overall_pct_vs_ccure_today": pct(head_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "live_employee_pct_vs_ccure_today": pct(live_emp, cc_emp),
            "live_contractor_pct_vs_ccure_today": pct(live_con, cc_con),
            "live_overall_pct_vs_ccure_today": pct(live_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "history_employee_pct_vs_ccure": pct(history_emp_avg, cc_emp),
            "history_contractor_pct_vs_ccure": pct(history_con_avg, cc_con),
            "history_overall_pct_vs_ccure": pct(history_overall_avg, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None)
        },
        "averages": {
            "history_avg_employee_last_7_days": history_emp_avg,
            "history_avg_contractor_last_7_days": history_con_avg,
            "history_avg_overall_last_7_days": history_overall_avg,
            "avg_headcount_last_7_days_db": averages.get("avg_headcount_last_7_days") or averages.get("avg_headcount_last_7_days_db"),
            "avg_headcount_per_site_last_7_days": averages.get("avg_headcount_per_site_last_7_days"),
            "employee_pct": averages.get("employee_pct"),
            "contractor_pct": averages.get("contractor_pct"),
            "overall_pct": averages.get("overall_pct"),
            **({k: v for k, v in averages.items() if k not in (
                "history_avg_employee_last_7_days",
                "history_avg_contractor_last_7_days",
                "history_avg_overall_last_7_days",
                "avg_headcount_last_7_days",
                "avg_headcount_last_7_days_db",
                "avg_headcount_per_site_last_7_days",
                "employee_pct","contractor_pct","overall_pct"
            )})
        },
        "notes": mapped.get("notes")
    }

    summary["headcount_details"] = {
        "total_visited_today": head_total,
        "employee": head_emp,
        "contractor": head_con,
        "by_location": head.get("by_location") if isinstance(head.get("by_location"), dict) else {}
    }
    summary["live_headcount_details"] = {
        "currently_present_total": live_total,
        "employee": live_emp,
        "contractor": live_con,
        "by_location": live_head.get("by_location") if isinstance(live_head.get("by_location"), dict) else {}
    }

    summary["ccure_active"] = {
        "active_employees": cc.get("active_employees") or cc.get("ccure_active_employees_reported"),
        "active_contractors": cc.get("active_contractors") or cc.get("ccure_active_contractors_reported"),
        "ccure_active_employees_reported": cc.get("ccure_active_employees_reported"),
        "ccure_active_contractors_reported": cc.get("ccure_active_contractors_reported")
    }

    return summary

# ---------- /ccure/verify (already present in your original) -----
@app.get("/ccure/verify")
def ccure_verify(
    raw: bool = Query(False, description="if true, include the raw compute payload for debugging"),
    start_date: Optional[str] = Query(None, description="YYYY-MM-DD start date (inclusive)"),
    end_date: Optional[str] = Query(None, description="YYYY-MM-DD end date (inclusive)")
):
    """
    Synchronous verification endpoint. Prefer compute_visit_averages() (synchronous call).
    If compute raises or fails, fall back to build_ccure_averages() so output shape remains consistent.
    """
    try:
        detailed = None
        try:
            from ccure_compare_service import compute_visit_averages
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
        except Exception:
            logger.exception("compute_visit_averages() failed inside /ccure/verify; falling back")
            detailed = None

        if isinstance(detailed, dict):
            mapped = _map_detailed_to_resp(detailed)
            summary = _build_verify_like_summary_from_mapped(mapped, include_raw=raw)
            if raw and isinstance(detailed, dict):
                summary["raw"] = detailed
            return JSONResponse(summary)
        else:
            fallback = build_ccure_averages(start_date, end_date)
            mapped_fallback = {
                "date": fallback.get("date"),
                "notes": fallback.get("notes"),
                "live_today": fallback.get("live_today", {}),
                "headcount": {
                    "total_visited_today": fallback.get("live_today", {}).get("total_from_details") or fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "live_headcount": {
                    "currently_present_total": fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "ccure_active": fallback.get("ccure_active", {}),
                "averages": fallback.get("averages", {})
            }

            summary = _build_verify_like_summary_from_mapped(mapped_fallback, include_raw=raw)
            if raw:
                summary["raw"] = mapped_fallback
       
            return JSONResponse(summary)
    except Exception as e:
        logger.exception("ccure_verify failed")
        raise HTTPException(status_code=500, detail=f"ccure verify error: {e}")

# ---------- /ccure/compare -> uses data_compare_service.compare_ccure_vs_sheets ----------
@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from data_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)




@app.get("/ccure/export_uploaded")
def ccure_export_uploaded():
    """
    Export the currently uploaded canonical sheets into one workbook.
    Returns JSON: { status: "ok", report_path: "<filename>" }
    Downloadable at /ccure/report/{filename}
    """
    try:
        from data_compare_service import export_uploaded_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed for export_uploaded")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    fname = export_uploaded_sheets()
    if not fname:
        raise HTTPException(status_code=500, detail="failed to export uploaded sheets")
    return JSONResponse({"status": "ok", "report_path": fname})





@app.get("/ccure/compare_v2")
def ccure_compare_v2(
    mode: str = Query("full"),
    stats_detail: str = Query("ActiveProfiles"),
    limit_list: int = Query(200, ge=1, le=5000),
    export: bool = Query(False),
    region_filter: Optional[str] = Query(None, description="Region code e.g. APAC"),
    location_city: Optional[str] = Query(None, description="Location city e.g. Pune"),
    location_state: Optional[str] = Query(None, description="Location state/province"),
    location_description: Optional[str] = Query(None, description="Location Description"),
    week_ref_date: Optional[str] = Query(None, description="YYYY-MM-DD for week (Mon-Fri)")
):
    try:
        from data_compare_service_v2 import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service_v2 import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(
        mode=mode,
        stats_detail=stats_detail,
        limit_list=limit_list,
        export=export,
        region_filter=region_filter,
        location_city=location_city,
        location_state=location_state,
        location_description=location_description,
        week_ref_date=week_ref_date
    )
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)




@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = OUTPUT_DIR / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(str(full),
                            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            filename=safe_name)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")
      

# End of app.py original content

# ---------------------------
# NEW: Duration API endpoint
# ---------------------------
# Endpoint: GET /duration
# Query params:
#   date (YYYY-MM-DD) — optional, defaults to today in Asia/Kolkata
#   regions (comma-separated) — optional, default all (apac,emea,laca,namer)
#   outdir — optional, path where CSVs will be written (default: OUTPUT_DIR/duration_reports)
#
# Response:
#  - JSON with summary for each region and first N rows per region (as list of dicts).
#  - Also returns URLs for CSVs if outdir is under OUTPUT_DIR (relative path).
#
@app.get("/duration")
async def api_duration(
    date: Optional[str] = Query(None, description="Target date YYYY-MM-DD. Defaults to today in Asia/Kolkata"),
    regions: Optional[str] = Query(None, description="Comma-separated list: apac,emea,laca,namer. Default: all"),
    outdir: Optional[str] = Query(None, description="Output directory for CSVs. Defaults to OUTPUT_DIR/duration_reports"),
    sample_rows: int = Query(10, ge=0, le=100, description="How many sample rows to include per region in response")
):
    try:
        # parse date
        if date:
            try:
                target_date = datetime.strptime(date, "%Y-%m-%d").date()
            except Exception:
                raise HTTPException(status_code=400, detail="Invalid date format. Use YYYY-MM-DD.")
        else:
            # default date in Asia/Kolkata
            from zoneinfo import ZoneInfo
            tz = ZoneInfo("Asia/Kolkata")
            target_date = datetime.now(tz).date()

        # parse regions
        if regions:
            regions_list = [r.strip().lower() for r in regions.split(",") if r.strip()]
        else:
            regions_list = ["apac", "emea", "laca", "namer"]

        # determine outdir
        if outdir:
            outdir_path = Path(outdir)
        else:
            outdir_path = OUTPUT_DIR / "duration_reports"
        outdir_path.mkdir(parents=True, exist_ok=True)

        # lazy import of duration_report to avoid startup failure if dependencies missing
        try:
            import duration_report
        except Exception as e:
            logger.exception("Failed importing duration_report module")
            raise HTTPException(status_code=500, detail=f"duration module import failed: {e}")

        # run blocking compute in thread with timeout
        loop = asyncio.get_running_loop()
        task = loop.run_in_executor(None, duration_report.run_for_date, target_date, regions_list, str(outdir_path))
        try:
            results = await asyncio.wait_for(task, timeout=COMPUTE_WAIT_TIMEOUT_SECONDS)
        except asyncio.TimeoutError:
            raise HTTPException(status_code=504, detail="Duration computation timed out")
        except Exception as e:
            logger.exception("Duration run_for_date failed")
            raise HTTPException(status_code=500, detail=f"duration run failed: {e}")

        # summarize results into JSON-serializable structure
        resp = {"date": target_date.isoformat(), "regions": {}}
        for r, df in results.items():
            try:
                # convert pandas DataFrame to small sample and counts
                if df is None:
                    resp["regions"][r] = {"rows": 0, "sample": []}
                    continue
                rows = int(len(df))
                sample = []
                if rows > 0 and sample_rows > 0:
                    # take head(sample_rows), convert datetimes to isoformat strings
                    head = df.head(sample_rows).copy()
                    # Convert Timestamp columns to iso strings if present
                    for c in ("FirstSwipe", "LastSwipe"):
                        if c in head.columns:
                            head[c] = head[c].apply(lambda v: v.isoformat() if (not pd.isna(v) and hasattr(v, "isoformat")) else (str(v) if not pd.isna(v) else None))
                    # ensure DurationSeconds as float/int
                    if "DurationSeconds" in head.columns:
                        head["DurationSeconds"] = head["DurationSeconds"].apply(lambda v: None if pd.isna(v) else float(v))
                    sample = head.to_dict(orient="records")
                # CSV path
                csv_name = f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
                csv_path = str((outdir_path / csv_name).resolve())
                csv_url = None
                try:
                    # If outdir is under OUTPUT_DIR, provide a report path relative to output for download via /ccure/report
                    out_abs = outdir_path.resolve()
                    try:
                        out_root = OUTPUT_DIR.resolve()
                        if str(out_abs).startswith(str(out_root)):
                            rel = out_abs.relative_to(out_root)
                            csv_url = f"/ccure/report/{(rel / csv_name).as_posix()}"
                    except Exception:
                        csv_url = None
                except Exception:
                    csv_url = None

                resp["regions"][r] = {
                    "rows": rows,
                    "sample": sample,
                    "csv_path": csv_path,
                    "csv_url": csv_url
                }
            except Exception:
                logger.exception("Failed to serialize duration result for region %s", r)
                resp["regions"][r] = {"rows": 0, "sample": [], "csv_path": None, "csv_url": None}

        return JSONResponse(resp)
    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("api_duration failed")
        raise HTTPException(status_code=500, detail=f"duration api error: {exc}")











# #C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py

# """
# duration_report.py

# Purpose:
#   - Connects to ACVSUJournal MSSQL databases for APAC, EMEA, LACA, NAMER regions (credentials provided by user)
#   - Extracts CardAdmitted swipe events for a specified date and computes daily duration per employee
#     using first swipe (min) and last swipe (max) timestamps for that date.
#   - Writes per-region CSV duration reports and returns pandas DataFrames for further use.
#   - Contains clear TODO hooks to later improve shift-aware logic.

# Notes & Usage:
#   - Place this file in:
#       C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics
#     (or any other folder).

#   - Requires Python packages: pyodbc, pandas, python-dateutil
#     Install (example):
#       pip install pyodbc pandas python-dateutil

#   - ODBC driver: adjust DRIVER in the connection string depending on the host environment.
#     Example uses 'ODBC Driver 17 for SQL Server' by default.
#     If you use a different driver (e.g. 18), change the DRIVER value.

#   - Example CLI usage:
#       python duration_report.py --date 2025-07-21 --regions apac,namer --outdir ./out_reports

#   - The script by default picks "today" in Asia/Kolkata timezone if --date is omitted.

# Security:
#   - This script contains credentials as provided by the user. In production, consider moving credentials
#     to environment variables or a secure vault.

# TODO (future improvements):
#   - Shift-aware logic: handle cases where employees have predefined shift windows spanning midnight,
#     and decide which swipes belong to which shift.
#   - Handle badge-in-only or badge-out-only cases (e.g., tele-getting or push-button exits)
#   - More advanced deduplication of multiple swipes in short windows.

# """

# import argparse
# import logging
# import os
# from datetime import datetime, timedelta, date
# from zoneinfo import ZoneInfo
# from pathlib import Path

# import pandas as pd

# # Optional: import pyodbc only when connecting (allows importing this module even without driver)
# try:
#     import pyodbc
# except Exception:
#     pyodbc = None

# # --------------------- Configuration ---------------------
# ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# REGION_CONFIG = {
#     "apac": {
#         "user": "GSOC_Test",
#         "password": "Westernccure@2025",
#         "server": "SRVWUPNQ0986V",
#         "database": "ACVSUJournal_00010029",
#         # partitions to include (from provided query)
#         "partitions": [
#             "APAC.Default", "CN.Beijing", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur"
#         ]
#     },
#     "emea": {
#         "user": "GSOC_Test",
#         "password": "Westernccure@2025",
#         "server": "SRVWUFRA0986V",
#         "database": "ACVSUJournal_00011028",  # note: user provided 00011028 in credentials block
#         "partitions": [
#             "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
#             "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
#         ]
#     },
#     "laca": {
#         "user": "GSOC_Test",
#         "password": "Westernccure@2025",
#         "server": "SRVWUSJO0986V",
#         "database": "ACVSUJournal_00010029",
#         "partitions": [
#             "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
#             "PA.Panama City", "PE.Lima"
#         ]
#     },
#     "namer": {
#         "user": "GSOC_Test",
#         "password": "Westernccure@2025",
#         "server": "SRVWUDEN0891V",
#         "database": "ACVSUJournal_00010029",
#         # For NAMER we'll filter by ObjectName2 patterns (HQ, Austin, Miami, NYC)
#         "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
#     }
# }


# # --------------------- SQL Builder ---------------------
# GENERIC_SQL_TEMPLATE = r"""
# SELECT
#     t1.[ObjectName1],
#     t1.[ObjectName2],
#     CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
#     t2.[PersonnelTypeID],
#     t3.[Name] AS PersonnelTypeName,
#     t1.ObjectIdentity1 AS EmployeeIdentity,
#     t1.PartitionName2,
#     DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
#     t1.MessageType
# FROM [{db}].[dbo].[ACVSUJournalLog] AS t1
# INNER JOIN [ACVSCore].[Access].[Personnel] AS t2 ON t1.ObjectIdentity1 = t2.GUID
# INNER JOIN [ACVSCore].[Access].[PersonnelType] AS t3 ON t2.PersonnelTypeID = t3.ObjectID
# WHERE t1.MessageType = 'CardAdmitted'
#   AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
#   {region_filter}
# ;
# """


# def build_region_query(region_key: str, target_date: date) -> str:
#     rc = REGION_CONFIG[region_key]
#     date_str = target_date.strftime("%Y-%m-%d")
#     region_filter = ""

#     if region_key in ("apac", "emea", "laca"):
#         partitions = rc.get("partitions", [])
#         # build partition IN clause
#         parts_sql = ", ".join(f"'{p}'" for p in partitions)
#         region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
#     elif region_key == "namer":
#         likes = rc.get("logical_like", [])
#         # build OR of LIKE patterns on ObjectName2
#         like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
#         region_filter = f"AND ({like_sql})"
#     else:
#         # no filter
#         region_filter = ""

#     return GENERIC_SQL_TEMPLATE.format(db=rc["database"], date=date_str, region_filter=region_filter)


# # --------------------- DB Utilities ---------------------
# def get_connection(region_key: str):
#     """Create and return a pyodbc connection for the region configuration.
#     Caller must ensure pyodbc is installed and the ODBC driver exists on the host.
#     """
#     if pyodbc is None:
#         raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

#     rc = REGION_CONFIG[region_key]
#     conn_str = (
#         f"DRIVER={{{ODBC_DRIVER}}};"
#         f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
#         "TrustServerCertificate=Yes;"
#     )
#     return pyodbc.connect(conn_str, autocommit=True)


# def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
#     """Fetch CardAdmitted swipes for the given region and target_date.
#     Returns a pandas DataFrame with columns including EmployeeID and LocaleMessageTime (UTC->locale adjusted).
#     """
#     sql = build_region_query(region_key, target_date)
#     logging.info("Built SQL for region %s, date %s", region_key, target_date)
#     if pyodbc is None:
#         # Return an empty DataFrame skeleton so the script can be inspected without DB driver installed.
#         logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
#         cols = ["ObjectName1", "ObjectName2", "EmployeeID", "PersonnelTypeID", "PersonnelTypeName",
#                 "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "MessageType"]
#         return pd.DataFrame(columns=cols)

#     conn = get_connection(region_key)
#     try:
#         df = pd.read_sql(sql, conn)
#     finally:
#         conn.close()
#     # Ensure LocaleMessageTime is datetime
#     if "LocaleMessageTime" in df.columns:
#         df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"])
#     return df


# # --------------------- Duration Calculation ---------------------
# def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
#     """Given swipe events DataFrame, compute first and last swipe per employee per day and duration.
#     Returns DataFrame with columns: EmployeeID, Date, FirstSwipe, LastSwipe, DurationSeconds, Duration (HH:MM:SS), CountSwipes
#     """
#     if swipes_df.empty:
#         return pd.DataFrame(columns=[
#             "EmployeeID", "Date", "FirstSwipe", "LastSwipe", "DurationSeconds", "Duration", "CountSwipes",
#             "PersonnelTypeName", "PartitionName2"
#         ])

#     df = swipes_df.copy()

#     # Normalize employee id and remove rows without EmployeeID
#     df = df[df["EmployeeID"].notna()].copy()
#     # Create date column (local date already applied in SQL)
#     df["Date"] = df["LocaleMessageTime"].dt.date

#     # Group by EmployeeID + Date (use EmployeeIdentity if you prefer)
#     group_cols = ["EmployeeID", "Date"]
#     agg = df.groupby(group_cols).agg(
#         FirstSwipe=pd.NamedAgg(column="LocaleMessageTime", aggfunc="min"),
#         LastSwipe=pd.NamedAgg(column="LocaleMessageTime", aggfunc="max"),
#         CountSwipes=pd.NamedAgg(column="LocaleMessageTime", aggfunc="count"),
#         PersonnelTypeName=pd.NamedAgg(column="PersonnelTypeName", aggfunc="first"),
#         PartitionName2=pd.NamedAgg(column="PartitionName2", aggfunc="first")
#     ).reset_index()

#     agg["DurationSeconds"] = (agg["LastSwipe"] - agg["FirstSwipe"]).dt.total_seconds()
#     # Negative durations (if any) will be clipped to 0
#     agg["DurationSeconds"] = agg["DurationSeconds"].clip(lower=0)

#     # Human readable HH:MM:SS
#     agg["Duration"] = agg["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))))
#     return agg


# # --------------------- Main Runner ---------------------
# def run_for_date(target_date: date, regions: list, outdir: str) -> dict:
#     """
#     Run duration reports for the requested regions on target_date.
#     Returns a dict of region->DataFrame (duration report).
#     """
#     outdir_path = Path(outdir)
#     outdir_path.mkdir(parents=True, exist_ok=True)

#     results = {}
#     for r in regions:
#         r = r.lower()
#         if r not in REGION_CONFIG:
#             logging.warning("Unknown region '%s' - skipping", r)
#             continue
#         logging.info("Fetching swipes for region %s on %s", r, target_date)
#         swipes = fetch_swipes_for_region(r, target_date)
#         durations = compute_daily_durations(swipes)
#         csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
#         durations.to_csv(csv_path, index=False)
#         logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
#         results[r] = durations
#     return results


# def parse_args():
#     p = argparse.ArgumentParser(description="Generate daily duration report from ACVSUJournal swipe logs.")
#     p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
#     p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
#                    default="apac,emea,laca,namer")
#     p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
#     return p.parse_args()


# if __name__ == "__main__":
#     logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
#     args = parse_args()

#     # Default date in Asia/Kolkata timezone
#     if args.date:
#         target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
#     else:
#         tz = ZoneInfo("Asia/Kolkata")
#         target_date = datetime.now(tz).date()

#     regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
#     outdir = args.outdir

#     logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
#     results = run_for_date(target_date, regions, outdir)

#     # Print a short summary
#     for r, df in results.items():
#         logging.info("Region %s: %d employees with computed durations", r, len(df))
#     logging.info("Completed. CSVs are in %s", Path(outdir).absolute())























"""
duration_report.py

Purpose:
  - Connects to ACVSUJournal MSSQL databases for APAC, EMEA, LACA, NAMER regions (credentials provided by user)
  - Extracts CardAdmitted swipe events for a specified date and computes daily duration per employee
    using first swipe (min) and last swipe (max) timestamps for that date.
  - Writes per-region CSV duration reports and returns pandas DataFrames for further use.
  - Contains clear TODO hooks to later improve shift-aware logic.

Notes & Usage:
  - Place this file in:
      C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics
    (or any other folder).

  - Requires Python packages: pyodbc, pandas, python-dateutil
    Install (example):
      pip install pyodbc pandas python-dateutil

  - ODBC driver: adjust DRIVER in the connection string depending on the host environment.
    Example uses 'ODBC Driver 17 for SQL Server' by default.
    If you use a different driver (e.g. 18), change the DRIVER value.

  - Example CLI usage:
      python duration_report.py --date 2025-07-21 --regions apac,namer --outdir ./out_reports

  - The script by default picks "today" in Asia/Kolkata timezone if --date is omitted.

Security:
  - This script contains credentials as provided by the user. In production, consider moving credentials
    to environment variables or a secure vault.

TODO (future improvements):
  - Shift-aware logic: handle cases where employees have predefined shift windows spanning midnight,
    and decide which swipes belong to which shift.
  - Handle badge-in-only or badge-out-only cases (e.g., tele-getting or push-button exits)
  - More advanced deduplication of multiple swipes in short windows.

"""

import argparse
import logging
import os
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from pathlib import Path

import pandas as pd

# Optional: import pyodbc only when connecting (allows importing this module even without driver)
try:
    import pyodbc
except Exception:
    pyodbc = None

# --------------------- Configuration ---------------------
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "database": "ACVSUJournal_00010029",
        # partitions to include (from provided query)
        "partitions": [
            "APAC.Default", "CN.Beijing", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur"
        ]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "database": "ACVSUJournal_00011028",  # note: user provided 00011028 in credentials block
        "partitions": [
            "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
            "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
        ]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "database": "ACVSUJournal_00010029",
        "partitions": [
            "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
            "PA.Panama City", "PE.Lima"
        ]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "database": "ACVSUJournal_00010029",
        # For NAMER we'll filter by ObjectName2 patterns (HQ, Austin, Miami, NYC)
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}


# --------------------- SQL Builder ---------------------
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1],
    t1.[ObjectName2],
    CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[PersonnelTypeID],
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType
FROM [{db}].[dbo].[ACVSUJournalLog] AS t1
INNER JOIN [ACVSCore].[Access].[Personnel] AS t2 ON t1.ObjectIdentity1 = t2.GUID
INNER JOIN [ACVSCore].[Access].[PersonnelType] AS t3 ON t2.PersonnelTypeID = t3.ObjectID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
;
"""


def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        # build partition IN clause
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        # build OR of LIKE patterns on ObjectName2
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        # no filter
        region_filter = ""

    return GENERIC_SQL_TEMPLATE.format(db=rc["database"], date=date_str, region_filter=region_filter)


# --------------------- DB Utilities ---------------------
def get_connection(region_key: str):
    """Create and return a pyodbc connection for the region configuration.
    Caller must ensure pyodbc is installed and the ODBC driver exists on the host.
    """
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)


def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    """Fetch CardAdmitted swipes for the given region and target_date.
    Returns a pandas DataFrame with columns including EmployeeID and LocaleMessageTime (UTC->locale adjusted).
    """
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    if pyodbc is None:
        # Return an empty DataFrame skeleton so the script can be inspected without DB driver installed.
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        cols = ["ObjectName1", "ObjectName2", "EmployeeID", "PersonnelTypeID", "PersonnelTypeName",
                "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "MessageType"]
        return pd.DataFrame(columns=cols)

    conn = get_connection(region_key)
    try:
        df = pd.read_sql(sql, conn)
    finally:
        conn.close()
    # Ensure LocaleMessageTime is datetime
    if "LocaleMessageTime" in df.columns:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"])
    return df


# --------------------- Duration Calculation ---------------------
def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    """Given swipe events DataFrame, compute first and last swipe per employee per day and duration.
    Returns DataFrame with columns: EmployeeID, Date, FirstSwipe, LastSwipe, DurationSeconds, Duration (HH:MM:SS), CountSwipes
    """
    if swipes_df.empty:
        return pd.DataFrame(columns=[
            "EmployeeID", "Date", "FirstSwipe", "LastSwipe", "DurationSeconds", "Duration", "CountSwipes",
            "PersonnelTypeName", "PartitionName2"
        ])

    df = swipes_df.copy()

    # Normalize employee id and remove rows without EmployeeID
    df = df[df["EmployeeID"].notna()].copy()
    # Create date column (local date already applied in SQL)
    df["Date"] = df["LocaleMessageTime"].dt.date

    # Group by EmployeeID + Date (use EmployeeIdentity if you prefer)
    group_cols = ["EmployeeID", "Date"]
    agg = df.groupby(group_cols).agg(
        FirstSwipe=pd.NamedAgg(column="LocaleMessageTime", aggfunc="min"),
        LastSwipe=pd.NamedAgg(column="LocaleMessageTime", aggfunc="max"),
        CountSwipes=pd.NamedAgg(column="LocaleMessageTime", aggfunc="count"),
        PersonnelTypeName=pd.NamedAgg(column="PersonnelTypeName", aggfunc="first"),
        PartitionName2=pd.NamedAgg(column="PartitionName2", aggfunc="first")
    ).reset_index()

    agg["DurationSeconds"] = (agg["LastSwipe"] - agg["FirstSwipe"]).dt.total_seconds()
    # Negative durations (if any) will be clipped to 0
    agg["DurationSeconds"] = agg["DurationSeconds"].clip(lower=0)

    # Human readable HH:MM:SS
    agg["Duration"] = agg["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))))
    return agg


# --------------------- Main Runner ---------------------
def run_for_date(target_date: date, regions: list, outdir: str) -> dict:
    """
    Run duration reports for the requested regions on target_date.
    Returns a dict of region->DataFrame (duration report).
    """
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        swipes = fetch_swipes_for_region(r, target_date)
        durations = compute_daily_durations(swipes)
        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        durations.to_csv(csv_path, index=False)
        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        results[r] = durations
    return results


def parse_args():
    p = argparse.ArgumentParser(description="Generate daily duration report from ACVSUJournal swipe logs.")
    p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
    p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
                   default="apac,emea,laca,namer")
    p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
    return p.parse_args()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    args = parse_args()

    # Default date in Asia/Kolkata timezone
    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    else:
        tz = ZoneInfo("Asia/Kolkata")
        target_date = datetime.now(tz).date()

    regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
    outdir = args.outdir

    logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
    results = run_for_date(target_date, regions, outdir)

    # Print a short summary
    for r, df in results.items():
        logging.info("Region %s: %d employees with computed durations", r, len(df))
    logging.info("Completed. CSVs are in %s", Path(outdir).absolute())
