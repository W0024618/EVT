# backend/duration_report.py
from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name (override with environment variable if needed)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
try:
    from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    # fallback — keep behaviour if config file unavailable
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

    def map_door_to_zone(door: object, direction: object = None) -> str:
        """
        Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
        (This is only used if config.door_zone can't be imported.)
        """
        try:
            if door is None:
                return None
            s = str(door).strip()
            if not s:
                return None
            s_l = s.lower()
            # fallback: direction-based inference
            if direction and isinstance(direction, str):
                d = direction.strip().lower()
                if "out" in d:
                    return OUT_OF_OFFICE_ZONE
                if "in" in d:
                    # assume reception/working
                    return "Reception Area"
            # heuristic fallback
            if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
                return OUT_OF_OFFICE_ZONE
            # else treat as working area
            return "Working Area"
        except Exception:
            return None

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "SG.Singapore", "PH.Manila","IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023"
        ],
        "partitions": ["LT.Vilnius","IT.Rome","UK.London","IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition","PA.Panama City","PE.Lima", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        # NOTE: these should match the PartitionName2 values used in DB query WHERE clause.
        # keep as DB-side partition names (example provided earlier used things like 'US.CO.OBS', 'USA/Canada Default', etc).
        "partitions": ["US.CO.OBS", "USA/Canada Default", "US.FL.Miami", "US.NYC"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]  # kept for reference but not used for filtering now
    }
}


# --- Note: Added AdjustedMessageTime into the generic template, date condition templated as {date_condition}
# Also added a hard filter: only PersonnelType = 'Employee' (push filtering to DB).
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND t3.[Name] = 'Employee'
  {date_condition}
  {region_filter}
"""

# Helpers (must exist before build_region_query)
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        # updated: use explicit PartitionName2 list for NAMER (matches the SQL example you provided)
        partitions = rc.get("partitions", [])
        if partitions:
            parts_sql = ", ".join(f"'{p}'" for p in partitions)
            region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
        else:
            # fallback to the old logical_like behaviour (kept for safety)
            likes = rc.get("logical_like", [])
            if likes:
                like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
                region_filter = f"AND ({like_sql})"
            else:
                region_filter = ""
    else:
        region_filter = ""

    # Build date_condition:
    # - For APAC we must fetch LocaleMessageTime rows for target_date AND target_date + 1 day
    #   because AdjustedMessageTime = LocaleMessageTime - 2 hours; some swipes near midnight on next day
    #   belong to the shifted date group.
    if region_key == "apac":
        next_date_str = (target_date + timedelta(days=1)).strftime("%Y-%m-%d")
        date_condition = (
            "AND (CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d1}' "
            "OR CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d2}')"
            .format(d1=date_str, d2=next_date_str)
        )
    else:
        date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql
    
# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    # use first database in list if present
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
    "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
    "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
    "Direction", "CompanyName", "PrimaryLocation"
]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    # ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Dates parsing
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    try:
        if "AdjustedMessageTime" in df.columns:
            df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
        else:
            df["AdjustedMessageTime"] = pd.NaT
    except Exception:
        df["AdjustedMessageTime"] = pd.to_datetime(df.get("AdjustedMessageTime").astype(str), errors="coerce") if "AdjustedMessageTime" in df.columns else pd.NaT

    # defensive: make text fields strings (avoid object type surprises)
    for tcol in ("Door", "PartitionName2", "PersonnelTypeName", "EmployeeName", "CompanyName", "PrimaryLocation"):
        if tcol in df.columns:
            df[tcol] = df[tcol].fillna("").astype(str)

    # Filter: only Employees (defensive; the SQL template already requests t3.Name = 'Employee')
    try:
        if "PersonnelTypeName" in df.columns:
            df = df[df["PersonnelTypeName"].str.strip().str.lower() == "employee"].copy()
    except Exception:
        # If PersonnelTypeName missing or unexpected, keep df as-is but log
        logging.debug("Could not apply PersonnelTypeName filter for region %s", region_key)

    # maintain person_uid same as compute logic
    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)

    # ----- REGION SPECIFIC NORMALIZATIONS -----
    # APAC: map PartitionName2/Door patterns to friendly names:
    if region_key == "apac" and not df.empty:
        def normalize_apac_partition(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part or ""
            # Door-based mappings
            if "APAC_PI" in d or d.startswith("APAC_PI"):
                return "Taguig City"
            if "APAC_PH" in d or d.startswith("APAC_PH"):
                return "Quezon City"
            # Pune detection (APAC.Default or PUN in door/partition)
            if "PUN" in d or "PUNE" in d or ("APAC.DEFAULT" in p.upper() or p.upper().strip() == "APAC.DEFAULT"):
                return "Pune"
            if "APAC_MY" in d or "MY.KUALA" in p.upper() or "KUALA" in d:
                return "MY.Kuala Lumpur"
            if "IN.HYD" in p.upper() or "HYD" in d:
                return "IN.HYD"
            if "SG.SINGAPORE" in p or "SINGAPORE" in d:
                return "SG.Singapore"
            # fallback to existing PartitionName2
            return part
        df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)

    # NAMER: normalize PartitionName2 and add LogicalLocation per the SQL example you provided
    if region_key == "namer" and not df.empty:
        def namer_partition_and_logical(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()
            normalized = part
            logical = "Other"

            if ("US.CO.HQ" in d) or ("HQ" in d and "HQ" in d[:20]) or ("DENVER" in d) or (p == "US.CO.OBS"):
                normalized = "US.CO.OBS"
                logical = "Denver-HQ"
            elif "AUSTIN" in d or "AUSTIN TX" in d or p == "USA/CANADA DEFAULT":
                normalized = "USA/Canada Default"
                logical = "Austin Texas"
            elif "MIAMI" in d or p == "US.FL.MIAMI":
                normalized = "US.FL.Miami"
                logical = "Miami"
            elif "NYC" in d or "NEW YORK" in d or p == "US.NYC":
                normalized = "US.NYC"
                logical = "New York"
            else:
                # fallback mappings for common PartitionName2 values
                if p == "US.CO.OBS":
                    normalized = "US.CO.OBS"; logical = "Denver-HQ"
                elif p == "USA/CANADA DEFAULT":
                    normalized = "USA/Canada Default"; logical = "Austin Texas"
                elif p == "US.FL.MIAMI":
                    normalized = "US.FL.Miami"; logical = "Miami"
                elif p == "US.NYC":
                    normalized = "US.NYC"; logical = "New York"
                else:
                    normalized = part
                    logical = "Other"
            return pd.Series({"PartitionName2": normalized, "LogicalLocation": logical})

        mapped = df.apply(namer_partition_and_logical, axis=1)
        df["PartitionName2"] = mapped["PartitionName2"].astype(str)
        df["LogicalLocation"] = mapped["LogicalLocation"].astype(str)

    # ensure PartitionName2 column exists as string
    if "PartitionName2" not in df.columns:
        df["PartitionName2"] = ""

    # ensure LogicalLocation exists (maybe empty for non-NAMER rows)
    if "LogicalLocation" not in df.columns:
        df["LogicalLocation"] = ""

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]


# ---------------------------------------------------------------------
# compute_daily_durations
# (restored implementation — used by run_for_date and trend_runner)
# ---------------------------------------------------------------------
def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "AdjustedMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes if present
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    if "AdjustedMessageTime" in df.columns and df["AdjustedMessageTime"].dtype == object:
        df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")

    # drop exact duplicates (defensive)
    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    try:
        df = df.drop_duplicates(subset=dedupe_cols, keep="first")
    except Exception:
        # if columns not present as expected, fallback to no-dedupe
        pass

    # Date assignment:
    # - For Pune (PartitionName2 == 'APAC.Default') and if AdjustedMessageTime exists, use adjusted date (shifted date)
    # - Otherwise use LocaleMessageTime.date()
    try:
        df["Date"] = df["LocaleMessageTime"].dt.date
        mask = (df.get("PartitionName2") == "APAC.Default") & (pd.notna(df.get("AdjustedMessageTime")))
        if mask.any():
            df.loc[mask, "Date"] = df.loc[mask, "AdjustedMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure a person_uid column (compat with other code)
    df["person_uid"] = df.apply(
        lambda row: row.get("person_uid")
        if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
        else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
        axis=1
    )
    df = df[df["person_uid"].notna()].copy()

    # Group and aggregate
    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", "first"),
            EmployeeName=("EmployeeName", "first"),
            CardNumber=("CardNumber", "first"),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        # fallback groupby implementation
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": first.get("EmployeeID"),
                "EmployeeName": first.get("EmployeeName"),
                "CardNumber": first.get("CardNumber"),
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(
        lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    )

    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    return grouped[out_cols]


# ---------------------------------------------------------------------
# run_for_date (already present in your newer file; kept and wired to compute_daily_durations)
# ---------------------------------------------------------------------
def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    """
    Fetch swipes for each region, compute durations and write CSV files.
    Returns a dict keyed by lower-case region name:
      { 'apac': {'swipes': DataFrame, 'durations': DataFrame}, ... }
    """
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        if not r:
            continue
        rkey = r.lower()
        if rkey not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", rkey, target_date)
        try:
            swipes = fetch_swipes_for_region(rkey, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", rkey)
            swipes = pd.DataFrame()

        # optional city filter (defensive): replicate existing behavior used elsewhere
        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                before = len(swipes)
                swipes = swipes[combined_mask].copy()
                logging.info("City filter '%s' applied for region %s: rows before=%d after=%d", city, rkey, before, len(swipes))
            else:
                logging.warning("City filter requested (%s) but no location columns present in swipes for region %s; skipping city filter", city, rkey)

        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", rkey)
            durations = pd.DataFrame()

        # write outputs for visibility/compatibility with existing pipeline
        try:
            csv_path = outdir_path / f"{rkey}_duration_{target_date.strftime('%Y%m%d')}.csv"
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", rkey)
        try:
            swipes_csv_path = outdir_path / f"{rkey}_swipes_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", rkey)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", rkey, csv_path if 'csv_path' in locals() else '<unknown>', len(durations) if durations is not None else 0)
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", rkey, swipes_csv_path if 'swipes_csv_path' in locals() else '<unknown>', len(swipes) if swipes is not None else 0)

        results[rkey] = {"swipes": swipes, "durations": durations}

    return results


# end of file























Once compare both file line by line and share me fully update one file carefully so i can easily swap file each other...

 PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python app.py
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 17, in <module>
    from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR, read_90day_cache
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 2042, in <module>
    from duration_report import run_for_date, compute_daily_durations
ImportError: cannot import name 'compute_daily_durations' from 'duration_report' (C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py)
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> 

# # backend/duration_report.py
# from __future__ import annotations

# import logging
# import os
# import re
# import warnings
# from datetime import date, datetime, timedelta
# from pathlib import Path
# from typing import Optional, List, Dict, Any

# import pandas as pd
# import numpy as np

# try:
#     import pyodbc
# except Exception:
#     pyodbc = None

# # ODBC driver name (override with environment variable if needed)
# ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# # Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
# try:
#     from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
# except Exception:
#     # fallback — keep behaviour if config file unavailable
#     BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
#     OUT_OF_OFFICE_ZONE = "Out of office"

#     def map_door_to_zone(door: object, direction: object = None) -> str:
#         """
#         Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
#         (This is only used if config.door_zone can't be imported.)
#         """
#         try:
#             if door is None:
#                 return None
#             s = str(door).strip()
#             if not s:
#                 return None
#             s_l = s.lower()
#             # fallback: direction-based inference
#             if direction and isinstance(direction, str):
#                 d = direction.strip().lower()
#                 if "out" in d:
#                     return OUT_OF_OFFICE_ZONE
#                 if "in" in d:
#                     # assume reception/working
#                     return "Reception Area"
#             # heuristic fallback
#             if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
#                 return OUT_OF_OFFICE_ZONE
#             # else treat as working area
#             return "Working Area"
#         except Exception:
#             return None

# # REGION configuration - databases list used to build UNION queries
# # REGION configuration - databases list used to build UNION queries
# REGION_CONFIG = {
#     "apac": {
#         "user": "GSOC_Test",
#         "password": "Westernccuredb@2026",
#         "server": "SRVWUPNQ0986V",
#         "databases": [
#             "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
#             "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
#         ],
#         "partitions": ["APAC.Default", "SG.Singapore", "PH.Manila","IN.HYD"]
#     },
#     "emea": {
#         "user": "GSOC_Test",
#         "password": "Westernccuredb@2026",
#         "server": "SRVWUFRA0986V",
#         "databases": [
#             "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
#             "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
#             "ACVSUJournal_00011023"
#         ],
#         "partitions": ["LT.Vilnius","IT.Rome","UK.London","IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
#     },
#     "laca": {
#         "user": "GSOC_Test",
#         "password": "Westernccuredb@2026",
#         "server": "SRVWUSJO0986V",
#         "databases": [
#             "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
#             "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
#         ],
#         "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition","PA.Panama City","PE.Lima", "MX.Mexico City"]
#     },
#     "namer": {
#         "user": "GSOC_Test",
#         "password": "Westernccuredb@2026",
#         "server": "SRVWUDEN0891V",
#         "databases": [
#             "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
#             "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
#         ],
#         "partitions": ["Denver", "Austin Texas", "Miami", "New York"],
#         "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
#     }
# }


# # --- Note: Added AdjustedMessageTime into the generic template, date condition templated as {date_condition}
# GENERIC_SQL_TEMPLATE = r"""
# SELECT
#     t1.[ObjectName1] AS EmployeeName,
#     t1.[ObjectName2] AS Door,
#     CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
#     t2.[Int1] AS Int1,
#     t2.[Text12] AS Text12,
#     t_xml.XmlMessage AS XmlMessage,
#     sc.value AS XmlShredValue,
#     COALESCE(
#       TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
#       TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
#       sc.value,
#       NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
#       t2.[Text12]
#     ) AS CardNumber,
#     t3.[Name] AS PersonnelTypeName,
#     t1.ObjectIdentity1 AS EmployeeIdentity,
#     t1.PartitionName2,
#     DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
#     DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
#     t1.MessageType,
#     t5d.value AS Direction,
#     t2.Text4 AS CompanyName,
#     t2.Text5 AS PrimaryLocation
# FROM [{db}].dbo.ACVSUJournalLog AS t1
# LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
# LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
# LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
#   ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
# LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
#   ON t1.XmlGUID = t_xml.GUID
# LEFT JOIN (
#   SELECT GUID, value
#   FROM [{db}].dbo.ACVSUJournalLogxmlShred
#   WHERE Name IN ('Card','CHUID')
# ) AS sc
#   ON t1.XmlGUID = sc.GUID
# WHERE t1.MessageType = 'CardAdmitted'
#   {date_condition}
#   {region_filter}
# """

# # Helpers (must exist before build_region_query)
# def _split_db_name(dbname: str):
#     m = re.match(r"^(.*?)(\d+)$", dbname)
#     if not m:
#         return dbname, None
#     return m.group(1), m.group(2)

# def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
#     prefix, digits = _split_db_name(db_base)
#     if digits is None:
#         return [db_base]
#     width = len(digits)
#     try:
#         cur = int(digits)
#     except Exception:
#         return [db_base]
#     out = []
#     for i in range(last_n):
#         num = cur - i
#         if num < 0:
#             break
#         out.append(f"{prefix}{str(num).zfill(width)}")
#     return out

# def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
#     if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
#         return rc["databases"]
#     base_db = rc.get("database")
#     if not base_db:
#         return []
#     last_n = int(rc.get("last_n_databases", 1) or 1)
#     if last_n <= 1:
#         return [base_db]
#     return _expand_databases_from_base(base_db, last_n)

# def _connect_master(rc: Dict[str, Any]):
#     if pyodbc is None:
#         logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
#         return None
#     try:
#         conn_str = (
#             f"DRIVER={{{ODBC_DRIVER}}};"
#             f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
#             "TrustServerCertificate=Yes;"
#         )
#         return pyodbc.connect(conn_str, autocommit=True)
#     except Exception:
#         logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
#         return None

# def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
#     if not candidates:
#         return []
#     master_conn = _connect_master(rc)
#     if master_conn is None:
#         logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
#         return candidates
#     try:
#         exists = []
#         cursor = master_conn.cursor()
#         for db in candidates:
#             try:
#                 cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
#                 row = cursor.fetchone()
#                 if row and row[0] and int(row[0]) > 0:
#                     exists.append(db)
#             except Exception:
#                 logging.exception("Error checking existence for database %s", db)
#         cursor.close()
#         logging.info("Databases present for server %s: %s", rc.get("server"), exists)
#         return exists if exists else candidates
#     finally:
#         try:
#             master_conn.close()
#         except Exception:
#             pass

# def build_region_query(region_key: str, target_date: date) -> str:
#     rc = REGION_CONFIG[region_key]
#     date_str = target_date.strftime("%Y-%m-%d")
#     region_filter = ""

#     if region_key in ("apac", "emea", "laca"):
#         partitions = rc.get("partitions", [])
#         parts_sql = ", ".join(f"'{p}'" for p in partitions)
#         region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
#     elif region_key == "namer":
#         likes = rc.get("logical_like", [])
#         like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
#         region_filter = f"AND ({like_sql})"
#     else:
#         region_filter = ""

#     # Build date_condition:
#     # - For APAC we must fetch LocaleMessageTime rows for target_date AND target_date + 1 day
#     #   because AdjustedMessageTime = LocaleMessageTime - 2 hours; some swipes near midnight on next day
#     #   belong to the shifted date group.
#     if region_key == "apac":
#         next_date_str = (target_date + timedelta(days=1)).strftime("%Y-%m-%d")
#         date_condition = (
#             "AND (CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d1}' "
#             "OR CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d2}')"
#             .format(d1=date_str, d2=next_date_str)
#         )
#     else:
#         date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

#     candidates = _get_candidate_databases(rc)
#     if not candidates:
#         candidates = [rc.get("database")]

#     valid_dbs = _filter_existing_databases(rc, candidates)

#     union_parts = []
#     for dbname in valid_dbs:
#         union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

#     if not union_parts:
#         dbname = rc.get("database")
#         return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

#     sql = "\nUNION ALL\n".join(union_parts)
#     return sql
    
# # DB connection & fetch
# def get_connection(region_key: str):
#     if pyodbc is None:
#         raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

#     rc = REGION_CONFIG[region_key]
#     # use first database in list if present
#     db = rc.get("databases", [rc.get("database")])[0]
#     conn_str = (
#         f"DRIVER={{{ODBC_DRIVER}}};"
#         f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
#         "TrustServerCertificate=Yes;"
#     )
#     return pyodbc.connect(conn_str, autocommit=True)

# def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
#     sql = build_region_query(region_key, target_date)
#     logging.info("Built SQL for region %s, date %s", region_key, target_date)
#     cols = [
#     "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
#     "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
#     "Direction", "CompanyName", "PrimaryLocation"
# ]

#     if pyodbc is None:
#         logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
#         return pd.DataFrame(columns=cols)

#     conn = None
#     try:
#         conn = get_connection(region_key)
#         with warnings.catch_warnings():
#             warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
#             df = pd.read_sql(sql, conn)
#     except Exception:
#         logging.exception("Failed to run query for region %s", region_key)
#         df = pd.DataFrame(columns=cols)
#     finally:
#         try:
#             if conn is not None:
#                 conn.close()
#         except Exception:
#             pass

#     for c in cols:
#         if c not in df.columns:
#             df[c] = None

#     try:
#         df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
#     except Exception:
#         df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

#     # NEW: convert AdjustedMessageTime (may be NULL for non-Pune rows)
#     try:
#         if "AdjustedMessageTime" in df.columns:
#             df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
#         else:
#             df["AdjustedMessageTime"] = pd.NaT
#     except Exception:
#         df["AdjustedMessageTime"] = pd.to_datetime(df.get("AdjustedMessageTime").astype(str), errors="coerce") if "AdjustedMessageTime" in df.columns else pd.NaT

#     # maintain person_uid same as compute logic
#     def make_person_uid(row):
#         eid = row.get("EmployeeIdentity")
#         if pd.notna(eid) and str(eid).strip() != "":
#             return str(eid).strip()
#         pieces = [
#             (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
#             (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
#             (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
#         ]
#         joined = "|".join([p for p in pieces if p])
#         return joined or None

#     if not df.empty:
#         df['person_uid'] = df.apply(make_person_uid, axis=1)

#     return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]

# # compute durations (unchanged largely) but Date assignment updated to use AdjustedMessageTime for APAC.Default
# def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
#     out_cols = [
#         "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
#         "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
#         "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
#         "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
#     ]

#     if swipes_df is None or swipes_df.empty:
#         return pd.DataFrame(columns=out_cols)

#     df = swipes_df.copy()
#     expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "AdjustedMessageTime", "Door",
#                 "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
#     for col in expected:
#         if col not in df.columns:
#             df[col] = None

#     if df["LocaleMessageTime"].dtype == object:
#         df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

#     if "AdjustedMessageTime" in df.columns and df["AdjustedMessageTime"].dtype == object:
#         df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")

#     dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
#     df = df.drop_duplicates(subset=dedupe_cols, keep="first")

#     # Date assignment:
#     # - For Pune (PartitionName2 == 'APAC.Default') and if AdjustedMessageTime exists, use adjusted date (shifted date)
#     # - Otherwise use LocaleMessageTime.date()
#     try:
#         # default to LocaleMessageTime date
#         df["Date"] = df["LocaleMessageTime"].dt.date
#         # mask for APAC.Default rows with valid AdjustedMessageTime
#         mask = (df.get("PartitionName2") == "APAC.Default") & (pd.notna(df.get("AdjustedMessageTime")))
#         if mask.any():
#             df.loc[mask, "Date"] = df.loc[mask, "AdjustedMessageTime"].dt.date
#     except Exception:
#         # fallback: naive conversion
#         try:
#             df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
#         except Exception:
#             df["Date"] = None

#     df["person_uid"] = df.apply(
#         lambda row: row["person_uid"]
#         if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
#         else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
#         axis=1
#     )
#     df = df[df["person_uid"].notna()].copy()

#     try:
#         df = df.sort_values("LocaleMessageTime")
#         grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
#             FirstSwipe=("LocaleMessageTime", "first"),
#             LastSwipe=("LocaleMessageTime", "last"),
#             FirstDoor=("Door", "first"),
#             LastDoor=("Door", "last"),
#             CountSwipes=("LocaleMessageTime", "count"),
#             EmployeeIdentity=("EmployeeIdentity", "first"),
#             EmployeeID=("EmployeeID", "first"),
#             EmployeeName=("EmployeeName", "first"),
#             CardNumber=("CardNumber", "first"),
#             PersonnelTypeName=("PersonnelTypeName", "first"),
#             PartitionName2=("PartitionName2", "first"),
#             CompanyName=("CompanyName", "first"),
#             PrimaryLocation=("PrimaryLocation", "first"),
#             FirstDirection=("Direction", "first"),
#             LastDirection=("Direction", "last")
#         ).reset_index()
#     except Exception:
#         def agg_for_group(g):
#             g_sorted = g.sort_values("LocaleMessageTime")
#             first = g_sorted.iloc[0]
#             last = g_sorted.iloc[-1]
#             return pd.Series({
#                 "person_uid": first["person_uid"],
#                 "EmployeeIdentity": first.get("EmployeeIdentity"),
#                 "EmployeeID": first.get("EmployeeID"),
#                 "EmployeeName": first.get("EmployeeName"),
#                 "CardNumber": first.get("CardNumber"),
#                 "Date": first["Date"],
#                 "FirstSwipe": first["LocaleMessageTime"],
#                 "LastSwipe": last["LocaleMessageTime"],
#                 "FirstDoor": first.get("Door"),
#                 "LastDoor": last.get("Door"),
#                 "CountSwipes": int(len(g_sorted)),
#                 "PersonnelTypeName": first.get("PersonnelTypeName"),
#                 "PartitionName2": first.get("PartitionName2"),
#                 "CompanyName": first.get("CompanyName"),
#                 "PrimaryLocation": first.get("PrimaryLocation"),
#                 "FirstDirection": first.get("Direction"),
#                 "LastDirection": last.get("Direction")
#             })
#         grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

#     grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
#     grouped["Duration"] = grouped["DurationSeconds"].apply(
#         lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
#     )

#     for c in out_cols:
#         if c not in grouped.columns:
#             grouped[c] = None

#     return grouped[out_cols]

# # runner helper: run_for_date
# def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
#     outdir_path = Path(outdir)
#     outdir_path.mkdir(parents=True, exist_ok=True)

#     results: Dict[str, Any] = {}
#     for r in regions:
#         r = r.lower()
#         if r not in REGION_CONFIG:
#             logging.warning("Unknown region '%s' - skipping", r)
#             continue
#         logging.info("Fetching swipes for region %s on %s", r, target_date)
#         try:
#             swipes = fetch_swipes_for_region(r, target_date)
#         except Exception:
#             logging.exception("Failed fetching swipes for region %s", r)
#             swipes = pd.DataFrame()

#         # optional city filter (defensive)
#         if city and not swipes.empty:
#             city_l = str(city).strip().lower()
#             mask_parts = []
#             for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
#                 if col in swipes.columns:
#                     # normalize to string and test contains
#                     mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
#             if mask_parts:
#                 combined_mask = mask_parts[0]
#                 for m in mask_parts[1:]:
#                     combined_mask = combined_mask | m
#                 before = len(swipes)
#                 swipes = swipes[combined_mask].copy()
#                 logging.info("City filter '%s' applied: rows before=%d after=%d", city, before, len(swipes))
#             else:
#                 # fallback: we expected a location column but none present - skip city filter
#                 logging.warning("City filter requested (%s) but no location columns present in swipes; skipping city filter", city)


#         try:
#             durations = compute_daily_durations(swipes)
#         except Exception:
#             logging.exception("Failed computing durations for region %s", r)
#             durations = pd.DataFrame()

#         csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
#         swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
#         try:
#             durations.to_csv(csv_path, index=False)
#         except Exception:
#             logging.exception("Failed writing durations CSV for %s", r)
#         try:
#             swipes.to_csv(swipes_csv_path, index=False)
#         except Exception:
#             logging.exception("Failed writing swipes CSV for %s", r)

#         logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
#         logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
#         results[r] = {"swipes": swipes, "durations": durations}

#     return results

# # end of file












# backend/duration_report.py
from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name (override with environment variable if needed)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
try:
    from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    # fallback — keep behaviour if config file unavailable
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

    def map_door_to_zone(door: object, direction: object = None) -> str:
        """
        Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
        (This is only used if config.door_zone can't be imported.)
        """
        try:
            if door is None:
                return None
            s = str(door).strip()
            if not s:
                return None
            s_l = s.lower()
            # fallback: direction-based inference
            if direction and isinstance(direction, str):
                d = direction.strip().lower()
                if "out" in d:
                    return OUT_OF_OFFICE_ZONE
                if "in" in d:
                    # assume reception/working
                    return "Reception Area"
            # heuristic fallback
            if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
                return OUT_OF_OFFICE_ZONE
            # else treat as working area
            return "Working Area"
        except Exception:
            return None

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "SG.Singapore", "PH.Manila","IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023"
        ],
        "partitions": ["LT.Vilnius","IT.Rome","UK.London","IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition","PA.Panama City","PE.Lima", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        # NOTE: these should match the PartitionName2 values used in DB query WHERE clause.
        # keep as DB-side partition names (example provided earlier used things like 'US.CO.OBS', 'USA/Canada Default', etc).
        "partitions": ["US.CO.OBS", "USA/Canada Default", "US.FL.Miami", "US.NYC"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]  # kept for reference but not used for filtering now
    }
}


# --- Note: Added AdjustedMessageTime into the generic template, date condition templated as {date_condition}
# Also added a hard filter: only PersonnelType = 'Employee' (push filtering to DB).
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND t3.[Name] = 'Employee'
  {date_condition}
  {region_filter}
"""

# Helpers (must exist before build_region_query)
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        # updated: use explicit PartitionName2 list for NAMER (matches the SQL example you provided)
        partitions = rc.get("partitions", [])
        if partitions:
            parts_sql = ", ".join(f"'{p}'" for p in partitions)
            region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
        else:
            # fallback to the old logical_like behaviour (kept for safety)
            likes = rc.get("logical_like", [])
            if likes:
                like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
                region_filter = f"AND ({like_sql})"
            else:
                region_filter = ""
    else:
        region_filter = ""

    # Build date_condition:
    # - For APAC we must fetch LocaleMessageTime rows for target_date AND target_date + 1 day
    #   because AdjustedMessageTime = LocaleMessageTime - 2 hours; some swipes near midnight on next day
    #   belong to the shifted date group.
    if region_key == "apac":
        next_date_str = (target_date + timedelta(days=1)).strftime("%Y-%m-%d")
        date_condition = (
            "AND (CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d1}' "
            "OR CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d2}')"
            .format(d1=date_str, d2=next_date_str)
        )
    else:
        date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql
    
# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    # use first database in list if present
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
    "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
    "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
    "Direction", "CompanyName", "PrimaryLocation"
]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    # ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Dates parsing
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    try:
        if "AdjustedMessageTime" in df.columns:
            df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
        else:
            df["AdjustedMessageTime"] = pd.NaT
    except Exception:
        df["AdjustedMessageTime"] = pd.to_datetime(df.get("AdjustedMessageTime").astype(str), errors="coerce") if "AdjustedMessageTime" in df.columns else pd.NaT

    # defensive: make text fields strings (avoid object type surprises)
    for tcol in ("Door", "PartitionName2", "PersonnelTypeName", "EmployeeName", "CompanyName", "PrimaryLocation"):
        if tcol in df.columns:
            df[tcol] = df[tcol].fillna("").astype(str)

    # Filter: only Employees (defensive; the SQL template already requests t3.Name = 'Employee')
    try:
        if "PersonnelTypeName" in df.columns:
            df = df[df["PersonnelTypeName"].str.strip().str.lower() == "employee"].copy()
    except Exception:
        # If PersonnelTypeName missing or unexpected, keep df as-is but log
        logging.debug("Could not apply PersonnelTypeName filter for region %s", region_key)

    # maintain person_uid same as compute logic
    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)

    # ----- REGION SPECIFIC NORMALIZATIONS -----
    # APAC: map PartitionName2/Door patterns to friendly names:
    if region_key == "apac" and not df.empty:
        def normalize_apac_partition(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part or ""
            # Door-based mappings
            if "APAC_PI" in d or d.startswith("APAC_PI"):
                return "Taguig City"
            if "APAC_PH" in d or d.startswith("APAC_PH"):
                return "Quezon City"
            # Pune detection (APAC.Default or PUN in door/partition)
            if "PUN" in d or "PUNE" in d or ("APAC.DEFAULT" in p.upper() or p.upper().strip() == "APAC.DEFAULT"):
                return "Pune"
            if "APAC_MY" in d or "MY.KUALA" in p.upper() or "KUALA" in d:
                return "MY.Kuala Lumpur"
            if "IN.HYD" in p.upper() or "HYD" in d:
                return "IN.HYD"
            if "SG.SINGAPORE" in p or "SINGAPORE" in d:
                return "SG.Singapore"
            # fallback to existing PartitionName2
            return part
        df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)

    # NAMER: normalize PartitionName2 and add LogicalLocation per the SQL example you provided
    if region_key == "namer" and not df.empty:
        def namer_partition_and_logical(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()
            normalized = part
            logical = "Other"

            if ("US.CO.HQ" in d) or ("HQ" in d and "HQ" in d[:20]) or ("DENVER" in d) or (p == "US.CO.OBS"):
                normalized = "US.CO.OBS"
                logical = "Denver-HQ"
            elif "AUSTIN" in d or "AUSTIN TX" in d or p == "USA/CANADA DEFAULT":
                normalized = "USA/Canada Default"
                logical = "Austin Texas"
            elif "MIAMI" in d or p == "US.FL.MIAMI":
                normalized = "US.FL.Miami"
                logical = "Miami"
            elif "NYC" in d or "NEW YORK" in d or p == "US.NYC":
                normalized = "US.NYC"
                logical = "New York"
            else:
                # fallback mappings for common PartitionName2 values
                if p == "US.CO.OBS":
                    normalized = "US.CO.OBS"; logical = "Denver-HQ"
                elif p == "USA/CANADA DEFAULT":
                    normalized = "USA/Canada Default"; logical = "Austin Texas"
                elif p == "US.FL.MIAMI":
                    normalized = "US.FL.Miami"; logical = "Miami"
                elif p == "US.NYC":
                    normalized = "US.NYC"; logical = "New York"
                else:
                    normalized = part
                    logical = "Other"
            return pd.Series({"PartitionName2": normalized, "LogicalLocation": logical})

        mapped = df.apply(namer_partition_and_logical, axis=1)
        df["PartitionName2"] = mapped["PartitionName2"].astype(str)
        df["LogicalLocation"] = mapped["LogicalLocation"].astype(str)

    # ensure PartitionName2 column exists as string
    if "PartitionName2" not in df.columns:
        df["PartitionName2"] = ""

    # ensure LogicalLocation exists (maybe empty for non-NAMER rows)
    if "LogicalLocation" not in df.columns:
        df["LogicalLocation"] = ""

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]




def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    """
    Fetch swipes for each region, compute durations and write CSV files.
    Returns a dict keyed by lower-case region name:
      { 'apac': {'swipes': DataFrame, 'durations': DataFrame}, ... }
    """
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        if not r:
            continue
        rkey = r.lower()
        if rkey not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", rkey, target_date)
        try:
            swipes = fetch_swipes_for_region(rkey, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", rkey)
            swipes = pd.DataFrame()

        # optional city filter (defensive): replicate existing behavior used elsewhere
        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                before = len(swipes)
                swipes = swipes[combined_mask].copy()
                logging.info("City filter '%s' applied for region %s: rows before=%d after=%d", city, rkey, before, len(swipes))
            else:
                logging.warning("City filter requested (%s) but no location columns present in swipes for region %s; skipping city filter", city, rkey)

        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", rkey)
            durations = pd.DataFrame()

        # write outputs for visibility/compatibility with existing pipeline
        try:
            csv_path = outdir_path / f"{rkey}_duration_{target_date.strftime('%Y%m%d')}.csv"
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", rkey)
        try:
            swipes_csv_path = outdir_path / f"{rkey}_swipes_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", rkey)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", rkey, csv_path if 'csv_path' in locals() else '<unknown>', len(durations) if durations is not None else 0)
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", rkey, swipes_csv_path if 'swipes_csv_path' in locals() else '<unknown>', len(swipes) if swipes is not None else 0)

        results[rkey] = {"swipes": swipes, "durations": durations}

    return results












