# app.py
from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Query
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import shutil
import uuid
import json
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
import re
import asyncio

# --- DB / models imports (your existing project modules) ---
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary

# --- settings (assumes these exist in your project) ---
try:
    from settings import UPLOAD_DIR, OUTPUT_DIR
except Exception:
    # fallback defaults
    UPLOAD_DIR = "./uploads"
    OUTPUT_DIR = "./output"

# --- app & logging setup ---
app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# -------------------------------
# CORS (allow frontend dev server to connect directly to Python SSE)
# Adjust origins as needed for production â€” do NOT use '*' in prod.
# -------------------------------
_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    # add if you have other dev hosts or ports
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# ------------------------------------------------------------------
# Small SSE broadcaster (in-process). Minimal, works for single-process deployments.
# Each connected client gets its own asyncio.Queue; we push payloads to all queues.
# ------------------------------------------------------------------
_broadcaster_clients = set()  # set of asyncio.Queue

def broadcast_ccure_update(payload: dict):
    """
    Non-blocking broadcast to all connected SSE clients.
    Uses asyncio.get_event_loop().call_soon_threadsafe to schedule queue.put_nowait
    so it is safe to call from sync endpoints.
    """
    if not _broadcaster_clients:
        return
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = None

    for q in list(_broadcaster_clients):
        try:
            if loop and loop.is_running():
                loop.call_soon_threadsafe(q.put_nowait, payload)
            else:
                q.put_nowait(payload)
        except Exception:
            logger.exception("Failed to push payload to SSE client (will remove client)")
            try:
                _broadcaster_clients.discard(q)
            except Exception:
                pass

async def _sse_event_generator(client_queue: asyncio.Queue):
    """
    Async generator yielding SSE data lines from provided queue.
    When client disconnects, finally block removes the queue from broadcaster set.
    """
    try:
        while True:
            payload = await client_queue.get()
            try:
                data = json.dumps(payload, default=str)
            except Exception:
                data = json.dumps({"error": "serialization error", "payload": str(payload)})
            # single "data:" block per payload
            yield f"data: {data}\n\n"
    finally:
        # Ensure client queue is removed when generator stops (disconnect)
        try:
            _broadcaster_clients.discard(client_queue)
        except Exception:
            pass
        return

@app.get("/ccure/stream")
async def ccure_stream():
    """
    SSE endpoint that streams ccure/averages updates.
    Frontend may connect directly to `http://localhost:8000/ccure/stream`
    (or set VITE_PY_BACKEND to point there in dev).
    """
    q = asyncio.Queue()
    _broadcaster_clients.add(q)
    generator = _sse_event_generator(q)

    # Use headers that help with streaming and intermediate proxies
    headers = {
        "Cache-Control": "no-cache",
        "X-Accel-Buffering": "no",  # for nginx buffering disable if used
    }
    return StreamingResponse(generator, media_type="text/event-stream", headers=headers)

# ------------------------------------------------------------------
# Helper functions (normalizers / region guesser)
# ------------------------------------------------------------------
def _guess_region_from_text(txt: str) -> str:
    if not txt:
        return "unknown"
    s = str(txt).strip().lower()
    s = re.sub(r"[,\-/()]", " ", s)
    if any(k in s for k in ("pune","mumbai","bangalore","bengaluru","hyderabad","chennai","manila","singapore","hong kong","beijing","shanghai","jakarta","kuala","osaka","tokyo","seoul","bangkok")):
        return "apac"
    if any(k in s for k in ("london","dublin","paris","frankfurt","amsterdam","stockholm","cape town","johannesburg","berlin","brussels","madrid","rome","milan")):
        return "emea"
    if any(k in s for k in ("mexico","bogota","buenos","santiago","sao","salvador","lima","caracas")):
        return "laca"
    if any(k in s for k in ("denver","new york","ny","chicago","toronto","vancouver","los angeles","san francisco","boston","houston","atlanta","miami")):
        return "namer"
    return "unknown"

# ------------------------------------------------------------------
# HEADCOUNT endpoint (exact '/headcount' path) - unchanged logic
# ------------------------------------------------------------------
@app.get("/headcount")
def api_headcount():
    try:
        totals = {"apac": 0, "emea": 0, "laca": 0, "namer": 0, "unknown": 0}
        with SessionLocal() as db:
            try:
                emp_rows = db.query(ActiveEmployee).all()
            except Exception:
                logger.exception("Failed to query ActiveEmployee")
                emp_rows = []
            try:
                contr_rows = db.query(ActiveContractor).all()
            except Exception:
                logger.exception("Failed to query ActiveContractor")
                contr_rows = []

            def _loc_from_employee(e):
                return getattr(e, "location_city", None) or getattr(e, "location", None) or getattr(e, "Location", None)

            def _loc_from_contractor(c):
                return getattr(c, "location", None) or getattr(c, "location_city", None) or getattr(c, "Location", None)

            for e in emp_rows:
                loc = _loc_from_employee(e)
                region = _guess_region_from_text(loc)
                totals[region] = totals.get(region, 0) + 1

            for c in contr_rows:
                loc = _loc_from_contractor(c)
                region = _guess_region_from_text(loc)
                totals[region] = totals.get(region, 0) + 1

        out = {
            "apac": int(totals.get("apac", 0)),
            "emea": int(totals.get("emea", 0)),
            "laca": int(totals.get("laca", 0)),
            "namer": int(totals.get("namer", 0))
        }
        return JSONResponse(out)
    except Exception as exc:
        logger.exception("api_headcount failed")
        raise HTTPException(status_code=500, detail=f"headcount error: {exc}")

# ------------------------------------------------------------------
# Build ccure averages payload (extracted helper) -- unchanged behaviour
# ------------------------------------------------------------------
def build_ccure_averages():
    try:
        today = date.today()

        with SessionLocal() as db:
            try:
                att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
            except Exception:
                logger.exception("Failed to query AttendanceSummary")
                att_rows = []

            active_employee_ids = set()
            active_contractor_primary_ids = set()

            try:
                emps = db.query(ActiveEmployee).all()
                for e in emps:
                    if getattr(e, "employee_id", None):
                        active_employee_ids.add(str(e.employee_id).strip())
            except Exception:
                logger.exception("Failed to load ActiveEmployee rows")

            try:
                contrs = db.query(ActiveContractor).all()
                for c in contrs:
                    primary = getattr(c, "worker_system_id", None) or getattr(c, "ipass_id", None) or getattr(c, "worker_id", None)
                    if primary:
                        active_contractor_primary_ids.add(str(primary).strip())
            except Exception:
                logger.exception("Failed to load ActiveContractor rows")

            # Map attendance keys -> employee/contractor counts.
            live_emp = 0
            live_contr = 0
            unknown_count = 0
            seen_keys = set()
            for a in att_rows:
                key = (a.employee_id or "").strip() if a.employee_id else None
                if not key:
                    try:
                        key = (a.derived.get('card_number') or "").strip() if (a.derived and isinstance(a.derived, dict)) else None
                    except Exception:
                        key = None
                if not key:
                    unknown_count += 1
                    continue
                if key in seen_keys:
                    continue
                seen_keys.add(key)
                if key in active_employee_ids:
                    live_emp += 1
                elif key in active_contractor_primary_ids:
                    live_contr += 1
                else:
                    numeric = re.sub(r'\D+', '', key)
                    if numeric:
                        if numeric in active_employee_ids or numeric.lstrip('0') in active_employee_ids:
                            live_emp += 1
                        elif numeric in active_contractor_primary_ids or numeric.lstrip('0') in active_contractor_primary_ids:
                            live_contr += 1
                        else:
                            unknown_count += 1
                    else:
                        unknown_count += 1

            live_total_reported = live_emp + live_contr + unknown_count
            live_total_details = len(att_rows)

            # 7-day average headcount (unique keys present per day)
            try:
                start_7 = today - timedelta(days=6)
                q = db.query(AttendanceSummary.date, AttendanceSummary.employee_id, AttendanceSummary.presence_count)\
                      .filter(AttendanceSummary.date >= start_7, AttendanceSummary.date <= today).all()
                by_date = {}
                for row in q:
                    d = row[0]
                    key = (row[1] or "").strip() if row[1] else None
                    if not key:
                        continue
                    if d not in by_date:
                        by_date[d] = set()
                    try:
                        presence_val = getattr(row, 'presence_count', row[2])
                        if int(presence_val) > 0:
                            by_date[d].add(key)
                    except Exception:
                        by_date[d].add(key)
                daily_counts = [len(by_date.get(start_7 + timedelta(days=i), set())) for i in range(7)]
                avg7 = int(round(sum(daily_counts) / 7.0)) if any(daily_counts) else 0
            except Exception:
                logger.exception("Failed computing 7-day average")
                avg7 = None

        # get CCURE stats if available
        ccure_stats = {}
        try:
            import ccure_client
            if hasattr(ccure_client, "get_global_stats"):
                ccure_stats = ccure_client.get_global_stats() or {}
        except Exception:
            logger.debug("ccure_client.get_global_stats not available", exc_info=True)

        cc_active_emps = None
        cc_active_contractors = None
        try:
            if isinstance(ccure_stats, dict):
                a = ccure_stats.get("ActiveEmployees") or ccure_stats.get("active_employees") or None
                b = ccure_stats.get("ActiveContractors") or ccure_stats.get("active_contractors") or None
                if a is not None and str(a).strip() != "":
                    cc_active_emps = int(a)
                if b is not None and str(b).strip() != "":
                    cc_active_contractors = int(b)
        except Exception:
            cc_active_emps = cc_active_contractors = None

        emp_pct = None
        contr_pct = None
        overall_pct = None
        try:
            if isinstance(cc_active_emps, int) and cc_active_emps > 0:
                emp_pct = round((live_emp / float(cc_active_emps)) * 100.0, 2)
            if isinstance(cc_active_contractors, int) and cc_active_contractors > 0:
                contr_pct = round((live_contr / float(cc_active_contractors)) * 100.0, 2)
            if isinstance(cc_active_emps, int) and isinstance(cc_active_contractors, int) and (cc_active_emps + cc_active_contractors) > 0:
                overall_pct = round(((live_emp + live_contr) / float(cc_active_emps + cc_active_contractors)) * 100.0, 2)
        except Exception:
            emp_pct = contr_pct = overall_pct = None

        resp = {
            "date": today.isoformat(),
            "notes": None,
            "live_today": {
                "employee": live_emp,
                "contractor": live_contr,
                "total_reported": live_total_reported,
                "total_from_details": live_total_details
            },
            "ccure_active": {
                "active_employees": cc_active_emps,
                "active_contractors": cc_active_contractors,
                "ccure_active_employees_reported": cc_active_emps,
                "ccure_active_contractors_reported": cc_active_contractors
            },
            "averages": {
                "employee_pct": emp_pct,
                "contractor_pct": contr_pct,
                "overall_pct": overall_pct,
                "avg_headcount_last_7_days": avg7,
                "head_emp_pct_vs_ccure_today": emp_pct,
                "head_contractor_pct_vs_ccure_today": contr_pct,
                "headcount_overall_pct_vs_ccure_today": overall_pct,
                "history_avg_overall_last_7_days": avg7
            }
        }

        return resp
    except Exception:
        logger.exception("build_ccure_averages failed")
        raise

# ------------------------------------------------------------------
# /ccure/averages endpoint - uses helper and broadcasts to SSE clients
# ------------------------------------------------------------------
@app.get("/ccure/averages")
def ccure_averages():
    try:
        resp = build_ccure_averages()
        try:
            broadcast_ccure_update(resp)
        except Exception:
            logger.exception("broadcast failed in /ccure/averages (non-fatal)")
        return JSONResponse(resp)
    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("ccure_averages failed")
        raise HTTPException(status_code=500, detail=f"ccure averages error: {exc}")

# ------------------------------------------------------------------
# The rest of your existing endpoints (compare, upload, ingest, reports)
# Minor changes: after ingest operations we attempt to broadcast updated averages
# ------------------------------------------------------------------

@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from ccure_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("ccure_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")
    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)


@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = Path(OUTPUT_DIR) / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(
            str(full),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            filename=safe_name
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")


@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = Path(UPLOAD_DIR) / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_employee_excel
    except Exception as e:
        logger.exception("ingest_excel import failed")
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_employee_excel(dest)
    # broadcast updated averages
    try:
        resp = build_ccure_averages()
        broadcast_ccure_update(resp)
    except Exception:
        logger.exception("Failed to broadcast after upload_active_employees")
    return {"status":"ok", "path": str(dest)}

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = Path(UPLOAD_DIR) / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_contractor_excel
    except Exception as e:
        logger.exception("ingest_excel import failed")
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_contractor_excel(dest)
    try:
        resp = build_ccure_averages()
        broadcast_ccure_update(resp)
    except Exception:
        logger.exception("Failed to broadcast after upload_active_contractors")
    return {"status":"ok", "path": str(dest)}


@app.post("/ingest/live-details")
async def ingest_live(request: Request):
    details = None
    try:
        body = await request.json()
        if isinstance(body, dict) and 'details' in body:
            details = body['details']
        else:
            details = body
    except Exception:
        try:
            form = await request.form()
            if 'details' in form:
                raw = form['details']
                if isinstance(raw, str):
                    details = json.loads(raw)
                else:
                    try:
                        details = json.loads((await raw.read()).decode('utf-8'))
                    except Exception:
                        details = list(form.getlist('details'))
            else:
                first = None
                for v in form.values():
                    first = v
                    break
                if isinstance(first, str):
                    details = json.loads(first)
                else:
                    raise HTTPException(status_code=400, detail="No JSON payload found")
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Could not parse request body as JSON or form: {e}")

    if not isinstance(details, (list, tuple)):
        raise HTTPException(status_code=400, detail="Expected top-level array (JSON list) of detail objects")

    try:
        from compare_service import ingest_live_details_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    try:
        res = ingest_live_details_list(details)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to ingest details: {e}")

    # After successful ingest, recompute/broadcast ccure averages (best-effort)
    try:
        resp = build_ccure_averages()
        broadcast_ccure_update(resp)
    except Exception:
        logger.exception("Failed to broadcast after ingest_live (non-fatal)")

    if isinstance(res, dict):
        return {"status": "ok", **res}
    return {"status": "ok", "inserted": len(details)}


@app.get("/ingest/fetch-all")
def fetch_all_and_ingest():
    try:
        import region_clients
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"region_clients unavailable: {e}")

    details = region_clients.fetch_all_details()
    if not isinstance(details, list):
        raise HTTPException(status_code=500, detail="Unexpected data from region_clients.fetch_all_details")

    try:
        from compare_service import ingest_live_details_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    res = ingest_live_details_list(details)
    # broadcast
    try:
        resp = build_ccure_averages()
        broadcast_ccure_update(resp)
    except Exception:
        logger.exception("Failed to broadcast after fetch-all")
    if isinstance(res, dict):
        return {"status":"ok", **res}
    return {"status":"ok", "inserted": len(details)}


@app.get("/reports/daily/{yyyymmdd}")
def daily_report(yyyymmdd: str):
    import datetime
    try:
        dt = datetime.datetime.strptime(yyyymmdd, "%Y%m%d").date()
    except Exception:
        raise HTTPException(status_code=400, detail="Date must be in YYYYMMDD format")

    try:
        from compare_service import compute_daily_attendance, compare_with_active
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    compute_daily_attendance(dt)
    summary = compare_with_active(dt)

    try:
        resp = build_ccure_averages()
        broadcast_ccure_update(resp)
    except Exception:
        logger.exception("Failed to broadcast after daily_report")

    return JSONResponse(summary)


@app.get("/ccure/stats")
def ccure_stats():
    try:
        import ccure_client
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ccure_client import failed: {e}")
    try:
        stats = ccure_client.get_global_stats()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ccure_client.get_global_stats failed: {e}")
    return stats







// AVERAGES: use SSE (direct to Python backend) with fallback initial fetch
useEffect(() => {
  let stopped = false;
  let es = null;
  let backoff = 1000;

  // Allow override via VITE_PY_BACKEND; otherwise assume python at :8000
  const PY_BACKEND = (import.meta.env.VITE_PY_BACKEND || `${window.location.protocol}//${window.location.hostname}:8000`).replace(/\/$/, '');

  const connect = () => {
    if (stopped) return;
    try {
      // Directly connect to Python SSE endpoint (bypasses Vite proxy for streaming)
      es = new EventSource(`${PY_BACKEND}/ccure/stream`);
    } catch (err) {
      console.warn('SSE creation failed', err);
      es = null;
    }

    if (!es) {
      // fallback to polling if EventSource not supported or creation failed
      initialFetch();
      return;
    }

    es.onopen = () => {
      console.info('[SSE] connected to', `${PY_BACKEND}/ccure/stream`);
      backoff = 1000;
      setAveragesError(null);
    };

    es.onmessage = (evt) => {
      try {
        const payload = JSON.parse(evt.data);
        setAverages(payload);
        setLoadingAverages(false);
        setAveragesError(null);
      } catch (e) {
        console.warn('Failed to parse SSE message', e);
      }
    };

    es.onerror = (err) => {
      console.warn('[SSE] error/closed, attempting reconnect', err);
      try { es.close(); } catch (e) {}
      es = null;
      if (stopped) return;
      // exponential backoff reconnect (capped)
      setTimeout(() => {
        backoff = Math.min(backoff * 2, 30000);
        connect();
      }, backoff);
    };
  };

  const initialFetch = async () => {
    setLoadingAverages(true);
    setAveragesError(null);
    try {
      const res = await api.get('/ccure/averages');
      setAverages(res.data);
      setLoadingAverages(false);
      setAveragesError(null);
    } catch (err) {
      console.warn('initial /ccure/averages fetch failed', err);
      setLoadingAverages(false);
      setAveragesError(err);
    }
  };

  // Start with initial fetch so UI is populated quickly, then open SSE
  initialFetch();
  connect();

  return () => {
    stopped = true;
    if (es) {
      try { es.close(); } catch (e) {}
      es = null;
    }
  };
}, []);
















When I Upadte this File We Got this Error in Console and 
and 

Live vs CCURE Summary
CCURE Active (reported)
8615
Active Employees
658
Active Contractors
Live Today
2025-08-26
1221
Employee
116
Contractor
Totals
Reported total

1303

and Here Live vs CCURE Live Employee and Contractor 
and reported Total Count is Not updated realtime this need to Fix ..



Failed to load resource: the server responded with a status of 404 (Not Found)

GlobalPage.jsx:335 [SSE] error/closed, attempting reconnect 
Event
:5173/api/ccure/stream:1 
 Failed to load resource: the server responded with a status of 404 (Not Found)
GlobalPage.jsx:335 [SSE] error/closed, attempting reconnect 
Event
:5173/api/ccure/stream:1 
 Failed to load resource: the server responded with a status of 404 (Not Found)
GlobalPage.jsx:335 [SSE] error/closed, attempting reconnect 
Event
:5173/api/ccure/stream:1 
 Failed to load resource: the server responded with a status of 404 (Not Found)
GlobalPage.jsx:335 [SSE] error/closed, attempting reconnect 
Event
:5173/api/ccure/stream:1 
 Failed to load resource: the server responded with a status of 404 (Not Found)
GlobalPage.jsx:335 [SSE] error/closed, attempting reconnect 
Event
stream:1 
 GET http://localhost:5173/api/ccure/stream 404 (Not Found)
GlobalPage.jsx:335 [SSE] error/closed, attempting reconnect 
Event {isTrusted: true, type: 'error', target: EventSource, currentTarget: EventSource, eventPhase: 2, â€¦}
ï»¿





C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py


# app.py
from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Query
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
import shutil
import uuid
import json
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
import re
import asyncio

# --- DB / models imports (your existing project modules) ---
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary

# optional: ccure compare service and others are imported inside endpoints when needed

# --- settings (assumes these exist in your project) ---
try:
    from settings import UPLOAD_DIR, OUTPUT_DIR
except Exception:
    # fallback defaults
    UPLOAD_DIR = "./uploads"
    OUTPUT_DIR = "./output"

# --- app & logging setup ---
app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# ------------------------------------------------------------------
# Small SSE broadcaster (in-process). Minimal, works for single-process deployments.
# Each connected client gets its own asyncio.Queue; we push payloads to all queues.
# ------------------------------------------------------------------
_broadcaster_clients = set()  # set of asyncio.Queue

def broadcast_ccure_update(payload: dict):
    """
    Non-blocking broadcast to all connected SSE clients.
    Uses asyncio.get_event_loop().call_soon_threadsafe to schedule queue.put_nowait
    so it is safe to call from sync endpoints.
    """
    if not _broadcaster_clients:
        return
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        # no running loop (shouldn't happen in normal uvicorn) - try to schedule via new loop
        loop = None

    for q in list(_broadcaster_clients):
        try:
            if loop and loop.is_running():
                loop.call_soon_threadsafe(q.put_nowait, payload)
            else:
                # fallback: attempt put_nowait (may raise)
                q.put_nowait(payload)
        except Exception:
            logger.exception("Failed to push payload to SSE client (will remove client)")
            try:
                _broadcaster_clients.discard(q)
            except Exception:
                pass

async def _sse_event_generator(client_queue: asyncio.Queue):
    """
    Async generator yielding SSE data lines from provided queue.
    The generator will exit when client disconnects and finally block will allow cleanup.
    """
    try:
        # Keep yielding as items arrive
        while True:
            payload = await client_queue.get()
            # send JSON payload as a single SSE "data:" event
            try:
                data = json.dumps(payload, default=str)
            except Exception:
                # fallback if non-serializable
                data = json.dumps({"error": "serialization error", "payload": str(payload)})
            yield f"data: {data}\n\n"
    finally:
        # cleanup of queue is handled by the endpoint wrapper
        return

@app.get("/ccure/stream")
async def ccure_stream():
    """
    SSE endpoint that streams ccure/averages updates.
    Frontend should connect to '/api/ccure/stream' (vite proxy routes to /ccure/stream).
    """
    q = asyncio.Queue()
    _broadcaster_clients.add(q)
    generator = _sse_event_generator(q)

    async def cleanup():
        try:
            _broadcaster_clients.discard(q)
        except Exception:
            pass

    # StreamingResponse will iterate the async generator and keep connection open
    response = StreamingResponse(generator, media_type="text/event-stream")
    # When the response is finished/closed, ensure the queue is removed
    # There is no direct callback here; but the generator's finally will be called on disconnect;
    # ensure queue removal as well (best-effort)
    # return response (queue removal occurs in generator finally / broadcast failure handlers)
    return response

# ------------------------------------------------------------------
# Helper functions (normalizers / region guesser)
# ------------------------------------------------------------------
def _guess_region_from_text(txt: str) -> str:
    """Return 'apac','emea','laca','namer' or 'unknown' using substring keywords."""
    if not txt:
        return "unknown"
    s = str(txt).strip().lower()
    s = re.sub(r"[,\-/()]", " ", s)
    # APAC keywords
    if any(k in s for k in ("pune","mumbai","bangalore","bengaluru","hyderabad","chennai","manila","singapore","hong kong","beijing","shanghai","jakarta","kuala","osaka","tokyo","seoul","bangkok")):
        return "apac"
    # EMEA keywords
    if any(k in s for k in ("london","dublin","paris","frankfurt","amsterdam","stockholm","cape town","johannesburg","berlin","brussels","madrid","rome","milan")):
        return "emea"
    # LACA keywords
    if any(k in s for k in ("mexico","bogota","buenos","santiago","sao","salvador","lima","caracas")):
        return "laca"
    # NAMER keywords
    if any(k in s for k in ("denver","new york","ny","chicago","toronto","vancouver","los angeles","san francisco","boston","houston","atlanta","miami")):
        return "namer"
    return "unknown"

# ------------------------------------------------------------------
# HEADCOUNT endpoint (exact '/headcount' path) - unchanged logic
# ------------------------------------------------------------------
@app.get("/headcount")
def api_headcount():
    try:
        totals = {"apac": 0, "emea": 0, "laca": 0, "namer": 0, "unknown": 0}
        with SessionLocal() as db:
            try:
                emp_rows = db.query(ActiveEmployee).all()
            except Exception:
                logger.exception("Failed to query ActiveEmployee")
                emp_rows = []
            try:
                contr_rows = db.query(ActiveContractor).all()
            except Exception:
                logger.exception("Failed to query ActiveContractor")
                contr_rows = []

            def _loc_from_employee(e):
                return getattr(e, "location_city", None) or getattr(e, "location", None) or getattr(e, "Location", None)

            def _loc_from_contractor(c):
                return getattr(c, "location", None) or getattr(c, "location_city", None) or getattr(c, "Location", None)

            for e in emp_rows:
                loc = _loc_from_employee(e)
                region = _guess_region_from_text(loc)
                totals[region] = totals.get(region, 0) + 1

            for c in contr_rows:
                loc = _loc_from_contractor(c)
                region = _guess_region_from_text(loc)
                totals[region] = totals.get(region, 0) + 1

        out = {
            "apac": int(totals.get("apac", 0)),
            "emea": int(totals.get("emea", 0)),
            "laca": int(totals.get("laca", 0)),
            "namer": int(totals.get("namer", 0))
        }
        return JSONResponse(out)
    except Exception as exc:
        logger.exception("api_headcount failed")
        raise HTTPException(status_code=500, detail=f"headcount error: {exc}")

# ------------------------------------------------------------------
# Build ccure averages payload (extracted helper)
# ------------------------------------------------------------------
def build_ccure_averages():
    """
    Builds the payload dictionary that /ccure/averages returns.
    This code mirrors the previous /ccure/averages handler (kept intact) but returns a dict.
    """
    try:
        today = date.today()

        with SessionLocal() as db:
            # Fetch AttendanceSummary rows for today
            try:
                att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
            except Exception:
                logger.exception("Failed to query AttendanceSummary")
                att_rows = []

            # Build active employee and contractor id sets
            active_employee_ids = set()
            active_contractor_primary_ids = set()

            try:
                emps = db.query(ActiveEmployee).all()
                for e in emps:
                    if getattr(e, "employee_id", None):
                        active_employee_ids.add(str(e.employee_id).strip())
            except Exception:
                logger.exception("Failed to load ActiveEmployee rows")

            try:
                contrs = db.query(ActiveContractor).all()
                for c in contrs:
                    primary = getattr(c, "worker_system_id", None) or getattr(c, "ipass_id", None) or getattr(c, "worker_id", None)
                    if primary:
                        active_contractor_primary_ids.add(str(primary).strip())
            except Exception:
                logger.exception("Failed to load ActiveContractor rows")

            # Map attendance keys -> employee/contractor counts.
            live_emp = 0
            live_contr = 0
            unknown_count = 0
            seen_keys = set()
            for a in att_rows:
                key = (a.employee_id or "").strip() if a.employee_id else None
                if not key:
                    try:
                        key = (a.derived.get('card_number') or "").strip() if (a.derived and isinstance(a.derived, dict)) else None
                    except Exception:
                        key = None
                if not key:
                    unknown_count += 1
                    continue
                if key in seen_keys:
                    continue
                seen_keys.add(key)
                if key in active_employee_ids:
                    live_emp += 1
                elif key in active_contractor_primary_ids:
                    live_contr += 1
                else:
                    numeric = re.sub(r'\D+', '', key)
                    if numeric:
                        if numeric in active_employee_ids or numeric.lstrip('0') in active_employee_ids:
                            live_emp += 1
                        elif numeric in active_contractor_primary_ids or numeric.lstrip('0') in active_contractor_primary_ids:
                            live_contr += 1
                        else:
                            unknown_count += 1
                    else:
                        unknown_count += 1

            live_total_reported = live_emp + live_contr + unknown_count
            live_total_details = len(att_rows)

            # 7-day average headcount (unique keys present per day)
            try:
                start_7 = today - timedelta(days=6)
                q = db.query(AttendanceSummary.date, AttendanceSummary.employee_id, AttendanceSummary.presence_count)\
                      .filter(AttendanceSummary.date >= start_7, AttendanceSummary.date <= today).all()
                by_date = {}
                for row in q:
                    d = row[0]
                    key = (row[1] or "").strip() if row[1] else None
                    if not key:
                        continue
                    if d not in by_date:
                        by_date[d] = set()
                    try:
                        presence_val = getattr(row, 'presence_count', row[2])
                        if int(presence_val) > 0:
                            by_date[d].add(key)
                    except Exception:
                        by_date[d].add(key)
                daily_counts = [len(by_date.get(start_7 + timedelta(days=i), set())) for i in range(7)]
                avg7 = int(round(sum(daily_counts) / 7.0)) if any(daily_counts) else 0
            except Exception:
                logger.exception("Failed computing 7-day average")
                avg7 = None

        # get CCURE stats if available
        ccure_stats = {}
        try:
            import ccure_client
            if hasattr(ccure_client, "get_global_stats"):
                ccure_stats = ccure_client.get_global_stats() or {}
        except Exception:
            logger.debug("ccure_client.get_global_stats not available", exc_info=True)

        cc_active_emps = None
        cc_active_contractors = None
        try:
            if isinstance(ccure_stats, dict):
                a = ccure_stats.get("ActiveEmployees") or ccure_stats.get("active_employees") or None
                b = ccure_stats.get("ActiveContractors") or ccure_stats.get("active_contractors") or None
                if a is not None and str(a).strip() != "":
                    cc_active_emps = int(a)
                if b is not None and str(b).strip() != "":
                    cc_active_contractors = int(b)
        except Exception:
            cc_active_emps = cc_active_contractors = None

        # percentages vs CCURE
        emp_pct = None
        contr_pct = None
        overall_pct = None
        try:
            if isinstance(cc_active_emps, int) and cc_active_emps > 0:
                emp_pct = round((live_emp / float(cc_active_emps)) * 100.0, 2)
            if isinstance(cc_active_contractors, int) and cc_active_contractors > 0:
                contr_pct = round((live_contr / float(cc_active_contractors)) * 100.0, 2)
            if isinstance(cc_active_emps, int) and isinstance(cc_active_contractors, int) and (cc_active_emps + cc_active_contractors) > 0:
                overall_pct = round(((live_emp + live_contr) / float(cc_active_emps + cc_active_contractors)) * 100.0, 2)
        except Exception:
            emp_pct = contr_pct = overall_pct = None

        resp = {
            "date": today.isoformat(),
            "notes": None,
            "live_today": {
                "employee": live_emp,
                "contractor": live_contr,
                "total_reported": live_total_reported,
                "total_from_details": live_total_details
            },
            "ccure_active": {
                "active_employees": cc_active_emps,
                "active_contractors": cc_active_contractors,
                "ccure_active_employees_reported": cc_active_emps,
                "ccure_active_contractors_reported": cc_active_contractors
            },
            "averages": {
                "employee_pct": emp_pct,
                "contractor_pct": contr_pct,
                "overall_pct": overall_pct,
                "avg_headcount_last_7_days": avg7,
                # backwards-compat fields UI may use
                "head_emp_pct_vs_ccure_today": emp_pct,
                "head_contractor_pct_vs_ccure_today": contr_pct,
                "headcount_overall_pct_vs_ccure_today": overall_pct,
                "history_avg_overall_last_7_days": avg7
            }
        }

        return resp
    except Exception:
        logger.exception("build_ccure_averages failed")
        raise

# ------------------------------------------------------------------
# /ccure/averages endpoint - uses helper and broadcasts to SSE clients
# ------------------------------------------------------------------
@app.get("/ccure/averages")
def ccure_averages():
    """
    Returns the ccure averages payload and broadcasts to connected SSE clients.
    """
    try:
        resp = build_ccure_averages()
        # broadcast to SSE clients (best-effort; non-blocking)
        try:
            broadcast_ccure_update(resp)
        except Exception:
            logger.exception("broadcast failed in /ccure/averages (non-fatal)")
        return JSONResponse(resp)
    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("ccure_averages failed")
        raise HTTPException(status_code=500, detail=f"ccure averages error: {exc}")

# ------------------------------------------------------------------
# The rest of your existing endpoints (compare, upload, ingest, reports)
# Minor change: after ingest_live successfully ingests, compute & broadcast new ccure averages
# ------------------------------------------------------------------

@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from ccure_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("ccure_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")
    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)


@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = Path(OUTPUT_DIR) / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(
            str(full),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            filename=safe_name
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")


@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = Path(UPLOAD_DIR) / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_employee_excel
    except Exception as e:
        logger.exception("ingest_excel import failed")
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_employee_excel(dest)
    # After upload/ingest, try to broadcast updated averages (best-effort)
    try:
        resp = build_ccure_averages()
        broadcast_ccure_update(resp)
    except Exception:
        logger.exception("Failed to broadcast after upload_active_employees")
    return {"status":"ok", "path": str(dest)}

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = Path(UPLOAD_DIR) / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_contractor_excel
    except Exception as e:
        logger.exception("ingest_excel import failed")
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_contractor_excel(dest)
    try:
        resp = build_ccure_averages()
        broadcast_ccure_update(resp)
    except Exception:
        logger.exception("Failed to broadcast after upload_active_contractors")
    return {"status":"ok", "path": str(dest)}


@app.post("/ingest/live-details")
async def ingest_live(request: Request):
    """
    Accepts:
      - Raw JSON array in body (preferred)
      - JSON object with {"details": [...]} in body
      - multipart/form-data where a form field 'details' contains a JSON string
    """
    details = None
    # Try direct JSON
    try:
        body = await request.json()
        if isinstance(body, dict) and 'details' in body:
            details = body['details']
        else:
            details = body
    except Exception:
        # not JSON, try form
        try:
            form = await request.form()
            if 'details' in form:
                raw = form['details']
                if isinstance(raw, str):
                    details = json.loads(raw)
                else:
                    try:
                        details = json.loads((await raw.read()).decode('utf-8'))
                    except Exception:
                        details = list(form.getlist('details'))
            else:
                # attempt first field
                first = None
                for v in form.values():
                    first = v
                    break
                if isinstance(first, str):
                    details = json.loads(first)
                else:
                    raise HTTPException(status_code=400, detail="No JSON payload found")
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Could not parse request body as JSON or form: {e}")

    if not isinstance(details, (list, tuple)):
        raise HTTPException(status_code=400, detail="Expected top-level array (JSON list) of detail objects")

    try:
        from compare_service import ingest_live_details_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    try:
        res = ingest_live_details_list(details)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to ingest details: {e}")

    # After successful ingest, recompute/broadcast ccure averages (best-effort)
    try:
        resp = build_ccure_averages()
        broadcast_ccure_update(resp)
    except Exception:
        logger.exception("Failed to broadcast after ingest_live (non-fatal)")

    if isinstance(res, dict):
        return {"status": "ok", **res}
    return {"status": "ok", "inserted": len(details)}


@app.get("/ingest/fetch-all")
def fetch_all_and_ingest():
    try:
        import region_clients
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"region_clients unavailable: {e}")

    details = region_clients.fetch_all_details()
    if not isinstance(details, list):
        raise HTTPException(status_code=500, detail="Unexpected data from region_clients.fetch_all_details")

    try:
        from compare_service import ingest_live_details_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    res = ingest_live_details_list(details)
    # broadcast
    try:
        resp = build_ccure_averages()
        broadcast_ccure_update(resp)
    except Exception:
        logger.exception("Failed to broadcast after fetch-all")
    if isinstance(res, dict):
        return {"status":"ok", **res}
    return {"status":"ok", "inserted": len(details)}


@app.get("/reports/daily/{yyyymmdd}")
def daily_report(yyyymmdd: str):
    import datetime
    try:
        dt = datetime.datetime.strptime(yyyymmdd, "%Y%m%d").date()
    except Exception:
        raise HTTPException(status_code=400, detail="Date must be in YYYYMMDD format")

    try:
        from compare_service import compute_daily_attendance, compare_with_active
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    # compute attendance summary (will build attendance_summary rows)
    compute_daily_attendance(dt)

    # compare vs active and CCURE
    summary = compare_with_active(dt)

    # broadcast new averages (best-effort)
    try:
        resp = build_ccure_averages()
        broadcast_ccure_update(resp)
    except Exception:
        logger.exception("Failed to broadcast after daily_report")

    return JSONResponse(summary)


# additional endpoint to fetch CCure stats directly (optional)
@app.get("/ccure/stats")
def ccure_stats():
    try:
        import ccure_client
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ccure_client import failed: {e}")
    try:
        stats = ccure_client.get_global_stats()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ccure_client.get_global_stats failed: {e}")
    return stats








# region_clients.py
import requests
from requests.exceptions import RequestException
from datetime import datetime
import logging

logger = logging.getLogger("region_clients")
logger.setLevel(logging.INFO)
if not logger.handlers:
    import sys
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Edit endpoints if your hosts/ports differ
endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/live-summary",
    "emea":  "http://10.199.22.57:3007/api/occupancy/live-summary",
    "laca":  "http://10.199.22.57:4000/api/occupancy/live-summary",
    "apac":  "http://10.199.22.57:3008/api/occupancy/live-summary"
}

# History endpoints (as provided)
history_endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/history",
    "emea":  "http://10.199.22.57:3007/api/occupancy/history",
    "apac":  "http://10.199.22.57:3008/api/occupancy/history",
    "laca":  "http://10.199.22.57:4000/api/occupancy/history"
}

def fetch_all_regions(timeout=6):
    """Return list of dicts: [{region: name, count: N}, ...]"""
    results = []
    for region, url in endpoints.items():
        try:
            r = requests.get(url, timeout=timeout)
            r.raise_for_status()
            data = r.json()
            realtime = data.get("realtime", {}) if isinstance(data, dict) else {}
            total = 0
            # realtime may be a dict of sites => siteobj
            for site in realtime.values():
                try:
                    total += int(site.get("total", 0))
                except Exception:
                    pass
            results.append({"region": region, "count": total})
        except RequestException as e:
            logger.warning(f"[region_clients] error fetching live-summary for {region} @ {url}: {e}")
            results.append({"region": region, "count": None})
        except Exception as e:
            logger.exception(f"[region_clients] unexpected error fetching live-summary for {region}: {e}")
            results.append({"region": region, "count": None})
    return results

def fetch_all_details(timeout=6):
    """
    Return flattened 'details' list across all regions (tagged with '__region').
    Returns an empty list if none available.
    """
    all_details = []
    for region, url in endpoints.items():
        try:
            r = requests.get(url, timeout=timeout)
            r.raise_for_status()
            data = r.json()
            details = data.get("details", []) if isinstance(data, dict) else []
            for d in details:
                d2 = dict(d)
                d2["__region"] = region
                all_details.append(d2)
        except RequestException as e:
            logger.warning(f"[region_clients] warning: cannot fetch details from {region}@{url}: {e}")
            continue
        except Exception as e:
            logger.exception(f"[region_clients] unexpected error for {region} details: {e}")
            continue
    return all_details

# ---------- new: history fetchers ------------------------------------------

def fetch_history_for_region(region, timeout=6):
    """
    Fetch history endpoint for a single region.
    Returns list of summaryByDate entries where each entry is a dict (or [] on failure).
    Each returned entry will have an added key '__region' with the region id.
    """
    url = history_endpoints.get(region)
    if not url:
        logger.debug(f"[fetch_history_for_region] no history endpoint for {region}")
        return []
    try:
        r = requests.get(url, timeout=timeout)
        r.raise_for_status()
        data = r.json()
        summary = data.get("summaryByDate", []) if isinstance(data, dict) else []
        out = []
        for s in summary:
            try:
                s2 = dict(s)
                s2["__region"] = region
                out.append(s2)
            except Exception:
                continue
        return out
    except RequestException as e:
        logger.warning(f"[region_clients] cannot fetch history for {region}@{url}: {e}")
        return []
    except Exception as e:
        logger.exception(f"[region_clients] unexpected error fetching history for {region}: {e}")
        return []

def fetch_all_history(timeout=6):
    """
    Fetch history summaryByDate for all regions.
    Returns a list of entries like:
     [ { "date": "2025-08-20", "region": {...}, "partitions": {...}, "__region":"laca" }, ... ]
    If a region fails, it's skipped.
    """
    all_entries = []
    for region in history_endpoints.keys():
        try:
            entries = fetch_history_for_region(region, timeout=timeout)
            if entries:
                all_entries.extend(entries)
        except Exception:
            logger.exception(f"[region_clients] error aggregating history for {region}")
            continue
    return all_entries







# ccure_compare_service.py
"""
Compare CCURE profiles/stats with local sheets + compute visit averages + compliance.

Key behaviors:
 - If AttendanceSummary for today is empty, attempt to call compute_daily_attendance() to build it from LiveSwipe.
 - Provide headcount (AttendanceSummary) and live_headcount (region_clients) with per-location breakdowns.
 - ccure_active exposes only reported ActiveEmployees and ActiveContractors (no derived fields).
 - Computes averages (last 7 days) and today's percentages vs CCURE reported counts.
 - Compliance (meets_5days_8h, meets_3days_8h, defaulters) computed using AttendanceSummary historical data.
"""

import re
import traceback
from datetime import date, datetime, timedelta
from typing import List, Dict, Any, Optional, Set

import logging

logger = logging.getLogger("ccure_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, AttendanceSummary, LiveSwipe
from settings import OUTPUT_DIR

# ---------- small helpers ----------------------------------------------------

def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

def _sanitize_for_json(value):
    try:
        import numpy as _np
    except Exception:
        _np = None
    if value is None:
        return None
    if isinstance(value, (str, bool, int)):
        return value
    if isinstance(value, float):
        if _np is not None and not _np.isfinite(value):
            return None
        return float(value)
    if _np is not None and isinstance(value, (_np.integer,)):
        return int(value)
    if isinstance(value, dict):
        out = {}
        for k, v in value.items():
            try:
                key = str(k)
            except Exception:
                key = repr(k)
            out[key] = _sanitize_for_json(v)
        return out
    if isinstance(value, (list, tuple, set)):
        return [_sanitize_for_json(v) for v in value]
    try:
        return str(value)
    except Exception:
        return None

# ---------- ccure helpers ---------------------------------------------------

def _fetch_ccure_stats():
    try:
        import ccure_client
        if hasattr(ccure_client, "get_global_stats"):
            return ccure_client.get_global_stats()
    except Exception:
        logger.debug("ccure_client.get_global_stats not available", exc_info=True)
    return None

def _fetch_ccure_profiles():
    try:
        import ccure_client
        for fn in ("fetch_all_employees_full", "fetch_all_employees", "fetch_all_profiles", "fetch_profiles", "fetch_all"):
            if hasattr(ccure_client, fn):
                try:
                    res = getattr(ccure_client, fn)()
                    if isinstance(res, list):
                        return res
                except Exception:
                    continue
    except Exception:
        pass
    return []

def _extract_ccure_locations_from_profiles(profiles: List[dict]) -> Set[str]:
    locs = set()
    for p in profiles:
        if not isinstance(p, dict):
            continue
        for k in ("Partition", "PartitionName", "Location", "Location City", "location_city", "location", "Site", "BaseLocation"):
            v = p.get(k) if isinstance(p, dict) else None
            if v and isinstance(v, str) and v.strip():
                locs.add(v.strip())
    return locs

# ---------- classification & partition helpers ------------------------------

def classify_personnel_from_detail(detail: dict) -> str:
    """Map many CCURE / live-summary personnel strings to 'employee' or 'contractor'."""
    try:
        if not isinstance(detail, dict):
            return "contractor"
        candidate_keys = [
            "PersonnelType", "personnelType", "personnel_type", "Personnel Type",
            "PersonnelTypeName", "Personnel", "Type", "personnel", "PersonType", "personType"
        ]
        val = None
        for k in candidate_keys:
            if k in detail and detail.get(k) is not None:
                val = str(detail.get(k)).strip().lower()
                break
        status_keys = ["Employee_Status", "Employee Status", "Status", "Profile_Disabled"]
        status_val = None
        for k in status_keys:
            if k in detail and detail.get(k) is not None:
                status_val = str(detail.get(k)).strip().lower()
                break

        if status_val is not None and "terminated" in status_val:
            return "employee"
        if val is None or val == "":
            return "contractor"
        if "employee" in val:
            return "employee"
        if "terminated" in val:
            return "employee"
        contractor_terms = ["contractor", "visitor", "property", "property management", "temp", "temp badge", "tempbadge"]
        for t in contractor_terms:
            if t in val:
                return "contractor"
        if "contract" in val or "visitor" in val:
            return "contractor"
        return "contractor"
    except Exception:
        return "contractor"

def pick_partition_from_detail(detail: dict) -> str:
    if not isinstance(detail, dict):
        return "Unknown"
    for k in ("PartitionName2","PartitionName1","Partition","PartitionName","Region","Location","Site","location_city","Location City"):
        if k in detail and detail.get(k):
            try:
                return str(detail.get(k)).strip()
            except Exception:
                continue
    if "__region" in detail and detail.get("__region"):
        return str(detail.get("__region")).strip()
    return "Unknown"

# ---------- WFH detection helper -------------------------------------------

def is_employee_wfh(active_emp_row: ActiveEmployee) -> bool:
    try:
        wfh_keywords = ("work from home", "wfh", "remote", "workfromhome", "home")
        for attr in ("is_wfh", "work_from_home", "wfh", "remote_flag"):
            if hasattr(active_emp_row, attr):
                try:
                    val = getattr(active_emp_row, attr)
                    if isinstance(val, bool) and val:
                        return True
                    if isinstance(val, str) and any(k in val.strip().lower() for k in wfh_keywords):
                        return True
                except Exception:
                    pass
        for attr in ("location_description", "location_desc", "location_description1", "base_location", "location", "location_city"):
            if hasattr(active_emp_row, attr):
                try:
                    v = getattr(active_emp_row, attr)
                    if v and isinstance(v, str):
                        s = v.strip().lower()
                        if any(k in s for k in wfh_keywords):
                            return True
                except Exception:
                    pass
        try:
            rr = getattr(active_emp_row, "raw_row", None)
            if rr and isinstance(rr, dict):
                for k, v in rr.items():
                    try:
                        if v and isinstance(v, str) and any(word in v.strip().lower() for word in wfh_keywords):
                            return True
                    except Exception:
                        continue
        except Exception:
            pass
    except Exception:
        pass
    return False

# ---------- utility: fallback headcount builder from LiveSwipe --------------

def build_headcount_from_liveswipes_for_today(session) -> (int, Dict[str, Dict[str, int]]):
    """
    When AttendanceSummary for today is empty, build headcount by scanning LiveSwipe rows for today
    Deduplicate by key (employee_id or card) and compute per-location counts.
    Returns (total_count, by_location dict)
    """
    start = datetime.combine(date.today(), datetime.min.time())
    end = datetime.combine(date.today(), datetime.max.time())
    swipes = session.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
    if not swipes:
        return 0, {}
    seen_keys = {}
    per_loc = {}
    for s in swipes:
        key = _normalize_employee_key(s.employee_id) or _normalize_card_like(s.card_number)
        if not key:
            key = f"nokey_{s.id}"
        rec = seen_keys.get(key)
        ts = s.timestamp
        if rec is None:
            seen_keys[key] = {"first_seen": ts, "last_seen": ts, "partition": (s.partition or "Unknown"), "class": None, "card": s.card_number, "raw": s.raw}
        else:
            if ts and rec.get("first_seen") and ts < rec["first_seen"]:
                rec["first_seen"] = ts
            if ts and rec.get("last_seen") and ts > rec["last_seen"]:
                rec["last_seen"] = ts
    for k, v in seen_keys.items():
        loc = v.get("partition") or "Unknown"
        if not isinstance(loc, str) or not loc.strip():
            loc = "Unknown"
        if loc not in per_loc:
            per_loc[loc] = {"total": 0, "employee": 0, "contractor": 0}
        per_loc[loc]["total"] += 1
        classified = "contractor"
        raw = v.get("raw")
        if isinstance(raw, dict):
            try:
                classified = classify_personnel_from_detail(raw)
            except Exception:
                classified = "contractor"
        per_loc[loc][classified] += 1
    total = sum(p["total"] for p in per_loc.values())
    return int(total), per_loc

# ---------- main compute function -----------------------------------------




def compute_visit_averages(timeout: int = 6) -> Dict[str, Any]:
    notes = []
    today = date.today()
    week_start = today - timedelta(days=6)  # last 7 days inclusive

    # --- try to get CCURE stats/profiles early for filtering & denominators
    ccure_stats = _fetch_ccure_stats()
    reported_active_emps = _safe_int(ccure_stats.get("ActiveEmployees")) if isinstance(ccure_stats, dict) else None
    reported_active_contractors = _safe_int(ccure_stats.get("ActiveContractors")) if isinstance(ccure_stats, dict) else None

    ccure_profiles = _fetch_ccure_profiles()
    ccure_locations = _extract_ccure_locations_from_profiles(ccure_profiles) if isinstance(ccure_profiles, list) else set()

    # --- HEADCOUNT (AttendanceSummary for today) with fallback
    head_total = 0
    head_per_location: Dict[str, Dict[str, int]] = {}
    try:
        session = SessionLocal()
        att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
        if not att_rows_today:
            # Attempt to build AttendanceSummary from LiveSwipe using compute_daily_attendance (if available)
            built_ok = False
            try:
                # import local compare_service.compute_daily_attendance if available
                from compare_service import compute_daily_attendance as _compute_daily_attendance
                try:
                    built = _compute_daily_attendance(today)
                    # If compute_daily_attendance returns rows, requery AttendanceSummary
                    if isinstance(built, list) and len(built) > 0:
                        att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                        built_ok = True
                        notes.append("AttendanceSummary was missing; built from LiveSwipe via compute_daily_attendance().")
                except Exception:
                    # fall through to fallback builder
                    logger.exception("compute_daily_attendance execution failed; falling back")
            except Exception:
                # compare_service not importable -> fallback
                logger.debug("compare_service.compute_daily_attendance not importable; falling back", exc_info=True)

            if not att_rows_today:
                # fallback: build headcount from LiveSwipe directly (non-persistent)
                built_total, built_per_loc = build_headcount_from_liveswipes_for_today(session)
                head_total = built_total
                head_per_location = built_per_loc
                if head_total > 0:
                    notes.append("AttendanceSummary for today empty; built headcount from LiveSwipe rows (non-persistent fallback).")
        if att_rows_today:
            # classify using ActiveEmployee / ActiveContractor sets
            act_emps = session.query(ActiveEmployee).all()
            act_contrs = session.query(ActiveContractor).all()
            emp_id_set = set()
            contr_id_set = set()
            card_to_emp = {}
            for e in act_emps:
                v = _normalize_employee_key(getattr(e, "employee_id", None))
                if v:
                    emp_id_set.add(v)
                try:
                    rr = getattr(e, "raw_row", None)
                    if rr and isinstance(rr, dict):
                        for ck in ("CardNumber","card_number","Card","Card No","CardNo","Badge","BadgeNo","IPassID","iPass ID","IPASSID"):
                            if ck in rr and rr.get(ck):
                                cn = _normalize_card_like(rr.get(ck))
                                if cn:
                                    card_to_emp[cn] = v
                except Exception:
                    pass
            for c in act_contrs:
                wid = _normalize_employee_key(getattr(c, "worker_system_id", None))
                ip = _normalize_employee_key(getattr(c, "ipass_id", None))
                primary = wid or ip
                if primary:
                    contr_id_set.add(primary)

            for a in att_rows_today:
                key = _normalize_employee_key(a.employee_id)
                partition = None
                try:
                    if a.derived and isinstance(a.derived, dict):
                        partition = a.derived.get("partition")
                except Exception:
                    partition = None
                loc = partition or "Unknown"
                if not isinstance(loc, str) or not loc.strip():
                    loc = "Unknown"
                if loc not in head_per_location:
                    head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                if (a.presence_count or 0) > 0:
                    head_total += 1
                    head_per_location[loc]["total"] += 1
                    cls = "contractor"
                    if key and key in emp_id_set:
                        cls = "employee"
                    elif key and key in contr_id_set:
                        cls = "contractor"
                    else:
                        try:
                            card = (a.derived.get("card_number") if (a.derived and isinstance(a.derived, dict)) else None)
                        except Exception:
                            card = None
                        cnorm = _normalize_card_like(card)
                        if cnorm and cnorm in card_to_emp:
                            cls = "employee" if card_to_emp.get(cnorm) in emp_id_set else "contractor"
                        else:
                            cls = "contractor"
                    head_per_location[loc][cls] += 1
        session.expunge_all()
    except Exception:
        logger.exception("Error computing HeadCount")
        notes.append("Failed to compute HeadCount from DB; see server logs.")
    finally:
        try:
            session.close()
        except Exception:
            pass

    # --- LIVE HEADCOUNT via region_clients (as before)
    live_total = 0
    live_per_location: Dict[str, Dict[str, int]] = {}
    sites_queried = 0
    details = []  # ensure defined for later fallbacks
    try:
        import region_clients
        regions_info = []
        try:
            if hasattr(region_clients, "fetch_all_regions"):
                regions_info = region_clients.fetch_all_regions(timeout=timeout) or []
        except Exception:
            logger.exception("region_clients.fetch_all_regions failed")
        try:
            if hasattr(region_clients, "fetch_all_details"):
                details = region_clients.fetch_all_details(timeout=timeout) or []
        except Exception:
            logger.exception("region_clients.fetch_all_details failed")
        sites_queried = len(regions_info) if isinstance(regions_info, list) else 0
        if regions_info:
            for r in regions_info:
                try:
                    c = r.get("count") if isinstance(r, dict) else None
                    ci = _safe_int(c)
                    if ci is not None:
                        live_total += int(ci)
                except Exception:
                    continue
        derived_detail_sum = 0
        if details and isinstance(details, list):
            for d in details:
                try:
                    loc = pick_partition_from_detail(d) or "Unknown"
                    if not isinstance(loc, str) or not loc.strip():
                        loc = "Unknown"
                    pclass = classify_personnel_from_detail(d)
                    if loc not in live_per_location:
                        live_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                    live_per_location[loc]["total"] += 1
                    live_per_location[loc][pclass] += 1
                    derived_detail_sum += 1
                except Exception:
                    continue
            if live_total == 0 and derived_detail_sum > 0:
                live_total = derived_detail_sum
            else:
                if live_total != derived_detail_sum:
                    notes.append(f"Region totals ({live_total}) differ from detail rows ({derived_detail_sum}); using region totals for overall and details for breakdown.")
        else:
            notes.append("No per-person details available from region_clients; live breakdown unavailable.")
    except Exception:
        logger.exception("Error computing Live HeadCount")
        notes.append("Failed to compute Live HeadCount; see logs.")
        live_total = live_total or 0

    # ---------- NEW FALLBACK: if head_total still zero, build from region_clients details ----------
    if (head_total == 0) and details:
        try:
            seen_keys = set()
            for d in details:
                try:
                    # Prefer EmployeeID or CardNumber or PersonGUID as dedupe key
                    key = _normalize_employee_key(d.get("EmployeeID")) or _normalize_card_like(d.get("CardNumber")) or (d.get("PersonGUID") if d.get("PersonGUID") else None)
                    if not key:
                        # try other possible id-like fields
                        key = _normalize_employee_key(d.get("employee_id")) or _normalize_card_like(d.get("Card")) or None
                    if not key:
                        continue
                    key = str(key)
                    if key in seen_keys:
                        continue
                    seen_keys.add(key)
                    loc = pick_partition_from_detail(d) or "Unknown"
                    pclass = classify_personnel_from_detail(d)
                    if loc not in head_per_location:
                        head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                    head_per_location[loc]["total"] += 1
                    head_per_location[loc][pclass] += 1
                    head_total += 1
                except Exception:
                    continue
            if head_total > 0:
                notes.append("AttendanceSummary and LiveSwipe empty; built headcount from region_clients live-summary details (fallback).")
        except Exception:
            logger.exception("Error building headcount from region details fallback")

    # --- CCURE active: exposed only as reported (not derived)
    # reported_active_emps, reported_active_contractors already from ccure_stats above

    # --- Compliance: compute using AttendanceSummary last 7 days (DB)
    compliance = {
        "meets_5days_8h": {"count": 0, "percent_of_ccure_employees": None, "by_location": {}},
        "meets_3days_8h": {"count": 0, "percent_of_ccure_employees": None, "by_location": {}},
        "defaulters": {"count": 0, "percent_of_ccure_employees": None, "by_location": {}, "sample": []}
    }

    try:
        session = SessionLocal()
        active_emps = session.query(ActiveEmployee).all()
        emp_map = {}
        card_to_emp = {}
        for e in active_emps:
            eid = _normalize_employee_key(getattr(e, "employee_id", None))
            emp_map[eid] = e
            try:
                rr = getattr(e, "raw_row", None)
                if rr and isinstance(rr, dict):
                    for ck in ("CardNumber","card_number","Card","Card No","CardNo","Badge","BadgeNo","IPassID","iPass ID","IPASSID"):
                        if ck in rr and rr.get(ck):
                            cn = _normalize_card_like(rr.get(ck))
                            if cn:
                                card_to_emp[cn] = eid
            except Exception:
                pass

        att_rows_range = session.query(AttendanceSummary).filter(AttendanceSummary.date >= week_start, AttendanceSummary.date <= today).all()
        rows_by_key = {}
        for r in att_rows_range:
            key = _normalize_employee_key(r.employee_id)
            if key not in rows_by_key:
                rows_by_key[key] = []
            rows_by_key[key].append(r)

        meets_5 = []
        meets_3 = []
        defaulters_list = []
        for eid, e in emp_map.items():
            candidate_rows = []
            if eid and eid in rows_by_key:
                candidate_rows.extend(rows_by_key[eid])
            for k in list(rows_by_key.keys()):
                if not k:
                    continue
                k_norm = _normalize_card_like(k)
                if k_norm and k_norm in card_to_emp and card_to_emp[k_norm] == eid:
                    candidate_rows.extend(rows_by_key[k])
            by_date = {}
            for r in candidate_rows:
                try:
                    d = r.date
                    if d not in by_date:
                        by_date[d] = r
                    else:
                        if (r.presence_count or 0) > (by_date[d].presence_count or 0):
                            by_date[d] = r
                except Exception:
                    continue
            days_with_8h = 0
            for d, row in by_date.items():
                if (row.presence_count or 0) > 0:
                    try:
                        if row.first_seen and row.last_seen:
                            dur = (row.last_seen - row.first_seen).total_seconds() / 3600.0
                            if dur >= 8.0:
                                days_with_8h += 1
                    except Exception:
                        pass
            meets5 = (days_with_8h >= 5)
            meets3 = (days_with_8h >= 3)
            wfh_flag = is_employee_wfh(e)
            location = None
            for loc_attr in ("location_city", "location", "base_location", "location_desc", "location_description"):
                if hasattr(e, loc_attr):
                    v = getattr(e, loc_attr)
                    if v and isinstance(v, str) and v.strip():
                        location = v.strip()
                        break
            if not location:
                try:
                    rr = getattr(e, "raw_row", None)
                    if rr and isinstance(rr, dict):
                        for ck in ("Partition","PartitionName","Location","Site","location_city","Location City"):
                            if ck in rr and rr.get(ck):
                                location = str(rr.get(ck)).strip()
                                break
                except Exception:
                    pass
            if not location:
                location = "Unknown"

            if meets5:
                meets_5.append((eid, e, location))
            if meets3:
                meets_3.append((eid, e, location))
            if (not meets5) and (not meets3):
                if not wfh_flag:
                    defaulters_list.append((eid, e, location))

        def _build_location_counts(list_of_tuples):
            loc_map = {}
            for (_id, e_obj, loc) in list_of_tuples:
                if not loc:
                    loc = "Unknown"
                if ccure_locations:
                    if loc not in ccure_locations:
                        continue
                if loc not in loc_map:
                    loc_map[loc] = {"count": 0}
                loc_map[loc]["count"] += 1
            return loc_map

        meets_5_count = len(meets_5)
        meets_3_count = len(meets_3)
        defaulter_count = len(defaulters_list)

        compliance["meets_5days_8h"]["count"] = int(meets_5_count)
        compliance["meets_5days_8h"]["by_location"] = {k: {"count": int(v["count"])} for k, v in _build_location_counts(meets_5).items()}
        compliance["meets_3days_8h"]["count"] = int(meets_3_count)
        compliance["meets_3days_8h"]["by_location"] = {k: {"count": int(v["count"])} for k, v in _build_location_counts(meets_3).items()}
        compliance["defaulters"]["count"] = int(defaulter_count)
        compliance["defaulters"]["by_location"] = {k: {"count": int(v["count"])} for k, v in _build_location_counts(defaulters_list).items()}

        denom_emp = reported_active_emps if reported_active_emps is not None else None
        if isinstance(denom_emp, int) and denom_emp > 0:
            compliance["meets_5days_8h"]["percent_of_ccure_employees"] = round((meets_5_count / denom_emp) * 100.0, 2)
            compliance["meets_3days_8h"]["percent_of_ccure_employees"] = round((meets_3_count / denom_emp) * 100.0, 2)
            compliance["defaulters"]["percent_of_ccure_employees"] = round((defaulter_count / denom_emp) * 100.0, 2)
        else:
            compliance["meets_5days_8h"]["percent_of_ccure_employees"] = None
            compliance["meets_3days_8h"]["percent_of_ccure_employees"] = None
            compliance["defaulters"]["percent_of_ccure_employees"] = None

        sample = []
        for (eid, e_obj, loc) in defaulters_list[:50]:
            try:
                sample.append({
                    "employee_id": _sanitize_for_json(eid),
                    "full_name": _sanitize_for_json(getattr(e_obj, "full_name", None)),
                    "location": _sanitize_for_json(loc),
                    "wfh_flag": bool(is_employee_wfh(e_obj))
                })
            except Exception:
                continue
        compliance["defaulters"]["sample"] = sample

        session.expunge_all()
        session.close()
    except Exception:
        logger.exception("Error computing compliance section")
        notes.append("Failed to compute compliance metrics; check server logs for trace.")
    finally:
        try:
            session.close()
        except Exception:
            pass

    # --- Averages: compute last 7 days headcount averages from AttendanceSummary (DB)
    avg_headcount_last_7_days = None
    avg_headcount_per_site_last_7_days = None
    # Per-location DB aggregates (new)
    avg_by_location_last_7_days: Dict[str, Dict[str, Any]] = {}

    try:
        session = SessionLocal()

        # Build ActiveEmployee/ActiveContractor maps for classification during per-day scans
        act_emps = session.query(ActiveEmployee).all()
        act_contrs = session.query(ActiveContractor).all()
        emp_id_set = set()
        contr_id_set = set()
        card_to_emp = {}
        for e in act_emps:
            eid = _normalize_employee_key(getattr(e, "employee_id", None))
            if eid:
                emp_id_set.add(eid)
            try:
                rr = getattr(e, "raw_row", None) or {}
                if isinstance(rr, dict):
                    for ck in ("CardNumber","card_number","Card","Card No","CardNo","IPassID","iPass ID","IPASSID","Badge","BadgeNo"):
                        if ck in rr and rr.get(ck):
                            cn = _normalize_card_like(rr.get(ck))
                            if cn:
                                card_to_emp[cn] = eid
            except Exception:
                pass
        for c in act_contrs:
            wid = _normalize_employee_key(getattr(c, "worker_system_id", None))
            ip = _normalize_employee_key(getattr(c, "ipass_id", None))
            primary = wid or ip
            if primary:
                contr_id_set.add(primary)
            try:
                rr = getattr(c, "raw_row", None) or {}
                if isinstance(rr, dict):
                    for ck in ("Worker System Id","Worker System ID","iPass ID","IPASSID","CardNumber","card_number"):
                        if ck in rr and rr.get(ck):
                            cn = _normalize_card_like(rr.get(ck))
                            if cn:
                                # map to contractor primary
                                card_to_emp[cn] = primary
            except Exception:
                pass

        # Prepare per-location day lists
        loc_day_vals: Dict[str, Dict[str, List[int]]] = {}
        days = []
        for i in range(0, 7):
            d = today - timedelta(days=i)
            days.append(d)
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            # compute per-location counts for that day
            per_loc_counts: Dict[str, Dict[str, int]] = {}
            if rows:
                for r in rows:
                    try:
                        if (r.presence_count or 0) <= 0:
                            continue
                        partition = None
                        try:
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                        except Exception:
                            partition = None
                        loc = partition or "Unknown"
                        if not isinstance(loc, str) or not loc.strip():
                            loc = "Unknown"
                        if loc not in per_loc_counts:
                            per_loc_counts[loc] = {"employee": 0, "contractor": 0, "total": 0}
                        # classify row
                        key = _normalize_employee_key(r.employee_id)
                        cls = "contractor"
                        if key and key in emp_id_set:
                            cls = "employee"
                        elif key and key in contr_id_set:
                            cls = "contractor"
                        else:
                            # try derived card number
                            try:
                                card = (r.derived.get("card_number") if (r.derived and isinstance(r.derived, dict)) else None)
                            except Exception:
                                card = None
                            cnorm = _normalize_card_like(card)
                            if cnorm and cnorm in card_to_emp and card_to_emp.get(cnorm) in emp_id_set:
                                cls = "employee"
                            elif cnorm and cnorm in card_to_emp and card_to_emp.get(cnorm) in contr_id_set:
                                cls = "contractor"
                            else:
                                # fallback: try to classify by looking at presence of explicit PersonnelType in derived/raw (rare for AttendanceSummary)
                                cls = "contractor"
                        per_loc_counts[loc][cls] += 1
                        per_loc_counts[loc]["total"] += 1
                    except Exception:
                        continue
            # for each location seen on that day, append day's counts
            for loc, counts in per_loc_counts.items():
                if loc not in loc_day_vals:
                    loc_day_vals[loc] = {"employee": [], "contractor": [], "total": []}
                loc_day_vals[loc]["employee"].append(counts.get("employee", 0))
                loc_day_vals[loc]["contractor"].append(counts.get("contractor", 0))
                loc_day_vals[loc]["total"].append(counts.get("total", 0))

        # compute per-location averages
        for loc, lists in loc_day_vals.items():
            emp_list = lists.get("employee", [])
            con_list = lists.get("contractor", [])
            tot_list = lists.get("total", [])
            days_counted = len(tot_list)
            avg_emp = round(sum(emp_list) / float(days_counted), 2) if days_counted and sum(emp_list) is not None else 0.0
            avg_con = round(sum(con_list) / float(days_counted), 2) if days_counted and sum(con_list) is not None else 0.0
            avg_tot = round(sum(tot_list) / float(days_counted), 2) if days_counted and sum(tot_list) is not None else 0.0
            avg_by_location_last_7_days[loc] = {
                "history_days_counted": int(days_counted),
                "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
            }

        # compute DB overall avg_headcount_last_7_days (previous behavior)
        days_totals = []
        for d in days:
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            day_total = 0
            if rows:
                for r in rows:
                    if (r.presence_count or 0) > 0:
                        day_total += 1
            days_totals.append(day_total)
        if days_totals:
            avg_headcount_last_7_days = round(sum(days_totals) / float(len(days_totals)), 2)
            if sites_queried and sites_queried > 0:
                avg_headcount_per_site_last_7_days = round((sum(days_totals) / float(len(days_totals))) / float(sites_queried), 2)

        session.close()
    except Exception:
        logger.exception("Error computing averages from AttendanceSummary")
        notes.append("Failed to compute historical averages from AttendanceSummary; partial results only.")

    # --- HISTORY AVERAGES: use region_clients history endpoints (new)
    history_emp_avg = None
    history_contractor_avg = None
    history_overall_avg = None
    history_days = 0
    # per-partition history averages (new)
    history_avg_by_location_last_7_days: Dict[str, Dict[str, Any]] = {}

    try:
        import region_clients
        if hasattr(region_clients, "fetch_all_history"):
            entries = region_clients.fetch_all_history(timeout=timeout) or []
            # aggregate by date across regions
            agg_by_date = {}  # date_str -> {"employee": int, "contractor": int, "total": int}
            # Also aggregate partitions per date
            agg_partitions_by_date: Dict[str, Dict[str, Dict[str, int]]] = {}  # date -> partition -> {employee, contractor, total}
            for e in entries:
                try:
                    dstr = e.get("date")
                    if not dstr:
                        continue
                    region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                    emp = None
                    con = None
                    tot = None
                    if region_obj and isinstance(region_obj, dict):
                        emp = _safe_int(region_obj.get("Employee"))
                        con = _safe_int(region_obj.get("Contractor"))
                        tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                    else:
                        emp = _safe_int(e.get("Employee") or (e.get("region") and e.get("region").get("Employee") if isinstance(e.get("region"), dict) else None))
                        con = _safe_int(e.get("Contractor") or (e.get("region") and e.get("region").get("Contractor") if isinstance(e.get("region"), dict) else None))
                        tot = _safe_int(e.get("total") or ((emp or 0) + (con or 0)))
                    if emp is None and con is None and tot is None:
                        try:
                            robj = e.get("region") or {}
                            if isinstance(robj, dict):
                                emp = _safe_int(robj.get("Employee"))
                                con = _safe_int(robj.get("Contractor"))
                                tot = _safe_int(robj.get("total"))
                        except Exception:
                            pass
                    if emp is None and con is None:
                        continue
                    if tot is None:
                        tot = (emp or 0) + (con or 0)
                    if dstr not in agg_by_date:
                        agg_by_date[dstr] = {"employee": 0, "contractor": 0, "total": 0, "counted_regions": 0}
                    agg_by_date[dstr]["employee"] += (emp or 0)
                    agg_by_date[dstr]["contractor"] += (con or 0)
                    agg_by_date[dstr]["total"] += (tot or 0)
                    agg_by_date[dstr]["counted_regions"] += 1

                    # partitions
                    parts = e.get("partitions") if isinstance(e.get("partitions"), dict) else {}
                    if dstr not in agg_partitions_by_date:
                        agg_partitions_by_date[dstr] = {}
                    for pname, pstat in parts.items():
                        try:
                            p_emp = _safe_int(pstat.get("Employee"))
                            p_con = _safe_int(pstat.get("Contractor"))
                            p_tot = _safe_int(pstat.get("total")) or ((p_emp or 0) + (p_con or 0))
                            if pname not in agg_partitions_by_date[dstr]:
                                agg_partitions_by_date[dstr][pname] = {"employee": 0, "contractor": 0, "total": 0}
                            agg_partitions_by_date[dstr][pname]["employee"] += (p_emp or 0)
                            agg_partitions_by_date[dstr][pname]["contractor"] += (p_con or 0)
                            agg_partitions_by_date[dstr][pname]["total"] += (p_tot or 0)
                        except Exception:
                            continue
                except Exception:
                    continue

            # Now compute per-date lists for last 7 days
            day_vals_emp = []
            day_vals_con = []
            day_vals_tot = []
            for i in range(0, 7):
                d_iso = (today - timedelta(days=i)).isoformat()
                entry = agg_by_date.get(d_iso)
                if entry:
                    day_vals_emp.append(entry.get("employee", 0))
                    day_vals_con.append(entry.get("contractor", 0))
                    day_vals_tot.append(entry.get("total", 0))
            if day_vals_emp:
                history_emp_avg = round(sum(day_vals_emp) / float(len(day_vals_emp)), 2)
            if day_vals_con:
                history_contractor_avg = round(sum(day_vals_con) / float(len(day_vals_con)), 2)
            if day_vals_tot:
                history_overall_avg = round(sum(day_vals_tot) / float(len(day_vals_tot)), 2)
            history_days = len(day_vals_tot)
            if history_days == 0:
                notes.append("History endpoints returned no usable last-7-day rows; history averages not available.")

            # Compute per-partition averages across last 7 days
            # Build partition -> lists
            partition_day_values: Dict[str, Dict[str, List[int]]] = {}
            for i in range(0, 7):
                d_iso = (today - timedelta(days=i)).isoformat()
                per_parts = agg_partitions_by_date.get(d_iso, {})
                for pname, pvals in per_parts.items():
                    if pname not in partition_day_values:
                        partition_day_values[pname] = {"employee": [], "contractor": [], "total": []}
                    partition_day_values[pname]["employee"].append(pvals.get("employee", 0))
                    partition_day_values[pname]["contractor"].append(pvals.get("contractor", 0))
                    partition_day_values[pname]["total"].append(pvals.get("total", 0))
            # finalize per-partition averages
            for pname, lists in partition_day_values.items():
                emp_list = lists.get("employee", [])
                con_list = lists.get("contractor", [])
                tot_list = lists.get("total", [])
                days_counted = len(tot_list)
                if days_counted == 0:
                    continue
                avg_emp = round(sum(emp_list) / float(days_counted), 2)
                avg_con = round(sum(con_list) / float(days_counted), 2)
                avg_tot = round(sum(tot_list) / float(days_counted), 2)
                history_avg_by_location_last_7_days[pname] = {
                    "history_days_counted": int(days_counted),
                    "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                    "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                    "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
                }

    except Exception:
        logger.exception("Error fetching/processing history endpoints")
        notes.append("Failed to compute history averages from region history endpoints; partial results.")

    # ---------- NEW: if DB-based 7-day avg empty, fallback to history_overall_avg ----------
    if (not avg_headcount_last_7_days or avg_headcount_last_7_days == 0) and history_overall_avg:
        try:
            avg_headcount_last_7_days = history_overall_avg
            avg_headcount_per_site_last_7_days = round(history_overall_avg / float(sites_queried), 2) if sites_queried and sites_queried > 0 else None
            notes.append("avg_headcount_last_7_days derived from region history endpoints due to missing AttendanceSummary historical data.")
        except Exception:
            pass

    # --- compute percentages (head/live vs CCURE reported)
    def safe_pct(n, denom):
        try:
            if n is None or denom is None:
                return None
            d = float(denom)
            if d == 0.0:
                return None
            return round((float(n) / d) * 100.0, 2)
        except Exception:
            return None

    cc_emp_denom = reported_active_emps
    cc_con_denom = reported_active_contractors
    cc_total_denom = None
    if isinstance(cc_emp_denom, int) and isinstance(cc_con_denom, int):
        cc_total_denom = cc_emp_denom + cc_con_denom

    head_emp_total = sum(v.get("employee", 0) for v in head_per_location.values())
    head_con_total = sum(v.get("contractor", 0) for v in head_per_location.values())
    live_emp_total = sum(v.get("employee", 0) for v in live_per_location.values())
    live_con_total = sum(v.get("contractor", 0) for v in live_per_location.values())

    # percent of CCURE employees/contractors present today (headcount basis)
    head_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_emp_total, cc_emp_denom))
    head_con_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_con_total, cc_con_denom))
    head_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_total, cc_total_denom))

    # Provide historical key name expected elsewhere (fix for NameError)
    head_contractor_pct_vs_ccure_today = head_con_pct_vs_ccure_today

    live_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_emp_total, cc_emp_denom))
    live_con_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_con_total, cc_con_denom))
    live_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_total, cc_total_denom))

    # history percentages vs CCURE (if denominators exist)
    history_emp_pct_vs_ccure = _sanitize_for_json(safe_pct(history_emp_avg, cc_emp_denom))
    history_con_pct_vs_ccure = _sanitize_for_json(safe_pct(history_contractor_avg, cc_con_denom))
    history_overall_pct_vs_ccure = _sanitize_for_json(safe_pct(history_overall_avg, cc_total_denom))

    result = {
        "date": today.isoformat(),
        "headcount": {
            "total_visited_today": int(head_total),
            "employee": int(head_emp_total),
            "contractor": int(head_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in head_per_location.items() }
        },
        "live_headcount": {
            "currently_present_total": int(live_total),
            "employee": int(live_emp_total),
            "contractor": int(live_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in live_per_location.items() }
        },
        "ccure_active": {
            "ccure_active_employees_reported": _safe_int(reported_active_emps),
            "ccure_active_contractors_reported": _safe_int(reported_active_contractors)
        },
        "averages": {
            # existing AttendanceSummary averages
            "head_emp_pct_vs_ccure_today": head_emp_pct_vs_ccure_today,
            "head_contractor_pct_vs_ccure_today": head_contractor_pct_vs_ccure_today,
            "headcount_overall_pct_vs_ccure_today": head_overall_pct_vs_ccure_today,
            "live_employee_pct_vs_ccure": live_emp_pct_vs_ccure_today,
            "live_contractor_pct_vs_ccure": live_con_pct_vs_ccure_today if False else live_con_pct_vs_ccure_today if 'live_con_pct_vs_ccure_today' in locals() else _sanitize_for_json(safe_pct(live_con_total, cc_con_denom)),
            "live_overall_pct_vs_ccure": live_overall_pct_vs_ccure_today,
            "avg_headcount_last_7_days": _sanitize_for_json(avg_headcount_last_7_days),
            "avg_headcount_per_site_last_7_days": _sanitize_for_json(avg_headcount_per_site_last_7_days),
            "avg_live_per_site": _sanitize_for_json(round(live_total / sites_queried, 2) if sites_queried and sites_queried > 0 else None),

            # NEW: history endpoint averages (region-provided)
            "history_avg_employee_last_7_days": _sanitize_for_json(history_emp_avg),
            "history_avg_contractor_last_7_days": _sanitize_for_json(history_contractor_avg),
            "history_avg_overall_last_7_days": _sanitize_for_json(history_overall_avg),
            "history_days_counted": int(history_days) if history_days is not None else None,
            "history_employee_pct_vs_ccure": history_emp_pct_vs_ccure,
            "history_contractor_pct_vs_ccure": history_con_pct_vs_ccure,
            "history_overall_pct_vs_ccure": history_overall_pct_vs_ccure,

            # NEW per-location aggregates:
            "avg_by_location_last_7_days": _sanitize_for_json(avg_by_location_last_7_days),
            "history_avg_by_location_last_7_days": _sanitize_for_json(history_avg_by_location_last_7_days)
        },
        "compliance": _sanitize_for_json(compliance),
        "sites_queried": int(sites_queried),
        "notes": " | ".join(notes) if notes else None
    }

    # sanitize and return
    return _sanitize_for_json(result)








C:\Users\W0024618\Desktop\global-page\frontend\src\pages\GlobalPage.jsx





//************************************************************** */

import React, { useState, useEffect, useRef } from 'react';
import {
  Box, Typography, CircularProgress, IconButton, Button, Paper, Divider,
  LinearProgress, Snackbar, Alert
} from '@mui/material';
import HomeIcon from '@mui/icons-material/Home';
import DescriptionIcon from '@mui/icons-material/Description';
import UploadFileIcon from '@mui/icons-material/UploadFile';
import MapChart from '../components/MapChart.jsx';
import api from '../api';
import { useNavigate, Link } from 'react-router-dom';

/*
  Important:
  - Do NOT mix /api/headcount and /api/ccure/averages.
  - Region cards (APAC/EMEA/LACA/NAMER) come only from /api/headcount.
  - Live vs CCURE Summary comes only from /api/ccure/averages.
  - Initial region totals are zero (keeps previous UI behaviour).
  - We implement polling for headcount and SSE for ccure/averages (realtime via SSE).
*/

export default function GlobalPage() {
  const navigate = useNavigate();

  // Region totals (headcount) - default to zeros so UI shows 0 immediately (preserve previous behaviour)
  const [counts, setCounts] = useState({ apac: 0, emea: 0, laca: 0, namer: 0 });
  const [selected, setSelected] = useState('global');

  // Averages/ccure state (left panel)
  const [averages, setAverages] = useState(null);
  const [loadingAverages, setLoadingAverages] = useState(true);
  const [averagesError, setAveragesError] = useState(null);

  // upload state
  const [uploading, setUploading] = useState(false);
  const [uploadResult, setUploadResult] = useState(null);
  const [uploadError, setUploadError] = useState(null);

  const fileInputEmpRef = useRef();
  const fileInputContrRef = useRef();
  const [snack, setSnack] = useState({ open: false, severity: 'info', message: '' });

  // Polling refs for safe scheduling and backoff
  const headcountRef = useRef({ timerId: null, failureCount: 0, isFetching: false });
  const averagesRef = useRef({ timerId: null, failureCount: 0, isFetching: false });

  // -----------------------
  // HEADCOUNT POLLING ONLY (unchanged)
  // -----------------------
  useEffect(() => {
    let mounted = true;

    const fetchHeadcount = async () => {
      if (!mounted) return;
      if (headcountRef.current.isFetching) return;
      headcountRef.current.isFetching = true;

      try {
        // Important: only call /headcount here (proxy rewrites /api -> backend)
        const res = await api.get('/headcount');
        if (!mounted) return;
        const d = res.data;
        // We expect an object with keys apac/emea/laca/namer (defensive)
        if (d && typeof d === 'object') {
          const newCounts = {
            apac: Number(d.apac || 0),
            emea: Number(d.emea || 0),
            laca: Number(d.laca || 0),
            namer: Number(d.namer || 0),
          };
          setCounts(prev => {
            if (
              prev.apac === newCounts.apac &&
              prev.emea === newCounts.emea &&
              prev.laca === newCounts.laca &&
              prev.namer === newCounts.namer
            ) {
              return prev;
            }
            return newCounts;
          });
        } else {
          console.warn('[headcount] unexpected response shape - ignoring', d);
        }
        headcountRef.current.failureCount = 0;
      } catch (err) {
        headcountRef.current.failureCount = (headcountRef.current.failureCount || 0) + 1;
        console.warn('[headcount] fetch failed:', err?.message || err);
      } finally {
        headcountRef.current.isFetching = false;
        const f = headcountRef.current.failureCount || 0;
        const backoffMs = 15000 * Math.pow(2, Math.min(Math.max(f - 1, 0), 4)); // 15s..240s
        headcountRef.current.timerId = setTimeout(fetchHeadcount, backoffMs);
      }
    };

    fetchHeadcount();

    return () => {
      mounted = false;
      if (headcountRef.current.timerId) clearTimeout(headcountRef.current.timerId);
      headcountRef.current.isFetching = false;
    };
  }, []); // run once

  // -----------------------
  // AVERAGES: use SSE (server-sent events) with fallback initial fetch
  // -----------------------
  useEffect(() => {
    let stopped = false;
    let es = null;
    let backoff = 1000;

    const connect = () => {
      if (stopped) return;
      try {
        es = new EventSource('/api/ccure/stream');
      } catch (err) {
        console.warn('SSE creation failed', err);
        es = null;
      }

      if (!es) {
        // fallback to polling if EventSource not supported or creation failed
        initialFetch(); // try initial fetch once
        return;
      }

      es.onopen = () => {
        console.info('[SSE] connected to /api/ccure/stream');
        backoff = 1000;
        setAveragesError(null);
      };

      es.onmessage = (evt) => {
        try {
          const payload = JSON.parse(evt.data);
          setAverages(payload);
          setLoadingAverages(false);
          setAveragesError(null);
        } catch (e) {
          console.warn('Failed to parse SSE message', e);
        }
      };

      es.onerror = (err) => {
        console.warn('[SSE] error/closed, attempting reconnect', err);
        try { es.close(); } catch (e) {}
        es = null;
        if (stopped) return;
        // exponential backoff reconnect
        setTimeout(() => {
          backoff = Math.min(backoff * 2, 30000);
          connect();
        }, backoff);
      };
    };

    const initialFetch = async () => {
      setLoadingAverages(true);
      setAveragesError(null);
      try {
        const res = await api.get('/ccure/averages');
        setAverages(res.data);
        setLoadingAverages(false);
        setAveragesError(null);
      } catch (err) {
        console.warn('initial /ccure/averages fetch failed', err);
        setLoadingAverages(false);
        setAveragesError(err);
      }
    };

    // Start with initial fetch so UI is populated quickly, then open SSE
    initialFetch();
    connect();

    return () => {
      stopped = true;
      if (es) {
        try { es.close(); } catch (e) {}
        es = null;
      }
    };
  }, []); // run once

  // -----------------------
  // Upload helper (unchanged)
  // -----------------------
  const handleUpload = async (file, type) => {
    if (!file) return;
    const endpoint = type === 'employee' ? '/upload/active-employees' : '/upload/active-contractors';
    const fd = new FormData();
    fd.append('file', file, file.name);

    setUploading(true);
    setUploadResult(null);
    setUploadError(null);

    try {
      const res = await api.post(endpoint, fd, {
        headers: { 'Content-Type': 'multipart/form-data' },
        timeout: 120000
      });
      setUploadResult(res.data);
      setSnack({ open: true, severity: 'success', message: `Upload successful: ${file.name}` });
      // Optionally re-fetch averages/headcount after successful upload:
      try { await api.get('/ccure/averages').then(r => setAverages(r.data)); } catch (_) {}
      try { await api.get('/headcount').then(r => {
        const d = r.data;
        if (d && typeof d === 'object') {
          setCounts(prev => ({
            apac: Number(d.apac || prev.apac || 0),
            emea: Number(d.emea || prev.emea || 0),
            laca: Number(d.laca || prev.laca || 0),
            namer: Number(d.namer || prev.namer || 0)
          }));
        }
      }) } catch (_) {}
    } catch (err) {
      console.error('Upload failed', err);
      setUploadError(err);
      setSnack({ open: true, severity: 'error', message: `Upload failed: ${file.name}` });
    } finally {
      setUploading(false);
    }
  };

  const onChooseEmployeeFile = (e) => { const f = e.target.files && e.target.files[0]; if (f) handleUpload(f, 'employee'); e.target.value = null; };
  const onChooseContractorFile = (e) => { const f = e.target.files && e.target.files[0]; if (f) handleUpload(f, 'contractor'); e.target.value = null; };

  // safe helper for nested averages paths
  const safe = (path, fallback = null) => {
    if (!averages) return fallback;
    try {
      return path.split('.').reduce((a, k) => (a && a[k] !== undefined ? a[k] : fallback), averages);
    } catch {
      return fallback;
    }
  };

  // derived
  const liveEmployee = safe('live_today.employee', safe('live_headcount.employee', null));
  const liveContractor = safe('live_today.contractor', safe('live_headcount.contractor', null));
  const liveTotalReported = safe('live_today.total_reported', safe('live_headcount.currently_present_total', null));
  const liveTotalDetails = safe('live_today.total_from_details', null);

  const ccureActiveEmployees = safe('ccure_active.active_employees', safe('ccure_active.ccure_active_employees_reported', null));
  const ccureActiveContractors = safe('ccure_active.active_contractors', safe('ccure_active.ccure_active_contractors_reported', null));

  const empPct = safe('averages.employee_pct', safe('averages.head_emp_pct_vs_ccure_today', null));
  const conPct = safe('averages.contractor_pct', safe('averages.head_contractor_pct_vs_ccure_today', null));
  const overallPct = safe('averages.overall_pct', safe('averages.headcount_overall_pct_vs_ccure_today', null));
  const avg7 = safe('averages.avg_headcount_last_7_days', safe('averages.history_avg_overall_last_7_days', null));

  // Render (unchanged)
  return (
    <Box sx={{ display: 'flex', flexDirection: 'column', height: '100vh', overflow: 'hidden' }}>
      {/* Header */}
      <Box px={2} py={1} sx={{ backgroundColor: 'black', color: '#fff', borderBottom: '4px solid #FFD700', display: 'flex', alignItems: 'center', justifyContent: 'space-between' }}>
        <Box>
          <IconButton component={Link} to="/" sx={{ color: '#FFC72C' }}><HomeIcon fontSize="medium" /></IconButton>
          <IconButton component={Link} to="/reports" sx={{ color: '#FFC72C', ml: 1 }}><DescriptionIcon fontSize="medium" /></IconButton>
        </Box>

        <Box sx={{ flexGrow: 1, display: 'flex', alignItems: 'center', justifyContent: 'center' }}>
          <Box component="img" src="/wu-head-logo.png" alt="WU Logo" sx={{ height: { xs: 30, md: 55 }, mr: 2 }} />
          <Typography variant="h5" sx={{ fontWeight: 'bold', color: 'primary.main' }}>Global Headcount Dashboard</Typography>
        </Box>

        <Box sx={{ width: 120 }} />
      </Box>

      {/* Region Cards */}
      <Box sx={{ display: 'flex', gap: 2, p: 2, flexWrap: 'wrap', justifyContent: 'center' }}>
        {[
          { key: 'apac', label: 'APAC', url: 'http://10.199.22.57:3000/', textColor: '#f5650c' },
          { key: 'emea', label: 'EMEA', url: 'http://10.199.22.57:3001/', textColor: '#11e6ed' },
          { key: 'laca', label: 'LACA', url: 'http://10.199.22.57:3003/', textColor: '#FF2DD1' },
          { key: 'namer', label: 'NAMER', url: 'http://10.199.22.57:3002/', textColor: '#a6e61c' },
        ].map(region => (
          <Box
            key={region.key}
            onClick={() => { window.location.href = region.url; }}
            sx={{
              cursor: 'pointer',
              width: 200,
              height: 80,
              display: 'flex',
              flexDirection: 'column',
              justifyContent: 'center',
              alignItems: 'center',
              border: '4px solid rgba(255, 204, 0, 0.89)',
              borderRadius: 2,
              boxShadow: 3,
              color: region.textColor,
              '&:hover': { opacity: 0.9 },
            }}
          >
            <Typography variant="subtitle1" sx={{ fontWeight: 'bold', color: region.textColor, fontSize: { xs: '1.3rem' } }}>{region.label}</Typography>
            <Typography variant="h3" sx={{ fontWeight: 700, fontSize: { xs: '1.5rem', sm: '1.8rem' }, color: region.textColor }}>
              {typeof counts[region.key] === 'number' ? counts[region.key] : 0}
            </Typography>
          </Box>
        ))}
      </Box>

      {/* Main */}
      <Box sx={{ display: 'flex', flex: 1, overflow: 'hidden' }}>
        <Box sx={{ width: 360, p: 2, bgcolor: 'background.paper', borderRight: '1px solid rgba(255,255,255,0.06)', overflowY: 'auto' }}>
          <Typography variant="h6" sx={{ mb: 1, color: 'primary.main' }}>Live vs CCURE Summary</Typography>

          {loadingAverages ? (
            <Box sx={{ py: 2 }}><LinearProgress /></Box>
          ) : averagesError ? (
            <Alert severity="error">Failed to load CCURE averages</Alert>
          ) : averages ? (
            <>
              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.03)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary">CCURE Active (reported)</Typography>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Box>
                    <Typography variant="h5" sx={{ fontWeight: 700 }}>{ccureActiveEmployees ?? 'â€”'}</Typography>
                    <Typography variant="caption" color="text.secondary">Active Employees</Typography>
                  </Box>
                  <Box sx={{ textAlign: 'right' }}>
                    <Typography variant="h5" sx={{ fontWeight: 700 }}>{ccureActiveContractors ?? 'â€”'}</Typography>
                    <Typography variant="caption" color="text.secondary">Active Contractors</Typography>
                  </Box>
                </Box>
              </Paper>

              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.03)' }} elevation={0}>
                <Box sx={{ display: 'flex', justifyContent: 'space-between' }}>
                  <Typography variant="subtitle2" color="text.secondary">Live Today</Typography>
                  <Typography variant="caption" color="text.secondary">{averages.date ?? ''}</Typography>
                </Box>

                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Box>
                    <Typography variant="h6" sx={{ fontWeight: 700 }}>{liveEmployee ?? 'â€”'}</Typography>
                    <Typography variant="caption" color="text.secondary">Employee</Typography>
                  </Box>
                  <Box>
                    <Typography variant="h6" sx={{ fontWeight: 700 }}>{liveContractor ?? 'â€”'}</Typography>
                    <Typography variant="caption" color="text.secondary">Contractor</Typography>
                  </Box>
                </Box>

                <Divider sx={{ my: 1 }} />

                <Box>
                  <Typography variant="caption" color="text.secondary">Totals</Typography>
                  <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                    <Typography variant="body2">Reported total</Typography>
                    <Typography variant="body2" sx={{ fontWeight: 700 }}>{liveTotalReported ?? 'â€”'}</Typography>
                  </Box>
                  {liveTotalDetails != null && (
                    <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                      <Typography variant="body2">Detail rows total</Typography>
                      <Typography variant="body2" sx={{ fontWeight: 700 }}>{liveTotalDetails}</Typography>
                    </Box>
                  )}
                </Box>
              </Paper>

              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.03)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary">Percentages vs CCURE</Typography>

                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Typography variant="body2">Employees</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{empPct != null ? `${empPct}%` : 'â€”'}</Typography>
                </Box>
                <Box sx={{ display: 'flex', justifyContent: 'space-between' }}>
                  <Typography variant="body2">Contractors</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{conPct != null ? `${conPct}%` : 'â€”'}</Typography>
                </Box>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                  <Typography variant="body2">Overall</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{overallPct != null ? `${overallPct}%` : 'â€”'}</Typography>
                </Box>

                <Divider sx={{ my: 1 }} />
                <Typography variant="caption" color="text.secondary">Averages</Typography>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Typography variant="body2">7-day avg headcount</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{avg7 ?? 'â€”'}</Typography>
                </Box>
              </Paper>

              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.03)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary" sx={{ mb: 1 }}>Upload Active Sheets</Typography>

                <input type="file" accept=".xls,.xlsx" style={{ display: 'none' }} ref={fileInputEmpRef} onChange={onChooseEmployeeFile} />
                <Button variant="contained" startIcon={<UploadFileIcon />} sx={{ mr: 1 }} onClick={() => fileInputEmpRef.current && fileInputEmpRef.current.click()} disabled={uploading}>
                  Upload Employees
                </Button>

                <input type="file" accept=".xls,.xlsx" style={{ display: 'none' }} ref={fileInputContrRef} onChange={onChooseContractorFile} />
                <Button variant="outlined" startIcon={<UploadFileIcon />} onClick={() => fileInputContrRef.current && fileInputContrRef.current.click()} disabled={uploading}>
                  Upload Contractors
                </Button>

                {uploading && <Box sx={{ mt: 1 }}><LinearProgress /></Box>}
                {uploadResult && <Typography variant="caption" color="success.main" sx={{ mt: 1, display: 'block' }}>Upload OK</Typography>}
                {uploadError && <Typography variant="caption" color="error.main" sx={{ mt: 1, display: 'block' }}>Upload error</Typography>}
              </Paper>

              {averages.notes && (
                <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.02)' }}>
                  {/* <Typography variant="caption" color="text.secondary">Notes</Typography> */}
                  <Typography variant="body2" sx={{ mt: 1 }}>{averages.notes}</Typography>
                </Paper>
              )}
            </>
          ) : (
            <Typography variant="body2" color="text.secondary">No data</Typography>
          )}
        </Box>

        <Box sx={{ flex: 1, height: '100%', position: 'relative' }}>
          <MapChart selected={selected} onClickSite={r => setSelected(r)} initialZoom={1.8} />
        </Box>
      </Box>

      <Snackbar open={snack.open} autoHideDuration={3500} onClose={() => setSnack(prev => ({ ...prev, open: false }))}>
        <Alert severity={snack.severity} onClose={() => setSnack(prev => ({ ...prev, open: false }))}>{snack.message}</Alert>
      </Snackbar>
    </Box>
  );
}





