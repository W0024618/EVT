check both file carefully and fix this issue 

#C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py
from flask import Flask, jsonify, request, send_from_directory, send_file
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re
import io
import base64
import os
import difflib
from typing import Optional, List, Dict, Any

from duration_report import REGION_CONFIG
from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR, read_90day_cache
from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE

# ---------- Ensure outputs directory exists early (so OVERRIDES_FILE can be defined safely) ----------
BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)

OVERRIDES_FILE = DEFAULT_OUTDIR / "overrides.csv"

def _load_overrides():
    if not OVERRIDES_FILE.exists():
        return {}
    try:
        df = pd.read_csv(OVERRIDES_FILE, dtype=str)
        out = {}
        for _, r in df.iterrows():
            emp = str(r.get('EmployeeID') or r.get('person_uid') or '').strip()
            if not emp:
                continue
            out[emp] = {
                'level': str(r.get('OverrideLevel') or '').strip(),
                'reason': str(r.get('Reason') or '').strip(),
                'ts': str(r.get('Timestamp') or '').strip()
            }
        return out
    except Exception:
        logging.exception("Failed reading overrides file")
        return {}

def _save_override(employee_key, level, reason):
    now = datetime.now().isoformat()
    row = {'EmployeeID': employee_key, 'OverrideLevel': level, 'Reason': reason or '', 'Timestamp': now}
    try:
        if OVERRIDES_FILE.exists():
            df = pd.read_csv(OVERRIDES_FILE, dtype=str)
            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        else:
            df = pd.DataFrame([row])
        df.to_csv(OVERRIDES_FILE, index=False)
        return True
    except Exception:
        logging.exception("Failed to save override")
        return False

# ODBC driver (keep existing env-based driver)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Backoff / connector helper for ACVSCore (restored so image lookups work)
_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20

def _get_acvscore_conn():
    """
    Try to connect to ACVSCore by reusing credentials from REGION_CONFIG.
    Loops through REGION_CONFIG entries and attempts:
      1) SQL auth (UID/PWD) to database "ACVSCore" using region server + credentials
      2) If SQL auth fails on that server, try Trusted_Connection (Windows auth) as a fallback
    Returns first successful pyodbc connection or None.
    Implements a short backoff after recent failure to reduce log noise.
    """
    try:
        import pyodbc
    except Exception:
        logging.exception("pyodbc not installed; ACVSCore lookups unavailable.")
        return None

    # basic backoff: skip attempts if we just failed recently
    from datetime import datetime
    now = datetime.now().timestamp()
    last = _acvscore_backoff.get("ts")
    if last and _acvscore_backoff.get("failed") and (now - last) < _ACVSCORE_BACKOFF_SECONDS:
        logging.debug("Skipping ACVSCore connection attempt (backoff active).")
        return None

    tried = []
    for region_key, rc in (REGION_CONFIG or {}).items():
        server = rc.get("server")
        user = rc.get("user")
        pwd = rc.get("password")
        if not server:
            continue

        # Try SQL auth first if credentials present
        if user and pwd:
            tried.append(f"{region_key}@{server}(sql)")
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={server};DATABASE=ACVSCore;UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;"
            )
            try:
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (sql auth).", server, region_key)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("SQL auth to %s failed: %s", server, e)

        # Try Trusted Connection fallback on same server
        tried.append(f"{region_key}@{server}(trusted)")
        conn_str_trusted = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={server};DATABASE=ACVSCore;Trusted_Connection=yes;"
            "TrustServerCertificate=Yes;"
        )
        try:
            conn = pyodbc.connect(conn_str_trusted, autocommit=True, timeout=5)
            logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (trusted connection).", server, region_key)
            _acvscore_backoff["ts"] = None
            _acvscore_backoff["failed"] = False
            return conn
        except Exception as e:
            logging.debug("Trusted connection to %s failed: %s", server, e)
            continue

    # Fallback: if a global ACVSCORE_DB_CONFIG exists, try it (safe check)
    try:
        if 'ACVSCORE_DB_CONFIG' in globals() and isinstance(globals().get('ACVSCORE_DB_CONFIG'), dict):
            cfg = globals().get('ACVSCORE_DB_CONFIG')
            server = cfg.get('server')
            user = cfg.get('user')
            pwd = cfg.get('password')
            database = cfg.get('database', 'ACVSCore')
            tried.append(f"ACVSCORE_DB_CONFIG@{server}")
            try:
                conn_str = (
                    f"DRIVER={{{ODBC_DRIVER}}};"
                    f"SERVER={server};DATABASE={database};UID={user};PWD={pwd};"
                    "TrustServerCertificate=Yes;"
                )
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore using ACVSCORE_DB_CONFIG (%s).", server)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("ACVSCORE_DB_CONFIG connection failed: %s", e)
    except Exception:
        logging.debug("ACVSCORE_DB_CONFIG fallback not available or failed.")

    # record failure to backoff
    _acvscore_backoff["ts"] = now
    _acvscore_backoff["failed"] = True
    logging.error("Failed to connect to ACVSCore using REGION_CONFIG servers. Tried: %s", tried)
    return None

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data

def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    """
    Try to resolve personnel record using a flexible lookup.
    Returns dict with keys: ObjectID (may be None), GUID, Name, EmailAddress, ManagerEmail
    If resolution fails returns empty dict.
    """
    out: Dict[str, Any] = {}
    if candidate_identifier is None:
        return out
    try:
        conn = _get_acvscore_conn()
    except Exception:
        conn = None
    if conn is None:
        return out

    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        params = (cand, cand, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()
        if row:
            try:
                out['ObjectID'] = row[0]
                out['GUID'] = row[1]
                out['Name'] = row[2]
                out['EmailAddress'] = row[3]
                out['ManagerEmail'] = row[4]
            except Exception:
                out = {
                    'ObjectID': row[0] if len(row) > 0 else None,
                    'GUID': row[1] if len(row) > 1 else None,
                    'Name': row[2] if len(row) > 2 else None,
                    'EmailAddress': row[3] if len(row) > 3 else None,
                    'ManagerEmail': row[4] if len(row) > 4 else None
                }
    except Exception:
        logging.exception("Failed personnel lookup for candidate: %s", candidate_identifier)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out

def get_person_image_bytes(parent_id) -> Optional[bytes]:
    """
    Query ACVSCore.Access.Images for top image where ParentId = parent_id and return raw bytes.
    Returns None if not found or on error.
    """
    try:
        conn = _get_acvscore_conn()
    except Exception:
        conn = None
    if conn is None:
        return None
    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 AI.Image
            FROM ACVSCore.Access.Images AI
            WHERE AI.ParentId = ?
              AND DATALENGTH(AI.Image) > 0
            ORDER BY AI.ObjectID DESC
        """
        cur.execute(sql, (str(parent_id),))
        row = cur.fetchone()
        if row and row[0] is not None:
            try:
                b = bytes(row[0])
                return b
            except Exception:
                return row[0]
    except Exception:
        logging.exception("Failed to fetch image for ParentId=%s", parent_id)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass
    return None


# ---------- New route to serve employee image ----------
# We'll import send_file earlier where used; define route after app created.

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

# optional import; used for styling
try:
    from openpyxl import load_workbook
    from openpyxl.styles import Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        return x
    try:
        return str(x)
    except Exception:
        return None

_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False

# Helper: format seconds to HH:MM:SS
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return None
        s = int(float(seconds))
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return None

_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _replace_placeholder_strings(df: pd.DataFrame) -> pd.DataFrame:
    """
    Replace common placeholder literal strings (e.g. 'nan', 'None', 'null', '-', 'n/a') in object/string columns with None.
    This prevents 'nan' strings showing up in JSON and ensures downstream logic treats them as missing values.
    Operates in-place on object columns and returns the dataframe (for chaining).
    """
    if df is None or df.empty:
        return df
    # operate only on object (string) columns to avoid corrupting numeric columns
    for col in df.columns:
        try:
            if df[col].dtype == object:
                df[col] = df[col].apply(lambda x: None if (isinstance(x, str) and x.strip().lower() in _PLACEHOLDER_STRS) else x)
        except Exception:
            # defensive: skip columns that error
            continue
    return df

_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None

def _resolve_field_from_record(record: dict, candidate_tokens: list):
    if record is None:
        return None

    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)

    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)

    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue

    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)

    return None

def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    if df is None or df.empty:
        return []
    df = df.copy()

    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # convert NaN -> None
    df = df.where(pd.notnull(df), None)

    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        if not emp_id:
            emp_tokens = ['Int1', 'Text12', 'EmployeeID', 'empid', 'id']
            resolved_emp = _resolve_field_from_record(r, emp_tokens)
            if resolved_emp is not None:
                try:
                    s = str(resolved_emp).strip()
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    if _looks_like_guid(s):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = s
                        emp_id = s
                except Exception:
                    if _looks_like_guid(resolved_emp):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = resolved_emp
                        emp_id = resolved_emp

        if out.get('CardNumber') in (None, '', 'nan'):
            card_tokens = ['CardNumber', 'CHUID', 'Card', 'card_no', 'cardnum']
            resolved_card = _resolve_field_from_record(r, card_tokens)
            if resolved_card is not None:
                try:
                    cs = str(resolved_card).strip()
                    if _looks_like_guid(cs) or _is_placeholder_str(cs):
                        out['CardNumber'] = None
                    else:
                        out['CardNumber'] = cs
                except Exception:
                    out['CardNumber'] = None

        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None
        if 'CardNumber' in out and isinstance(out['CardNumber'], str) and _looks_like_guid(out['CardNumber']):
            out['CardNumber'] = None

        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            if emp_id not in (None, '', 'nan') and not _looks_like_guid(emp_id):
                out['EmployeeName'] = str(emp_id)
            else:
                out['EmployeeName'] = None

        cleaned.append(out)
    return cleaned

@app.route('/')
def root():
    return "Trend Analysis API — Pune test"

# ---------------- Single run endpoint (merged & deduped) ----------------
@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.json or {}

    date_str = params.get('date')
    start_str = params.get('start')
    end_str = params.get('end')

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    # parse regions param (accept comma/pipe/semicolon-separated list)
    regions_param = params.get('regions') or params.get('region')
    if regions_param:
        regions = [r.strip().lower() for r in re.split(r'[;,|]', str(regions_param)) if r.strip()]
    else:
        # default: use all region keys from duration_report.REGION_CONFIG
        try:
            regions = [k.lower() for k in list(REGION_CONFIG.keys())]
        except Exception:
            regions = ['apac']  # fallback

    # validate region names against REGION_CONFIG
    valid_regions = []
    for r in regions:
        if r in (REGION_CONFIG or {}):
            valid_regions.append(r)
        else:
            logging.warning("Requested region '%s' not in REGION_CONFIG - skipping", r)
    if not valid_regions:
        valid_regions = [k.lower() for k in REGION_CONFIG.keys()] if REGION_CONFIG else ['apac']

    # Save for later use in response (so UI can show which regions were run)
    params['_regions_to_run'] = valid_regions

    combined_rows = []
    files = []

    # ---------- TRY CACHE FAST-PATH ----------
    try:
        cache_df, cache_start, cache_end = read_90day_cache(str(DEFAULT_OUTDIR), window_days=90)
    except Exception:
        cache_df = None
        cache_start = None
        cache_end = None

    use_cache = False
    if cache_df is not None and not cache_df.empty and cache_start and cache_end:
        try:
            all_in = all((d >= cache_start and d <= cache_end) for d in dates)
            if all_in:
                use_cache = True
        except Exception:
            use_cache = False

    if use_cache:
        try:
            if 'Date' in cache_df.columns:
                try:
                    cache_df['Date'] = pd.to_datetime(cache_df['Date'], errors='coerce').dt.date
                except Exception:
                    pass
            dates_set = set(dates)
            filtered = cache_df[cache_df['Date'].isin(dates_set)].copy()
            if not filtered.empty:
                combined_rows.append(filtered)
                files.append(Path(DEFAULT_OUTDIR).name + "/" + (_CACHE_FILENAME if '_CACHE_FILENAME' in globals() else "trend_pune_90day_cache.csv"))
            else:
                use_cache = False
        except Exception:
            logging.exception("Cache fast-path failed; falling back to per-date generation.")
            use_cache = False

    if not use_cache:
        for d in dates:
            try:
                # prefer new signature with regions
                try:
                    df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR))
                except TypeError:
                    # fallback: older run_trend_for_date may not accept regions -> try duration_report.run_for_date
                    logging.info("run_trend_for_date does not accept 'regions' param, falling back to duration_report.run_for_date to fetch multi-region data.")
                    try:
                        from duration_report import run_for_date as _dr_run_for_date
                        region_results = _dr_run_for_date(d, valid_regions, str(DEFAULT_OUTDIR))
                        # combine durations across regions into single DF so downstream logic gets combined view
                        combined_list = []
                        for rkey, res in (region_results or {}).items():
                            try:
                                df_dur = res.get('durations')
                                if df_dur is not None and not df_dur.empty:
                                    combined_list.append(df_dur)
                            except Exception:
                                continue
                        df = pd.concat(combined_list, ignore_index=True) if combined_list else pd.DataFrame()
                    except Exception:
                        # last fallback: call run_trend_for_date with old signature
                        df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
            except Exception as e:
                logging.exception("run_trend_for_date failed for %s", d)
                return jsonify({"error": f"runner failed for {d}: {e}"}), 500

            csv_path = DEFAULT_OUTDIR / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
            if csv_path.exists():
                files.append(csv_path.name)

            if df is None or df.empty:
                continue

            if 'IsFlagged' not in df.columns:
                df['IsFlagged'] = False
            if 'Reasons' not in df.columns:
                df['Reasons'] = None

            combined_rows.append(df)

    combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()

    # Normalize placeholder literal strings (e.g. 'nan') in string columns to actual None
    combined_df = _replace_placeholder_strings(combined_df)

    # ---------- Dedupe + aggregate (same as previous logic) ----------
    def _norm_id_val_local(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                f = float(s)
                if math.isfinite(f) and f.is_integer():
                    s = str(int(f))
        except Exception:
            pass
        return s

    def _aggregate_group(g):
        if 'AnomalyScore' in g.columns and not g['AnomalyScore'].isnull().all():
            try:
                idx = g['AnomalyScore'].astype(float).idxmax()
                base = g.loc[idx].to_dict()
            except Exception:
                base = g.iloc[0].to_dict()
        else:
            if 'Date' in g.columns:
                try:
                    g_dates = pd.to_datetime(g['Date'], errors='coerce')
                    if not g_dates.isnull().all():
                        idx = g_dates.idxmax()
                        base = g.loc[idx].to_dict()
                    else:
                        base = g.iloc[0].to_dict()
                except Exception:
                    base = g.iloc[0].to_dict()
            else:
                base = g.iloc[0].to_dict()

        reasons_set = set()
        explanations_set = []
        for _, row in g.iterrows():
            r = row.get('Reasons')
            if r:
                parts = [x.strip() for x in re.split(r'[;|,]', str(r)) if x and x.strip()]
                for p in parts:
                    if p and not _is_placeholder_str(p):
                        reasons_set.add(p)
            ex = row.get('Explanation')
            if ex and isinstance(ex, str):
                for seg in [s.strip() for s in re.split(r'(?<=[\.\?\!])\s+', ex) if s and s.strip()]:
                    if seg not in explanations_set:
                        explanations_set.append(seg)

        reasons_combined = "; ".join(sorted(reasons_set)) if reasons_set else (base.get('Reasons') if not _is_placeholder_str(base.get('Reasons')) else None)
        explanation_combined = " ".join([s if s.endswith('.') else s + '.' for s in explanations_set]) if explanations_set else base.get('Explanation')

        is_flagged_any = False
        if 'IsFlagged' in g.columns:
            try:
                is_flagged_any = bool(g['IsFlagged'].any())
            except Exception:
                is_flagged_any = any([bool(r.get('IsFlagged')) for _, r in g.iterrows()])

        base['Reasons'] = reasons_combined
        base['Explanation'] = explanation_combined
        base['IsFlagged'] = bool(is_flagged_any)
        return base

    deduped_df = pd.DataFrame()
    if not combined_df.empty:
        id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
        id_col = next((c for c in id_candidates if c in combined_df.columns), None)

        if id_col is None:
            deduped_df = combined_df.copy()
        else:
            combined_df['_norm_id'] = combined_df[id_col].apply(_norm_id_val_local)
            if 'person_uid' in combined_df.columns:
                combined_df['_norm_id'] = combined_df['_norm_id'].fillna(combined_df['person_uid'].astype(str).replace('nan','').replace('None',''))
            combined_df['_norm_id'] = combined_df['_norm_id'].apply(lambda x: x if (x is not None and str(x).strip()!='') else None)
            grouped = []

            non_null_mask = combined_df['_norm_id'].notna()
            if non_null_mask.any():
                for nid, g in combined_df[non_null_mask].groupby('_norm_id'):
                    try:
                        aggregated = _aggregate_group(g)
                        if isinstance(aggregated, dict):
                            aggregated['_norm_id'] = nid
                        else:
                            try:
                                aggregated = dict(aggregated)
                                aggregated['_norm_id'] = nid
                            except Exception:
                                pass
                        grouped.append(aggregated)
                    except Exception:
                        row = g.iloc[0].to_dict()
                        row['_norm_id'] = nid
                        grouped.append(row)

            noid_rows = combined_df[~non_null_mask]
            for _, r in noid_rows.iterrows():
                grouped.append(r.to_dict())

            if grouped:
                try:
                    deduped_df = pd.DataFrame(grouped)
                except Exception:
                    deduped_df = combined_df.copy()
            else:
                deduped_df = combined_df.copy()
    else:
        deduped_df = combined_df.copy()

    # Final safety: ensure IsFlagged/Reasons exist
    if 'IsFlagged' not in deduped_df.columns:
        deduped_df['IsFlagged'] = False
    if 'Reasons' not in deduped_df.columns:
        deduped_df['Reasons'] = None
    if 'Explanation' not in deduped_df.columns:
        deduped_df['Explanation'] = None

    # ----------------- Build sample / counts / response -----------------
    flagged = pd.DataFrame()
    max_rows_to_send = 10
    samples = []
    reasons_count = {}
    risk_counts = { "Low":0, "Low Medium":0, "Medium":0, "Medium High":0, "High":0 }
    raw_unique_person_uids = 0
    analysis_count = 0
    flagged_count = 0
    flagged_rate_pct = 0.0

    try:
        try:
            if not combined_df.empty:
                if 'person_uid' in combined_df.columns:
                    raw_ids = combined_df['person_uid'].dropna().astype(str).apply(lambda s: _norm_id_val_local(s))
                    raw_ids = raw_ids[raw_ids.fillna('').astype(str).str.strip() != '']
                    raw_unique_person_uids = int(raw_ids.nunique())
                elif id_col and id_col in combined_df.columns:
                    raw_ids = combined_df[id_col].dropna().astype(str).apply(lambda s: _norm_id_val_local(s))
                    raw_ids = raw_ids[raw_ids.fillna('').astype(str).str.strip() != '']
                    raw_unique_person_uids = int(raw_ids.nunique())
                else:
                    raw_unique_person_uids = int(len(combined_df))
            else:
                raw_unique_person_uids = 0
        except Exception:
            raw_unique_person_uids = int(len(combined_df)) if combined_df is not None else 0

        try:
            if not deduped_df.empty and 'IsFlagged' in deduped_df.columns:
                flagged = deduped_df[deduped_df['IsFlagged'] == True].copy()
            else:
                flagged = pd.DataFrame()
        except Exception:
            flagged = pd.DataFrame()

        MAX_SAMPLE = 2000
        if not flagged.empty:
            sample_df_for_output = flagged.copy()
        else:
            sample_df_for_output = deduped_df.copy() if not deduped_df.empty else pd.DataFrame()

        try:
            max_rows_to_send = min(int(len(sample_df_for_output)), MAX_SAMPLE) if sample_df_for_output is not None else 10
        except Exception:
            max_rows_to_send = min(10, MAX_SAMPLE)

        try:
            samples = _clean_sample_df(sample_df_for_output, max_rows=max_rows_to_send)
        except Exception:
            samples = _clean_sample_df(deduped_df.head(10), max_rows=10) if not deduped_df.empty else []

        reasons_count = {}
        try:
            if not flagged.empty and 'Reasons' in flagged.columns:
                for v in flagged['Reasons'].dropna().astype(str):
                    for part in re.split(r'[;,\|]', v):
                        key = part.strip()
                        if key and not _is_placeholder_str(key):
                            reasons_count[key] = reasons_count.get(key, 0) + 1
        except Exception:
            logging.exception("Failed computing reasons_count for response")
            reasons_count = {}

        risk_counts = { "Low":0, "Low Medium":0, "Medium":0, "Medium High":0, "High":0 }
        try:
            if not flagged.empty and 'RiskLevel' in flagged.columns:
                for v in flagged['RiskLevel'].fillna('').astype(str):
                    if v:
                        risk_counts[v] = risk_counts.get(v, 0) + 1
        except Exception:
            logging.exception("Failed computing risk_counts for response")
            risk_counts = { "Low":0, "Low Medium":0, "Medium":0, "Medium High":0, "High":0 }

        try:
            if not deduped_df.empty:
                if '_norm_id' in deduped_df.columns:
                    ac_series = deduped_df['_norm_id'].dropna().astype(str).str.strip()
                    ac_series = ac_series[ac_series != '']
                    analysis_count = int(ac_series.nunique())
                elif 'person_uid' in deduped_df.columns:
                    ac_series = deduped_df['person_uid'].dropna().astype(str).apply(lambda s: _norm_id_val_local(s))
                    ac_series = ac_series[ac_series.fillna('').astype(str).str.strip() != '']
                    analysis_count = int(ac_series.nunique()) if len(ac_series) > 0 else int(len(deduped_df))
                else:
                    analysis_count = int(len(deduped_df))
            else:
                analysis_count = 0
        except Exception:
            analysis_count = int(len(deduped_df)) if deduped_df is not None else 0

        try:
            flagged_count = int(len(flagged)) if flagged is not None else 0
            flagged_rate_pct = float((flagged_count / analysis_count * 100.0) if analysis_count and analysis_count > 0 else 0.0)
        except Exception:
            flagged_count = int(len(flagged)) if flagged is not None else 0
            flagged_rate_pct = 0.0

    except Exception:
        logging.exception("Failed preparing sample / counts; falling back to safe defaults.")
        if not samples:
            samples = _clean_sample_df(deduped_df.head(10), max_rows=10) if not deduped_df.empty else []
        reasons_count = reasons_count or {}
        risk_counts = risk_counts or { "Low":0, "Low Medium":0, "Medium":0, "Medium High":0, "High":0 }
        raw_unique_person_uids = raw_unique_person_uids or (int(len(combined_df)) if combined_df is not None else 0)
        analysis_count = analysis_count or (int(len(deduped_df)) if deduped_df is not None else 0)
        flagged_count = flagged_count or 0
        flagged_rate_pct = flagged_rate_pct or 0.0

    try:
        flagged_persons_cleaned = _clean_sample_df(flagged, max_rows=max_rows_to_send) if (flagged is not None and not flagged.empty) else (samples if samples else [])
    except Exception:
        flagged_persons_cleaned = samples if samples else []

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "aggregated_rows_total_raw": int(len(combined_df)),
        "aggregated_unique_persons": int(analysis_count),
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "flagged_rate_percent": float(flagged_rate_pct),
        "files": files,
        "sample": (samples[:max_rows_to_send] if isinstance(samples, list) else samples),
        "reasons_count": reasons_count,
        "risk_counts": risk_counts,
        "flagged_persons": (flagged_persons_cleaned[:max_rows_to_send] if isinstance(flagged_persons_cleaned, list) else flagged_persons_cleaned),
        "_raw_unique_person_uids": int(raw_unique_person_uids),
        "regions_run": params.get('_regions_to_run', [])
    }

    return jsonify(resp)


@app.route('/latest', methods=['GET'])
def latest_results():
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]
    # try to infer the date from the filename (trend_pune_YYYYMMDD.csv)
    start_date_iso = None
    end_date_iso = None
    try:
        m = re.search(r'(\d{8})', latest.name)
        if m:
            ymd = m.group(1)
            dt = datetime.strptime(ymd, "%Y%m%d").date()
            start_date_iso = dt.isoformat()
            end_date_iso = dt.isoformat()
    except Exception:
        start_date_iso = None
        end_date_iso = None

    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)

    # normalize placeholder strings
    df = _replace_placeholder_strings(df)

    # try to compute unique persons (use same id preference as run_trend)
    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in df.columns), None)

    def _norm_val_for_latest(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                fv = float(s)
                if math.isfinite(fv) and fv.is_integer():
                    s = str(int(fv))
        except Exception:
            pass
        return s

    if id_col is None:
        unique_persons = int(len(df))
    else:
        ids_series = df[id_col].apply(_norm_val_for_latest) if id_col in df.columns else pd.Series([None]*len(df))
        if id_col != 'person_uid' and 'person_uid' in df.columns:
            ids_series = ids_series.fillna(df['person_uid'].astype(str).replace('nan','').replace('None',''))
        unique_persons = int(len(set([x for x in ids_series.unique() if x])))

    sample = _clean_sample_df(df, max_rows=5)
    resp = {
        "file": latest.name,
        "rows_raw": int(len(df)),
        "rows": unique_persons,
        "sample": sample,
        # new: what date the latest file represents (helpful for frontend)
        "start_date": start_date_iso,
        "end_date": end_date_iso
    }
    return jsonify(resp)


@app.route('/record', methods=['GET'])
def get_record():
    """
    /record?employee_id=... or /record?person_uid=...
    Returns matching aggregated trend rows and filtered raw swipe rows (only for flagged persons).
    Updated: read ALL trend CSVs (not just the latest), so previous days are included.
    Also add Zone and SwipeGap (seconds) to raw_swipes returned as evidence.

    New behaviour:
      - By default, raw_swipes are returned only for aggregated rows where IsFlagged == True
      - Set include_unflagged=1 (or true/yes) to also fetch evidence for unflagged aggregated rows.
      - If an aggregated row has no Date/FirstSwipe/LastSwipe, we fall back to scanning all swipes files.
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    # Read all CSVs (concat) so /record will search previous days too
    df_list = []
    for fp in csvs:
        try:
            # removed deprecated infer_datetime_format argument
            tmp = pd.read_csv(fp, parse_dates=['FirstSwipe','LastSwipe'])
        except Exception:
            try:
                tmp = pd.read_csv(fp, dtype=str)
            except Exception:
                continue
        df_list.append(tmp)
    if not df_list:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200
    df = pd.concat(df_list, ignore_index=True)

    # normalize placeholder literal strings
    df = _replace_placeholder_strings(df)

    if q is None:
        cleaned = _clean_sample_df(df, max_rows=10)
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    q_str = str(q).strip()

    def normalize_series(s):
        if s is None:
            return pd.Series([''] * len(df))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if math.isfinite(fv) and fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    found_mask = pd.Series(False, index=df.index)

    if 'EmployeeID' in df.columns:
        emp_series = normalize_series(df['EmployeeID'])
        found_mask = found_mask | (emp_series == q_str)

    if 'person_uid' in df.columns:
        uid_series = normalize_series(df['person_uid'])
        found_mask = found_mask | (uid_series == q_str)

    # also check Int1 (Personnel.Int1) if present in CSV
    if 'Int1' in df.columns and not found_mask.any():
        int1_series = normalize_series(df['Int1'])
        found_mask = found_mask | (int1_series == q_str)

    if not found_mask.any():
        # try numeric equivalence
        try:
            q_numeric = float(q_str)
            if 'EmployeeID' in df.columns:
                emp_numeric = pd.to_numeric(df['EmployeeID'], errors='coerce')
                found_mask = found_mask | (emp_numeric == q_numeric)
            if 'Int1' in df.columns and not found_mask.any():
                int_numeric = pd.to_numeric(df['Int1'], errors='coerce')
                found_mask = found_mask | (int_numeric == q_numeric)
        except Exception:
            pass

    matched = df[found_mask]
    if matched.empty:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    cleaned_matched = _clean_sample_df(matched, max_rows=len(matched))


    # --- ENRICHMENT START ---
    # cleaned_matched is a list of dicts
    # Try to map each cleaned row to a personnel record and attach email/manager/image info
    try:
        # Build a quick index over matched DataFrame to find best lookup candidate for each cleaned row
        matched_indexed = matched.reset_index(drop=True)
        for idx_c, cleaned in enumerate(cleaned_matched):
            # Try to find a matching row in matched DataFrame by person_uid -> EmployeeID -> EmployeeName
            candidate_row = None
            try:
                if cleaned.get('person_uid'):
                    if 'person_uid' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('person_uid', '').astype(str) == str(cleaned['person_uid'])]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                if candidate_row is None and cleaned.get('EmployeeID'):
                    if 'EmployeeID' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('EmployeeID', '').astype(str) == str(cleaned['EmployeeID'])]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                if candidate_row is None and cleaned.get('EmployeeName'):
                    # names exact match (defensive)
                    if 'EmployeeName' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('EmployeeName', '').astype(str).str.strip().fillna('') == str(cleaned['EmployeeName']).strip()]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                # fallback: take first matched row if nothing else
                if candidate_row is None and len(matched_indexed) > 0:
                    candidate_row = matched_indexed.iloc[0].to_dict()
            except Exception:
                candidate_row = None

            # Determine the best candidate identifier to query Personnel
            lookup_candidates = []
            if candidate_row:
                for k in ('EmployeeObjID', 'EmployeeObjId', 'EmployeeIdentity', 'ObjectID', 'GUID', 'EmployeeID', 'Int1', 'Text12', 'EmployeeName'):
                    if k in candidate_row and candidate_row.get(k) not in (None, '', 'nan'):
                        lookup_candidates.append(candidate_row.get(k))
            # also include cleaned fields
            for k in ('EmployeeID', 'person_uid', 'EmployeeName'):
                if cleaned.get(k) not in (None, '', 'nan'):
                    lookup_candidates.append(cleaned.get(k))

            # try each lookup candidate until we find personnel info
            personnel_info = {}
            for cand in lookup_candidates:
                if cand is None:
                    continue
                try:
                    info = get_personnel_info(cand)
                    if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None or info.get('ManagerEmail') is not None):
                        personnel_info = info
                        break
                except Exception:
                    continue

            # attach if found
            if personnel_info:
                # safe assignments
                cleaned['EmployeeObjID'] = personnel_info.get('ObjectID')
                cleaned['EmployeeEmail'] = personnel_info.get('EmailAddress')
                cleaned['ManagerEmail'] = personnel_info.get('ManagerEmail')
                # HasImage and imageUrl
                if personnel_info.get('ObjectID') is not None:
                    cleaned['imageUrl'] = f"/employee/{personnel_info.get('ObjectID')}/image"
                    # quick check if image exists (non-blocking: try fetch bytes but ignore failure)
                    try:
                        b = get_person_image_bytes(personnel_info.get('ObjectID'))
                        cleaned['HasImage'] = True if b else False
                    except Exception:
                        cleaned['HasImage'] = False
                else:
                    cleaned['imageUrl'] = None
                    cleaned['HasImage'] = False
    except Exception:
        logging.exception("Failed to enrich aggregated rows with personnel/email/image info")
    # --- ENRICHMENT END ---


    # Resolve raw swipe file names by Date (collect all dates present in matched rows)
    raw_files = set()
    raw_swipes_out = []

    # Helper to add and dedupe swipe rows
    seen_swipe_keys = set()
    def _append_swipe(out_row, source_name):
        # create a dedupe key (date,time,door,direction,card)
        key = (
            out_row.get('Date') or '',
            out_row.get('Time') or '',
            (out_row.get('Door') or '').strip(),
            (out_row.get('Direction') or '').strip(),
            (out_row.get('CardNumber') or out_row.get('Card') or '').strip()
        )
        if key in seen_swipe_keys:
            return
        seen_swipe_keys.add(key)
        out_row['_source'] = source_name
        raw_swipes_out.append(out_row)

    # iterate matched aggregated rows and search raw swipe files
    for idx, agg_row in matched.iterrows():
        person_uid = agg_row.get('person_uid') if 'person_uid' in agg_row else None
        empid = agg_row.get('EmployeeID') if 'EmployeeID' in agg_row else None
        if (not empid) and 'Int1' in agg_row:
            empid = agg_row.get('Int1')
        card = agg_row.get('CardNumber') if 'CardNumber' in agg_row else None

       
       
      
      
      
      
      
      
      
      
              # build dates_for_row from Date / FirstSwipe / LastSwipe (include AdjustedMessageTime / ShiftedDate / OriginalLocaleMessageTime if present)
        dates_for_row = set()
        if 'Date' in agg_row and pd.notna(agg_row['Date']):
            try:
                d = pd.to_datetime(agg_row['Date']).date()
                dates_for_row.add(d.isoformat())
            except Exception:
                pass
        for col in ('FirstSwipe', 'LastSwipe'):
            if col in agg_row and pd.notna(agg_row[col]):
                try:
                    d = pd.to_datetime(agg_row[col]).date()
                    dates_for_row.add(d.isoformat())
                except Exception:
                    pass

        # include AdjustedMessageTime / ShiftedDate / OriginalLocaleMessageTime if present
        for cand_col in ('AdjustedMessageTime', 'ShiftedDate', 'OriginalLocaleMessageTime'):
            if cand_col in agg_row and pd.notna(agg_row.get(cand_col)):
                try:
                    d = pd.to_datetime(agg_row.get(cand_col)).date()
                    dates_for_row.add(d.isoformat())
                except Exception:
                    # if it's already a date string fallback
                    try:
                        if isinstance(agg_row.get(cand_col), str) and len(agg_row.get(cand_col)) >= 8:
                            dates_for_row.add(str(agg_row.get(cand_col))[:10])
                    except Exception:
                        pass

        # If the aggregated row is not flagged and include_unflagged is False, skip fetching raw evidence
        is_flagged = bool(agg_row.get('IsFlagged', False))
        if (not is_flagged) and (not include_unflagged):
            continue

        # If no dates or if date-specific candidates are missing, scan all swipes files as a fallback
        dates_to_scan = dates_for_row or {None}

        for d in dates_to_scan:
            try:
                candidates = []
                if d is None:
                    candidates = list(Path(DEFAULT_OUTDIR).glob("swipes_*.csv"))
                else:
                    dd = str(d)[:10]  # 'YYYY-MM-DD'
                    target = dd.replace('-', '')
                    candidates = list(Path(DEFAULT_OUTDIR).glob(f"swipes_*_{target}.csv"))

                # fallback: if exact per-date candidates not found, search all swipes files
                if not candidates:
                    candidates = list(Path(DEFAULT_OUTDIR).glob("swipes_*.csv"))
                    if not candidates:
                        continue


                for fp in candidates:
                    raw_name = fp.name
                    raw_files.add(raw_name)
                    ...



                for fp in candidates:
                    raw_name = fp.name
                    raw_files.add(raw_name)
                    try:
                        # removed deprecated infer_datetime_format argument
                        raw_df = pd.read_csv(fp, parse_dates=['LocaleMessageTime'])
                    except Exception:
                        try:
                            raw_df = pd.read_csv(fp, dtype=str)
                        except Exception:
                            continue

                    # normalize placeholder strings in raw_df as well (object columns)
                    raw_df = _replace_placeholder_strings(raw_df)

                    cols_lower = {c.lower(): c for c in raw_df.columns}
                    tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
                    emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                    name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
                    card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
                    door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
                    dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
                    note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None

                    # build filter mask
                    mask = pd.Series(False, index=raw_df.index)
                    if person_uid is not None and 'person_uid' in raw_df.columns:
                        mask = mask | (raw_df['person_uid'].astype(str).str.strip() == str(person_uid).strip())
                    if emp_col:
                        if empid is not None:
                            try:
                                cmp_val = str(empid).strip()
                                if '.' in cmp_val:
                                    fv = float(cmp_val)
                                    if math.isfinite(fv) and fv.is_integer():
                                        cmp_val = str(int(fv))
                            except Exception:
                                cmp_val = str(empid).strip()
                            mask = mask | (raw_df[emp_col].astype(str).str.strip() == cmp_val)
                    if card_col and card is not None:
                        mask = mask | (raw_df[card_col].astype(str).str.strip() == str(card).strip())

                    if (not mask.any()) and name_col and 'EmployeeName' in agg_row and pd.notna(agg_row.get('EmployeeName')):
                        mask = mask | (raw_df[name_col].astype(str).str.strip() == str(agg_row.get('EmployeeName')).strip())

                    # filter by date if possible
                    if d is not None and tcol and tcol in raw_df.columns:
                        try:
                            raw_df[tcol] = pd.to_datetime(raw_df[tcol], errors='coerce')
                            mask = mask & (raw_df[tcol].dt.date == pd.to_datetime(d).date())
                        except Exception:
                            pass

                    filtered = raw_df[mask].copy()
                    if filtered.empty:
                        # xml value fallback for embedded card values
                        if card is not None:
                            for ccol in raw_df.columns:
                                cl = ccol.lower()
                                if 'xml' in cl or 'msg' in cl or 'value' == cl:
                                    try:
                                        vals = raw_df[ccol].dropna().astype(str)
                                        match_mask = vals.apply(lambda x: (_extract_card_from_xml_text(x) == str(card).strip()))
                                        if match_mask.any():
                                            idxs = match_mask.index[match_mask]
                                            filtered = raw_df.loc[idxs].copy()
                                            break
                                    except Exception:
                                        continue
                        if filtered.empty:
                            continue

                    # enrich filtered rows and append to output (deduped)
                    try:
                        if tcol and tcol in filtered.columns:
                            filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                        else:
                            if 'localemessagetime' in filtered.columns:
                                filtered['localemessagetime'] = pd.to_datetime(filtered['localemessagetime'], errors='coerce')
                                tcol = 'localemessagetime'
                    except Exception:
                        pass
                    if tcol and tcol in filtered.columns:
                        filtered = filtered.sort_values(by=tcol)
                        filtered['_prev_ts'] = filtered[tcol].shift(1)
                        try:
                            filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                        except Exception:
                            filtered['_swipe_gap_seconds'] = 0.0

                        # ZERO gaps when the previous swipe belongs to a different calendar day
                        # (so the first swipe of a logical day shows 00:00:00 gap).
                        try:
                            # compute series of current/previous dates (local date part)
                            cur_dates = filtered[tcol].dt.date
                            prev_dates = cur_dates.shift(1)
                            # any row where prev_date != cur_date or prev_ts is NaT should be treated as day-start
                            day_start_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
                            filtered.loc[day_start_mask, '_swipe_gap_seconds'] = 0.0
                        except Exception:
                            # non-fatal: leave computed gaps as-is if any error occurs
                            pass

                    else:
                        filtered['_swipe_gap_seconds'] = 0.0

                    try:
                        if door_col and door_col in filtered.columns:
                            if dir_col and dir_col in filtered.columns:
                                filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                            else:
                                filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
                        else:
                            if 'PartitionName2' in filtered.columns:
                                filtered['_zone'] = filtered['PartitionName2'].fillna('').astype(str).apply(lambda x: x if x else None)
                            else:
                                filtered['_zone'] = None
                    except Exception:
                        filtered['_zone'] = None

                    # normalize and append
                    for _, r in filtered.iterrows():
                        out = {}
                        out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in raw_df.columns else _to_python_scalar(agg_row.get('EmployeeName') or agg_row.get('person_uid'))

                        # EmployeeID
                        emp_val = None
                        if 'int1' in cols_lower and cols_lower.get('int1') in raw_df.columns:
                            emp_val = _to_python_scalar(r.get(cols_lower.get('int1')))
                        elif emp_col and emp_col in raw_df.columns:
                            emp_val = _to_python_scalar(r.get(emp_col))
                        else:
                            possible_emp = None
                            for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                                if cand.lower() in cols_lower:
                                    possible_emp = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                                    if possible_emp not in (None, '', 'nan'):
                                        break
                            emp_val = possible_emp if possible_emp not in (None, '', 'nan') else _to_python_scalar(agg_row.get('EmployeeID'))

                        if emp_val is not None:
                            try:
                                s = str(emp_val).strip()
                                if '.' in s:
                                    f = float(s)
                                    if math.isfinite(f) and f.is_integer():
                                        s = str(int(f))
                                if _looks_like_guid(s) or _is_placeholder_str(s):
                                    emp_val = None
                                else:
                                    emp_val = s
                            except Exception:
                                if _looks_like_guid(emp_val):
                                    emp_val = None
                        out['EmployeeID'] = emp_val

                        # CardNumber
                        card_val = None
                        if 'cardnumber' in cols_lower and cols_lower.get('cardnumber') in raw_df.columns:
                            card_val = _to_python_scalar(r.get(cols_lower.get('cardnumber')))
                        elif card_col and card_col in raw_df.columns:
                            card_val = _to_python_scalar(r.get(card_col))
                        else:
                            possible_card = None
                            for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                                if cand.lower() in cols_lower:
                                    possible_card = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                                    if possible_card not in (None, '', 'nan'):
                                        break
                            if possible_card in (None, '', 'nan'):
                                for ccc in raw_df.columns:
                                    cl = ccc.lower()
                                    if 'xml' in cl or 'msg' in cl or 'value' == cl:
                                        try:
                                            txt = r.get(ccc)
                                            extracted = _extract_card_from_xml_text(str(txt)) if txt is not None else None
                                            if extracted:
                                                possible_card = extracted
                                                break
                                        except Exception:
                                            continue
                            card_val = possible_card if possible_card not in (None, '', 'nan') else _to_python_scalar(agg_row.get('CardNumber'))

                        if card_val is not None:
                            try:
                                cs = str(card_val).strip()
                                if _looks_like_guid(cs) or _is_placeholder_str(cs):
                                    card_val = None
                                else:
                                    card_val = cs
                            except Exception:
                                card_val = None
                        out['CardNumber'] = card_val

                        # timestamp -> Date/Time
                        if tcol and tcol in raw_df.columns:
                            ts = r.get(tcol)
                            try:
                                ts_py = pd.to_datetime(ts)
                                out['Date'] = ts_py.date().isoformat()
                                out['Time'] = ts_py.time().isoformat()
                            except Exception:
                                txt = str(r.get(tcol))
                                out['Date'] = txt[:10]
                                out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                        else:
                            out['Date'] = d if d is not None else None
                            out['Time'] = None

                        out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
                        out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in r else None
                        out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None

                        try:
                            out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else map_door_to_zone(out['Door'], out['Direction'])
                        except Exception:
                            out['Zone'] = None
                        try:
                            gap = r.get('_swipe_gap_seconds') if '_swipe_gap_seconds' in r else None
                            out['SwipeGapSeconds'] = float(gap) if gap is not None else None
                            out['SwipeGap'] = format_seconds_to_hms(out['SwipeGapSeconds'])
                        except Exception:
                            out['SwipeGapSeconds'] = None
                            out['SwipeGap'] = None

                        _append_swipe(out, raw_name)
            except Exception as e:
                logging.exception("Error processing raw swipe file for date %s: %s", d, e)
                continue

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": sorted(list(raw_files)),
        "raw_swipes": raw_swipes_out
    }), 200


@app.route('/override', methods=['POST'])
def set_override():
    """
    POST JSON: { "employee_id": "<EmployeeID or person_uid>", "level": "Low|Medium|High|Clear", "reason": "justification text" }
    Saves override to outputs/overrides.csv. Run-time uses this file to adjust RiskLevel display.
    """
    try:
        payload = request.get_json(force=True)
        emp = payload.get('employee_id')
        level = payload.get('level')
        reason = payload.get('reason', '')
        if not emp or not level:
            return jsonify({'error':'employee_id and level required'}), 400
        ok = _save_override(str(emp).strip(), str(level).strip(), str(reason).strip())
        if not ok:
            return jsonify({'error':'failed to save override'}), 500
        return jsonify({'status':'ok'}), 200
    except Exception as e:
        logging.exception("Override save error")
        return jsonify({'error': str(e)}), 500

# ... rest of file (export, download_swipes, train, chatbot_query, serve_employee_image) remain unchanged ...
# (note: serve_employee_image below uses get_person_image_bytes which now works because _get_acvscore_conn is restored)

@app.route('/record/export', methods=['GET'])
def export_record_excel():
    """
    /record/export?employee_id=...&date=YYYY-MM-DD  OR /record/export?person_uid=...&date=YYYY-MM-DD
    Produces an Excel file (xlsx) filtered for the requested employee and date (if provided).
    Two sheets:
      - "Details — Evidence": EmployeeName, EmployeeID, Door, Direction, Zone, Date, LocaleMessageTime, SwipeGapSeconds, PartitionName2, _source_file
      - "Swipe timeline": Employee Name, Employee ID, Card, Date, Time, SwipeGapSeconds, Door, Direction, Zone, Note
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    date_str = request.args.get('date')  # optional 'YYYY-MM-DD' (single date)
    if not q:
        return jsonify({"error":"employee_id or person_uid is required"}), 400

    # Determine list of raw swipe files to scan. If date provided, restrict to that date only.
    files_to_scan = []
    p = Path(DEFAULT_OUTDIR)
    if date_str:
        try:
            dd = pd.to_datetime(date_str).date()
            target = dd.strftime("%Y%m%d")
            candidates = list(p.glob(f"swipes_*_{target}.csv"))
            files_to_scan = candidates
        except Exception:
            return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400
    else:
        # scan all swipes files (most recent first)
        files_to_scan = sorted(p.glob("swipes_*.csv"), reverse=True)

    if not files_to_scan:
        return jsonify({"error":"no raw swipe files found for requested date / outputs"}), 404

    all_rows = []
    for fp in files_to_scan:
        try:
            # removed deprecated infer_datetime_format argument
            raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
        except Exception:
            try:
                raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

        # normalize column names
        raw_df = _replace_placeholder_strings(raw_df)

        cols_lower = {c.lower(): c for c in raw_df.columns}
        # pick possible columns
        tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
        emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
        name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
        card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
        door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
        dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
        note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
        person_uid_col = cols_lower.get('person_uid')

        # build mask matching requested q: try person_uid, emp_col, card, name
        mask = pd.Series(False, index=raw_df.index)
        if person_uid_col and person_uid_col in raw_df.columns:
            mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
        if emp_col and emp_col in raw_df.columns:
            mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
        # try matching numeric equivalence
        if not mask.any() and emp_col and emp_col in raw_df.columns:
            try:
                q_numeric = float(q)
                emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                mask = mask | (emp_numeric == q_numeric)
            except Exception:
                pass
        # also try name match if nothing matched
        if not mask.any() and name_col and name_col in raw_df.columns:
            mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())

        if not mask.any():
            # nothing to include from this file
            continue

        filtered = raw_df[mask].copy()
        if filtered.empty:
            continue

        # ensure timestamp col exists and parsed
        if tcol and tcol in filtered.columns:
            try:
                filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
            except Exception:
                pass

        # compute swipe gaps (seconds)
        if tcol and tcol in filtered.columns:
            filtered = filtered.sort_values(by=tcol)
            filtered['_prev_ts'] = filtered[tcol].shift(1)
            try:
                filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
            except Exception:
                filtered['_swipe_gap_seconds'] = 0.0
        else:
            filtered['_swipe_gap_seconds'] = 0.0

        # compute zone per row (use door+direction if available)
        try:
            if door_col and door_col in filtered.columns:
                if dir_col and dir_col in filtered.columns:
                    filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                else:
                    filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
            else:
                filtered['_zone'] = filtered.get('PartitionName2', None)
        except Exception:
            filtered['_zone'] = None

        # normalize columns into common shape and append rows
        for _, r in filtered.iterrows():
            row = {}
            row['EmployeeName'] = _to_python_scalar(r.get(name_col)) if (name_col and name_col in filtered.columns) else None
            # EmployeeID attempts: Int1/Text12/EmployeeID
            emp_val = None
            if emp_col and emp_col in filtered.columns:
                emp_val = _to_python_scalar(r.get(emp_col))
            else:
                # fallbacks
                for cand in ('int1','text12','employeeid','employee_identity','employeeidentity'):
                    if cand in cols_lower and cols_lower[cand] in filtered.columns:
                        emp_val = _to_python_scalar(r.get(cols_lower[cand]))
                        if emp_val:
                            break
            row['EmployeeID'] = emp_val
            row['Card'] = _to_python_scalar(r.get(card_col)) if (card_col and card_col in filtered.columns) else None

            # Date and Time
            if tcol and tcol in filtered.columns:
                ts = r.get(tcol)
                try:
                    ts_py = pd.to_datetime(ts)
                    row['Date'] = ts_py.date().isoformat()
                    row['Time'] = ts_py.time().isoformat()
                    row['LocaleMessageTime'] = ts_py.isoformat()
                except Exception:
                    txt = str(r.get(tcol))
                    row['Date'] = txt[:10]
                    row['Time'] = txt[11:19] if len(txt) >= 19 else None
                    row['LocaleMessageTime'] = txt
            else:
                row['Date'] = None
                row['Time'] = None
                row['LocaleMessageTime'] = None

            row['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
            row['SwipeGap'] = format_seconds_to_hms(row['SwipeGapSeconds'])

            row['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
            row['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else None
            row['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None

            # Zone (computed above)
            try:
                zone_val = r.get('_zone') if '_zone' in r else None
                if zone_val is None:
                    # fallback from door/direction
                    zone_val = map_door_to_zone(row['Door'], row['Direction'])
                row['Zone'] = _to_python_scalar(zone_val)
            except Exception:
                row['Zone'] = None

            row['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
            row['_source_file'] = fp.name
            all_rows.append(row)

    if not all_rows:
        return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

    df_out = pd.DataFrame(all_rows)

    # Build two sheets as requested (with exact requested column order)
    details_cols = ['EmployeeName','EmployeeID','Door','Direction','Zone','Date','LocaleMessageTime','SwipeGapSeconds','PartitionName2','_source_file']
    timeline_cols = ['EmployeeName','EmployeeID','Card','Date','Time','SwipeGapSeconds','Door','Direction','Zone','Note','_source_file']

    details_df = df_out[[c for c in details_cols if c in df_out.columns]].copy()
    timeline_df = df_out[[c for c in timeline_cols if c in df_out.columns]].copy()

    # Create excel in-memory
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            details_df.to_excel(writer, sheet_name='Details — Evidence', index=False)
            timeline_df.to_excel(writer, sheet_name='Swipe timeline', index=False)
            writer.save()
            output.seek(0)
    except Exception as e:
        logging.exception("Failed to create Excel: %s", e)
        return jsonify({"error":"failed to create excel"}), 500

    # If openpyxl available, apply formatting (bold header, center align, borders)
    if OPENPYXL_AVAILABLE:
        try:
            wb = load_workbook(output)
            thin = Side(border_style="thin", color="000000")
            thick = Side(border_style="medium", color="000000")
            for ws in wb.worksheets:
                # header styling
                header = ws[1]
                for cell in header:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
                # data rows: center & thin border
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                        cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
                # autosize columns (best-effort)
                for col in ws.columns:
                    max_len = 0
                    col_letter = col[0].column_letter
                    for cell in col:
                        try:
                            v = str(cell.value) if cell.value is not None else ""
                        except Exception:
                            v = ""
                        if len(v) > max_len:
                            max_len = len(v)
                    # limit column width
                    width = min(max(10, max_len + 2), 50)
                    ws.column_dimensions[col_letter].width = width
            # write back to bytes
            out2 = io.BytesIO()
            wb.save(out2)
            out2.seek(0)
            return send_file(out2, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        except Exception:
            logging.exception("Excel styling failed, returning raw file")
            output.seek(0)
            return send_file(output, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    else:
        # fallback: return raw excel binary without styling
        output.seek(0)
        return send_file(output, as_attachment=True,
                         download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                         mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")


@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    """
    Serve raw swipe CSVs from outputs/ (filename should be the file name only).
    """
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    # send file
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)


@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500


# ------------------------------
# Improved chatbot endpoint & helpers (replacement)
# ------------------------------

# try to import helpers from trend_runner (they may or may not be present)
try:
    from trend_runner import _read_past_trend_csvs, _normalize_id_val, SCENARIO_EXPLANATIONS
except Exception:
    _read_past_trend_csvs = None
    _normalize_id_val = None
    SCENARIO_EXPLANATIONS = {}

# helper to load the most recent single trend CSV (fast for "today" queries)
def _load_latest_trend_df(outdir: Path):
    csvs = sorted(outdir.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return None, None
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        try:
            df = pd.read_csv(latest, dtype=str)
        except Exception:
            return None, None
    # normalize placeholder strings
    df = _replace_placeholder_strings(df)
    return df, latest.name

# helper: find person rows across recent days (useful for "last N days")
def _find_person_rows(identifier: str, days: int = 90, outdir: Path = DEFAULT_OUTDIR):
    # normalize token (strip ".0" floats etc)
    if _normalize_id_val:
        norm = _normalize_id_val(identifier)
    else:
        norm = str(identifier).strip()
        if '.' in norm:
            try:
                f = float(norm)
                if f.is_integer():
                    norm = str(int(f))
            except Exception:
                pass
    # read past trend CSVs using trend_runner helper when available
    today = datetime.now().date()
    try:
        if _read_past_trend_csvs:
            past = _read_past_trend_csvs(str(outdir), days, today)
        else:
            # fallback: read files manually
            files = sorted(Path(outdir).glob("trend_pune_*.csv"), reverse=True)
            dfs = []
            cutoff = today - timedelta(days=days)
            for fp in files:
                try:
                    tmp = pd.read_csv(fp, parse_dates=['Date'])
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                    tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                    dfs.append(tmp)
                except Exception:
                    try:
                        tmp = pd.read_csv(fp, dtype=str)
                        if 'Date' in tmp.columns:
                            tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                            tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                            dfs.append(tmp)
                    except Exception:
                        continue
            past = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
    except Exception:
        past = pd.DataFrame()

    if past is None or past.empty:
        return pd.DataFrame()

    # normalize placeholders
    past = _replace_placeholder_strings(past)

    # search by EmployeeID, person_uid, EmployeeIdentity, CardNumber, Int1/Text12 if present
    match_mask = pd.Series(False, index=past.index)
    for col in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
        if col in past.columns:
            try:
                match_mask = match_mask | (past[col].astype(str).fillna('').str.strip() == str(norm).strip())
            except Exception:
                continue

    # also try numeric equality
    if not match_mask.any():
        try:
            qnum = float(norm)
            for col in ('EmployeeID','Int1'):
                if col in past.columns:
                    try:
                        numcol = pd.to_numeric(past[col], errors='coerce')
                        match_mask = match_mask | (numcol == qnum)
                    except Exception:
                        continue
        except Exception:
            pass

    # fuzzy name match if nothing matched above
    if not match_mask.any() and 'EmployeeName' in past.columns:
        names = past['EmployeeName'].dropna().astype(str).unique().tolist()
        close = difflib.get_close_matches(str(identifier), names, n=5, cutoff=0.7)
        if close:
            match_mask = match_mask | past['EmployeeName'].astype(str).isin(close)

    return past[match_mask].copy()

# simple sentence builder for scenario codes -> human text
def _explain_scenario_code(code):
    if not code:
        return None
    code = str(code).strip()
    # prefer full mapping if available
    if code in SCENARIO_EXPLANATIONS:
        try:
            fn = SCENARIO_EXPLANATIONS.get(code)
            try:
                txt = fn({})
                return txt
            except Exception:
                return code.replace("_", " ").replace(">= ", "≥ ")
        except Exception:
            return code.replace("_", " ").replace(">= ", "≥ ")
    # fallback readable version
    return code.replace("_", " ").replace(">=", "≥")

# fallback mapping for numeric RiskScore -> (score, label)
def _map_score_to_label_fallback(score: float):
    try:
        s = float(score)
    except Exception:
        return (0.0, "Low")
    if s >= 0.75:
        return (s, "High")
    if s >= 0.4:
        return (s, "Medium")
    return (s, "Low")

@app.route('/chatbot/query', methods=['POST'])
def chatbot_query():
    """
    Improved chatbot query handler.
    Payload: { "q": "<free text query>", "top_k": 5, "lang": "en" }
    Recognised intents:
      - who is high risk today / who is low risk today
      - show me <EmployeeID|name> last <N> days
      - explain <scenario_code>
      - trend details for today / top reasons today
    Returns: { "answer": "<nl text>", "evidence": [ {source, snippet}, ... ] }
    """
    payload = request.get_json(force=True)
    q = (payload.get('q') or '').strip()
    if not q:
        return jsonify({"error":"query text 'q' required"}), 400

    lang = payload.get('lang')  # optional, frontend can provide (we will echo answer in same language string)
    q_l = q.lower().strip()

    # intent: who is high/low risk today
    if re.search(r"\bwho is (high|low) risk\b", q_l) or re.search(r"\b(high|low) risk (people|persons|people) (today)?\b", q_l):
        want = 'high' if 'high' in q_l else 'low' if 'low' in q_l else None
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer": "No trend data available.", "evidence": []})
        # ensure RiskLevel column exists
        if 'RiskLevel' not in df.columns:
            # try to map RiskScore to label if RiskScore present
            if 'RiskScore' in df.columns:
                def _map_rs(s):
                    try:
                        if pd.isna(s):
                            return 'Low'
                    except Exception:
                        pass
                    try:
                        # if external helper available, prefer it
                        if 'map_score_to_label' in globals() and callable(globals().get('map_score_to_label')):
                            try:
                                return globals().get('map_score_to_label')(float(s))[1]
                            except Exception:
                                pass
                        # fallback
                        return _map_score_to_label_fallback(float(s))[1]
                    except Exception:
                        return 'Low'
                df['RiskLevel'] = df['RiskScore'].apply(lambda s: _map_rs(s))
            else:
                df['RiskLevel'] = df.get('RiskLevel', 'Low')

        if want == 'high':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'high']
        elif want == 'low':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'low']
        else:
            sel = df

        names = sel['EmployeeName'].dropna().astype(str).unique().tolist()
        if not names:
            ans = f"No {want} risk persons found in the latest data." if want else "No persons found."
            return jsonify({"answer": ans, "evidence": []})
        else:
            # limit list length
            ans = f"{want.capitalize()} risk persons today: " + ", ".join(names[:40])
            sample = _clean_sample_df(sel.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})

    # intent: explain <scenario>
    m = re.match(r".*\bexplain\s+([A-Za-z0-9_\-]+)\b.*", q_l)
    if m:
        code = m.group(1).strip()
        explanation = _explain_scenario_code(code)
        ans = f"Explanation for '{code}': {explanation}"
        return jsonify({"answer": ans, "evidence": []})

    # intent: trend details for today — top reasons
    if 'trend details' in q_l or 'top reasons' in q_l or 'trend details for today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        # aggregate Reasons column
        if 'Reasons' in df.columns:
            reasons = {}
            for v in df['Reasons'].dropna().astype(str):
                for part in re.split(r'[;,\|]', v):
                    key = part.strip()
                    if key and not _is_placeholder_str(key):
                        reasons[key] = reasons.get(key, 0) + 1
            # sort
            top = sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:10]
            if not top:
                return jsonify({"answer":"No reason counts available today.","evidence":[]})
            ans = "Top reasons today: " + ", ".join([f"{k} ({c})" for k,c in top])
            # include small evidence of rows where those reasons occurred
            sample = []
            try:
                top_reasons = [k for k,_ in top]
                mask = df['Reasons'].astype(str).apply(lambda s: any(tr in s for tr in top_reasons))
                sample_df = df[mask].head(10)
                sample = _clean_sample_df(sample_df, max_rows=10)
            except Exception:
                sample = []
            return jsonify({"answer": ans, "evidence": sample})

    # intent: show me <id|name> last N days
    m = re.match(r".*\bshow (?:me )?([A-Za-z0-9\-\:\s]+?) (?:for )?(?:last )?(\d+)\s*days\b", q_l)
    if not m:
        # also accept "show me 320172 last 90 days" or "show 320172 last 90 days" (looser)
        m = re.match(r".*\b(show|display)\s+(?:me\s+)?([A-Za-z0-9\-\:\s]+?)\s+last\s+(\d+)\s*days\b", q_l)
    if m:
        if len(m.groups()) == 2:
            identifier, days = m.group(1).strip(), int(m.group(2))
        else:
            # if earlier regex produced different groups
            identifier = m.group(1).strip()
            days = int(m.group(2))
        # find person rows in past `days`
        rows = _find_person_rows(identifier, days=days, outdir=DEFAULT_OUTDIR)
        if rows is None or rows.empty:
            return jsonify({"answer": f"No records found for '{identifier}' in last {days} days.", "evidence": []})
        # prepare textual summary
        flagged = rows[rows.get('IsFlagged', False) == True] if 'IsFlagged' in rows.columns else pd.DataFrame()
        flagged_count = int(len(flagged))
        total_days = int(len(rows))
        latest_row = rows.sort_values('Date', ascending=False).iloc[0].to_dict()
        name = latest_row.get('EmployeeName') or latest_row.get('person_uid') or latest_row.get('EmployeeID')
        ans = f"Found {total_days} day(s) for {name} in the last {days} days. Flagged days: {flagged_count}."
        sample = _clean_sample_df(rows.sort_values('Date', ascending=False).head(10), max_rows=10)
        return jsonify({"answer": ans, "evidence": sample})

    # fallback: try simple heuristics like "who is present today" or "present today"
    if 'present today' in q_l or 'who is present today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'PresentToday' in df.columns:
            present = df[df['PresentToday'] == True]
            names = present['EmployeeName'].dropna().unique().tolist()
            ans = f"Present today: {', '.join(names[:40]) if names else 'None'}"
            sample = _clean_sample_df(present.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})
        else:
            return jsonify({"answer":"PresentToday field not available in latest trends.","evidence":[]})

    # final graceful fallback with helpful hint
    hint = "I can answer: 'Who is high risk today', 'Who is low risk today', 'Show me <EmployeeID|Name> last 90 days', 'Explain <scenario_code>', 'Trend details for today — top reasons'."
    return jsonify({"answer": f"I can help with trend & risk questions. I recognized: {q}. Try: {hint}", "evidence":[]})

# Serve employee image route (defined after app)
@app.route('/employee/<empid>/image', methods=['GET'])
def serve_employee_image(empid):
    """
    Serve the portrait for a personnel record where ParentId = empid (ACVSCore.Images).
    Uses the query you provided:
      SELECT AI.Image AS ImageBuffer FROM ACVSCore.Access.Images AI WHERE AI.ParentId = @id
    If found, returns binary with best-effort content-type detection.
    """
    if empid is None:
        return jsonify({"error": "employee id required"}), 400

    try:
        img_bytes = get_person_image_bytes(empid)
        if not img_bytes:
            return jsonify({"error": "no image found"}), 404

        # try to detect jpeg/png
        header = img_bytes[:8]
        content_type = 'application/octet-stream'
        if len(header) >= 2 and header[0] == 0xFF and header[1] == 0xD8:
            content_type = 'image/jpeg'
        elif header.startswith(b'\x89PNG\r\n\x1a\n'):
            content_type = 'image/png'
        # send as file-like
        bio = io.BytesIO(img_bytes)
        bio.seek(0)
        return send_file(bio, mimetype=content_type)
    except Exception:
        logging.exception("Error serving image for employee %s", empid)
        return jsonify({"error": "failed to serve image"}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)














# backend/trend_runner.py
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re
import os
import calendar
import json
from collections import defaultdict
from datetime import datetime as _datetime
from datetime import timedelta as _timedelta
from typing import Optional, List
from duration_report import run_for_date, REGION_CONFIG

# IMPORTANT: duration_report must exist and expose run_for_date(date, regions, outdir, city)
# and compute_daily_durations(swipes_df)
from duration_report import run_for_date, compute_daily_durations

# try to import config door->zone mapping, but keep fallback
try:
    from config.door_zone import map_door_to_zone as config_map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    config_map_door_to_zone = None
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

# HIST_PATH: try a few likely locations (project config, repository root, absolute path)
CANDIDATE_HISTORY = [
    Path(__file__).parent / "config" / "current_analysis.csv",
    Path(__file__).parent.parent / "config" / "current_analysis.csv",
    Path.cwd() / "current_analysis.csv",
    Path(__file__).parent / "current_analysis.csv"
]
HIST_PATH = None
for p in CANDIDATE_HISTORY:
    if p.exists():
        HIST_PATH = p
        break

if HIST_PATH is None:
    logging.warning("Historical profile file current_analysis.csv not found in candidate locations.")
    HIST_DF = pd.DataFrame()
else:
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)

# ----- small shared helpers: treat empty/placeholder tokens as None -----
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _normalize_id_val(v):
    """
    Normalize an id-like value: strip, convert floats like '320172.0' -> '320172'
    Return None for NaN/empty/placeholder.
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    # strip .0 if integer-like
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s

# GUID / name helpers
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')

def _looks_like_guid(s: object) -> bool:
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        if _looks_like_guid(st):
            return False
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    """
    Create canonical person uid:
      - prefer EmployeeID (normalized) -> 'emp:<id>' only if it is not a GUID
      - else EmployeeIdentity -> 'uid:<val>'
      - else EmployeeName -> hash-based 'name:<shorthash>'
    """
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None

# small helper to extract Card from XML-like strings
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None

# use config_map_door_to_zone if available, else fallback
try:
    _BREAK_ZONES = BREAK_ZONES
    _OUT_OF_OFFICE_ZONE = OUT_OF_OFFICE_ZONE
except Exception:
    _BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    _OUT_OF_OFFICE_ZONE = "Out of office"

def map_door_to_zone(door: object, direction: object = None) -> str:
    try:
        if config_map_door_to_zone is not None:
            return config_map_door_to_zone(door, direction)
    except Exception:
        pass
    try:
        if door is None:
            return None
        s = str(door).strip()
        if not s:
            return None
        s_l = s.lower()
        if direction and isinstance(direction, str):
            d = direction.strip().lower()
            if "out" in d:
                return _OUT_OF_OFFICE_ZONE
            if "in" in d:
                return "Reception Area"
        if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
            return _OUT_OF_OFFICE_ZONE
        return "Working Area"
    except Exception:
        return None

# ----- Config and scenarios -----
VIOLATION_WINDOW_DAYS = 90
RISK_THRESHOLDS = [
    (0.5, "Low"),
    (1.5, "Low Medium"),
    (2.5, "Medium"),
    (4.0, "Medium High"),
    (float("inf"), "High"),
]

def map_score_to_label(score: float) -> (int, str):
    try:
        if score is None:
            score = 0.0
        s = float(score)
    except Exception:
        s = 0.0
    bucket = 1
    label = "Low"
    for i, (threshold, lbl) in enumerate(RISK_THRESHOLDS, start=1):
        if s <= threshold:
            bucket = i
            label = lbl
            break
    return bucket, label

# scenario functions (kept from your improved version)
def scenario_long_gap(row):
    try:
        gap = int(row.get('MaxSwipeGapSeconds') or 0)
        return gap >= int(4.5 * 3600)
    except Exception:
        return False

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = int(row.get('CountSwipes') or 0)
    dur = float(row.get('DurationMinutes') or 0.0)
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = rec.get('TotalSwipes_median', np.nan)
            if pd.notna(median) and median > 0:
                return (cur > 3 * float(median)) and (dur < 60)
    except Exception:
        pass
    try:
        if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
            global_med = HIST_DF['TotalSwipes_median'].median()
            if pd.notna(global_med) and global_med > 0:
                return (cur > 3 * float(global_med)) and (dur < 60)
    except Exception:
        pass
    return (cur > 50) and (dur < 60)

def scenario_high_swipes_benign(row):
    cur = int(row.get('CountSwipes') or 0)
    dur = float(row.get('DurationMinutes') or 0.0)
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = rec.get('TotalSwipes_median', np.nan)
            if pd.notna(median) and median > 0:
                return (cur > 3 * float(median)) and (dur >= 60)
    except Exception:
        pass
    try:
        if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
            global_med = HIST_DF['TotalSwipes_median'].median()
            if pd.notna(global_med) and global_med > 0:
                return (cur > 3 * float(global_med)) and (dur >= 60)
    except Exception:
        pass
    return (cur > 50) and (dur >= 60)

def scenario_behaviour_shift(row, hist_df=None, minutes_threshold=180):
    try:
        if pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None:
            return False
        first_ts = pd.to_datetime(row.get('FirstSwipe'))
        today_minutes = first_ts.hour * 60 + first_ts.minute
        empid = row.get('EmployeeID')
        hist = hist_df if hist_df is not None else (HIST_DF if (HIST_DF is not None and not HIST_DF.empty) else None)
        if hist is None or hist.empty or empid is None:
            return False
        try:
            rec = hist[hist['EmployeeID'] == empid]
            if rec.empty:
                return False
            if 'FirstSwipeMinutes_median' in rec.columns:
                median_min = rec.iloc[0].get('FirstSwipeMinutes_median')
            else:
                median_min = rec.iloc[0].get('AvgFirstSwipeMins_median', None)
            if pd.isna(median_min) or median_min is None:
                return False
            diff = abs(today_minutes - float(median_min))
            return diff >= int(minutes_threshold)
        except Exception:
            return False
    except Exception:
        return False

def scenario_repeated_short_breaks(row):
    try:
        break_count = int(row.get('BreakCount') or 0)
        total_break_mins = float(row.get('TotalBreakMinutes') or 0.0)
        long_break_count = int(row.get('LongBreakCount') or 0)
        short_gap_count = int(row.get('ShortGapCount') or 0)
        if break_count >= 2:
            return True
        if short_gap_count >= 5:
            return True
        if total_break_mins >= 180 and short_gap_count >= 2:
            return True
        return False
    except Exception:
        return False

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map

def scenario_shortstay_longout_repeat(row):
    return bool(row.get('PatternShortLongRepeat', False))

SCENARIOS = [
    ("long_gap_>=4.5h", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap),
    ("high_swipes_benign", scenario_high_swipes_benign),
    ("behaviour_shift", scenario_behaviour_shift),
    ("shortstay_longout_repeat", scenario_shortstay_longout_repeat)
]



# --- improved human-readable scenario explanations (use hours for duration/gaps) ---
def _hrs_from_minutes(mins):
    try:
        m = float(mins or 0.0)
        return round(m / 60.0, 1)
    except Exception:
        return None

def _hrs_from_seconds(sec):
    try:
        s = float(sec or 0.0)
        return round(s / 3600.0, 1)
    except Exception:
        return None

SCENARIO_EXPLANATIONS = {
    "long_gap_>=4.5h": lambda r: (
        (lambda h: f"Long gap between swipes (~{h} h)." if h is not None else "Long gap between swipes.")
        (_hrs_from_seconds(r.get('MaxSwipeGapSeconds')))
    ),
    "short_duration_<4h": lambda r: (
        # if duration is zero but we only saw only_in/only_out, be explicit
        "Only 'IN' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyIn', 0)) == 1 else
        "Only 'OUT' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyOut', 0)) == 1 else
        (lambda h: f"Short total presence (~{h} h)." if h is not None else "Short total presence.")(_hrs_from_minutes(r.get('DurationMinutes')))
    ),
    "coffee_badging": lambda r: "Multiple quick swipes in short time.",
    "low_swipe_count_<=2": lambda r: "Very few swipes on day.",
    "single_door": lambda r: "Only a single door used during the day.",
    "only_in": lambda r: "Only 'IN' events recorded.",
    "only_out": lambda r: "Only 'OUT' events recorded.",
    "overtime_>=10h": lambda r: "Overtime detected (>=10 hours).",
    "very_long_duration_>=16h": lambda r: "Very long presence (>=16 hours).",
    "zero_swipes": lambda r: "No swipes recorded on this day.",
    "unusually_high_swipes": lambda r: "Unusually high number of swipes compared to peers/history.",
    "repeated_short_breaks": lambda r: "Many short gaps between swipes.",
    "multiple_location_same_day": lambda r: "Multiple locations/partitions used in same day.",
    "weekend_activity": lambda r: "Activity recorded on weekend day.",
    "repeated_rejection_count": lambda r: "Multiple rejection events recorded.",
    "badge_sharing_suspected": lambda r: "Same card used by multiple users on same day — possible badge sharing.",
    "early_arrival_before_06": lambda r: "First swipe earlier than 06:00.",
    "late_exit_after_22": lambda r: "Last swipe after 22:00.",
    "shift_inconsistency": lambda r: "Duration deviates from historical shift patterns.",
    "trending_decline": lambda r: "Employee shows trending decline in presence.",
    "consecutive_absent_days": lambda r: "Consecutive absent days observed historically.",
    "high_variance_duration": lambda r: "High variance in daily durations historically.",
    "short_duration_on_high_presence_days": lambda r: "Short duration despite normally high presence days.",
    "swipe_overlap": lambda r: "Overlap in swipe times with other persons on same door.",
    "behaviour_shift": lambda r: "Significant change in arrival time compared to historical baseline.",
    "shortstay_longout_repeat": lambda r: "Repeated pattern: short in → long out → short return."
}



def _explain_scenarios_detected(row, detected_list):
    pieces = []
    name = row.get('EmployeeName') or row.get('EmployeeID') or row.get('person_uid') or "Employee"
    prefix = f"{name} - "
    for sc in detected_list:
        sc = sc.strip()
        fn = SCENARIO_EXPLANATIONS.get(sc)
        try:
            if fn:
                pieces.append(fn(row))
            else:
                pieces.append(sc.replace("_", " ").replace(">=", "≥"))
        except Exception:
            pieces.append(sc)
    if not pieces:
        return None
    explanation = " ".join([p if p.endswith('.') else p + '.' for p in pieces])
    return prefix + " " + explanation

# ---------------- compute_features (robust merged version) ----------------
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)
  
  
  
    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
    else:
        if 'Date' in sw.columns:
            sw['LocaleMessageTime'] = None
            try:
                sw['LocaleMessageTime'] = pd.to_datetime(sw['Date'], errors='coerce')
            except Exception:
                sw['LocaleMessageTime'] = None

    # By default Date comes from LocaleMessageTime (local, human timestamps).
    # However if an AdjustedMessageTime column exists (the Pune 2AM boundary) prefer that
    # for date assignment so trend grouping matches compute_daily_durations().
    if 'AdjustedMessageTime' in sw.columns and sw['AdjustedMessageTime'].notna().any():
        try:
            sw['AdjustedMessageTime'] = pd.to_datetime(sw['AdjustedMessageTime'], errors='coerce')
            # Prefer adjusted date for rows where it exists (this mirrors duration_report logic).
            mask_adj = sw['AdjustedMessageTime'].notna()
            # Ensure LocaleMessageTime parsed for those not adjusted
            sw.loc[~mask_adj, 'Date'] = pd.to_datetime(sw.loc[~mask_adj, 'LocaleMessageTime'], errors='coerce').dt.date
            sw.loc[mask_adj, 'Date']  = sw.loc[mask_adj,  'AdjustedMessageTime'].dt.date
        except Exception:
            # fallback to LocaleMessageTime date
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
    else:
        # normal path
        sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date




    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    try:
        if dir_col and dir_col in sw.columns:
            sw['Direction'] = sw[dir_col]
        if door_col and door_col in sw.columns:
            sw['Door'] = sw[door_col]
        if empid_col and empid_col in sw.columns:
            sw['EmployeeID'] = sw[empid_col]
        if name_col and name_col in sw.columns:
            sw['EmployeeName'] = sw[name_col]
        if card_col and card_col in sw.columns:
            sw['CardNumber'] = sw[card_col]
        if 'LocaleMessageTime' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
        elif 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
    except Exception:
        logging.exception("Normalization of swipe columns failed.")

    # PersonnelType filtering (tolerant) - avoid dropping if column absent
    if 'PersonnelTypeName' in sw.columns:
        sw['PersonnelTypeName'] = sw['PersonnelTypeName'].astype(str).str.strip()
        mask = sw['PersonnelTypeName'].str.lower().str.contains(r'employee|terminated', na=False)
        logging.info("PersonnelTypeName values example: %s", list(sw['PersonnelTypeName'].dropna().unique()[:6]))
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelTypeName filter applied: before=%d after=%d", before, len(sw))
    elif 'PersonnelType' in sw.columns:
        sw['PersonnelType'] = sw['PersonnelType'].astype(str).str.strip()
        mask = sw['PersonnelType'].str.lower().str.contains(r'employee|terminated', na=False)
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelType filter applied: before=%d after=%d", before, len(sw))

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # person_uid canonical
    if 'person_uid' not in sw.columns:
        def make_person_uid_local(r):
            empid_val = None
            if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
                empid_val = r.get(empid_col)
            elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
                empid_val = r.get('EmployeeID')
            empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
            name_val = None
            if name_col and name_col in r:
                name_val = r.get(name_col)
            elif 'EmployeeName' in r:
                name_val = r.get('EmployeeName')
            elif 'ObjectName1' in r:
                name_val = r.get('ObjectName1')
            return _canonical_person_uid({
                'EmployeeID': empid_val,
                'EmployeeIdentity': empident_val,
                'EmployeeName': name_val
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
                    'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
                    'EmployeeIdentity'])
    if name_col: sel_cols.add(name_col)
    if empid_col: sel_cols.add(empid_col)
    if card_col: sel_cols.add(card_col)
    if door_col: sel_cols.add(door_col)
    if dir_col: sel_cols.add(dir_col)
    sel_cols = [c for c in sel_cols if c in sw.columns]

    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0

        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

        # card extraction
        card_numbers = []
        if card_col and card_col in g.columns:
            card_numbers = list(pd.unique(g[card_col].dropna()))
        if not card_numbers and 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna()))
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
                    try:
                        vals = list(pd.unique(g[c].dropna()))
                        if vals:
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'xml' in cl:
                    for raw in g[c].dropna().astype(str):
                        extracted = _extract_card_from_xml(raw)
                        if extracted:
                            card_numbers.append(extracted)
        card_numbers = list(dict.fromkeys(card_numbers))
        card_number = None
        for c in card_numbers:
            n = _normalize_id_val(c)
            if n and not _looks_like_guid(n):
                card_number = n
                break

        # stable id/name
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None

        if empid_col and empid_col in g.columns:
            vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        elif 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized

        if (not employee_id) and 'PersonnelType' in g.columns:
            try:
                pvals = g['PersonnelType'].dropna().astype(str)
                if not pvals.empty:
                    p0 = pvals.iloc[0]
                    if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
                        for c in g.columns:
                            if c.lower() == 'text12':
                                vals = g[c].dropna().astype(str).map(lambda x: x.strip())
                                employee_id = _pick_first_non_guid_value(vals)
                                if employee_id:
                                    break
            except Exception:
                pass

        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]

        candidate_name_vals = None
        if name_col and name_col in g.columns:
            candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
        elif 'EmployeeName' in g.columns:
            candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
        elif 'ObjectName1' in g.columns:
            candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

        if candidate_name_vals is not None and not candidate_name_vals.empty:
            employee_name = _pick_first_non_guid_value(candidate_name_vals)
            if employee_name is None:
                for v in candidate_name_vals:
                    if _looks_like_name(v) and not _is_placeholder_str(v):
                        employee_name = str(v).strip()
                        break

        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        first_swipe = None
        last_swipe = None
        if times:
            first_swipe = times[0]
            last_swipe = times[-1]

        # timeline & segments with mapping to zones
        timeline = []
        for _, row in g.sort_values('LocaleMessageTime').iterrows():
            t = row.get('LocaleMessageTime')
            dname = None
            if door_col and door_col in row and pd.notna(row.get(door_col)):
                dname = row.get(door_col)
            elif 'Door' in row and pd.notna(row.get('Door')):
                dname = row.get('Door')
            direction = None
            if dir_col and dir_col in row and pd.notna(row.get(dir_col)):
                direction = row.get(dir_col)
            elif 'Direction' in row and pd.notna(row.get('Direction')):
                direction = row.get('Direction')
            zone = map_door_to_zone(dname, direction)
            timeline.append((t, dname, direction, zone))

        segments = []
        if timeline:
            cur_zone = None
            seg_start = timeline[0][0]
            seg_label = None
            for (t, dname, direction, zone) in timeline:
                if zone in _BREAK_ZONES:
                    lbl = 'break'
                elif zone == _OUT_OF_OFFICE_ZONE:
                    lbl = 'out_of_office'
                else:
                    lbl = 'work'
                if cur_zone is None:
                    cur_zone = zone
                    seg_label = lbl
                    seg_start = t
                else:
                    if lbl != seg_label:
                        segments.append({
                            'label': seg_label,
                            'start': seg_start,
                            'end': t,
                            'start_zone': cur_zone
                        })
                        seg_start = t
                        seg_label = lbl
                        cur_zone = zone
                    else:
                        cur_zone = cur_zone or zone
            if seg_label is not None:
                segments.append({
                    'label': seg_label,
                    'start': seg_start,
                    'end': timeline[-1][0],
                    'start_zone': cur_zone
                })

        break_count = 0
        long_break_count = 0
        total_break_minutes = 0.0

        BREAK_MINUTES_THRESHOLD = 60
        OUT_OFFICE_COUNT_MINUTES = 180
        LONG_BREAK_FLAG_MINUTES = 120

        for i, s in enumerate(segments):
            lbl = s.get('label')
            start = s.get('start')
            end = s.get('end')
            dur_mins = ((end - start).total_seconds() / 60.0) if (start and end) else 0.0
            if lbl == 'break':
                if dur_mins >= BREAK_MINUTES_THRESHOLD:
                    break_count += 1
                    total_break_minutes += dur_mins
                    if dur_mins >= LONG_BREAK_FLAG_MINUTES:
                        long_break_count += 1
            elif lbl == 'out_of_office':
                prev_lbl = segments[i-1]['label'] if i > 0 else None
                next_lbl = segments[i+1]['label'] if i < len(segments)-1 else None
                if prev_lbl == 'work' and next_lbl == 'work' and dur_mins >= OUT_OFFICE_COUNT_MINUTES:
                    break_count += 1
                    total_break_minutes += dur_mins
                    if dur_mins >= LONG_BREAK_FLAG_MINUTES:
                        long_break_count += 1

        pattern_flag = False
        pattern_sequence_readable = None
        try:
            seq = []
            for s in segments:
                dur_mins = (s['end'] - s['start']).total_seconds() / 60.0 if (s['end'] and s['start']) else 0
                seq.append((s['label'], int(round(dur_mins))))
            for i in range(len(seq)-2):
                a = seq[i]
                b = seq[i+1]
                c = seq[i+2]
                if (a[0] == 'work' and a[1] < 60) and \
                   (b[0] in ('out_of_office','break') and b[1] >= LONG_BREAK_FLAG_MINUTES) and \
                   (c[0] == 'work' and c[1] < 60):
                    pattern_flag = True
                    seq_fragment = [a, b, c]
                    pattern_sequence_readable = " -> ".join([f"{lbl} ({mins}m)" for lbl, mins in seq_fragment])
                    break
        except Exception:
            pattern_flag = False
            pattern_sequence_readable = None

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type,
            'FirstSwipe': first_swipe,
            'LastSwipe': last_swipe,
            'BreakCount': int(break_count),
            'LongBreakCount': int(long_break_count),
            'TotalBreakMinutes': float(round(total_break_minutes,1)),
            'PatternShortLongRepeat': bool(pattern_flag),
            'PatternSequenceReadable': pattern_sequence_readable,
            'PatternSequence': None
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = grouped.apply(agg_swipe_group).reset_index()

    # POST-PROCESS: merge early-morning fragments into previous day (heuristic)
    try:
        grouped['FirstSwipe_dt'] = pd.to_datetime(grouped['FirstSwipe'], errors='coerce')
        grouped['LastSwipe_dt']  = pd.to_datetime(grouped['LastSwipe'],  errors='coerce')
        rows_to_drop = set()
        MERGE_GAP_SECONDS = int(4 * 3600)
        for pid, sub in grouped.sort_values(['person_uid','Date']).groupby('person_uid'):
            prev_idx = None
            for idx, r in sub.reset_index().iterrows():
                real_idx = int(r['index']) if 'index' in r else r.name
                cur_first = pd.to_datetime(grouped.at[real_idx, 'FirstSwipe_dt'])
                if prev_idx is not None:
                    prev_last = pd.to_datetime(grouped.at[prev_idx, 'LastSwipe_dt'])
                    if (not pd.isna(cur_first)) and (not pd.isna(prev_last)):
                        gap = (cur_first - prev_last).total_seconds()
                        if 0 <= gap <= MERGE_GAP_SECONDS and cur_first.time().hour < 2:
                            try:
                                grouped.at[prev_idx, 'CountSwipes'] = int(grouped.at[prev_idx, 'CountSwipes']) + int(grouped.at[real_idx, 'CountSwipes'])
                                grouped.at[prev_idx, 'MaxSwipeGapSeconds'] = max(int(grouped.at[prev_idx, 'MaxSwipeGapSeconds'] or 0), int(grouped.at[real_idx, 'MaxSwipeGapSeconds'] or 0), int(gap))
                                if not pd.isna(grouped.at[real_idx, 'LastSwipe_dt']):
                                    if pd.isna(grouped.at[prev_idx, 'LastSwipe_dt']) or grouped.at[real_idx, 'LastSwipe_dt'] > grouped.at[prev_idx, 'LastSwipe_dt']:
                                        grouped.at[prev_idx, 'LastSwipe_dt'] = grouped.at[real_idx, 'LastSwipe_dt']
                                        grouped.at[prev_idx, 'LastSwipe'] = grouped.at[real_idx, 'LastSwipe']
                                if not grouped.at[prev_idx, 'CardNumber']:
                                    grouped.at[prev_idx, 'CardNumber'] = grouped.at[real_idx, 'CardNumber']
                                grouped.at[prev_idx, 'UniqueDoors'] = int(max(int(grouped.at[prev_idx].get('UniqueDoors') or 0), int(grouped.at[real_idx].get('UniqueDoors') or 0)))
                                grouped.at[prev_idx, 'UniqueLocations'] = int(max(int(grouped.at[prev_idx].get('UniqueLocations') or 0), int(grouped.at[real_idx].get('UniqueLocations') or 0)))
                                rows_to_drop.add(real_idx)
                                continue
                            except Exception:
                                pass
                prev_idx = real_idx
        if rows_to_drop:
            grouped = grouped.drop(index=list(rows_to_drop)).reset_index(drop=True)
    except Exception:
        logging.exception("Failed merge-early-morning fragments (non-fatal).")

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # coalesce duplicated columns (_x/_y) produced by merge
    def _coalesce_merge_columns(df, bases):
        for base in bases:
            x = base + "_x"
            y = base + "_y"
            try:
                has_base = base in df.columns
                base_all_null = False
                if has_base:
                    base_all_null = df[base].isnull().all()
            except Exception:
                has_base = base in df.columns
                base_all_null = True
            if (not has_base) or base_all_null:
                if x in df.columns and y in df.columns:
                    try:
                        df[base] = df[x].combine_first(df[y])
                    except Exception:
                        try:
                            df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
                        except Exception:
                            if x in df.columns:
                                df[base] = df[x]
                            elif y in df.columns:
                                df[base] = df[y]
                elif x in df.columns:
                    df[base] = df[x]
                elif y in df.columns:
                    df[base] = df[y]
    _coalesce_merge_columns(merged, [
        "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
    ])
    drop_cols = [c for c in merged.columns if c.endswith("_x") or c.endswith("_y")]
    if drop_cols:
        try:
            merged.drop(columns=drop_cols, inplace=True)
        except Exception:
            for c in drop_cols:
                if c in merged.columns:
                    try:
                        merged.drop(columns=[c], inplace=True)
                    except Exception:
                        pass

    # ensure columns exist and normalized
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)
    ensure_col(merged, 'BreakCount', 0)
    ensure_col(merged, 'LongBreakCount', 0)
    ensure_col(merged, 'TotalBreakMinutes', 0.0)
    ensure_col(merged, 'PatternShortLongRepeat', False)
    ensure_col(merged, 'PatternSequenceReadable', None)
    ensure_col(merged, 'PatternSequence', None)

    if 'EmployeeName' in merged.columns:
        def choose_best_name(row):
            gname = row.get('EmployeeName')
            dname = None
            for cand in ('EmployeeName', 'employee_name', 'objectname1', 'ObjectName1'):
                if cand in row and row.get(cand) is not None:
                    dname = row.get(cand)
                    break
            if _looks_like_name(gname):
                return str(gname).strip()
            if _looks_like_name(dname):
                return str(dname).strip()
            if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
                return str(gname).strip()
            if dname and not _is_placeholder_str(dname):
                return str(dname).strip()
            return None
        merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
    else:
        if not dur.empty:
            def fill_name_from_dur(row):
                gname = row.get('EmployeeName')
                if _looks_like_name(gname) and not _is_placeholder_str(gname):
                    return gname
                for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
                    if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
                        return row[cand]
                return None
            merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

    def normalize_empid(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            try:
                if '.' in s:
                    f = float(s)
                    if math.isfinite(f) and f.is_integer():
                        return str(int(f))
            except Exception:
                pass
            return s
        except Exception:
            return None

    merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

    def normalize_card(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            return s
        except Exception:
            return None

    merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

    if 'DurationSeconds' not in merged.columns or merged['DurationSeconds'].isnull().all():
        try:
            merged['DurationSeconds'] = (pd.to_datetime(merged['LastSwipe']) - pd.to_datetime(merged['FirstSwipe'])).dt.total_seconds().clip(lower=0).fillna(0)
        except Exception:
            merged['DurationSeconds'] = merged.get('DurationSeconds', 0)

    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)
    merged['BreakCount'] = merged['BreakCount'].fillna(0).astype(int)
    merged['LongBreakCount'] = merged['LongBreakCount'].fillna(0).astype(int)
    merged['TotalBreakMinutes'] = merged['TotalBreakMinutes'].fillna(0.0).astype(float)
    merged['PatternShortLongRepeat'] = merged['PatternShortLongRepeat'].fillna(False).astype(bool)

    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
        if c in merged.columns:
            def _clean_str_val(v):
                if v is None:
                    return None
                try:
                    s = str(v).strip()
                    if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                        return None
                    return s
                except Exception:
                    return None
            merged[c] = merged[c].apply(_clean_str_val)

    if 'EmployeeName' in merged.columns:
        merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

    return merged

# ---------------- SCENARIO WEIGHTS ----------------
WEIGHTS = {
    "long_gap_>=4.5h": 0.3,
    "short_duration_<4h": 1.0,
    "coffee_badging": 1.0,
    "low_swipe_count_<=2": 0.5,
    "single_door": 0.25,
    "only_in": 0.8,
    "only_out": 0.8,
    "overtime_>=10h": 0.2,
    "very_long_duration_>=16h": 1.5,
    "zero_swipes": 0.4,
    "unusually_high_swipes": 1.5,
    "repeated_short_breaks": 0.5,
    "multiple_location_same_day": 0.6,
    "weekend_activity": 0.6,
    "repeated_rejection_count": 0.8,
    "badge_sharing_suspected": 2.0,
    "early_arrival_before_06": 0.4,
    "late_exit_after_22": 0.4,
    "shift_inconsistency": 1.2,
    "trending_decline": 0.7,
    "consecutive_absent_days": 1.2,
    "high_variance_duration": 0.8,
    "short_duration_on_high_presence_days": 1.1,
    "swipe_overlap": 2.0,
    "high_swipes_benign": 0.1,
    "shortstay_longout_repeat": 2.0
}
ANOMALY_THRESHOLD = 1.5

def _read_past_trend_csvs(outdir: str, window_days: int, target_date: date):
    p = Path(outdir)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return pd.DataFrame()
    dfs = []
    cutoff = target_date - timedelta(days=window_days)
    for fp in csvs:
        try:
            df = pd.read_csv(fp, parse_dates=['Date'])
            if 'Date' in df.columns:
                try:
                    df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                except Exception:
                    pass
                # include target_date in the window (cutoff <= Date <= target_date)
                def _date_in_window(d):
                    try:
                        return d is not None and (d >= cutoff and d <= target_date)
                    except Exception:
                        return False
                df = df[df['Date'].apply(_date_in_window)]
            dfs.append(df)
        except Exception:
            try:
                df = pd.read_csv(fp, dtype=str)
                if 'Date' in df.columns:
                    try:
                        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                        # include target_date in the window (cutoff <= Date <= target_date)
                        def _date_in_window(d):
                            try:
                                return d is not None and (d >= cutoff and d <= target_date)
                            except Exception:
                                return False
                        df = df[df['Date'].apply(_date_in_window)]
                    except Exception:
                        pass
                dfs.append(df)
            except Exception:
                continue
    if not dfs:
        return pd.DataFrame()
    try:
        out = pd.concat(dfs, ignore_index=True)
        return out
    except Exception:
        return pd.DataFrame()

def _read_scenario_counts_by_person(outdir: str, window_days: int, target_date: date, scenario_col: str):
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty or scenario_col not in df.columns:
        return {}
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass
    id_cols = [c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in df.columns]
    out = defaultdict(int)
    q = df[df[scenario_col] == True] if df[scenario_col].dtype == bool else df[df[scenario_col].astype(str).str.lower() == 'true']
    for _, r in q.iterrows():
        for col in id_cols:
            try:
                raw = r.get(col)
                if raw in (None, '', float('nan')):
                    continue
                norm = _normalize_id_val(raw)
                if norm:
                    out[str(norm)] += 1
                    stripped = _strip_uid_prefix(str(norm))
                    if stripped != str(norm):
                        out[str(stripped)] += 1
            except Exception:
                continue
        for fallback in ('Int1', 'Text12'):
            if fallback in r and r.get(fallback) not in (None, '', 'nan'):
                try:
                    norm = _normalize_id_val(r.get(fallback))
                    if norm:
                        out[str(norm)] += 1
                except Exception:
                    continue
    return dict(out)

def _compute_weeks_with_threshold(past_df: pd.DataFrame,
                                  person_col: str = 'person_uid',
                                  date_col: str = 'Date',
                                  scenario_col: str = 'short_duration_<4h',
                                  threshold_days: int = 3) -> dict:
    if past_df is None or past_df.empty:
        return {}
    df = past_df.copy()
    if date_col not in df.columns:
        return {}
    try:
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.date
    except Exception:
        pass
    if scenario_col not in df.columns:
        return {}
    try:
        if df[scenario_col].dtype == bool:
            df['__scenario_flag__'] = df[scenario_col].astype(bool)
        else:
            df['__scenario_flag__'] = df[scenario_col].astype(str).str.strip().str.lower().isin({'true', '1', 'yes', 'y', 't'})
    except Exception:
        df['__scenario_flag__'] = df[scenario_col].apply(lambda v: str(v).strip().lower() in ('true','1','yes','y','t') if v is not None else False)
    df = df[df['__scenario_flag__'] == True].copy()
    if df.empty:
        return {}
    if person_col not in df.columns:
        fallback = next((c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in past_df.columns), None)
        if fallback is None:
            return {}
        person_col = fallback
    def _week_monday(d):
        try:
            if d is None or (isinstance(d, float) and np.isnan(d)):
                return None
            iso = d.isocalendar()
            return date.fromisocalendar(iso[0], iso[1], 1)
        except Exception:
            return None
    df['__week_monday__'] = df[date_col].apply(_week_monday)
    df = df.dropna(subset=['__week_monday__', person_col])
    if df.empty:
        return {}
    week_counts = (df.groupby([person_col, '__week_monday__'])
                     .size()
                     .reset_index(name='days_flagged'))
    valid_weeks = week_counts[week_counts['days_flagged'] >= int(threshold_days)].copy()
    if valid_weeks.empty:
        return {}
    person_weeks = {}
    for person, grp in valid_weeks.groupby(person_col):
        wlist = sorted(pd.to_datetime(grp['__week_monday__']).dt.date.unique(), reverse=True)
        person_weeks[str(person)] = wlist
    def _consecutive_week_count(week_dates_desc):
        if not week_dates_desc:
            return 0
        count = 1
        prev = week_dates_desc[0]
        for cur in week_dates_desc[1:]:
            try:
                if (prev - cur).days == 7:
                    count += 1
                    prev = cur
                else:
                    break
            except Exception:
                break
        return count
    out = {}
    for pid, weeks in person_weeks.items():
        c = _consecutive_week_count(weeks)
        out[str(pid)] = int(c)
        try:
            stripped = _strip_uid_prefix(str(pid))
            if stripped and stripped != str(pid):
                out[str(stripped)] = int(c)
        except Exception:
            pass
    return out

def _strip_uid_prefix(s):
    try:
        if s is None:
            return s
        st = str(s)
        for p in ('emp:', 'uid:', 'name:'):
            if st.startswith(p):
                return st[len(p):]
        return st
    except Exception:
        return s

def compute_violation_days_map(outdir: str, window_days: int, target_date: date):
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty:
        return {}
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass
    id_cols = []
    for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber'):
        if c in df.columns:
            id_cols.append(c)
    if 'IsFlagged' not in df.columns:
        if 'AnomalyScore' in df.columns:
            df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: float(s) >= ANOMALY_THRESHOLD if not pd.isna(s) else False)
        else:
            df['IsFlagged'] = False
    ident_dates = defaultdict(set)
    try:
        flagged = df[df['IsFlagged'] == True]
        for _, r in flagged.iterrows():
            d = r.get('Date')
            if d is None:
                continue
            for col in id_cols:
                try:
                    raw = r.get(col)
                    if raw is None:
                        continue
                    norm = _normalize_id_val(raw)
                    if norm:
                        ident_dates[str(norm)].add(d)
                        stripped = _strip_uid_prefix(str(norm))
                        if stripped != str(norm):
                            ident_dates[str(stripped)].add(d)
                except Exception:
                    continue
            for fallback in ('Int1', 'Text12'):
                if fallback in r and r.get(fallback) not in (None, '', 'nan'):
                    try:
                        norm = _normalize_id_val(r.get(fallback))
                        if norm:
                            ident_dates[str(norm)].add(d)
                            stripped = _strip_uid_prefix(str(norm))
                            if stripped != str(norm):
                                ident_dates[str(stripped)].add(d)
                    except Exception:
                        continue
    except Exception:
        logging.exception("Error building violation days map from history.")
    out = {k: int(len(v)) for k, v in ident_dates.items()}
    return out

# ---------------- run_trend_for_date: main entry point ----------------
# def run_trend_for_date(target_date: date, outdir: str = "./outputs", city='Pune', as_dict: bool = False):
#     logging.info("run_trend_for_date: date=%s (city=%s)", target_date, city)
#     results = run_for_date(target_date, regions=['apac'], outdir=outdir, city=city)
#     apac = results.get('apac', {})
#     swipes = apac.get('swipes', pd.DataFrame())
#     durations = apac.get('durations', pd.DataFrame())

#     # save raw swipes for evidence (full raw)
#     try:
#         if swipes is not None and not swipes.empty:
#             sw_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}.csv"
#             swipes.to_csv(sw_out, index=False)
#             logging.info("Saved raw swipes to %s", sw_out)
#     except Exception as e:
#         logging.warning("Failed to save raw swipes: %s", e)

#     # Pune 2AM boundary logic (default for 'pun' city)
#     use_pune_2am_boundary = False
#     try:
#         if city and isinstance(city, str) and 'pun' in city.strip().lower():
#             use_pune_2am_boundary = True
#         else:
#             if os.getenv("PUNE_2AM_BOUNDARY", "0") == "1":
#                 use_pune_2am_boundary = True
#     except Exception:
#         use_pune_2am_boundary = False

#     sw_for_features = swipes.copy() if swipes is not None else pd.DataFrame()
#     durations_for_features = durations.copy() if durations is not None else pd.DataFrame()






#     if use_pune_2am_boundary and (sw_for_features is not None) and (not sw_for_features.empty):
#         try:
#             # ensure LocaleMessageTime exists and is datetime
#             if 'LocaleMessageTime' in sw_for_features.columns:
#                 sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features['LocaleMessageTime'], errors='coerce')
#             else:
#                 for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
#                     if cand in sw_for_features.columns:
#                         sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features[cand], errors='coerce')
#                         break

#             # keep original local timestamps for display/evidence
#             sw_for_features['OriginalLocaleMessageTime'] = sw_for_features['LocaleMessageTime']

#             # IMPORTANT: create AdjustedMessageTime (local time shifted -2h) *without* overwriting LocaleMessageTime.
#             # compute_daily_durations will use AdjustedMessageTime (for APAC.Default) to assign Date like duration_report.py.
#             sw_for_features['AdjustedMessageTime'] = sw_for_features['LocaleMessageTime'] - pd.Timedelta(hours=2)

#             try:
#                 durations_for_features = compute_daily_durations(sw_for_features)
#             except Exception:
#                 logging.exception("Failed to recompute durations for Pune 2AM boundary; falling back to original durations.")
#                 durations_for_features = durations.copy() if durations is not None else pd.DataFrame()

#             try:
#                 sw_shifted_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}_adjusted.csv"
#                 cols_keep = [c for c in sw_for_features.columns if c in ('person_uid','EmployeeID','CardNumber','LocaleMessageTime','OriginalLocaleMessageTime','AdjustedMessageTime','Door','Direction','PartitionName2')]
#                 sw_for_features[cols_keep].to_csv(sw_shifted_out, index=False)
#             except Exception:
#                 logging.debug("Could not write adjusted swipes evidence file (non-fatal).")
#         except Exception:
#             logging.exception("Failed preparing adjusted swipes for Pune 2AM logic; using original swipes/durations.")
#             sw_for_features = swipes.copy() if swipes is not None else pd.DataFrame()
#             durations_for_features = durations.copy() if durations is not None else pd.DataFrame()




#             try:
#                 sw_shifted_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}_shifted.csv"
#                 cols_keep = [c for c in sw_for_features.columns if c in ('person_uid','EmployeeID','CardNumber','LocaleMessageTime','OriginalLocaleMessageTime','Door','Direction','PartitionName2')]
#                 sw_for_features[cols_keep].to_csv(sw_shifted_out, index=False)
#             except Exception:
#                 logging.debug("Could not write shifted swipes evidence file (non-fatal).")
#         except Exception:
#             logging.exception("Failed preparing shifted swipes for Pune 2AM logic; using original swipes/durations.")
#             sw_for_features = swipes.copy() if swipes is not None else pd.DataFrame()
#             durations_for_features = durations.copy() if durations is not None else pd.DataFrame()

#     # compute features (possibly using shifted swipes)
#     features = compute_features(sw_for_features, durations_for_features)
#     if features.empty:
#         logging.warning("run_trend_for_date: no features computed")
#         # Create an empty CSV with expected filename so caller sees file list
#         out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
#         try:
#             empty_df = pd.DataFrame(columns=[
#                 'person_uid','Date','EmployeeID','EmployeeName','CountSwipes','DurationMinutes','FirstSwipe','LastSwipe','IsFlagged'
#             ])
#             empty_df.to_csv(out_csv, index=False)
#             logging.info("Wrote empty trend CSV %s (rows=0)", out_csv)
#         except Exception:
#             logging.exception("Failed to write empty trend CSV.")
#         if as_dict:
#             return {
#                 'rows': 0,
#                 'flagged_rows': 0,
#                 'aggregated_unique_persons': 0,
#                 'sample': [],
#                 'reasons_count': {},
#                 'risk_counts': {},
#                 'files': [str(out_csv.name)]
#             }
#         return pd.DataFrame()




#     # ------------------ FIXED: restore 2am boundary then compute DisplayDate ------------------
#     try:
#         # We used AdjustedMessageTime for date assignment but we kept LocaleMessageTime / FirstSwipe in local time,
#         # so no +2 hour restore is required. Compute DisplayDate from FirstSwipe (local) or fallback to Date.
#         try:
#             if 'FirstSwipe' in features.columns:
#                 features['DisplayDate'] = pd.to_datetime(features['FirstSwipe'], errors='coerce').dt.date
#             else:
#                 features['DisplayDate'] = features.get('Date', None)
#         except Exception:
#             features['DisplayDate'] = features.get('Date', None)
#     except Exception:
#         logging.exception("Failed computing DisplayDate (non-fatal).")


#     # ------------------------------------------------------------------------------------------

#     # historical scenario counts
#     try:
#         hist_pattern_counts = _read_scenario_counts_by_person(outdir, VIOLATION_WINDOW_DAYS, target_date, 'shortstay_longout_repeat')
#         hist_rep_breaks = _read_scenario_counts_by_person(outdir, VIOLATION_WINDOW_DAYS, target_date, 'repeated_short_breaks')
#         hist_short_duration = _read_scenario_counts_by_person(outdir, VIOLATION_WINDOW_DAYS, target_date, 'short_duration_<4h')
#         def get_hist_count_for_row(row, hist_map):
#             for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
#                 if k in row and row.get(k) not in (None, '', float('nan')):
#                     try:
#                         norm = _normalize_id_val(row.get(k))
#                         if norm and str(norm) in hist_map:
#                             return int(hist_map.get(str(norm), 0))
#                         stripped = _strip_uid_prefix(str(norm)) if norm else None
#                         if stripped and str(stripped) in hist_map:
#                             return int(hist_map.get(str(stripped), 0))
#                     except Exception:
#                         continue
#             return 0
#         features['HistPatternShortLongCount90'] = features.apply(lambda r: get_hist_count_for_row(r, hist_pattern_counts), axis=1)
#         features['HistRepeatedShortBreakCount90'] = features.apply(lambda r: get_hist_count_for_row(r, hist_rep_breaks), axis=1)
#         features['HistShortDurationCount90'] = features.apply(lambda r: get_hist_count_for_row(r, hist_short_duration), axis=1)

#         pat_mask = features['HistPatternShortLongCount90'].fillna(0).astype(int) >= 3
#         if pat_mask.any():
#             features.loc[pat_mask, 'RiskScore'] = 5
#             features.loc[pat_mask, 'RiskLevel'] = 'High'
#             features.loc[pat_mask, 'IsFlagged'] = True

#         rep_mask = features['HistRepeatedShortBreakCount90'].fillna(0).astype(int) >= 5
#         if rep_mask.any():
#             features.loc[rep_mask, 'RiskScore'] = 5
#             features.loc[rep_mask, 'RiskLevel'] = 'High'
#             features.loc[rep_mask, 'IsFlagged'] = True
#     except Exception:
#         logging.exception("Failed to compute historical scenario counts.")

#     # ===== START FIX: reconcile zero CountSwipes with raw swipe files =====
#     try:
#         if swipes is not None and not swipes.empty and 'person_uid' in swipes.columns:
#             if use_pune_2am_boundary and sw_for_features is not None and not sw_for_features.empty:
#                 tsw = sw_for_features.copy()
#             else:
#                 tsw = swipes.copy()
#             if 'LocaleMessageTime' in tsw.columns:
#                 tsw['LocaleMessageTime'] = pd.to_datetime(tsw['LocaleMessageTime'], errors='coerce')
#             else:
#                 for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
#                     if cand in tsw.columns:
#                         tsw['LocaleMessageTime'] = pd.to_datetime(tsw[cand], errors='coerce')
#                         break

#             if 'Date' not in tsw.columns or tsw['Date'].isnull().all():
#                 # Prefer AdjustedMessageTime for date grouping if present (Pune boundary)
#                 if 'AdjustedMessageTime' in tsw.columns and tsw['AdjustedMessageTime'].notna().any():
#                     tsw['AdjustedMessageTime'] = pd.to_datetime(tsw['AdjustedMessageTime'], errors='coerce')
#                     tsw['Date'] = tsw['AdjustedMessageTime'].dt.date
#                 elif 'LocaleMessageTime' in tsw.columns:
#                     tsw['Date'] = tsw['LocaleMessageTime'].dt.date
#                 else:
#                     for cand in ('date','Date'):
#                         if cand in tsw.columns:
#                             try:
#                                 tsw['Date'] = pd.to_datetime(tsw[cand], errors='coerce').dt.date
#                             except Exception:
#                                 tsw['Date'] = None
#                             break





#             try:
#                 grp = tsw.dropna(subset=['person_uid', 'Date']).groupby(['person_uid', 'Date'])
#                 counts = grp.size().to_dict()
#                 firsts = grp['LocaleMessageTime'].min().to_dict()
#                 lasts = grp['LocaleMessageTime'].max().to_dict()
#             except Exception:
#                 counts = {}
#                 firsts = {}
#                 lasts = {}
#             def _fix_row_by_raw(idx, row):
#                 key = (row.get('person_uid'), row.get('Date'))
#                 if key in counts and (row.get('CountSwipes', 0) == 0 or pd.isna(row.get('CountSwipes'))):
#                     try:
#                         c = int(counts.get(key, 0))
#                         features.at[idx, 'CountSwipes'] = c
#                         f = firsts.get(key)
#                         l = lasts.get(key)
#                         if pd.notna(f) and (pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None):
#                             features.at[idx, 'FirstSwipe'] = pd.to_datetime(f)
#                         if pd.notna(l) and (pd.isna(row.get('LastSwipe')) or row.get('LastSwipe') is None):
#                             features.at[idx, 'LastSwipe'] = pd.to_datetime(l)
#                         try:
#                             fs = features.at[idx, 'FirstSwipe']
#                             ls = features.at[idx, 'LastSwipe']
#                             if pd.notna(fs) and pd.notna(ls):
#                                 dursec = (pd.to_datetime(ls) - pd.to_datetime(fs)).total_seconds()
#                                 dursec = max(0, dursec)
#                                 features.at[idx, 'DurationSeconds'] = float(dursec)
#                                 features.at[idx, 'DurationMinutes'] = float(dursec / 60.0)
#                         except Exception:
#                             pass
#                     except Exception:
#                         pass
#             for ix, r in features[features['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
#                 try:
#                     _fix_row_by_raw(ix, r)
#                 except Exception:
#                     logging.debug("Failed to reconcile row %s with raw swipes", ix)
#     except Exception:
#         logging.exception("Error while reconciling aggregated features with raw swipes (zero-swipe fix).")
#     # ===== END FIX =====

#     # Build badge map and swipe overlap maps for higher-severity scenarios
#     badge_map = {}
#     try:
#         if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
#             tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
#             if not tmp.empty:
#                 grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
#                 badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}
#     except Exception:
#         badge_map = {}

#     swipe_overlap_map = {}
#     try:
#         overlap_window_seconds = 2
#         if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
#             tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
#             if not tmp.empty:
#                 tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
#                 for (d, door), g in tmp.groupby(['Date', 'Door']):
#                     items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
#                     n = len(items)
#                     for i in range(n):
#                         t_i, uid_i = items[i]
#                         j = i+1
#                         while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
#                             uid_j = items[j][1]
#                             if uid_i != uid_j:
#                                 swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
#                                 swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
#                             j += 1
#     except Exception:
#         logging.exception("Failed to build swipe overlap map (non-fatal).")

#     # Evaluate scenarios
#     for name, fn in SCENARIOS:
#         if name == "badge_sharing_suspected":
#             features[name] = features.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
#         elif name == "swipe_overlap":
#             features[name] = features.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map), axis=1)
#         else:
#             features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

#     def compute_score(r):
#         score = 0.0
#         detected = []
#         for name, _ in SCENARIOS:
#             val = bool(r.get(name))
#             w = WEIGHTS.get(name, 0.0)
#             if val and w > 0:
#                 score += float(w)
#                 detected.append(name)
#         return score, detected

#     scores = features.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
#     features['AnomalyScore'] = scores['AnomalyScore'].astype(float)
#     features['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
#     features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

#     # monitoring / weekly adjustments
#     try:
#         features['PresentToday'] = features['CountSwipes'].fillna(0).astype(int) > 0
#         if 'ViolationDaysLast90' not in features.columns:
#             features['ViolationDaysLast90'] = 0

#         def _append_monitor_note(idx, row):
#             try:
#                 vd = int(row.get('ViolationDaysLast90') or 0)
#             except Exception:
#                 vd = 0
#             if vd <= 0:
#                 return row.get('Explanation')
#             if not row.get('PresentToday', False):
#                 return row.get('Explanation')
#             note = f"Note: Previously flagged {vd} time{'s' if vd!=1 else ''} in the last {VIOLATION_WINDOW_DAYS} days — monitor when present today."
#             ex = row.get('Explanation') or ''
#             if ex and not ex.strip().endswith('.'):
#                 ex = ex.strip() + '.'
#             if note in ex:
#                 return ex
#             return (ex + ' ' + note).strip()

#         features['Explanation'] = features.apply(lambda r: _append_monitor_note(r.name, r), axis=1)
#         features['MonitorFlag'] = features.apply(lambda r: (int(r.get('ViolationDaysLast90') or 0) > 0) and bool(r.get('PresentToday')), axis=1)

#         past_df = _read_past_trend_csvs(outdir, VIOLATION_WINDOW_DAYS, target_date)
#         week_runs = _compute_weeks_with_threshold(past_df, person_col='person_uid', date_col='Date', scenario_col='short_duration_<4h', threshold_days=3)
#         def _get_week_run_for_row(r):
#             for k in ('person_uid', 'EmployeeID'):
#                 if k in r and r.get(k):
#                     key = str(r.get(k))
#                     if key in week_runs:
#                         return int(week_runs[key])
#                     stripped = _strip_uid_prefix(key)
#                     if stripped in week_runs:
#                         return int(week_runs[stripped])
#             return 0
#         features['ConsecWeeksShort4hrs'] = features.apply(_get_week_run_for_row, axis=1)

#         if 'AnomalyScore' not in features.columns:
#             features['AnomalyScore'] = 0.0
#         mask1 = features['ConsecWeeksShort4hrs'].fillna(0).astype(int) >= 1
#         mask2 = features['ConsecWeeksShort4hrs'].fillna(0).astype(int) >= 2
#         if mask1.any():
#             features.loc[mask1, 'AnomalyScore'] = features.loc[mask1, 'AnomalyScore'].astype(float) + 0.5
#         if mask2.any():
#             features.loc[mask2, 'AnomalyScore'] = features.loc[mask2, 'AnomalyScore'].astype(float) + 1.0
#         features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))
#         def _map_risk_after_bump(r):
#             score = r.get('AnomalyScore') or 0.0
#             bucket, label = map_score_to_label(score)
#             return int(bucket), label
#         rs2 = features.apply(lambda r: pd.Series(_map_risk_after_bump(r), index=['RiskScore', 'RiskLevel']), axis=1)
#         features['RiskScore'] = rs2['RiskScore']
#         features['RiskLevel'] = rs2['RiskLevel']
#     except Exception:
#         logging.exception("Failed post-scoring weekly-run / monitoring augmentation.")

#     # reasons/explanation columns
#     def reasons_for_row(r):
#         if not bool(r.get('IsFlagged')):
#             return None, None
#         ds_raw = r.get('DetectedScenarios')
#         if ds_raw:
#             ds = [s.strip() for s in ds_raw.split(";") if s and s.strip()]
#             explanation = _explain_scenarios_detected(r, ds)
#             reasons_codes = "; ".join(ds) if ds else None
#             return reasons_codes, explanation
#         return None, None

#     reason_tuples = features.apply(lambda r: pd.Series(reasons_for_row(r), index=['Reasons', 'Explanation']), axis=1)

#     def _sanitize_reason_val(v):
#         if v is None:
#             return None
#         try:
#             s = str(v).strip()
#             if s == "" or _is_placeholder_str(s):
#                 return None
#             return s
#         except Exception:
#             return None

#     features['Reasons'] = reason_tuples['Reasons'].apply(_sanitize_reason_val)

#     # ----------------- FIX B: Merge reason-derived explanation with any existing monitor-note style explanation -----------------
#     # Merge reason-derived explanation with any existing monitor-note style explanation
#     def _merge_explanations(row, reason_expl):
#         prior = row.get('Explanation')  # this is the earlier monitor note or empty
#         try:
#             # sanitize reason_expl
#             if reason_expl is None or _is_placeholder_str(reason_expl):
#                 reason_expl = None
#         except Exception:
#             reason_expl = None

#         parts = []
#         if reason_expl:
#             parts.append(reason_expl.rstrip('. ').strip() + '.')  # ensure trailing period
#         if prior:
#             # prior already has a trailing sentence (see earlier _append_monitor_note),
#             # append if not duplicated
#             if prior.strip() and prior.strip() not in (reason_expl or ''):
#                 parts.append(prior.strip() if prior.strip().endswith('.') else prior.strip() + '.')
#         if not parts:
#             return None
#         return " ".join(parts)

#     features['Explanation'] = features.apply(
#     lambda r, rt=reason_tuples: _merge_explanations(
#         r,
#         rt.at[r.name, 'Explanation'] if ('Explanation' in rt.columns and r.name in rt.index) else None
#     ),
#     axis=1
# )

#     # ensure placeholder / empty normalized
#     features['Explanation'] = features['Explanation'].apply(lambda v: None if _is_placeholder_str(v) else (str(v).strip() if v is not None else None))
#     # -------------------------------------------------------------------------------------------------------------------------

#     def _ensure_reason_for_flagged(row):
#         if bool(row.get('IsFlagged')) and (row.get('Reasons') is None or row.get('Reasons') == ''):
#             ds = row.get('DetectedScenarios')
#             if ds and not _is_placeholder_str(ds):
#                 parts = [p.strip() for p in re.split(r'[;,\|]', str(ds)) if p and not _is_placeholder_str(p)]
#                 if parts:
#                     return "; ".join(parts)
#             if int(row.get('ConsecWeeksShort4hrs') or 0) >= 1:
#                 return "consecutive_short_weeks"
#             if int(row.get('ViolationDaysLast90') or 0) > 0:
#                 return "historical_monitoring"
#             return None
#         return row.get('Reasons')

#     if 'IsFlagged' in features.columns:
#         features['Reasons'] = features.apply(_ensure_reason_for_flagged, axis=1)
#     else:
#         features['Reasons'] = features['Reasons'].apply(_sanitize_reason_val)

#     if 'OverlapWith' not in features.columns:
#         def overlap_with_fn(r):
#             d = r.get('Date')
#             uid = r.get('person_uid')
#             if (d, uid) in swipe_overlap_map:
#                 return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
#             return None
#         features['OverlapWith'] = features.apply(overlap_with_fn, axis=1)

#     try:
#         violation_map = compute_violation_days_map(outdir, VIOLATION_WINDOW_DAYS, target_date)
#         def map_violation_days(r):
#             candidates = []
#             for k in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
#                 if k in r and r.get(k) not in (None, '', float('nan')):
#                     val = r.get(k)
#                     try:
#                         norm = _normalize_id_val(val)
#                         if norm:
#                             candidates.append(str(norm))
#                             stripped = _strip_uid_prefix(str(norm))
#                             if stripped != str(norm):
#                                 candidates.append(str(stripped))
#                     except Exception:
#                         continue
#             for cand in candidates:
#                 if cand in violation_map:
#                     return int(violation_map[cand])
#             return 0
#         features['ViolationDaysLast90'] = features.apply(map_violation_days, axis=1)
#     except Exception:
#         features['ViolationDaysLast90'] = 0

#     try:
#         def map_risk(r):
#             score = r.get('AnomalyScore') or 0.0
#             bucket, label = map_score_to_label(score)
#             return int(bucket), label
#         rs = features.apply(lambda r: pd.Series(map_risk(r), index=['RiskScore', 'RiskLevel']), axis=1)
#         features['RiskScore'] = rs['RiskScore']
#         features['RiskLevel'] = rs['RiskLevel']
#     except Exception:
#         features['RiskScore'] = 1
#         features['RiskLevel'] = 'Low'

#     try:
#         features['ViolationDaysLast90'] = features['ViolationDaysLast90'].fillna(0).astype(int)
#         high_violation_mask = features['ViolationDaysLast90'] >= 4
#         if high_violation_mask.any():
#             features.loc[high_violation_mask, 'RiskScore'] = 5
#             features.loc[high_violation_mask, 'RiskLevel'] = 'High'
#     except Exception:
#         pass

#     cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
#     if cols_to_drop:
#         for c in cols_to_drop:
#             base = c[:-2]
#             if base in features.columns:
#                 try:
#                     features.drop(columns=[c], inplace=True)
#                 except Exception:
#                     pass
#             else:
#                 try:
#                     features.rename(columns={c: base}, inplace=True)
#                 except Exception:
#                     pass
#     features = features.loc[:, ~features.columns.duplicated()]

#     for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
#         if col in features.columns:
#             features[col] = features[col].astype(bool)

#     # write CSV with native types
#     out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
#     try:
#         write_df = features.copy()
#         for dtcol in ('FirstSwipe', 'LastSwipe'):
#             if dtcol in write_df.columns:
#                 write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
#         if 'Date' in write_df.columns:
#             try:
#                 write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
#                 write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
#             except Exception:
#                 pass
#         write_df = write_df.where(pd.notnull(write_df), None)
#         write_df.to_csv(out_csv, index=False)
#         logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
#     except Exception as e:
#         logging.exception("Failed to write trend CSV: %s", e)

#     if 'DisplayDate' in write_df.columns:
#         try:
#             write_df['DisplayDate'] = pd.to_datetime(write_df['DisplayDate'], errors='coerce').dt.date
#             write_df['DisplayDate'] = write_df['DisplayDate'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
#         except Exception:
#             pass

#     if as_dict:
#         try:
#             rec_df = features.copy()
#             for dtcol in ('FirstSwipe', 'LastSwipe'):
#                 if dtcol in rec_df.columns:
#                     rec_df[dtcol] = pd.to_datetime(rec_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
#             if 'Date' in rec_df.columns:
#                 try:
#                     rec_df['Date'] = pd.to_datetime(rec_df['Date'], errors='coerce').dt.date
#                     rec_df['Date'] = rec_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
#                 except Exception:
#                     pass
#             rec_df = rec_df.where(pd.notnull(rec_df), None)
#             total_rows = int(len(rec_df))
#             flagged_rows = int(rec_df['IsFlagged'].sum()) if 'IsFlagged' in rec_df.columns else 0
#             reasons_count = {}
#             flagged_df = rec_df[rec_df['IsFlagged'] == True] if 'IsFlagged' in rec_df.columns else pd.DataFrame()
#             if 'Reasons' in flagged_df.columns and not flagged_df.empty:
#                 for v in flagged_df['Reasons'].dropna().astype(str):
#                     for part in re.split(r'[;,\|]', v):
#                         key = part.strip()
#                         if key:
#                             reasons_count[key] = reasons_count.get(key, 0) + 1
#             risk_counts = {}
#             if 'RiskLevel' in flagged_df.columns and not flagged_df.empty:
#                 for v in flagged_df['RiskLevel'].fillna('').astype(str):
#                     if v:
#                         risk_counts[v] = risk_counts.get(v, 0) + 1
#             sample_records = flagged_df.to_dict(orient='records') if not flagged_df.empty else []
#             return {
#                 'rows': total_rows,
#                 'flagged_rows': flagged_rows,
#                 'aggregated_unique_persons': total_rows,
#                 'sample': sample_records,
#                 'reasons_count': reasons_count,
#                 'risk_counts': risk_counts,
#                 'files': [str(out_csv.name)]
#             }
#         except Exception:
#             logging.exception("Failed to build dict output for run_trend_for_date")
#             return {
#                 'rows': len(features),
#                 'flagged_rows': int(features['IsFlagged'].sum() if 'IsFlagged' in features.columns else 0),
#                 'sample': [],
#                 'reasons_count': {},
#                 'risk_counts': {},
#                 'files': [str(out_csv.name)]
#             }

#     return features

def run_trend_for_date(target_date: date, regions: Optional[List[str]] = None, outdir: str = None) -> pd.DataFrame:
    """
    Run full trend pipeline for `target_date` across specified `regions`.
    - regions: list of region keys, e.g. ['apac','emea']
    - outdir: outputs folder (passed-through)
    Returns: combined DataFrame containing aggregated trend rows (same format your app expects)
    """
    if regions is None:
        regions = list(REGION_CONFIG.keys())

    # normalize region keys
    regions = [r.lower() for r in regions if r]

    # fetch durations & swipes per region (duration_report.run_for_date returns dict by region)
    results = run_for_date(target_date, regions, outdir or "outputs")

    # combine durations across regions into one DataFrame for scoring/aggregation
    dur_list = []
    for rkey, rr in (results or {}).items():
        try:
            dfdur = rr.get('durations')
            if dfdur is not None and not dfdur.empty:
                # optionally add region marker
                dfdur['region'] = rkey
                dur_list.append(dfdur)
        except Exception:
            continue

    combined = pd.concat(dur_list, ignore_index=True) if dur_list else pd.DataFrame()

    # ---- existing scoring / ML / scenario detection logic should run here on `combined` ----
    # e.g. apply your feature calculations and model predictions to produce 'IsFlagged', 'Reasons', etc.
    # At the end return the final trend df (same format as before).
    # For backward compatibility, if your old implementation returned a single 'trend_pune_{date}.csv' dataframe,
    # keep the same column names and semantics.

    # Placeholder: if you have an existing function `score_trends_from_durations(combined)` use it:
    try:
        trend_df = score_trends_from_durations(combined)  # <--- replace with your actual scoring fn
    except Exception:
        # if scoring not available yet, just return combined durations
        trend_df = combined

    return trend_df


# ---------------- helper wrappers ----------------
def _ensure_date_obj(d):
    if d is None:
        return None
    if isinstance(d, date):
        return d
    if isinstance(d, _datetime):
        return d.date()
    if isinstance(d, str):
        try:
            return _datetime.strptime(d, "%Y-%m-%d").date()
        except Exception:
            try:
                return _datetime.fromisoformat(d).date()
            except Exception:
                raise ValueError(f"Unsupported date string: {d}")
    raise ValueError(f"Unsupported date type: {type(d)}")

def build_monthly_training(start_date=None, end_date=None, outdir: str = None, city: str = 'Pune', as_dict: bool = False):
    od = Path(outdir) if outdir else OUTDIR
    od.mkdir(parents=True, exist_ok=True)
    if start_date is None and end_date is None:
        today = date.today()
        first = date(today.year, today.month, 1)
        last = date(today.year, today.month, calendar.monthrange(today.year, today.month)[1])
    else:
        if start_date is None:
            raise ValueError("start_date must be provided when end_date is provided")
        first = _ensure_date_obj(start_date)
        if end_date is None:
            last = date(first.year, first.month, calendar.monthrange(first.year, first.month)[1])
        else:
            last = _ensure_date_obj(end_date)
    if last < first:
        raise ValueError("end_date must be >= start_date")
    cur = first
    ran = []
    errors = {}
    total_flagged = 0
    total_rows = 0
    while cur <= last:
        try:
            logging.info("build_monthly_training: running for %s (city=%s)", cur.isoformat(), city)
            res = run_trend_for_date(cur, outdir=str(od), city=city, as_dict=as_dict)
            ran.append({'date': cur.isoformat(), 'result': res})
            if isinstance(res, dict):
                total_flagged += int(res.get('flagged_rows', 0) or 0)
                total_rows += int(res.get('rows', 0) or 0)
        except KeyboardInterrupt:
            raise
        except Exception as e:
            logging.exception("build_monthly_training: failed for %s", cur)
            errors[cur.isoformat()] = str(e)
        cur = cur + _timedelta(days=1)
    summary = {
        'start_date': first.isoformat(),
        'end_date': last.isoformat(),
        'dates_attempted': (last - first).days + 1,
        'dates_succeeded': len([r for r in ran if r.get('result') is not None]),
        'dates_failed': len(errors),
        'errors': errors,
        'total_rows': total_rows,
        'total_flagged': total_flagged
    }
    if as_dict:
        return summary
    return ran

def read_90day_cache(outdir: str = None):
    od = Path(outdir) if outdir else OUTDIR
    fp = od / "90day_cache.json"
    if not fp.exists():
        return {}
    try:
        with fp.open("r", encoding="utf8") as fh:
            return json.load(fh)
    except Exception:
        logging.exception("read_90day_cache: failed to read %s", str(fp))
        return {}

if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today, as_dict=False)
    print("Completed; rows:", len(df) if df is not None else 0)







