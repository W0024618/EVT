Now In APAC we dont get taguig and MY.Kulampur data 
 taguig display in Quezon city 
Only logic is Working for Pune and HYD .... Remenanimg for all location we got blank data Which is need to fix ..

Once check file by line also check console details and fix this isue ...



  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 19, in <module>
    from duration_report import REGION_CONFIG
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 1877
    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: 'return' outside function
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python app.py
>>
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 19, in <module>
    from duration_report import REGION_CONFIG
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 2199
    if "PrimaryLocation" in swipes.columns:
IndentationError: unexpected indent
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> ^C
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python app.py
>>
 * Serving Flask app 'app'
 * Debug mode: on
INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8002
 * Running on http://10.199.47.235:8002
INFO:werkzeug:Press CTRL+C to quit
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 134-209-644
INFO:root:Fetching swipes for region apac on 2025-11-11
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-11
INFO:root:City filter 'pune' applied for region apac: rows before=24215 after=18755
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1910: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251111.csv (rows=831)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251111.csv (rows=18755)
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1910: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:PersonnelTypeName values example: ['Employee']
INFO:root:PersonnelTypeName filter applied: before=18755 after=18755
INFO:root:Saved raw swipes to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\swipes_pune_20251111.csv
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2230: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2305: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2305: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1760: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1826: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  df['PresentToday'] = df['CountSwipes'].fillna(0).astype(int) > 0
INFO:root:run_trend_for_date: wrote C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\trend_pune_20251111.csv (rows=833)
INFO:root:get_personnel_info: lookup called with candidate_identifier=249246
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=249246 -> ObjectID=2097193272 Email=Abhinav.RGupta@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=321516
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=321516 -> ObjectID=2097199908 Email=Shashank.Dubey@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=321516
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=321516 -> ObjectID=2097199908 Email=Shashank.Dubey@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=323801
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=323801 -> ObjectID=2097203282 Email=Ayush.Gupta@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=323878
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=323878 -> ObjectID=2097203362 Email=Abhijeet.Agrawal@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=323976
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=323976 -> ObjectID=2097203498 Email=Shubham.Soni@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=325804
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=325804 -> ObjectID=2097206013 Email=JohnPascal.Gaddam@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=325804
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=325804 -> ObjectID=2097206013 Email=JohnPascal.Gaddam@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=326724
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=326724 -> ObjectID=2097207139 Email=Parth.Goyal@westernunion.com
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:17:27] "GET /run?start=2025-11-11&end=2025-11-11&full=true&region=apac&city=Pune HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:17:35] "GET /record?employee_id=321516 HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:17:44] "GET /record?employee_id=321516 HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:18:11] "GET /record?employee_id=321516 HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:18:20] "GET /record?employee_id=326724 HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:18:29] "GET /record?employee_id=323976 HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:18:39] "GET /record?employee_id=323801 HTTP/1.1" 200 -
INFO:root:Fetching swipes for region apac on 2025-11-11
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-11
INFO:root:City filter 'inhyd' applied for region apac: rows before=24215 after=294
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1910: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251111.csv (rows=32)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251111.csv (rows=294)
INFO:root:PersonnelTypeName values example: ['Employee']
INFO:root:PersonnelTypeName filter applied: before=294 after=294
INFO:root:Saved raw swipes to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\swipes_inhyd_20251111.csv
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2230: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2305: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2305: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1760: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1826: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  df['PresentToday'] = df['CountSwipes'].fillna(0).astype(int) > 0
INFO:root:run_trend_for_date: wrote C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\trend_inhyd_20251111.csv (rows=32)
INFO:root:get_personnel_info: lookup called with candidate_identifier=325365
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=325365 -> ObjectID=2097205378 Email=Rajeev.Mago@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=326610
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=326610 -> ObjectID=2097206996 Email=Vineet.Das@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=329467
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=329467 -> ObjectID=2097210324 Email=None     
INFO:root:get_personnel_info: lookup called with candidate_identifier=329750
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=329750 -> ObjectID=2097210583 Email=None
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:18:58] "GET /run?start=2025-11-11&end=2025-11-11&full=true&region=apac&city=IN.HYD HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:19:08] "GET /record?employee_id=329750 HTTP/1.1" 200 -
INFO:root:Fetching swipes for region apac on 2025-11-11
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-11
INFO:root:City filter 'sgsingapore' applied for region apac: rows before=24215 after=186
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1910: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251111.csv (rows=19)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251111.csv (rows=186)
INFO:root:PersonnelTypeName values example: ['Employee']
INFO:root:PersonnelTypeName filter applied: before=186 after=186
INFO:root:Saved raw swipes to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\swipes_sgsingapore_20251111.csv
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2230: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2305: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2305: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1760: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1826: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  df['PresentToday'] = df['CountSwipes'].fillna(0).astype(int) > 0
INFO:root:run_trend_for_date: wrote C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\trend_sgsingapore_20251111.csv (rows=19)
INFO:root:get_personnel_info: lookup called with candidate_identifier=236252
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=236252 -> ObjectID=2097176648 Email=Bharat.Dua@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=312276
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=312276 -> ObjectID=2097188996 Email=LieOnnDoris.Chu@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=312296
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=312296 -> ObjectID=2097183946 Email=LeeYing.Kong@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=313731
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=313731 -> ObjectID=2097185777 Email=CheeHao.Ong@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=316667
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=316667 -> ObjectID=2097192391 Email=Sudeep.Narula@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=318450
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=318450 -> ObjectID=2097195975 Email=Wilson.Huang@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=319272
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=319272 -> ObjectID=2097196982 Email=Bhavin.Shah1@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=322301
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=322301 -> ObjectID=2097201042 Email=AndreaYenny.Sjah@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=322569
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=322569 -> ObjectID=2097201472 Email=Serene.Lim@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=323137
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=323137 -> ObjectID=2097202310 Email=ShiLong.Cheon@wu.com
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:19:28] "GET /run?start=2025-11-11&end=2025-11-11&full=true&region=apac&city=SG.Singapore HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:19:43] "GET /record?employee_id=322569 HTTP/1.1" 200 -
INFO:root:Fetching swipes for region apac on 2025-11-11
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-11
INFO:root:City filter 'taguig-city' applied for region apac: rows before=24215 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251111.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251111.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:20:04] "GET /run?start=2025-11-11&end=2025-11-11&full=true&region=apac&city=Taguig%20City HTTP/1.1" 200 -
INFO:root:Fetching swipes for region apac on 2025-11-10
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-10
INFO:root:City filter 'taguig-city' applied for region apac: rows before=18628 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251110.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251110.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:root:Fetching swipes for region apac on 2025-11-11
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-11
INFO:root:City filter 'taguig-city' applied for region apac: rows before=24215 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251111.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251111.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:root:Fetching swipes for region apac on 2025-11-12
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-12
INFO:root:City filter 'taguig-city' applied for region apac: rows before=25527 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251112.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251112.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:root:Fetching swipes for region apac on 2025-11-13
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-13
INFO:root:City filter 'taguig-city' applied for region apac: rows before=17135 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251113.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251113.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:root:Fetching swipes for region apac on 2025-11-14
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-14
INFO:root:City filter 'taguig-city' applied for region apac: rows before=9777 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251114.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251114.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:root:Fetching swipes for region apac on 2025-11-15
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-15
INFO:root:City filter 'taguig-city' applied for region apac: rows before=1841 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251115.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251115.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:21:32] "GET /run?start=2025-11-10&end=2025-11-15&full=true&region=apac&city=Taguig%20City HTTP/1.1" 200 -
INFO:root:Fetching swipes for region apac on 2025-11-10
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-10
INFO:root:City filter 'mykuala-lumpur' applied for region apac: rows before=18628 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251110.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251110.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:root:Fetching swipes for region apac on 2025-11-11
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-11
INFO:root:City filter 'mykuala-lumpur' applied for region apac: rows before=24215 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251111.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251111.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:root:Fetching swipes for region apac on 2025-11-12
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-12
INFO:root:City filter 'mykuala-lumpur' applied for region apac: rows before=25527 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251112.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251112.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:root:Fetching swipes for region apac on 2025-11-13
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-13
INFO:root:City filter 'mykuala-lumpur' applied for region apac: rows before=17135 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251113.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251113.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:root:Fetching swipes for region apac on 2025-11-14
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-14
INFO:root:City filter 'mykuala-lumpur' applied for region apac: rows before=9777 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251114.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251114.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:root:Fetching swipes for region apac on 2025-11-15
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-15
INFO:root:City filter 'mykuala-lumpur' applied for region apac: rows before=1841 after=0
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251115.csv (rows=0)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251115.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:22:55] "GET /run?start=2025-11-10&end=2025-11-15&full=true&region=apac&city=MY.Kuala%20Lumpur HTTP/1.1" 200 -
INFO:root:Fetching swipes for region apac on 2025-11-11
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-11
INFO:root:City filter 'quezon-city' applied for region apac: rows before=24215 after=4980
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1910: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251111.csv (rows=288)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251111.csv (rows=4980)
INFO:root:PersonnelTypeName values example: ['Employee']
INFO:root:PersonnelTypeName filter applied: before=4980 after=4980
INFO:root:Saved raw swipes to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\swipes_quezon-city_20251111.csv
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2230: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2305: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2305: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1760: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1826: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  df['PresentToday'] = df['CountSwipes'].fillna(0).astype(int) > 0
INFO:root:run_trend_for_date: wrote C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\trend_quezon-city_20251111.csv (rows=288)
INFO:root:get_personnel_info: lookup called with candidate_identifier=237782
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=237782 -> ObjectID=2097185863 Email=Karren.Digno@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=237809
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=237809 -> ObjectID=2097185934 Email=Alexander.Dee@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=239360
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=239360 -> ObjectID=2097186159 Email=MaCecilia.Acance@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=239679
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=239679 -> ObjectID=2097186478 Email=Jielyn.Avenido@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=239684
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=239684 -> ObjectID=2097186023 Email=Maricar.Mendoza@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=240102
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=240102 -> ObjectID=2097185941 Email=Jerri.Bote@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=241033
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=241033 -> ObjectID=2097186035 Email=Rodeleon.Castro@Westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=242439
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=242439 -> ObjectID=2097184050 Email=JayAnne.Tumolva@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=242488
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=242488 -> ObjectID=2097185456 Email=MariaCriselda.Cortez@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=246589
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=246589 -> ObjectID=2097183431 Email=Ronnel.Organisa@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=246665
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=246665 -> ObjectID=2097185995 Email=Jeen.Marasigan@Westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=246691
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=246691 -> ObjectID=2097186101 Email=Lerma.Agbayani@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=246700
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=246700 -> ObjectID=2097185954 Email=April.Magistrado@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=246742
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=246742 -> ObjectID=2097185852 Email=PhilipFerdinand.Alvarez@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=248732
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=248732 -> ObjectID=2097185556 Email=Sharon.Rasco@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=248734
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=248734 -> ObjectID=2097185557 Email=AnnaKatrina.Sagun@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=249269
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=249269 -> ObjectID=2097185447 Email=SymphonyCassandra.Esguerra@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=301256
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=301256 -> ObjectID=2097185657 Email=PatriciaMae.Parian@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=301258
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=301258 -> ObjectID=2097186229 Email=ABIGIEL.PANGILINAN@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=301259
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=301259 -> ObjectID=2097186223 Email=ROSEMARIE.OLAGUER@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=303159
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=303159 -> ObjectID=2097185960 Email=EmmaRuth.Digap@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=303167
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=303167 -> ObjectID=2097166849 Email=Jane.Santos@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=303300
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=303300 -> ObjectID=2097186082 Email=Lourdes.Codera@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=304211
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=304211 -> ObjectID=2097186470 Email=RichardManuel.Maranan@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=304609
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=304609 -> ObjectID=2097186009 Email=MARITES.MARQUEZ@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=304726
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=304726 -> ObjectID=2097186033 Email=Denelyn.Miram@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=304942
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=304942 -> ObjectID=2097185972 Email=Maureen.Domingo@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=304943
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=304943 -> ObjectID=2097185496 Email=VIDA.VILLANUEVA@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=304967
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=304967 -> ObjectID=2097185401 Email=JamesPaul.Mosende@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=305505
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=305505 -> ObjectID=2097176700 Email=Dimsdaille.Paulo@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=305600
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=305600 -> ObjectID=2097185861 Email=JuanRamon.AlcazarJr@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=306690
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=306690 -> ObjectID=2097185953 Email=MariePearl.Magaway@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=308738
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=308738 -> ObjectID=2097185892 Email=LouSheena.Lao@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=309121
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=309121 -> ObjectID=2097185844 Email=Fe.Alcantara@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=309209
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=309209 -> ObjectID=2097185666 Email=neilcarlo.ragandap@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=309425
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=309425 -> ObjectID=2097185517 Email=Arvin.Vasquez@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=309803
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=309803 -> ObjectID=2097185989 Email=JingerGale.Dysangco@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=314541
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=314541 -> ObjectID=2097188451 Email=Grace.Cristobal@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=315546
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=315546 -> ObjectID=2097189833 Email=AprilCadorniga.Zacate@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=316444
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=316444 -> ObjectID=2097191775 Email=Lissy.Cruz@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=316816
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=316816 -> ObjectID=2097192634 Email=Geramie.Gerona@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=318439
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=318439 -> ObjectID=2097195954 Email=Nancy.Salvador@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=323336
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=323336 -> ObjectID=2097202610 Email=Ruth.Sulit@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=324552
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=324552 -> ObjectID=2097204321 Email=Blue.Legaspi@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=325373
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=325373 -> ObjectID=2097205386 Email=Geraldine.Soriano@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=326237
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=326237 -> ObjectID=2097206475 Email=None
INFO:root:get_personnel_info: lookup called with candidate_identifier=326292
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=326292 -> ObjectID=2097206546 Email=None
INFO:root:get_personnel_info: lookup called with candidate_identifier=326315
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=326315 -> ObjectID=2097206569 Email=None
INFO:root:get_personnel_info: lookup called with candidate_identifier=326501
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=326501 -> ObjectID=2097206849 Email=None     
INFO:root:get_personnel_info: lookup called with candidate_identifier=326756
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=326756 -> ObjectID=2097207186 Email=None     
INFO:root:get_personnel_info: lookup called with candidate_identifier=326801
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=326801 -> ObjectID=2097207244 Email=None     
INFO:root:get_personnel_info: lookup called with candidate_identifier=326886
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=326886 -> ObjectID=2097207367 Email=None     
INFO:root:get_personnel_info: lookup called with candidate_identifier=326915
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=326915 -> ObjectID=2097207405 Email=None     
INFO:root:get_personnel_info: lookup called with candidate_identifier=327172
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=327172 -> ObjectID=2097207697 Email=None
INFO:root:get_personnel_info: lookup called with candidate_identifier=327364
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=327364 -> ObjectID=2097207869 Email=None
INFO:root:get_personnel_info: lookup called with candidate_identifier=327365
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=327365 -> ObjectID=2097207870 Email=None     
INFO:root:get_personnel_info: lookup called with candidate_identifier=327555
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=327555 -> ObjectID=2097208157 Email=None     
INFO:root:get_personnel_info: lookup called with candidate_identifier=327601
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=327601 -> ObjectID=2097208242 Email=None
INFO:root:get_personnel_info: lookup called with candidate_identifier=328117
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=328117 -> ObjectID=2097208730 Email=None
INFO:root:get_personnel_info: lookup called with candidate_identifier=328986
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=328986 -> ObjectID=2097209723 Email=None     
INFO:root:get_personnel_info: lookup called with candidate_identifier=329030
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=329030 -> ObjectID=2097209778 Email=None
INFO:root:get_personnel_info: lookup called with candidate_identifier=329061
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=329061 -> ObjectID=2097209834 Email=None
INFO:root:get_personnel_info: lookup called with candidate_identifier=329119
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=329119 -> ObjectID=2097209908 Email=None     
INFO:root:get_personnel_info: lookup called with candidate_identifier=329127
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=329127 -> ObjectID=2097209946 Email=None
INFO:root:get_personnel_info: lookup called with candidate_identifier=329134
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=329134 -> ObjectID=2097209955 Email=None     
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:23:29] "GET /run?start=2025-11-11&end=2025-11-11&full=true&region=apac&city=Quezon%20City HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:23:44] "GET /record?employee_id=239360 HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:23:54] "GET /record?employee_id=309121 HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:24:00] "GET /record?employee_id=239360 HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:24:06] "GET /record?employee_id=246691 HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:24:19] "GET /record?employee_id=314541 HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:24:30] "GET /record?employee_id=326501 HTTP/1.1" 200 -
INFO:root:Fetching swipes for region emea on 2025-11-11
INFO:root:Databases present for server SRVWUFRA0986V: ['ACVSUJournal_00011029', 'ACVSUJournal_00011028', 'ACVSUJournal_00011027', 'ACVSUJournal_00011026', 'ACVSUJournal_00011025', 'ACVSUJournal_00011024', 'ACVSUJournal_00011023']
INFO:root:Built SQL for region emea, date 2025-11-11
ERROR:root:Failed fetching swipes for region emea
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 2118, in run_for_date     
    swipes = fetch_swipes_for_region(rkey, target_date)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 1822, in fetch_swipes_for_region
    df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'normalize_apac_partition' where it is not associated with a value
INFO:root:Wrote duration CSV for emea to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\emea_duration_20251111.csv (rows=0)
INFO:root:Wrote swipes CSV for emea to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\emea_swipes_20251111.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:25:52] "GET /run?start=2025-11-11&end=2025-11-11&full=true&region=emea&city=LT.Vilnius HTTP/1.1" 200 -
INFO:root:Fetching swipes for region emea on 2025-11-11
INFO:root:Databases present for server SRVWUFRA0986V: ['ACVSUJournal_00011029', 'ACVSUJournal_00011028', 'ACVSUJournal_00011027', 'ACVSUJournal_00011026', 'ACVSUJournal_00011025', 'ACVSUJournal_00011024', 'ACVSUJournal_00011023']
INFO:root:Built SQL for region emea, date 2025-11-11
ERROR:root:Failed fetching swipes for region emea
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 2118, in run_for_date     
    swipes = fetch_swipes_for_region(rkey, target_date)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 1822, in fetch_swipes_for_region
    df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'normalize_apac_partition' where it is not associated with a value
INFO:root:Wrote duration CSV for emea to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\emea_duration_20251111.csv (rows=0)
INFO:root:Wrote swipes CSV for emea to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\emea_swipes_20251111.csv (rows=0)
WARNING:root:run_trend_for_date: no features computed
INFO:werkzeug:127.0.0.1 - - [18/Nov/2025 18:26:51] "GET /run?start=2025-11-11&end=2025-11-11&full=true&region=emea&city=IT.Rome HTTP/1.1" 200 -









# backend/duration_report.py
from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np
import hashlib

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name (override with environment variable if needed)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
try:
    from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    # fallback  keep behaviour if config file unavailable
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

    def map_door_to_zone(door: object, direction: object = None) -> str:
        """
        Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
        """
        try:
            if door is None:
                return None
            s = str(door).strip()
            if not s:
                return None
            s_l = s.lower()
            if direction and isinstance(direction, str):
                d = direction.strip().lower()
                if "out" in d:
                    return OUT_OF_OFFICE_ZONE
                if "in" in d:
                    return "Reception Area"
            if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
                return OUT_OF_OFFICE_ZONE
            return "Working Area"
        except Exception:
            return None

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "SG.Singapore", "PH.Manila","IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023"
        ],
        "partitions": ["LT.Vilnius","IT.Rome","UK.London","IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition","PA.Panama City","PE.Lima", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["US.CO.OBS", "USA/Canada Default", "US.FL.Miami", "US.NYC"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# GENERIC SQL template - we keep AdjustedMessageTime in SELECT for debugging but the Python
# logic in this file no longer depends on it for date assignment.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND t3.[Name] = 'Employee'
  {date_condition}
  {region_filter}
"""

# Helpers
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

# GUID / placeholders helpers
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '', '', 'none', 'null'])

def _looks_like_guid(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
        except Exception:
            continue
        if not s:
            continue
        if _is_placeholder_str(s):
            continue
        if _looks_like_guid(s):
            continue
        return s
    return None

def _canonical_person_uid_from_row(row):
    """
    Produce canonical person_uid in the form:
      - 'emp:<employeeid>' (if a sensible non-GUID EmployeeID present),
      - 'uid:<EmployeeIdentity>' (if present),
      - 'name:<sha1 10chars>' fallback when name present,
      - otherwise None.
    """
    empid = None
    for cand in ('EmployeeID', 'Int1', 'Text12'):
        if cand in row and row.get(cand) not in (None, '', float('nan')):
            empid = row.get(cand)
            break
    empident = row.get("EmployeeIdentity", None)
    name = row.get("EmployeeName", None)

    # normalize empid numeric floats -> ints
    if empid is not None:
        try:
            s = str(empid).strip()
            if '.' in s:
                f = float(s)
                if f.is_integer():
                    s = str(int(f))
            if s and not _looks_like_guid(s) and not _is_placeholder_str(s):
                return f"emp:{s}"
        except Exception:
            pass

    if empident not in (None, '', float('nan')):
        try:
            si = str(empident).strip()
            if si:
                return f"uid:{si}"
        except Exception:
            pass

    if name not in (None, '', float('nan')):
        try:
            sn = str(name).strip()
            if sn and not _looks_like_guid(sn) and not _is_placeholder_str(sn):
                h = hashlib.sha1(sn.lower().encode('utf8')).hexdigest()[:10]
                return f"name:{h}"
        except Exception:
            pass

    return None

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        partitions = rc.get("partitions", [])
        if partitions:
            parts_sql = ", ".join(f"'{p}'" for p in partitions)
            region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
        else:
            likes = rc.get("logical_like", [])
            if likes:
                like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
                region_filter = f"AND ({like_sql})"
            else:
                region_filter = ""
    else:
        region_filter = ""

    # NOTE: AdjustedMessageTime / 2AM boundary logic removed from date selection.
    date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
        "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
        "Direction", "CompanyName", "PrimaryLocation"
    ]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    # ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Dates parsing
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # Keep AdjustedMessageTime if present for debugging but we do NOT use it for date boundaries anymore.
    try:
        if "AdjustedMessageTime" in df.columns:
            df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
        else:
            df["AdjustedMessageTime"] = pd.NaT
    except Exception:
        df["AdjustedMessageTime"] = pd.NaT

    # defensive: make text fields strings (avoid object type surprises)
    for tcol in ("Door", "PartitionName2", "PersonnelTypeName", "EmployeeName", "CompanyName", "PrimaryLocation"):
        if tcol in df.columns:
            df[tcol] = df[tcol].fillna("").astype(str)

    # Filter: only Employees (defensive; the SQL template already requests t3.Name = 'Employee')
    try:
        if "PersonnelTypeName" in df.columns:
            df = df[df["PersonnelTypeName"].str.strip().str.lower() == "employee"].copy()
    except Exception:
        logging.debug("Could not apply PersonnelTypeName filter for region %s", region_key)

    # canonical person_uid (consistent with trend_runner)
    def make_person_uid(row):
        try:
            return _canonical_person_uid_from_row(row)
        except Exception:
            # fallback: try simple concatenation if canonical fails
            try:
                eid = row.get("EmployeeIdentity")
                if pd.notna(eid) and str(eid).strip() != "":
                    return str(eid).strip()
            except Exception:
                pass
            parts = []
            for c in ("EmployeeID", "CardNumber", "EmployeeName"):
                try:
                    v = row.get(c)
                    if v not in (None, '', float('nan')):
                        parts.append(str(v).strip())
                except Exception:
                    continue
            return "|".join(parts) if parts else None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)



    # # APAC partition normalization (more robust)
    # if region_key == "apac" and not df.empty:
    #     def normalize_apac_partition(row):
    #         door = str(row.get("Door") or "") or ""
    #         part = str(row.get("PartitionName2") or "") or ""
    #         d = door.upper()
    #         p = part.upper()

    #         # explicit token -> canonical partition mapping (extend as needed)
    #         token_map = {
    #             # Pune variants
    #             "APAC_IN_PUN": "Pune",
    #             "APAC_PUN": "Pune",
    #             "PUN": "Pune",
    #             "PUNE": "Pune",
    #             # Hyderabad variants
    #             "APAC_IN_HYD": "IN.HYD",
    #             "HYD": "IN.HYD",
    #             "IN.HYD": "IN.HYD",
    #             # Taguig / Philippines
    #             "APAC_PI": "Taguig City",
    #             "PH": "Quezon City",
    #             "APAC_PH": "Quezon City",
    #             "MANILA": "Quezon City",
    #             # Singapore
    #             "SG": "SG.Singapore",
    #             "SINGAPORE": "SG.Singapore",
    #             # Kuala Lumpur / Malaysia
    #             "APAC_MY": "MY.Kuala Lumpur",
    #             "MY": "MY.Kuala Lumpur",
    #             "KUALA": "MY.Kuala Lumpur",
    #         }

    #         # helper: split door/partition into alphanumeric tokens
    #         def make_tokens(s: str):
    #             toks = [t for t in re.split(r'[^A-Z0-9]+', s or "") if t]
    #             return toks

    #         # 1) If PartitionName2 already contains a known canonical token, prefer it.
    #         #    (handles rows where SQL PartitionName2 was useful)
    #         for key, canonical in token_map.items():
    #             if key in p:
    #                 return canonical

    #         # 2) Tokenize Door and PartitionName2 and check tokens
    #         door_tokens = make_tokens(d)
    #         part_tokens = make_tokens(p)
    #         all_tokens = door_tokens + part_tokens

    #         for t in all_tokens:
    #             if t in token_map:
    #                 return token_map[t]

    #         # 3) Substring / regex fallbacks for common patterns (more permissive)
    #         if re.search(r'\bPUN(E)?\b', d) or re.search(r'\bPUN(E)?\b', p):
    #             return "Pune"
    #         if re.search(r'\bHYD\b', d) or re.search(r'\bHYD\b', p):
    #             return "IN.HYD"
    #         if re.search(r'\bSINGAPORE\b', d) or re.search(r'\bSG\b', d) or re.search(r'\bSINGAPORE\b', p):
    #             return "SG.Singapore"
    #         if re.search(r'\bKUALA\b', d) or re.search(r'\bMY\b', d) or re.search(r'\bKUALA\b', p):
    #             return "MY.Kuala Lumpur"
    #         if re.search(r'\bMANILA\b', d) or re.search(r'\bPH\b', d) or re.search(r'\bMANILA\b', p):
    #             return "Quezon City"

    #         # 4) If PartitionName2 is a non-empty meaningful value, keep it (no change)
    #         if p and p.strip():
    #             return part

    #         # 5) Unknown: return original partition (likely empty string)  caller can filter strictly
    #         return part

    #     df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)

    # APAC partition normalization (more robust)
    if region_key == "apac" and not df.empty:
       def normalize_apac_partition(row):
        door = str(row.get("Door") or "") or ""
        part = str(row.get("PartitionName2") or "") or ""
        d = door.upper()
        p = part.upper()

        # explicit token -> canonical partition mapping (extend as needed)
        token_map = {
            # Pune variants
            "APAC_IN_PUN": "Pune",
            "APAC_PUN": "Pune",
            "VIS_PUN": "Pune",
            "VIS_PUN_177": "Pune",
            "PUN": "Pune",
            "PUNE": "Pune",
            # Hyderabad variants
            "APAC_IN_HYD": "IN.HYD",
            "APAC_HYD": "IN.HYD",
            "HYD": "IN.HYD",
            "IN.HYD": "IN.HYD",
            # Taguig / Philippines
            "APAC_PI": "Taguig City",
            "APAC_PH": "Quezon City",
            "PH": "Quezon City",
            "MANILA": "Quezon City",
            # Singapore
            "SG": "SG.Singapore",
            "SINGAPORE": "SG.Singapore",
            # Kuala Lumpur / Malaysia
            "APAC_MY": "MY.Kuala Lumpur",
            "MY": "MY.Kuala Lumpur",
            "KUALA": "MY.Kuala Lumpur",
            # generic fallbacks that sometimes appear in door names
            "TAGUIG": "Taguig City",
            "QUEZON": "Quezon City",
        }

        # helper: split door/partition into alphanumeric tokens
        def make_tokens(s: str):
            # keep both pure alpha tokens and mixed alpha+digits tokens
            toks = [t for t in re.split(r'[^A-Z0-9]+', s or "") if t]
            return toks

        # 1) If PartitionName2 already contains a known canonical token, prefer it.
        for key, canonical in token_map.items():
            if key in p:
                return canonical

        # 2) Tokenize Door and PartitionName2 and check tokens
        door_tokens = make_tokens(d)
        part_tokens = make_tokens(p)
        all_tokens = door_tokens + part_tokens

        for t in all_tokens:
            if t in token_map:
                return token_map[t]

        # 3) Substring / regex fallbacks for common patterns (more permissive)
        if re.search(r'\bPUN(E)?\b', d) or re.search(r'\bPUN(E)?\b', p):
            return "Pune"
        if re.search(r'\bHYD\b', d) or re.search(r'\bHYD\b', p):
            return "IN.HYD"
        if re.search(r'\bSINGAPORE\b', d) or re.search(r'\bSG\b', d) or re.search(r'\bSINGAPORE\b', p):
            return "SG.Singapore"
        if re.search(r'\bKUALA\b', d) or re.search(r'\bMY\b', d) or re.search(r'\bKUALA\b', p):
            return "MY.Kuala Lumpur"
        if re.search(r'\bMANILA\b', d) or re.search(r'\bPH\b', d) or re.search(r'\bMANILA\b', p):
            return "Quezon City"

        # 4) If PartitionName2 is a non-empty meaningful value, keep it (no change)
        if p and p.strip():
            return part

        # 5) Unknown: return original partition (likely empty string)  caller can filter strictly
        return part

    df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)






    # NAMER: normalize PartitionName2 and add LogicalLocation per previous behaviour
    if region_key == "namer" and not df.empty:
        def namer_partition_and_logical(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()
            normalized = part
            logical = "Other"

            if ("US.CO.HQ" in d) or ("HQ" in d and "HQ" in d[:20]) or ("DENVER" in d) or (p == "US.CO.OBS"):
                normalized = "US.CO.OBS"
                logical = "Denver-HQ"
            elif "AUSTIN" in d or "AUSTIN TX" in d or p == "USA/CANADA DEFAULT":
                normalized = "USA/Canada Default"
                logical = "Austin Texas"
            elif "MIAMI" in d or p == "US.FL.MIAMI":
                normalized = "US.FL.Miami"
                logical = "Miami"
            elif "NYC" in d or "NEW YORK" in d or p == "US.NYC":
                normalized = "US.NYC"
                logical = "New York"
            else:
                if p == "US.CO.OBS":
                    normalized = "US.CO.OBS"; logical = "Denver-HQ"
                elif p == "USA/CANADA DEFAULT":
                    normalized = "USA/Canada Default"; logical = "Austin Texas"
                elif p == "US.FL.MIAMI":
                    normalized = "US.FL.Miami"; logical = "Miami"
                elif p == "US.NYC":
                    normalized = "US.NYC"; logical = "New York"
                else:
                    normalized = part
                    logical = "Other"
            return pd.Series({"PartitionName2": normalized, "LogicalLocation": logical})

        mapped = df.apply(namer_partition_and_logical, axis=1)
        df["PartitionName2"] = mapped["PartitionName2"].astype(str)
        df["LogicalLocation"] = mapped["LogicalLocation"].astype(str)

    # ensure PartitionName2 column exists as string
    if "PartitionName2" not in df.columns:
        df["PartitionName2"] = ""

    # ensure LogicalLocation exists
    if "LogicalLocation" not in df.columns:
        df["LogicalLocation"] = ""

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]

# ---------------------------------------------------------------------
# compute_daily_durations (single robust implementation)
# ---------------------------------------------------------------------
def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "DurationMinutes", "DurationDisplay", "DurationHMS",
        "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes if present
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # drop near-duplicate swipes (round to seconds)
    try:
        df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
        dedupe_subset = ["person_uid", "_lts_rounded", "CardNumber", "Door"]
        df = df.drop_duplicates(subset=dedupe_subset, keep="first").copy()
    except Exception:
        dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
        try:
            df = df.drop_duplicates(subset=dedupe_cols, keep="first")
        except Exception:
            pass

    # Date assignment (STRICT: only use LocaleMessageTime.date())
    try:
        df["Date"] = df["LocaleMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure person_uid (keep existing canonicalization where present)
    df["person_uid"] = df.apply(
        lambda row: row.get("person_uid")
        if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
        else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
        axis=1
    )
    df = df[df["person_uid"].notna()].copy()

    # Group and aggregate  pick first non-empty EmployeeName and EmployeeID
    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", lambda s: _pick_first_non_guid_value(s)),
            EmployeeName=("EmployeeName", lambda s: _pick_first_non_guid_value(s)),
            CardNumber=("CardNumber", lambda s: _pick_first_non_guid_value(s)),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            empid = _pick_first_non_guid_value(g_sorted["EmployeeID"]) if "EmployeeID" in g_sorted else first.get("EmployeeID")
            ename = _pick_first_non_guid_value(g_sorted["EmployeeName"]) if "EmployeeName" in g_sorted else first.get("EmployeeName")
            cnum = _pick_first_non_guid_value(g_sorted["CardNumber"]) if "CardNumber" in g_sorted else first.get("CardNumber")
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": empid,
                "EmployeeName": ename,
                "CardNumber": cnum,
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    # # compute duration using LocaleMessageTime first/last (wall-clock)
    # grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    # grouped["Duration"] = grouped["DurationSeconds"].apply(
    #     lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    # )



        # compute duration using LocaleMessageTime first/last (wall-clock)
    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)

    # Format Duration as total hours:minutes (e.g. "02:13" or "26:05")
    def _seconds_to_hhmm(seconds_val):
        try:
            if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
                return None
            total = int(round(float(seconds_val)))
            hours = total // 3600
            minutes = (total % 3600) // 60
            # hours may be >24; keep full hours
            return f"{hours}:{minutes:02d}"
        except Exception:
            return None

    grouped["Duration"] = grouped["DurationSeconds"].apply(_seconds_to_hhmm)


    # helper: format minutes to "Hh Mm"
    def _format_minutes_to_hhmm(seconds_val):
        try:
            if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
                return None
            total_minutes = int(round(float(seconds_val) / 60.0))
            h = total_minutes // 60
            m = total_minutes % 60
            return f"{h}h {m}m"
        except Exception:
            return None

    # Duration in integer minutes (useful where UI currently shows '689 min')
    # grouped["DurationMinutes"] = grouped["DurationSeconds"].apply(lambda s: int(round(s/60)) if pd.notna(s) else None)

    # Human readable hours display, e.g. "11h 29m"
    grouped["DurationDisplay"] = grouped["DurationSeconds"].apply(_format_minutes_to_hhmm)

    # Also provide a standard H:MM:SS string
    grouped["DurationHMS"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    # ensure all output cols exist
    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    # drop helper column if present
    if "_lts_rounded" in grouped.columns:
        try:
            grouped = grouped.drop(columns=["_lts_rounded"])
        except Exception:
            pass
    if "_prev_ts" in grouped.columns:
        try:
            grouped = grouped.drop(columns=["_prev_ts"])
        except Exception:
            pass

    return grouped[out_cols]






def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    # Defensive: normalize incoming regions list
    try:
        requested_regions = [r.lower() for r in (regions or []) if r]
    except Exception:
        requested_regions = []

    # If user provided a city/site, try to map that city to one or more region keys
    def _normalize_token(s: str) -> str:
        return re.sub(r'[^a-z0-9]', '', str(s or '').strip().lower())

    if city:
        city_raw = str(city).strip()
        city_norm = _normalize_token(city_raw)

        # find regions whose partitions or logical_like look like this city
        matched_regions = []
        for rkey, rc in (REGION_CONFIG or {}).items():
            parts = rc.get("partitions", []) or []
            likes = rc.get("logical_like", []) or []
            tokens = set()
            for p in parts:
                if not p:
                    continue
                tokens.add(_normalize_token(p))
                # also split on punctuation/dot and add parts (e.g. "LT.Vilnius" -> "vilnius")
                for part_piece in re.split(r'[.\-/\s]', str(p)):
                    if part_piece:
                        tokens.add(_normalize_token(part_piece))
            for lk in likes:
                tokens.add(_normalize_token(lk))
            # also include server/database names as a fallback
            if city_norm and city_norm in tokens:
                matched_regions.append(rkey)

        # If we could map city to specific region(s), only run those
        if matched_regions:
            requested_regions = [m for m in matched_regions]

    # fallback to all regions in config if none requested
    try:
        if not requested_regions:
            requested_regions = [k.lower() for k in list(REGION_CONFIG.keys())]
    except Exception:
        requested_regions = ['apac']

    results: Dict[str, Any] = {}
    for r in requested_regions:
        if not r:
            continue
        rkey = r.lower()
        if rkey not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", rkey, target_date)
        try:
            swipes = fetch_swipes_for_region(rkey, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", rkey)
            swipes = pd.DataFrame()

        # If a city was requested, apply a strict (but defensive) city filter
        if city and not swipes.empty:
            city_raw = str(city).strip()
            city_norm = _normalize_token(city_raw)

            alt_tokens = set()
            alt_tokens.add(city_raw)
            alt_tokens.add(city_raw.replace('-', ' '))
            alt_tokens.add(city_raw.replace('_', ' '))
            alt_tokens.add(city_raw.replace('.', ' '))
            alt_tokens.add(city_raw.replace(' ', '-'))
            alt_tokens.update({t.title() for t in list(alt_tokens)})
            if city_norm:
                alt_tokens.add(city_norm)

            def _norm_for_cmp(s):
                try:
                    if s is None:
                        return ''
                    return re.sub(r'[^a-z0-9]', '', str(s).strip().lower())
                except Exception:
                    return ''

            # precompute normalized columns (safe defaults)
            try:
                part_norm = swipes["PartitionName2"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "PartitionName2" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
                door_norm = swipes["Door"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "Door" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
                pl_norm = swipes["PrimaryLocation"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "PrimaryLocation" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
            except Exception:
                part_norm = pd.Series([''] * len(swipes), index=swipes.index)
                door_norm = pd.Series([''] * len(swipes), index=swipes.index)
                pl_norm = pd.Series([''] * len(swipes), index=swipes.index)

            # Build mask: start False
            mask = pd.Series(False, index=swipes.index)

            # 1) Strict: prefer PartitionName2 exact matches (canonical names from normalize)
            for t in alt_tokens:
                t_norm = _norm_for_cmp(t)
                if not t_norm:
                    continue
                try:
                    mask = mask | (part_norm == t_norm)
                except Exception:
                    continue

            # 2) For rows where PartitionName2 is empty / unknown, allow Door token contains match (permissive)
            try:
                no_part_mask = part_norm.fillna('').astype(str) == ''
                if no_part_mask.any():
                    door_mask = pd.Series(False, index=swipes.index)
                    for t in alt_tokens:
                        t_norm = _norm_for_cmp(t)
                        if not t_norm:
                            continue
                        door_mask = door_mask | door_norm.str.contains(t_norm, na=False)
                    # only accept door matches when partition is empty
                    mask = mask | (door_mask & no_part_mask)
            except Exception:
                logging.debug("Door-based fallback match failed for city filter in region %s", rkey)

            # 3) Allow PrimaryLocation match ONLY for rows with empty PartitionName2 and not already matched
            try:
                remaining_mask = ~mask
                pl_mask = pd.Series(False, index=swipes.index)
                for t in alt_tokens:
                    t_norm = _norm_for_cmp(t)
                    if not t_norm:
                        continue
                    pl_mask = pl_mask | pl_norm.str.contains(t_norm, na=False)
                mask = mask | (pl_mask & (part_norm.fillna('') == '') & remaining_mask)
            except Exception:
                logging.debug("PrimaryLocation fallback match failed for city filter in region %s", rkey)

            # 4) Finally, also check Door and EmployeeName contains across all rows (additional permissive matches)
            try:
                for col in ("Door", "EmployeeName"):
                    if col in swipes.columns:
                        col_norm = swipes[col].fillna("").astype(str).str.lower().apply(_norm_for_cmp)
                        col_mask = pd.Series(False, index=swipes.index)
                        for t in alt_tokens:
                            t_norm = _norm_for_cmp(t)
                            if not t_norm:
                                continue
                            col_mask = col_mask | col_norm.str.contains(t_norm, na=False)
                        mask = mask | col_mask
            except Exception:
                logging.debug("Door/EmployeeName contains-match fallback failed for city filter in region %s", rkey)

            # Apply the mask strictly
            before = len(swipes)
            swipes = swipes[mask].copy()
            logging.info("City filter '%s' applied for region %s: rows before=%d after=%d", city_raw, rkey, before, len(swipes))

        # compute durations for this region
        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", rkey)
            durations = pd.DataFrame()

        # frontend-friendly display columns for swipes
        try:
            if "LocaleMessageTime" in swipes.columns:
                swipes["LocaleMessageTime"] = pd.to_datetime(swipes["LocaleMessageTime"], errors="coerce")
                swipes["DateOnly"] = swipes["LocaleMessageTime"].dt.date
                swipes["Time"] = swipes["LocaleMessageTime"].dt.strftime("%H:%M:%S")
            else:
                if "Date" in swipes.columns and "Time" in swipes.columns:
                    swipes["DateOnly"] = swipes["Date"]
        except Exception:
            logging.debug("Frontend display enrichment failed for region %s", rkey)

        if "AdjustedMessageTime" not in swipes.columns:
            swipes["AdjustedMessageTime"] = pd.NaT

        # write outputs
        try:
            csv_path = outdir_path / f"{rkey}_duration_{target_date.strftime('%Y%m%d')}.csv"
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", rkey)
        try:
            swipes_csv_path = outdir_path / f"{rkey}_swipes_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", rkey)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", rkey, csv_path if 'csv_path' in locals() else '<unknown>', len(durations) if durations is not None else 0)
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", rkey, swipes_csv_path if 'swipes_csv_path' in locals() else '<unknown>', len(swipes) if swipes is not None else 0)

        results[rkey] = {"swipes": swipes, "durations": durations}

    return results


# end of file








