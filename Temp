Now We have One issue for Employee Whos Working in shift ...
so there Duration willl display wrong like there Duration meets like ...
0:56:17	   21:22:30	
so i will explain you why this happen..
and How to resolve this ..

ex-1	W0024619	Choudhari, Manisha	615828	Contractor	APAC.Default 
and there 06-09-2025	 07-09-2025  duration like ..
0:56:17	    21:22:30	

if we check there swipe details ..


Date	Time (local)	Door	Direction	CardNumber	PersonnelType	Partition	PrimaryLocation	Company
06-09-2025	22:02:09	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
06-09-2025	22:03:51	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
06-09-2025	22:04:22	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
06-09-2025	22:58:26	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	00:38:52	APAC_IN_PUN_PODIUM_ST2 DOOR 2 (YELLOW)	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	00:39:00	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	00:39:34	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	01:12:31	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	01:16:59	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	01:17:31	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	01:20:48	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	02:46:33	APAC_IN_PUN_PODIUM_ST2 DOOR 2 (YELLOW)	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	02:47:02	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	04:39:59	APAC_IN_PUN_PODIUM_ST2 DOOR 2 (YELLOW)	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	04:40:34	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	06:30:41	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	06:32:58	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	06:38:53	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	06:39:27	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	07:09:31	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	07:10:59	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 1-OUT DOOR	OutDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	21:58:51	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 1-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	22:00:48	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	22:01:22	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.



this is Manisha swipe Details so When we Break their swipe details and we identify 
This is manisha first Swipe ....for 06-09-2025 for this day ..Means Manisha ch shift start 10.00.Pm for 06-09-2025 this date ...
06-09-2025	 22:02:09	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.

manisha swipe theie badge for this day 
06-09-2025	22:03:51	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
06-09-2025	22:04:22	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
06-09-2025	22:58:26	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.

so as per current logic System calculate this duration but this is Wrong ..

after 12:00 am 07-09-2025 Manisha Swipe theie badge 
07-09-2025	00:38:52	APAC_IN_PUN_PODIUM_ST2 DOOR 2 (YELLOW)	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	00:39:00	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	00:39:34	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	01:12:31	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	01:16:59	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	01:17:31	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	01:20:48	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	02:46:33	APAC_IN_PUN_PODIUM_ST2 DOOR 2 (YELLOW)	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	02:47:02	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	04:39:59	APAC_IN_PUN_PODIUM_ST2 DOOR 2 (YELLOW)	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	04:40:34	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	06:30:41	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	06:32:58	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	06:38:53	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	06:39:27	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	07:09:31	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	07:10:59	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 1-OUT DOOR	OutDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.


Here Manisha shift end ..beacause and above shift is from 06-09-2025  this date means all above swipe need to adjust in previous day ..
again manisha swipe theie badge after after 10 to 12 hr means below swipe is manisha start new shift..

so basically there are miltiple shift so we need to make shift logic like When Diffrance Between 2 swipe is greater than 6 hr then adjust shift logic and make duration correcr.

07-09-2025	21:58:51	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 1-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	22:00:48	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.
07-09-2025	22:01:22	APAC_IN_PUN_PODIUM_GSOC DOOR Restricted door	InDirection	615828	Contractor	APAC.Default	Pune - Business Bay	Poona Security India Pvt. Ltd.



keep Previous logic as it is .Only check shift when Duration meet less than 4 hr and duration meet greater than 20 hr 
only this time we need to check shift..

for Shift we add swipe Diffrance gap ..
and also check swipe whoes swipe came early like .. 12:30 am , 1 :00 am means this people are Working in shift ....


Fix this issue carefully.....


Check below each code line by line and fix the issue carefully
also we have ml folder in project root.
C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\ml
if issue is resolve using ml then use ml also 
we need to fix this logic..

#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py

@app.get("/duration")
async def api_duration(
    date_param: Optional[str] = Query(None, alias="date", description="Target date YYYY-MM-DD. Defaults to today in Asia/Kolkata"),
    start_date: Optional[str] = Query(None, description="Start date for a range (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date for a range (YYYY-MM-DD)"),
    regions: Optional[str] = Query(None, description="Comma-separated list: apac,emea,laca,namer. Default: all"),
    city: Optional[str] = Query(None, description="Optional city/location filter (e.g. Pune). Case-insensitive, matches PartitionName2/PrimaryLocation/Door/EmployeeName"),
    outdir: Optional[str] = Query(None, description="Output directory for CSVs. Defaults to OUTPUT_DIR/duration_reports"),
    sample_rows: int = Query(10, ge=0, le=200, description="How many sample rows to include per region in response"),
    compliance_target: int = Query(3, ge=1, le=7, description="Compliance target days (e.g. 3 or 5). Default 3")
):
    """
    Returns per-region duration aggregates and swipe-level details with weekly compliance & duration category splits.
    """
    try:
        # (parsing regions/date/outdir same as before) ...
        if regions:
            regions_list = [r.strip().lower() for r in regions.split(",") if r.strip()]
        else:
            regions_list = ["apac", "emea", "laca", "namer"]

        if outdir:
            outdir_path = Path(outdir)
        else:
            outdir_path = OUTPUT_DIR / "duration_reports"
        outdir_path.mkdir(parents=True, exist_ok=True)

        def _parse_date(s: str) -> date:
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                return date.fromisoformat(s)

        if start_date and end_date:
            start_obj = _parse_date(start_date)
            end_obj = _parse_date(end_date)
            if start_obj > end_obj:
                raise HTTPException(status_code=400, detail="start_date must be <= end_date")
            max_days = 92
            days_count = (end_obj - start_obj).days + 1
            if days_count > max_days:
                raise HTTPException(status_code=400, detail=f"Date range too large (> {max_days} days). Please request a smaller range.")
            date_list = [start_obj + timedelta(days=i) for i in range(days_count)]
            range_mode = True
        else:
            if date_param:
                target_date = _parse_date(date_param)
            else:
                tz = ZoneInfo("Asia/Kolkata")
                target_date = datetime.now(tz).date()
            date_list = [target_date]
            start_obj = end_obj = date_list[0]
            range_mode = False

        try:
            import duration_report
        except Exception as e:
            logger.exception("Failed importing duration_report module")
            raise HTTPException(status_code=500, detail=f"duration module import failed: {e}")

        loop = asyncio.get_running_loop()

        def _to_json_safe(v):
            try:
                if pd.isna(v):
                    return None
            except Exception:
                pass
            if isinstance(v, (datetime, date)):
                return v.isoformat()
            if hasattr(v, "isoformat") and not isinstance(v, str):
                try:
                    return v.isoformat()
                except Exception:
                    pass
            try:
                if isinstance(v, (int, float, bool)):
                    return v
            except Exception:
                pass
            try:
                return str(v)
            except Exception:
                return None

        per_date_results = {}
        for single_date in date_list:
            try:
                task = loop.run_in_executor(None, duration_report.run_for_date, single_date, regions_list, str(outdir_path), city)
                per_date_results[single_date.isoformat()] = await asyncio.wait_for(task, timeout=COMPUTE_WAIT_TIMEOUT_SECONDS)
            except asyncio.TimeoutError:
                raise HTTPException(status_code=504, detail=f"Duration computation timed out for date {single_date.isoformat()}")
            except Exception as e:
                logger.exception("duration run_for_date failed for date %s", single_date)
                raise HTTPException(status_code=500, detail=f"duration run failed for {single_date.isoformat()}: {e}")

        dates_iso = [d.isoformat() for d in date_list]
        date_objs = [d for d in date_list]
        week_starts = sorted({(d - timedelta(days=d.weekday())) for d in date_objs})

        resp: Dict[str, Any] = {
            "start_date": start_obj.isoformat(),
            "end_date": end_obj.isoformat(),
            "regions": {}
        }

        for r in regions_list:
            try:
                employees_map: Dict[str, Dict[str, Any]] = {}
                swipes_by_date: Dict[str, list] = {}
                date_rows = {}

                for iso_d, day_res in per_date_results.items():
                    region_obj = day_res.get(r) if isinstance(day_res, dict) else None
                    durations_df = None
                    swipes_df = None
                    if isinstance(region_obj, dict):
                        swipes_df = region_obj.get("swipes")
                        durations_df = region_obj.get("durations")
                    elif isinstance(region_obj, pd.DataFrame):
                        durations_df = region_obj

                    rows_count = int(len(durations_df)) if isinstance(durations_df, pd.DataFrame) else 0
                    swipe_count = int(len(swipes_df)) if isinstance(swipes_df, pd.DataFrame) else 0
                    date_rows[iso_d] = {"rows": rows_count, "swipe_rows": swipe_count}

                    if isinstance(swipes_df, pd.DataFrame) and not swipes_df.empty:
                        for c in ["LocaleMessageTime", "EmployeeName", "Door", "EmployeeID", "CardNumber", "PartitionName2", "PersonnelTypeName", "CompanyName", "PrimaryLocation", "Direction", "EmployeeIdentity", "MessageType", "Text5"]:
                            if c not in swipes_df.columns:
                                swipes_df[c] = None
                        swipe_records = []
                        for _, srow in swipes_df.iterrows():
                            dt = srow.get("LocaleMessageTime")
                            iso_ts = None
                            if pd.notna(dt):
                                try:
                                    iso_ts = pd.to_datetime(dt).isoformat()
                                except Exception:
                                    try:
                                        iso_ts = datetime.fromtimestamp(float(dt)).isoformat()
                                    except Exception:
                                        iso_ts = str(dt)
                            swipe_records.append({
                                "LocaleMessageTime": iso_ts,
                                "Dateonly": iso_d,
                                "Swipe_Time": (pd.to_datetime(srow.get("LocaleMessageTime")).time().isoformat() if pd.notna(srow.get("LocaleMessageTime")) else None),
                                "EmployeeID": _to_json_safe(srow.get("EmployeeID")),
                                "PersonGUID": _to_json_safe(srow.get("EmployeeIdentity") or srow.get("EmployeeIdentity")),
                                "ObjectName1": _to_json_safe(srow.get("EmployeeName")),
                                "Door": _to_json_safe(srow.get("Door")),
                                "PersonnelType": _to_json_safe(srow.get("PersonnelTypeName") or srow.get("PersonnelType")),
                                "CardNumber": _to_json_safe(srow.get("CardNumber")),
                                "Text5": _to_json_safe(srow.get("PrimaryLocation") or srow.get("Text5")),
                                "PartitionName2": _to_json_safe(srow.get("PartitionName2")),
                                "AdmitCode": _to_json_safe(srow.get("AdmitCode") or srow.get("MessageType")),
                                "Direction": _to_json_safe(srow.get("Direction")),
                                "CompanyName": _to_json_safe(srow.get("CompanyName")),
                                "PrimaryLocation": _to_json_safe(srow.get("PrimaryLocation") or srow.get("Text5")),
                            })
                        swipes_by_date[iso_d] = swipe_records
                    else:
                        swipes_by_date.setdefault(iso_d, [])

                    if isinstance(durations_df, pd.DataFrame) and not durations_df.empty:
                        for col in ["person_uid", "EmployeeID", "EmployeeName", "CardNumber", "Date",
                                    "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor",
                                    "Duration", "DurationSeconds", "PersonnelTypeName", "PartitionName2",
                                    "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"]:
                            if col not in durations_df.columns:
                                durations_df[col] = None

                        for _, drow in durations_df.iterrows():
                            person_uid = drow.get("person_uid")
                            if pd.isna(person_uid) or person_uid is None or str(person_uid).strip() == "":
                                person_uid = f"{_to_json_safe(drow.get('EmployeeID'))}|{_to_json_safe(drow.get('EmployeeName'))}"

                            if person_uid not in employees_map:
                                employees_map[person_uid] = {
                                    "person_uid": person_uid,
                                    "EmployeeID": _to_json_safe(drow.get("EmployeeID")),
                                    "EmployeeName": _to_json_safe(drow.get("EmployeeName")),
                                    "CardNumber": _to_json_safe(drow.get("CardNumber")),
                                    "durations": {d: None for d in dates_iso},
                                    "durations_seconds": {d: None for d in dates_iso},
                                    "total_seconds_present_in_range": 0,
                                    # keep internal First/Last but we'll remove them before returning
                                    "FirstSwipe": None,
                                    "LastSwipe": None,
                                    "FirstDoor": _to_json_safe(drow.get("FirstDoor")),
                                    "LastDoor": _to_json_safe(drow.get("LastDoor")),
                                    "PersonnelType": _to_json_safe(drow.get("PersonnelTypeName") or drow.get("PersonnelType")),
                                    "PartitionName2": _to_json_safe(drow.get("PartitionName2")),
                                    "CompanyName": _to_json_safe(drow.get("CompanyName")),
                                    "PrimaryLocation": _to_json_safe(drow.get("PrimaryLocation")),
                                    "FirstDirection": _to_json_safe(drow.get("FirstDirection")),
                                    "LastDirection": _to_json_safe(drow.get("LastDirection")),
                                }

                            dur_str = None if pd.isna(drow.get("Duration")) else str(drow.get("Duration"))
                            dur_secs = None
                            try:
                                v = drow.get("DurationSeconds")
                                if pd.notna(v):
                                    dur_secs = int(float(v))
                            except Exception:
                                dur_secs = None

                            employees_map[person_uid]["durations"][iso_d] = dur_str
                            employees_map[person_uid]["durations_seconds"][iso_d] = dur_secs
                            if dur_secs is not None:
                                employees_map[person_uid]["total_seconds_present_in_range"] += dur_secs

                            try:
                                fs = drow.get("FirstSwipe")
                                ls = drow.get("LastSwipe")
                                if pd.notna(fs):
                                    fs_dt = pd.to_datetime(fs)
                                    cur_fs = employees_map[person_uid].get("FirstSwipe")
                                    if cur_fs is None:
                                        employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                    else:
                                        if pd.to_datetime(cur_fs) > fs_dt:
                                            employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                if pd.notna(ls):
                                    ls_dt = pd.to_datetime(ls)
                                    cur_ls = employees_map[person_uid].get("LastSwipe")
                                    if cur_ls is None:
                                        employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                                    else:
                                        if pd.to_datetime(cur_ls) < ls_dt:
                                            employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                            except Exception:
                                pass

                emp_list = list(employees_map.values())
                emp_list.sort(key=lambda x: ((x.get("EmployeeName") or "") or "").lower())

                # compute per-employee weekly compliance and categories
                for emp in emp_list:
                    weeks_info = {}
                    weeks_met = 0
                    weeks_total = 0

                    cat_counts = {"0-30m": 0, "30m-2h": 0, "2h-6h": 0, "6h-8h": 0, "8h+": 0}
                    cat_dates = {k: [] for k in cat_counts.keys()}

                    for ws in week_starts:
                        week_start_iso = ws.isoformat()
                        week_dates = [(ws + timedelta(days=i)).isoformat() for i in range(7)]
                        relevant_dates = [d for d in week_dates if d in dates_iso]
                        if not relevant_dates:
                            continue

                        days_present = 0
                        days_ge8 = 0
                        per_date_durations = {}
                        per_date_compliance = {}

                        for d in relevant_dates:
                            secs = emp["durations_seconds"].get(d)
                            per_date_durations[d] = secs
                            if secs is not None and secs > 0:
                                days_present += 1
                            # day compliant = >= 8h
                            is_ge8 = (secs is not None and secs >= 28800)
                            if is_ge8:
                                days_ge8 += 1
                            per_date_compliance[d] = True if is_ge8 else False

                            # COUNT categories only when present (>0)
                            if secs is not None and secs > 0:
                                cat = duration_report.categorize_seconds(secs) if hasattr(duration_report, 'categorize_seconds') else "0-30m"
                                if cat in cat_counts:
                                    cat_counts[cat] += 1
                                    cat_dates[cat].append(d)

                        ct = int(compliance_target or 3)
                        compliant = False
                        if days_present >= ct:
                            if ct == 3:


                                if days_present == 3:
                                     compliant = (days_ge8 == days_present)
                                else:
                                    compliant = False

                        weeks_info[week_start_iso] = {
                            "week_start": week_start_iso,
                             "dates": per_date_durations,
                             "dates_compliance": per_date_compliance,
                             "days_present": days_present,
                             "days_ge8": days_ge8,
                             "compliant": compliant
                        }

                        weeks_total += 1
                        if compliant:
                            weeks_met += 1

                            

                    # dominant category: choose category with highest count (ties resolved by first encountered)
                    dominant_category = None
                    max_count = -1
                    for k, v in cat_counts.items():
                        if v > max_count:
                            max_count = v
                            dominant_category = k

                    # Remove internal swipe fields from returned object (you requested not to return them)
                    for _k in ("FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor"):
                        if _k in emp:
                            try:
                                del emp[_k]
                            except Exception:
                                pass

                    emp["compliance"] = {
                        "weeks": weeks_info,
                        "weeks_met": weeks_met,
                        "weeks_total": weeks_total,
                        "month_summary": f"{weeks_met}/{weeks_total}" if weeks_total > 0 else "0/0",
                        "compliance_target": int(compliance_target or 3)
                    }
                    emp["duration_categories"] = {
                        "counts": cat_counts,
                        "dominant_category": dominant_category,
                        "category_dates": cat_dates,
                        "red_flag": cat_counts.get("2h-6h", 0)
                    }

                durations_sample = emp_list[:sample_rows] if sample_rows and sample_rows > 0 else []

                resp["regions"][r] = {
                    "dates": dates_iso,
                    "employees": emp_list,
                    "durations_sample": durations_sample,
                    "date_rows": date_rows,
                    "swipes_by_date": swipes_by_date
                }
            except Exception:
                logger.exception("Failed to aggregate range results for region %s", r)
                resp["regions"][r] = {"dates": [d.isoformat() for d in date_list], "employees": [], "durations_sample": [], "date_rows": {}, "swipes_by_date": {}}

        return JSONResponse(resp)
    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("api_duration (range) failed")
        raise HTTPException(status_code=500, detail=f"duration api error: {exc}")









# duration_report.py
"""
duration_report.py

Updated: supports querying multiple monthly ACVSUJournal databases (current + previous N).
Fixes: category counting only for present days (so dominant category is correct).
"""
import argparse
import logging
import os
import re
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd

# Optional: import pyodbc only when connecting (allows importing this module even without driver)
try:
    import pyodbc
except Exception:
    pyodbc = None

ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# (REGION_CONFIG unchanged - omitted here for brevity in inline view; keep your config)
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "partitions": [
            "APAC.Default", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur"
        ]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "database": "ACVSUJournal_00011028",
        "last_n_databases": 2,
        "partitions": [
            "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
            "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
        ]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "partitions": [
            "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
            "PA.Panama City", "PE.Lima"
        ]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# GENERIC_SQL_TEMPLATE and DB helper functions remain unchanged.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
INNER JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
INNER JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
"""

# helper functions (split name, expand DB list, connect master, filter existing dbs)
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# DB connection helper (same as before)
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "CardNumber", "PersonnelTypeName", "EmployeeIdentity",
        "PartitionName2", "LocaleMessageTime", "MessageType", "Direction", "CompanyName", "PrimaryLocation"
    ]
    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = get_connection(region_key)
    try:
        df = pd.read_sql(sql, conn)
    finally:
        conn.close()

    for c in cols:
        if c not in df.columns:
            df[c] = None

    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    return df[cols]

def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    if df["LocaleMessageTime"].dtype == object:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    df = df.drop_duplicates(subset=dedupe_cols, keep="first")

    df["Date"] = df["LocaleMessageTime"].dt.date

    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    df["person_uid"] = df.apply(make_person_uid, axis=1)
    df = df[df["person_uid"].notna()].copy()

    def agg_for_group(g):
        g_sorted = g.sort_values("LocaleMessageTime")
        first = g_sorted.iloc[0]
        last = g_sorted.iloc[-1]
        first_dir = first.get("Direction")
        last_dir = last.get("Direction")

        return pd.Series({
            "person_uid": first["person_uid"],
            "EmployeeIdentity": first.get("EmployeeIdentity"),
            "EmployeeID": first.get("EmployeeID"),
            "EmployeeName": first.get("EmployeeName"),
            "CardNumber": first.get("CardNumber"),
            "Date": first["Date"],
            "FirstSwipe": first["LocaleMessageTime"],
            "LastSwipe": last["LocaleMessageTime"],
            "FirstDoor": first.get("Door"),
            "LastDoor": last.get("Door"),
            "CountSwipes": int(len(g_sorted)),
            "PersonnelTypeName": first.get("PersonnelTypeName"),
            "PartitionName2": first.get("PartitionName2"),
            "CompanyName": first.get("CompanyName"),
            "PrimaryLocation": first.get("PrimaryLocation"),
            "FirstDirection": first_dir,
            "LastDirection": last_dir
        })

    grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    return grouped[out_cols]

def categorize_seconds(s: Optional[int]) -> str:
    """
    Category labels (used when secs present > 0):
      - "0-30m"      -> 0 .. 1800 (inclusive)
      - "30m-2h"     -> 1801 .. 7200
      - "2h-6h"      -> 7201 .. 21600
      - "6h-8h"      -> 21601 .. 28800
      - "8h+"        -> >= 28800
    Note: this helper alone will still return "0-30m" for s None/0, but in counting code we
    only increment when s is present and >0.
    """
    try:
        if s is None or s <= 0:
            return "0-30m"
        s = int(s)
        if s <= 1800:
            return "0-30m"
        if s <= 7200:
            return "30m-2h"
        if s <= 21600:
            return "2h-6h"
        if s < 28800:
            return "6h-8h"
        return "8h+"
    except Exception:
        return "0-30m"

def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        try:
            swipes = fetch_swipes_for_region(r, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", r)
            swipes = pd.DataFrame()

        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                swipes = swipes[combined_mask].copy()
            else:
                logging.debug("City filter provided but no matching columns to filter on for region %s", r)

        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", r)
            durations = pd.DataFrame()

        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
        try:
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", r)
        try:
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", r)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
        results[r] = {"swipes": swipes, "durations": durations}

    return results

# CLI runner (unchanged)
def parse_args():
    p = argparse.ArgumentParser(description="Generate daily duration report from ACVSUJournal swipe logs.")
    p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
    p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
                   default="apac,emea,laca,namer")
    p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
    p.add_argument("--city", help="Optional city/location filter (e.g. Pune)", default=None)
    return p.parse_args()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    args = parse_args()

    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    else:
        tz = ZoneInfo("Asia/Kolkata")
        target_date = datetime.now(tz).date()

    regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
    outdir = args.outdir

    logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
    results = run_for_date(target_date, regions, outdir, city=args.city)

    for r, obj in results.items():
        dur = obj.get("durations")
        logging.info("Region %s: %d persons with computed durations", r, len(dur) if dur is not None else 0)
    logging.info("Completed. CSVs are in %s", Path(outdir).absolute())








// frontend/src/pages/DurationPage.jsx
import React, { useState, useMemo } from "react";
import axios from "axios";
import {
  Box,
  Grid,
  Paper,
  Typography,
  TextField,
  Button,
  Table,
  TableHead,
  TableRow,
  TableCell,
  TableBody,
  TableContainer,
  CircularProgress,
  IconButton,
  Tooltip,
  Card,
  CardContent,
  Dialog,
  DialogTitle,
  DialogContent,
  Divider,
} from "@mui/material";
import DateRangeIcon from "@mui/icons-material/DateRange";
import SearchIcon from "@mui/icons-material/Search";
import DownloadIcon from "@mui/icons-material/CloudDownload";
import ClearIcon from "@mui/icons-material/Clear";
import VisibilityIcon from "@mui/icons-material/Visibility";

const API_BASE = import.meta.env.VITE_API_BASE || import.meta.env.REACT_APP_API_BASE || "http://localhost:8000";

const REGIONS = [
  { value: "apac", label: "APAC" },
  { value: "emea", label: "EMEA" },
  { value: "laca", label: "LACA" },
  { value: "namer", label: "NAMER" },
];

function secondsToHMS(s) {
  if (s == null) return "";
  const sec = Number(s);
  if (!Number.isFinite(sec)) return "";
  const h = Math.floor(sec / 3600);
  const m = Math.floor((sec % 3600) / 60);
  const r = Math.floor(sec % 60);
  return `${h}:${String(m).padStart(2, "0")}:${String(r).padStart(2, "0")}`;
}

function isoToDDMMYYYY(iso) {
  if (!iso) return iso;
  const dt = new Date(iso.includes("T") ? iso : `${iso}T00:00:00Z`);
  if (Number.isNaN(dt.getTime())) return iso;
  const dd = String(dt.getUTCDate()).padStart(2, "0");
  const mm = String(dt.getUTCMonth() + 1).padStart(2, "0");
  const yyyy = dt.getUTCFullYear();
  return `${dd}-${mm}-${yyyy}`;
}

function isoToLongDateNoCommas(iso) {
  // produce a header string without internal commas to avoid CSV splitting
  if (!iso) return iso;
  const dt = new Date(iso.includes("T") ? iso : `${iso}T00:00:00Z`);
  if (Number.isNaN(dt.getTime())) return iso;
  const weekday = dt.toLocaleDateString(undefined, { weekday: "short" }); // Mon/Tue...
  const day = String(dt.getUTCDate()).padStart(2, "0");
  const month = String(dt.getUTCMonth() + 1).padStart(2, "0");
  const year = dt.getUTCFullYear();
  return `${weekday} ${day}-${month}-${year}`; // no commas
}

export default function DurationPage() {
  const [region, setRegion] = useState("apac");
  const [city, setCity] = useState("");
  const [startDate, setStartDate] = useState("");
  const [endDate, setEndDate] = useState("");
  const [singleDate, setSingleDate] = useState("");
  const [useRange, setUseRange] = useState(true);

  const [data, setData] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState("");

  const [searchEmployeeId, setSearchEmployeeId] = useState("");
  const [searchEmployeeName, setSearchEmployeeName] = useState("");
  const [searchCardNumber, setSearchCardNumber] = useState("");

  const [swipeDialogOpen, setSwipeDialogOpen] = useState(false);
  const [selectedEmployee, setSelectedEmployee] = useState(null);
  const [selectedSwipes, setSelectedSwipes] = useState([]);

  // --- fetch durations ---
  const fetchDurations = async () => {
    setError("");
    setLoading(true);
    setData(null);

    try {
      const params = {};
      if (useRange && startDate && endDate) {
        params.start_date = startDate;
        params.end_date = endDate;
      } else if (!useRange && singleDate) {
        params.date = singleDate;
      } else {
        if (singleDate) params.date = singleDate;
      }

      if (region) params.regions = region;
      if (city) params.city = city;

      const res = await axios.get(`${API_BASE}/duration`, { params, timeout: 12000000 });
      setData(res.data);
    } catch (err) {
      console.error(err);
      setError(err?.response?.data?.detail || err.message || "Failed to fetch duration data");
    } finally {
      setLoading(false);
    }
  };

  const regionObj = useMemo(() => {
    if (!data || !region) return null;
    return data.regions?.[region] || null;
  }, [data, region]);

  // helpers to compute week starts from regionObj.dates
  const computeWeekStarts = (datesIso) => {
    if (!datesIso || datesIso.length === 0) return [];
    const dateObjs = datesIso.map(d => new Date(d + "T00:00:00Z"));
    const weekStartSet = new Set();
    dateObjs.forEach(dt => {
      const day = dt.getUTCDay(); // 0 Sun..6 Sat
      const diff = (day + 6) % 7; // days since Monday
      const monday = new Date(dt);
      monday.setUTCDate(dt.getUTCDate() - diff);
      weekStartSet.add(monday.toISOString().slice(0,10));
    });
    return Array.from(weekStartSet).sort();
  };

  // --- CSV exports ---

  const quote = (s) => `"${String(s ?? "").replace(/"/g, '""')}"`;

  const exportSummaryCsv = () => {
    if (!regionObj) return;
    const dates = regionObj.dates || [];
    const rows = regionObj.employees || [];

    const header = ["EmployeeID", "EmployeeName", "TotalSecondsPresentInRange", "DominantCategory", "ComplianceSummary", ...dates.map(d => d)];
    const csvRows = [header.map(h => quote(h)).join(",")];

    rows.forEach((r) => {
      const complianceText = r.compliance?.month_summary || "";
      const complianceCell = `'${complianceText}`; // force text in Excel
      const row = [
        quote(r.EmployeeID || ""),
        quote(r.EmployeeName || ""),
        r.total_seconds_present_in_range ?? "",
        quote(r.duration_categories?.dominant_category || ""),
        quote(complianceCell),
        ...dates.map(d => quote(r.durations?.[d] ?? ""))
      ];
      csvRows.push(row.join(","));
    });

    const blob = new Blob([csvRows.join("\n")], { type: "text/csv;charset=utf-8;" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    const filename = `duration_summary_${region}_${data?.start_date || ""}_to_${data?.end_date || ""}.csv`;
    a.download = filename;
    document.body.appendChild(a);
    a.click();
    a.remove();
    URL.revokeObjectURL(url);
  };







 const exportReportCsv = () => {
    if (!regionObj) return;
    const datesIso = regionObj.dates || [];
    const weekStarts = computeWeekStarts(datesIso);

    // base header
    const baseHeader = [
      "Sr.No", "EmployeeID", "EmployeeName", "CardNumber", "PersonnelType", "PartitionName2", "TotalSecondsPresentInRange"
    ];

    // per-day headers (Mon..Sun for each week) - use a no-comma format
    const perDayHeaders = [];
    weekStarts.forEach(ws => {
      for (let i = 0; i < 7; i++) {
        const d = new Date(ws + "T00:00:00Z");
        d.setUTCDate(d.getUTCDate() + i);
        const iso = d.toISOString().slice(0,10);
        perDayHeaders.push(isoToLongDateNoCommas(iso));
      }
    });

    // per-week compliance headers
    const perWeekHeaders = weekStarts.map(ws => `Week compliance ${ws}`);

    const header = [...baseHeader, ...perDayHeaders, ...perWeekHeaders, "DominantCategory", "ComplianceSummary"];
    const csvRows = [header.map(h => quote(h)).join(",")];

    const rows = regionObj.employees || [];
    rows.forEach((r, idx) => {
      const srNo = idx + 1;
      const employeeId = r.EmployeeID ?? "";
      const employeeName = r.EmployeeName ?? "";
      const cardNumber = r.CardNumber ?? "";
      const personnelType = r.PersonnelType ?? r.PersonnelTypeName ?? "";
      const partition = r.PartitionName2 ?? "";
      const totalSeconds = r.total_seconds_present_in_range ?? "";

      // per-day durations (string) or "0" when absent
      const perDayVals = [];
      weekStarts.forEach(ws => {
        for (let i = 0; i < 7; i++) {
          const d = new Date(ws + "T00:00:00Z");
          d.setUTCDate(d.getUTCDate() + i);
          const iso = d.toISOString().slice(0,10);

          // prefer API-provided week map if present
          const wk = r.compliance && r.compliance.weeks ? r.compliance.weeks[ws] : null;
          let outVal = "0";

          if (wk && wk.dates && Object.prototype.hasOwnProperty.call(wk.dates, iso)) {
            const secs = wk.dates[iso];
            if (secs !== null && secs !== undefined) {
              // prefer already formatted string if available
              outVal = (r.durations && r.durations[iso]) ? r.durations[iso] : secondsToHMS(secs);
            } else {
              outVal = "0";
            }
          } else if (r.durations && Object.prototype.hasOwnProperty.call(r.durations, iso) && r.durations[iso]) {
            outVal = r.durations[iso];
          } else if (r.durations_seconds && Object.prototype.hasOwnProperty.call(r.durations_seconds, iso) && r.durations_seconds[iso]) {
            outVal = secondsToHMS(r.durations_seconds[iso]);
          } else {
            outVal = "0";
          }

          perDayVals.push(outVal);
        }
      });

      // per-week compliance Yes/No
      const perWeekVals = weekStarts.map(ws => {
        const wk = r.compliance && r.compliance.weeks ? r.compliance.weeks[ws] : null;
        return wk && wk.compliant ? "Yes" : "No";
      });

      const complianceText = r.compliance?.month_summary || "";
      const complianceCell = `'${complianceText}`;

      const row = [
        `${srNo}`,
        quote(employeeId),
        quote(employeeName),
        quote(cardNumber),
        quote(personnelType),
        quote(partition),
        `${totalSeconds}`,
        ...perDayVals.map(v => quote(v)),
        ...perWeekVals.map(v => quote(v)),
        quote(r.duration_categories?.dominant_category || ""),
        quote(complianceCell)
      ];

      csvRows.push(row.join(","));
    });

    const blob = new Blob([csvRows.join("\n")], { type: "text/csv;charset=utf-8;" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    const filename = `duration_report_${region}_${data?.start_date || ""}_to_${data?.end_date || ""}.csv`;
    a.download = filename;
    document.body.appendChild(a);
    a.click();
    a.remove();
    URL.revokeObjectURL(url);
  };





  const exportSwipesCsv = () => {
    if (!regionObj) return;
    const swipesByDate = regionObj.swipes_by_date || {};
    const rows = [];
    Object.entries(swipesByDate).forEach(([d, arr]) => {
      arr.forEach(s => {
        rows.push({
          Date: d,
          LocaleMessageTime: s.LocaleMessageTime || "",
          Swipe_Time: s.Swipe_Time || "",
          EmployeeID: s.EmployeeID || "",
          PersonGUID: s.PersonGUID || "",
          ObjectName1: s.ObjectName1 || "",
          Door: s.Door || "",
          PersonnelType: s.PersonnelType || "",
          CardNumber: s.CardNumber || "",
          PrimaryLocation: s.PrimaryLocation || s.Text5 || "",
          PartitionName2: s.PartitionName2 || "",
          AdmitCode: s.AdmitCode || "",
          Direction: s.Direction || "",
          CompanyName: s.CompanyName || ""
        });
      });
    });

    if (rows.length === 0) {
      alert("No swipe rows available for current selection to export.");
      return;
    }

    const headers = [
      "Date",
      "LocaleMessageTime",
      "Swipe_Time",
      "EmployeeID",
      "PersonGUID",
      "ObjectName1",
      "Door",
      "PersonnelType",
      "CardNumber",
      "PrimaryLocation",
      "PartitionName2",
      "AdmitCode",
      "Direction",
      "CompanyName"
    ];
    const csvRows = [headers.map(h => quote(h)).join(",")];
    rows.forEach(r => {
      const line = headers.map(h => quote(r[h] ?? "")).join(",");
      csvRows.push(line);
    });

    const blob = new Blob([csvRows.join("\n")], { type: "text/csv;charset=utf-8;" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    const filename = `swipes_${region}_${data?.start_date || ""}_to_${data?.end_date || ""}.csv`;
    a.download = filename;
    document.body.appendChild(a);
    a.click();
    a.remove();
    URL.revokeObjectURL(url);
  };

  // --- Render helpers ---
  const getFilteredRows = () => {
    if (!regionObj) return [];
    const rows = regionObj.employees || [];
    return rows.filter(r => {
      const matchesEmployeeId = !searchEmployeeId || String(r.EmployeeID || "").toLowerCase().includes(searchEmployeeId.toLowerCase());
      const matchesName = !searchEmployeeName || String(r.EmployeeName || "").toLowerCase().includes(searchEmployeeName.toLowerCase());
      const matchesCard = !searchCardNumber || String(r.CardNumber || "").toLowerCase().includes(searchCardNumber.toLowerCase());
      return matchesEmployeeId && matchesName && matchesCard;
    });
  };

  const openSwipeDialogFor = (emp) => {
    setSelectedEmployee(emp);
    const swipesByDate = regionObj?.swipes_by_date || {};
    const matches = [];
    Object.entries(swipesByDate).forEach(([d, arr]) => {
      arr.forEach(s => {
        const matchByEmployeeId = emp.EmployeeID && s.EmployeeID && String(s.EmployeeID) === String(emp.EmployeeID);
        const matchByCard = emp.CardNumber && s.CardNumber && String(s.CardNumber) === String(emp.CardNumber);
        const matchByPersonGuid = emp.person_uid && s.PersonGUID && String(s.PersonGUID) === String(emp.person_uid);
        if (matchByEmployeeId || matchByCard || matchByPersonGuid) {
          matches.push({ ...s, Date: d });
        }
      });
    });
    matches.sort((a,b) => {
      const ta = a.LocaleMessageTime ? new Date(a.LocaleMessageTime).getTime() : 0;
      const tb = b.LocaleMessageTime ? new Date(b.LocaleMessageTime).getTime() : 0;
      return ta - tb;
    });

    setSelectedSwipes(matches);
    setSwipeDialogOpen(true);
  };

  const renderTable = () => {
    if (!regionObj) return <Typography>No data for selected region.</Typography>;

    const dates = regionObj.dates || [];
    const rows = getFilteredRows();

    const weekStarts = computeWeekStarts(dates);

    return (
      <TableContainer component={Paper} sx={{ mt: 2, width: "100%", overflowX: "auto" }}>
        <Table size="small" stickyHeader>
          <TableHead>
            <TableRow>
              <TableCell><b>Sr.No</b></TableCell>
              <TableCell><b>EmployeeID</b></TableCell>
              <TableCell><b>EmployeeName</b></TableCell>
              <TableCell><b>CardNumber</b></TableCell>
              <TableCell><b>PersonnelType</b></TableCell>
              <TableCell><b>PartitionName2</b></TableCell>
              <TableCell align="right"><b>Total (hh:mm:ss)</b></TableCell>
              {dates.map((d) => (
                <TableCell key={d} align="center"><b>{isoToDDMMYYYY(d)}</b></TableCell>
              ))}
              {/* add week-level compliance columns */}
              {weekStarts.map(ws => (
                <TableCell key={ws} align="center"><b>{`Week ${ws} compliant`}</b></TableCell>
              ))}
              <TableCell align="center"><b>Dominant Category</b></TableCell>
              <TableCell align="center"><b>Compliance (weeks met/total)</b></TableCell>
              <TableCell align="center"><b>View</b></TableCell>
            </TableRow>
          </TableHead>
          <TableBody>
            {rows.length === 0 ? (
              <TableRow>
                <TableCell colSpan={9 + (regionObj.dates || []).length + weekStarts.length} align="center">No employees in the response.</TableCell>
              </TableRow>
            ) : (
              rows.map((r, idx) => (
                <TableRow key={r.person_uid || `${r.EmployeeID}-${r.EmployeeName}`}>
                  <TableCell>{idx + 1}</TableCell>
                  <TableCell>{r.EmployeeID || "-"}</TableCell>
                  <TableCell>{r.EmployeeName || "-"}</TableCell>
                  <TableCell>{r.CardNumber || "-"}</TableCell>
                  <TableCell>{r.PersonnelType || r.PersonnelTypeName || "-"}</TableCell>
                  <TableCell>{r.PartitionName2 || "-"}</TableCell>
                  <TableCell align="right">{secondsToHMS(r.total_seconds_present_in_range)}</TableCell>
                  {dates.map((d) => (
                    <TableCell key={d} align="center">{r.durations?.[d] ?? "-"}</TableCell>
                  ))}
                  {weekStarts.map(ws => {
                    const wk = r.compliance && r.compliance.weeks ? r.compliance.weeks[ws] : null;
                    return <TableCell key={ws} align="center">{wk && wk.compliant ? "Yes" : "No"}</TableCell>;
                  })}
                  <TableCell align="center">
                    {r.duration_categories?.dominant_category || "-"}
                    {r.duration_categories?.red_flag > 0 ? " " : ""}
                  </TableCell>
                  <TableCell align="center">{r.compliance?.month_summary || "-"}</TableCell>
                  <TableCell align="center">
                    <Tooltip title="View swipe records for this employee">
                      <IconButton size="small" onClick={() => openSwipeDialogFor(r)}>
                        <VisibilityIcon />
                      </IconButton>
                    </Tooltip>
                  </TableCell>
                </TableRow>
              ))
            )}
          </TableBody>
        </Table>
      </TableContainer>
    );
  };

  // --- JSX UI (top controls) ---
  return (
    <Box sx={{ p: 3, width: "100%", maxWidth: "100vw", boxSizing: "border-box" }}>
      <Typography variant="h5" gutterBottom>
        Duration Reports (with Compliance & Category)
      </Typography>

      <Grid container spacing={2}>
        <Grid item xs={12}>
          <Card>
            <CardContent>
              <Grid container spacing={2} alignItems="center">
                <Grid item xs={12} md={2}>
                  <TextField
                    select
                    fullWidth
                    label="Region"
                    value={region}
                    onChange={(e) => setRegion(e.target.value)}
                  >
                    {REGIONS.map((r) => (
                      <option key={r.value} value={r.value}>{r.label}</option>
                    ))}
                  </TextField>
                </Grid>

                <Grid item xs={12} md={3}>
                  <TextField
                    fullWidth
                    label="City / Partition (optional)"
                    placeholder="e.g. CR.Costa Rica Partition"
                    value={city}
                    onChange={(e) => setCity(e.target.value)}
                  />
                </Grid>

                <Grid item xs={12} md={5} sx={{ display: "flex", gap: 1, justifyContent: "flex-end" }}>
                  <Button
                    startIcon={<DateRangeIcon />}
                    variant={useRange ? "contained" : "outlined"}
                    onClick={() => setUseRange(true)}
                  >
                    Range
                  </Button>
                  <Button
                    startIcon={<DateRangeIcon />}
                    variant={!useRange ? "contained" : "outlined"}
                    onClick={() => setUseRange(false)}
                  >
                    Single Day
                  </Button>
                </Grid>

                {useRange ? (
                  <>
                    <Grid item xs={12} md={3}>
                      <TextField
                        label="Start date"
                        type="date"
                        fullWidth
                        InputLabelProps={{ shrink: true }}
                        value={startDate}
                        onChange={(e) => setStartDate(e.target.value)}
                      />
                    </Grid>
                    <Grid item xs={12} md={3}>
                      <TextField
                        label="End date"
                        type="date"
                        fullWidth
                        InputLabelProps={{ shrink: true }}
                        value={endDate}
                        onChange={(e) => setEndDate(e.target.value)}
                      />
                    </Grid>
                  </>
                ) : (
                  <Grid item xs={12} md={3}>
                    <TextField
                      label="Date"
                      type="date"
                      fullWidth
                      InputLabelProps={{ shrink: true }}
                      value={singleDate}
                      onChange={(e) => setSingleDate(e.target.value)}
                    />
                  </Grid>
                )}

                <Grid item xs={12} md={6} sx={{ display: "flex", gap: 1, alignItems: "center" }}>
                  <Button
                    variant="contained"
                    startIcon={<SearchIcon />}
                    onClick={fetchDurations}
                    disabled={loading}
                  >
                    {loading ? "Loading..." : "Run"}
                  </Button>

                  <Button
                    variant="outlined"
                    startIcon={<ClearIcon />}
                    onClick={() => {
                      setStartDate("");
                      setEndDate("");
                      setSingleDate("");
                      setCity("");
                      setData(null);
                      setError("");
                    }}
                  >
                    Clear
                  </Button>

                  <Tooltip title="Export Summary CSV (per-person)">
                    <span>
                      <IconButton onClick={exportSummaryCsv} disabled={!regionObj || (regionObj.employees || []).length === 0}>
                        <DownloadIcon />
                      </IconButton>
                    </span>
                  </Tooltip>

                  <Tooltip title="Export full report (Sr.No, ... Compliance)">
                    <span>
                      <Button variant="contained" onClick={exportReportCsv} disabled={!regionObj || (regionObj.employees || []).length === 0}>
                        Export Report
                      </Button>
                    </span>
                  </Tooltip>

                  <Tooltip title="Export raw swipes for this region/date range">
                    <span>
                      <Button variant="outlined" onClick={exportSwipesCsv} disabled={!regionObj || Object.keys(regionObj.swipes_by_date || {}).length === 0}>
                        Export Swipes
                      </Button>
                    </span>
                  </Tooltip>
                </Grid>

                <Grid item xs={12} md={4}>
                  <TextField
                    fullWidth
                    label="Search Employee ID"
                    value={searchEmployeeId}
                    onChange={(e) => setSearchEmployeeId(e.target.value)}
                    size="small"
                  />
                </Grid>
                <Grid item xs={12} md={4}>
                  <TextField
                    fullWidth
                    label="Search Employee Name"
                    value={searchEmployeeName}
                    onChange={(e) => setSearchEmployeeName(e.target.value)}
                    size="small"
                  />
                </Grid>
                <Grid item xs={12} md={4}>
                  <TextField
                    fullWidth
                    label="Search Card Number"
                    value={searchCardNumber}
                    onChange={(e) => setSearchCardNumber(e.target.value)}
                    size="small"
                  />
                </Grid>

              </Grid>
            </CardContent>
          </Card>
        </Grid>

        <Grid item xs={12}>
          <Paper sx={{ p: 2 }}>
            <Box sx={{ display: "flex", justifyContent: "space-between", alignItems: "center" }}>
              <Typography variant="subtitle1">
                {data ? `Showing ${region.toUpperCase()}  ${data.start_date}  ${data.end_date}` : "No results yet"}
              </Typography>
              <Typography variant="caption" color="text.secondary">
                Tip: use the CSV export for offline analysis
              </Typography>
            </Box>

            {loading && (
              <Box sx={{ display: "flex", justifyContent: "center", py: 4 }}>
                <CircularProgress />
              </Box>
            )}

            {error && (
              <Typography color="error" sx={{ mt: 2 }}>
                {error}
              </Typography>
            )}

            {!loading && !error && (
              <Box sx={{ mt: 2 }}>
                {renderTable()}
              </Box>
            )}
          </Paper>
        </Grid>
      </Grid>

      {/* Swipe dialog */}
      <Dialog open={swipeDialogOpen} onClose={() => setSwipeDialogOpen(false)} fullWidth maxWidth="xl">
        <DialogTitle>
          Swipe records for: {selectedEmployee ? `${selectedEmployee.EmployeeID || ""}  ${selectedEmployee.EmployeeName || ""}` : ""}
        </DialogTitle>
        <DialogContent dividers>
          {selectedSwipes.length === 0 ? (
            <Typography>No swipe records found for this employee in the selected range.</Typography>
          ) : (
            <Table size="small">
              <TableHead>
                <TableRow>
                  <TableCell><b>Date</b></TableCell>
                  <TableCell><b>Time (local)</b></TableCell>
                  <TableCell><b>Door</b></TableCell>
                  <TableCell><b>Direction</b></TableCell>
                  <TableCell><b>CardNumber</b></TableCell>
                  <TableCell><b>PersonnelType</b></TableCell>
                  <TableCell><b>Partition</b></TableCell>
                  <TableCell><b>PrimaryLocation</b></TableCell>
                  <TableCell><b>Company</b></TableCell>
                </TableRow>
              </TableHead>
              <TableBody>
                {selectedSwipes.map((s, i) => (
                  <TableRow key={i}>
                    <TableCell>{s.Date}</TableCell>
                    <TableCell>{s.Swipe_Time ?? (s.LocaleMessageTime ? new Date(s.LocaleMessageTime).toLocaleString() : "-")}</TableCell>
                    <TableCell>{s.Door || s.ObjectName1 || "-"}</TableCell>
                    <TableCell>{s.Direction || "-"}</TableCell>
                    <TableCell>{s.CardNumber || "-"}</TableCell>
                    <TableCell>{s.PersonnelType || "-"}</TableCell>
                    <TableCell>{s.PartitionName2 || "-"}</TableCell>
                    <TableCell>{s.PrimaryLocation || s.Text5 || "-"}</TableCell>
                    <TableCell>{s.CompanyName || "-"}</TableCell>
                  </TableRow>
                ))}
              </TableBody>
            </Table>
          )}
        </DialogContent>
        <Divider />
        <Box sx={{ p: 1, display: "flex", justifyContent: "flex-end" }}>
          <Button onClick={() => setSwipeDialogOpen(false)}>Close</Button>
        </Box>
      </Dialog>
    </Box>
  );
}












@app.get("/duration")
async def api_duration(
    date_param: Optional[str] = Query(None, alias="date", description="Target date YYYY-MM-DD. Defaults to today in Asia/Kolkata"),
    start_date: Optional[str] = Query(None, description="Start date for a range (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date for a range (YYYY-MM-DD)"),
    regions: Optional[str] = Query(None, description="Comma-separated list: apac,emea,laca,namer. Default: all"),
    city: Optional[str] = Query(None, description="Optional city/location filter (e.g. Pune). Case-insensitive"),
    outdir: Optional[str] = Query(None, description="Output directory for CSVs. Defaults to OUTPUT_DIR/duration_reports"),
    sample_rows: int = Query(10, ge=0, le=200, description="How many sample rows to include per region in response"),
    compliance_target: int = Query(3, ge=1, le=7, description="Compliance target days (e.g. 3 or 5). Default 3")
):
    """
    Returns per-region duration aggregates and swipe-level details with weekly compliance & duration category splits.
    This endpoint is defensive: if duration_report or DB is slow/unavailable, returns partial results with diagnostic messages.
    """
    try:
        # parse regions / outdir
        if regions:
            regions_list = [r.strip().lower() for r in regions.split(",") if r.strip()]
        else:
            regions_list = ["apac", "emea", "laca", "namer"]

        if outdir:
            outdir_path = Path(outdir)
        else:
            outdir_path = OUTPUT_DIR / "duration_reports"
        outdir_path.mkdir(parents=True, exist_ok=True)

        def _parse_date(s: str) -> date:
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                return date.fromisoformat(s)

        if start_date and end_date:
            start_obj = _parse_date(start_date)
            end_obj = _parse_date(end_date)
            if start_obj > end_obj:
                raise HTTPException(status_code=400, detail="start_date must be <= end_date")
            max_days = 92
            days_count = (end_obj - start_obj).days + 1
            if days_count > max_days:
                raise HTTPException(status_code=400, detail=f"Date range too large (> {max_days} days). Please request a smaller range.")
            date_list = [start_obj + timedelta(days=i) for i in range(days_count)]
            range_mode = True
        else:
            if date_param:
                target_date = _parse_date(date_param)
            else:
                tz = ZoneInfo("Asia/Kolkata")
                target_date = datetime.now(tz).date()
            date_list = [target_date]
            start_obj = end_obj = date_list[0]
            range_mode = False

        # import duration_report (guarded)
        try:
            import importlib
            duration_report = importlib.import_module("duration_report")
        except Exception as e:
            logger.exception("Failed importing duration_report module")
            raise HTTPException(status_code=500, detail=f"duration module import failed: {e}")

        # timeout seconds (fallback if not defined)
        timeout_seconds = globals().get("COMPUTE_WAIT_TIMEOUT_SECONDS", None)
        if timeout_seconds is None:
            timeout_seconds = int(os.getenv("DURATION_COMPUTE_TIMEOUT", "120"))  # default 120s
        else:
            timeout_seconds = int(timeout_seconds)

        loop = asyncio.get_running_loop()

        # helper to run the blocking run_for_date in a thread with timeout
        async def _run_date_in_thread(single_date):
            start_ts = time.time()
            logger.info("Scheduling duration computation for date %s", single_date)
            try:
                # prefer asyncio.to_thread when available (py>=3.9)
                if hasattr(asyncio, "to_thread"):
                    coro = asyncio.to_thread(duration_report.run_for_date, single_date, regions_list, str(outdir_path), city)
                else:
                    # fallback: run in executor
                    coro = loop.run_in_executor(None, duration_report.run_for_date, single_date, regions_list, str(outdir_path), city)
                result = await asyncio.wait_for(coro, timeout=timeout_seconds)
                logger.info("Completed duration computation for date %s in %.1fs", single_date, time.time() - start_ts)
                return result
            except asyncio.TimeoutError:
                logger.exception("Duration computation timed out for date %s after %s seconds", single_date, timeout_seconds)
                raise
            except Exception as e:
                logger.exception("Duration computation failed for date %s: %s", single_date, e)
                raise

        # schedule all date jobs concurrently but keep per-date timeout (gather with return_exceptions=True)
        tasks = [asyncio.create_task(_run_date_in_thread(d)) for d in date_list]
        per_date_results: Dict[str, Any] = {}
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for iso_d, res in zip([d.isoformat() for d in date_list], results):
            if isinstance(res, asyncio.TimeoutError) or isinstance(res, asyncio.CancelledError):
                per_date_results[iso_d] = {"__error": f"timeout after {timeout_seconds}s"}
            elif isinstance(res, Exception):
                # an exception object returned by gather
                logger.exception("Duration run_for_date raised for date %s: %s", iso_d, res)
                per_date_results[iso_d] = {"__error": f"{res}"}
            else:
                # res expected: dict(region -> {"swipes": DataFrame, "durations": DataFrame})
                per_date_results[iso_d] = res if isinstance(res, dict) else {}

        dates_iso = [d.isoformat() for d in date_list]
        date_objs = [d for d in date_list]
        week_starts = sorted({(d - timedelta(days=d.weekday())) for d in date_objs})

        resp: Dict[str, Any] = {
            "start_date": start_obj.isoformat(),
            "end_date": end_obj.isoformat(),
            "regions": {},
            "diagnostics": {"per_date_status": {}}
        }

        # helper: if a returned value is a filesystem path to CSV(s), try reading with pandas
        def _coerce_result_region(region_val):
            # Accept dict with DataFrames or CSV paths (strings). Return dict {'durations': DataFrame, 'swipes': DataFrame}
            out = {"durations": pd.DataFrame(), "swipes": pd.DataFrame()}
            if not region_val:
                return out
            if isinstance(region_val, dict):
                dur = region_val.get("durations")
                sw = region_val.get("swipes")

                # DURATIONS handling
                try:
                    if isinstance(dur, str):
                        p = Path(dur)
                        if p.exists():
                            try:
                                out["durations"] = _read_csv_compat(p, parse_dates=["LocaleMessageTime"], dtype=str)
                            except Exception:
                                logger.exception("Failed to read durations CSV path %s", p)
                                out["durations"] = pd.DataFrame()
                        else:
                            out["durations"] = pd.DataFrame()
                    elif isinstance(dur, pd.DataFrame):
                        out["durations"] = dur.copy()
                except Exception:
                    logger.exception("Failed to coerce durations into DataFrame (dur may be %s)", type(dur))

                # SWIPES handling
                try:
                    if isinstance(sw, str):
                        p = Path(sw)
                        if p.exists():
                            try:
                                out["swipes"] = _read_csv_compat(p, parse_dates=["LocaleMessageTime"], dtype=str)
                            except Exception:
                                logger.exception("Failed to read swipes CSV path %s", p)
                                out["swipes"] = pd.DataFrame()
                        else:
                            out["swipes"] = pd.DataFrame()
                    elif isinstance(sw, pd.DataFrame):
                        out["swipes"] = sw.copy()
                except Exception:
                    logger.exception("Failed to coerce swipes into DataFrame (sw may be %s)", type(sw))

                return out

            # If caller provided a DataFrame directly
            if isinstance(region_val, pd.DataFrame):
                return {"durations": region_val.copy(), "swipes": pd.DataFrame()}

            # unexpected types
            return out

        # aggregate by region (unchanged logic from original but uses per_date_results)
        for r in regions_list:
            try:
                employees_map: Dict[str, Dict[str, Any]] = {}
                swipes_by_date: Dict[str, List[Dict[str, Any]]] = {}
                date_rows = {}

                # aggregate per-date
                for iso_d, day_res in per_date_results.items():
                    # check for error recorded earlier
                    if isinstance(day_res, dict) and "__error" in day_res:
                        resp["diagnostics"]["per_date_status"][iso_d] = day_res["__error"]
                        # mark empty row counts for this date
                        date_rows.setdefault(iso_d, {"rows": 0, "swipe_rows": 0})
                        swipes_by_date.setdefault(iso_d, [])
                        continue

                    region_obj = day_res.get(r) if isinstance(day_res, dict) else None
                    coerced = _coerce_result_region(region_obj)
                    durations_df = coerced.get("durations")
                    swipes_df = coerced.get("swipes")

                    rows_count = int(len(durations_df)) if isinstance(durations_df, pd.DataFrame) else 0
                    swipe_count = int(len(swipes_df)) if isinstance(swipes_df, pd.DataFrame) else 0
                    date_rows[iso_d] = {"rows": rows_count, "swipe_rows": swipe_count}

                    # SWIPES -> convert to list of dicts (safe)
                    if isinstance(swipes_df, pd.DataFrame) and not swipes_df.empty:
                        for c in ["LocaleMessageTime", "EmployeeName", "Door", "EmployeeID", "CardNumber", "PartitionName2", "PersonnelTypeName", "CompanyName", "PrimaryLocation", "Direction", "EmployeeIdentity", "MessageType", "Text5"]:
                            if c not in swipes_df.columns:
                                swipes_df[c] = None
                        swipe_records = []
                        for _, srow in swipes_df.iterrows():
                            dt = srow.get("LocaleMessageTime")
                            iso_ts = None
                            if pd.notna(dt):
                                try:
                                    iso_ts = pd.to_datetime(dt).isoformat()
                                except Exception:
                                    try:
                                        iso_ts = datetime.fromtimestamp(float(dt)).isoformat()
                                    except Exception:
                                        iso_ts = str(dt)
                            swipe_records.append({
                                "LocaleMessageTime": iso_ts,
                                "Dateonly": iso_d,
                                "Swipe_Time": (pd.to_datetime(srow.get("LocaleMessageTime")).time().isoformat() if pd.notna(srow.get("LocaleMessageTime")) else None),
                                "EmployeeID": srow.get("EmployeeID") if srow.get("EmployeeID") is not None else None,
                                "PersonGUID": srow.get("EmployeeIdentity") or srow.get("EmployeeIdentity"),
                                "ObjectName1": srow.get("EmployeeName"),
                                "Door": srow.get("Door"),
                                "PersonnelType": srow.get("PersonnelTypeName") or srow.get("PersonnelType"),
                                "CardNumber": srow.get("CardNumber"),
                                "Text5": srow.get("PrimaryLocation") or srow.get("Text5"),
                                "PartitionName2": srow.get("PartitionName2"),
                                "AdmitCode": srow.get("AdmitCode") or srow.get("MessageType"),
                                "Direction": srow.get("Direction"),
                                "CompanyName": srow.get("CompanyName"),
                                "PrimaryLocation": srow.get("PrimaryLocation") or srow.get("Text5"),
                            })
                        swipes_by_date[iso_d] = swipe_records
                    else:
                        swipes_by_date.setdefault(iso_d, [])

                    # DURATIONS -> iterate rows and aggregate per person
                    if isinstance(durations_df, pd.DataFrame) and not durations_df.empty:
                        # ensure columns exist
                        for col in ["person_uid", "EmployeeID", "EmployeeName", "CardNumber", "Date",
                                    "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor",
                                    "Duration", "DurationSeconds", "PersonnelTypeName", "PartitionName2",
                                    "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"]:
                            if col not in durations_df.columns:
                                durations_df[col] = None

                        for _, drow in durations_df.iterrows():
                            try:
                                person_uid = drow.get("person_uid")
                                if pd.isna(person_uid) or person_uid is None or str(person_uid).strip() == "":
                                    person_uid = f"{_to_json_safe(drow.get('EmployeeID'))}|{_to_json_safe(drow.get('EmployeeName'))}"

                                if person_uid not in employees_map:
                                    employees_map[person_uid] = {
                                        "person_uid": person_uid,
                                        "EmployeeID": _to_json_safe(drow.get("EmployeeID")),
                                        "EmployeeName": _to_json_safe(drow.get("EmployeeName")),
                                        "CardNumber": _to_json_safe(drow.get("CardNumber")),
                                        "durations": {d: None for d in dates_iso},
                                        "durations_seconds": {d: None for d in dates_iso},
                                        "total_seconds_present_in_range": 0,
                                        # keep internal First/Last but we'll remove them before returning
                                        "FirstSwipe": None,
                                        "LastSwipe": None,
                                        "FirstDoor": _to_json_safe(drow.get("FirstDoor")),
                                        "LastDoor": _to_json_safe(drow.get("LastDoor")),
                                        "PersonnelType": _to_json_safe(drow.get("PersonnelTypeName") or drow.get("PersonnelType")),
                                        "PartitionName2": _to_json_safe(drow.get("PartitionName2")),
                                        "CompanyName": _to_json_safe(drow.get("CompanyName")),
                                        "PrimaryLocation": _to_json_safe(drow.get("PrimaryLocation")),
                                        "FirstDirection": _to_json_safe(drow.get("FirstDirection")),
                                        "LastDirection": _to_json_safe(drow.get("LastDirection")),
                                    }

                                dur_str = None if pd.isna(drow.get("Duration")) else str(drow.get("Duration"))
                                dur_secs = None
                                try:
                                    v = drow.get("DurationSeconds")
                                    if pd.notna(v):
                                        dur_secs = int(float(v))
                                except Exception:
                                    dur_secs = None

                                employees_map[person_uid]["durations"][iso_d] = dur_str
                                employees_map[person_uid]["durations_seconds"][iso_d] = dur_secs
                                if dur_secs is not None:
                                    employees_map[person_uid]["total_seconds_present_in_range"] += dur_secs

                                # First/Last swipe times: keep earliest first, latest last
                                try:
                                    fs = drow.get("FirstSwipe")
                                    ls = drow.get("LastSwipe")
                                    if pd.notna(fs):
                                        fs_dt = pd.to_datetime(fs)
                                        cur_fs = employees_map[person_uid].get("FirstSwipe")
                                        if cur_fs is None:
                                            employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                        else:
                                            if pd.to_datetime(cur_fs) > fs_dt:
                                                employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                    if pd.notna(ls):
                                        ls_dt = pd.to_datetime(ls)
                                        cur_ls = employees_map[person_uid].get("LastSwipe")
                                        if cur_ls is None:
                                            employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                                        else:
                                            if pd.to_datetime(cur_ls) < ls_dt:
                                                employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                                except Exception:
                                    pass
                            except Exception:
                                logger.exception("Failed processing duration row for region %s date %s", r, iso_d)

                # (rest of aggregation / shift-fix / compliance/category code unchanged)
                # ... (copy the exact same logic you had for shift-fix, compliance computation, sorting,
                #  building emp_list, and building resp["regions"][r]) ...

                # Build list sorted by name
                emp_list = list(employees_map.values())
                emp_list.sort(key=lambda x: ((x.get("EmployeeName") or "") or "").lower())

                # SHIFT-FIX, compute weeks, categories, cleanup, etc. (unchanged original code)
                # --- (for brevity, reuse your existing block here exactly) ---
                # After that block, set:
                durations_sample = emp_list[:sample_rows] if sample_rows and sample_rows > 0 else []

                resp["regions"][r] = {
                    "dates": dates_iso,
                    "employees": emp_list,
                    "durations_sample": durations_sample,
                    "date_rows": date_rows,
                    "swipes_by_date": swipes_by_date
                }
            except Exception:
                logger.exception("Failed to aggregate range results for region %s", r)
                resp["regions"][r] = {"dates": [d.isoformat() for d in date_list], "employees": [], "durations_sample": [], "date_rows": {}, "swipes_by_date": {}}

        # ensure all numpy/pandas types are converted to serializable Python types
        safe_content = jsonable_encoder(resp)
        return JSONResponse(safe_content)

    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("api_duration (range) failed")
        raise HTTPException(status_code=500, detail=f"duration api error: {exc}")















Understand issue is Check below App.py file also i will shar you another file also 
We have isse for Duration API 

like Duration page continous in loading sate it not display anything on UI also it not display any error on Ui so lets debug issue and fix this issue carefully..



@app.get("/duration")
async def api_duration(
    date_param: Optional[str] = Query(None, alias="date", description="Target date YYYY-MM-DD. Defaults to today in Asia/Kolkata"),
    start_date: Optional[str] = Query(None, description="Start date for a range (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date for a range (YYYY-MM-DD)"),
    regions: Optional[str] = Query(None, description="Comma-separated list: apac,emea,laca,namer. Default: all"),
    city: Optional[str] = Query(None, description="Optional city/location filter (e.g. Pune). Case-insensitive"),
    outdir: Optional[str] = Query(None, description="Output directory for CSVs. Defaults to OUTPUT_DIR/duration_reports"),
    sample_rows: int = Query(10, ge=0, le=200, description="How many sample rows to include per region in response"),
    compliance_target: int = Query(3, ge=1, le=7, description="Compliance target days (e.g. 3 or 5). Default 3")
):
    """
    Returns per-region duration aggregates and swipe-level details with weekly compliance & duration category splits.
    This endpoint is defensive: if duration_report or DB is slow/unavailable, returns partial results with diagnostic messages.
    """
    try:
        # parse regions / outdir
        if regions:
            regions_list = [r.strip().lower() for r in regions.split(",") if r.strip()]
        else:
            regions_list = ["apac", "emea", "laca", "namer"]

        if outdir:
            outdir_path = Path(outdir)
        else:
            outdir_path = OUTPUT_DIR / "duration_reports"
        outdir_path.mkdir(parents=True, exist_ok=True)

        def _parse_date(s: str) -> date:
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                return date.fromisoformat(s)

        if start_date and end_date:
            start_obj = _parse_date(start_date)
            end_obj = _parse_date(end_date)
            if start_obj > end_obj:
                raise HTTPException(status_code=400, detail="start_date must be <= end_date")
            max_days = 92
            days_count = (end_obj - start_obj).days + 1
            if days_count > max_days:
                raise HTTPException(status_code=400, detail=f"Date range too large (> {max_days} days). Please request a smaller range.")
            date_list = [start_obj + timedelta(days=i) for i in range(days_count)]
            range_mode = True
        else:
            if date_param:
                target_date = _parse_date(date_param)
            else:
                tz = ZoneInfo("Asia/Kolkata")
                target_date = datetime.now(tz).date()
            date_list = [target_date]
            start_obj = end_obj = date_list[0]
            range_mode = False

        try:
            import duration_report
        except Exception as e:
            logger.exception("Failed importing duration_report module")
            raise HTTPException(status_code=500, detail=f"duration module import failed: {e}")

        loop = asyncio.get_running_loop()

        def _to_json_safe(v):
            try:
                if pd.isna(v):
                    return None
            except Exception:
                pass
            if isinstance(v, (datetime, date)):
                return v.isoformat()
            if hasattr(v, "isoformat") and not isinstance(v, str):
                try:
                    return v.isoformat()
                except Exception:
                    pass
            try:
                if isinstance(v, (int, float, bool)):
                    return v
            except Exception:
                pass
            try:
                return str(v)
            except Exception:
                return None

        per_date_results: Dict[str, Any] = {}
        # run duration_report.run_for_date for each date concurrently but guarded by a per-date timeout
        for single_date in date_list:
            try:
                # schedule on threadpool
                fut = loop.run_in_executor(None, duration_report.run_for_date, single_date, regions_list, str(outdir_path), city)
                result = await asyncio.wait_for(fut, timeout=COMPUTE_WAIT_TIMEOUT_SECONDS)
                # result expected: dict(region -> {"swipes": DataFrame, "durations": DataFrame})
                per_date_results[single_date.isoformat()] = result if isinstance(result, dict) else {}
            except asyncio.TimeoutError:
                logger.exception("Duration computation timed out for date %s", single_date)
                # store diagnostic so front-end can show partial failure
                per_date_results[single_date.isoformat()] = {"__error": f"timeout after {COMPUTE_WAIT_TIMEOUT_SECONDS}s"}
            except Exception as e:
                logger.exception("duration run_for_date failed for date %s: %s", single_date, e)
                per_date_results[single_date.isoformat()] = {"__error": f"{e}"}

        dates_iso = [d.isoformat() for d in date_list]
        date_objs = [d for d in date_list]
        week_starts = sorted({(d - timedelta(days=d.weekday())) for d in date_objs})

        resp: Dict[str, Any] = {
            "start_date": start_obj.isoformat(),
            "end_date": end_obj.isoformat(),
            "regions": {},
            "diagnostics": {"per_date_status": {}}
        }

        # helper: if a returned value is a filesystem path to CSV(s), try reading with pandas
        def _coerce_result_region(region_val):
            # Accept dict with DataFrames or CSV paths (strings). Return dict {'durations': DataFrame, 'swipes': DataFrame}
            out = {"durations": pd.DataFrame(), "swipes": pd.DataFrame()}
            if not region_val:
                return out
            if isinstance(region_val, dict):
                dur = region_val.get("durations")
                sw = region_val.get("swipes")

                # DURATIONS handling
                try:
                    if isinstance(dur, str):
                        p = Path(dur)
                        if p.exists():
                            try:
                                out["durations"] = _read_csv_compat(p, parse_dates=["LocaleMessageTime"], dtype=str)
                            except Exception:
                                logger.exception("Failed to read durations CSV path %s", p)
                                out["durations"] = pd.DataFrame()
                        else:
                            out["durations"] = pd.DataFrame()
                    elif isinstance(dur, pd.DataFrame):
                        out["durations"] = dur.copy()
                except Exception:
                    logger.exception("Failed to coerce durations into DataFrame (dur may be %s)", type(dur))

                # SWIPES handling
                try:
                    if isinstance(sw, str):
                        p = Path(sw)
                        if p.exists():
                            try:
                                out["swipes"] = _read_csv_compat(p, parse_dates=["LocaleMessageTime"], dtype=str)
                            except Exception:
                                logger.exception("Failed to read swipes CSV path %s", p)
                                out["swipes"] = pd.DataFrame()
                        else:
                            out["swipes"] = pd.DataFrame()
                    elif isinstance(sw, pd.DataFrame):
                        out["swipes"] = sw.copy()
                except Exception:
                    logger.exception("Failed to coerce swipes into DataFrame (sw may be %s)", type(sw))

                return out

            # If caller provided a DataFrame directly
            if isinstance(region_val, pd.DataFrame):
                return {"durations": region_val.copy(), "swipes": pd.DataFrame()}

            # unexpected types
            return out

        for r in regions_list:
            try:
                employees_map: Dict[str, Dict[str, Any]] = {}
                swipes_by_date: Dict[str, List[Dict[str, Any]]] = {}
                date_rows = {}

                # aggregate per-date
                for iso_d, day_res in per_date_results.items():
                    # check for error recorded earlier
                    if isinstance(day_res, dict) and "__error" in day_res:
                        resp["diagnostics"]["per_date_status"][iso_d] = day_res["__error"]
                        # mark empty row counts for this date
                        date_rows.setdefault(iso_d, {"rows": 0, "swipe_rows": 0})
                        swipes_by_date.setdefault(iso_d, [])
                        continue

                    region_obj = day_res.get(r) if isinstance(day_res, dict) else None
                    coerced = _coerce_result_region(region_obj)
                    durations_df = coerced.get("durations")
                    swipes_df = coerced.get("swipes")

                    rows_count = int(len(durations_df)) if isinstance(durations_df, pd.DataFrame) else 0
                    swipe_count = int(len(swipes_df)) if isinstance(swipes_df, pd.DataFrame) else 0
                    date_rows[iso_d] = {"rows": rows_count, "swipe_rows": swipe_count}

                    # SWIPES -> convert to list of dicts (safe)
                    if isinstance(swipes_df, pd.DataFrame) and not swipes_df.empty:
                        for c in ["LocaleMessageTime", "EmployeeName", "Door", "EmployeeID", "CardNumber", "PartitionName2", "PersonnelTypeName", "CompanyName", "PrimaryLocation", "Direction", "EmployeeIdentity", "MessageType", "Text5"]:
                            if c not in swipes_df.columns:
                                swipes_df[c] = None
                        swipe_records = []
                        for _, srow in swipes_df.iterrows():
                            dt = srow.get("LocaleMessageTime")
                            iso_ts = None
                            if pd.notna(dt):
                                try:
                                    iso_ts = pd.to_datetime(dt).isoformat()
                                except Exception:
                                    try:
                                        iso_ts = datetime.fromtimestamp(float(dt)).isoformat()
                                    except Exception:
                                        iso_ts = str(dt)
                            swipe_records.append({
                                "LocaleMessageTime": iso_ts,
                                "Dateonly": iso_d,
                                "Swipe_Time": (pd.to_datetime(srow.get("LocaleMessageTime")).time().isoformat() if pd.notna(srow.get("LocaleMessageTime")) else None),
                                "EmployeeID": srow.get("EmployeeID") if srow.get("EmployeeID") is not None else None,
                                "PersonGUID": srow.get("EmployeeIdentity") or srow.get("EmployeeIdentity"),
                                "ObjectName1": srow.get("EmployeeName"),
                                "Door": srow.get("Door"),
                                "PersonnelType": srow.get("PersonnelTypeName") or srow.get("PersonnelType"),
                                "CardNumber": srow.get("CardNumber"),
                                "Text5": srow.get("PrimaryLocation") or srow.get("Text5"),
                                "PartitionName2": srow.get("PartitionName2"),
                                "AdmitCode": srow.get("AdmitCode") or srow.get("MessageType"),
                                "Direction": srow.get("Direction"),
                                "CompanyName": srow.get("CompanyName"),
                                "PrimaryLocation": srow.get("PrimaryLocation") or srow.get("Text5"),
                            })
                        swipes_by_date[iso_d] = swipe_records
                    else:
                        swipes_by_date.setdefault(iso_d, [])

                    # DURATIONS -> iterate rows and aggregate per person
                    if isinstance(durations_df, pd.DataFrame) and not durations_df.empty:
                        # ensure columns exist
                        for col in ["person_uid", "EmployeeID", "EmployeeName", "CardNumber", "Date",
                                    "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor",
                                    "Duration", "DurationSeconds", "PersonnelTypeName", "PartitionName2",
                                    "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"]:
                            if col not in durations_df.columns:
                                durations_df[col] = None

                        for _, drow in durations_df.iterrows():
                            try:
                                person_uid = drow.get("person_uid")
                                if pd.isna(person_uid) or person_uid is None or str(person_uid).strip() == "":
                                    person_uid = f"{_to_json_safe(drow.get('EmployeeID'))}|{_to_json_safe(drow.get('EmployeeName'))}"

                                if person_uid not in employees_map:
                                    employees_map[person_uid] = {
                                        "person_uid": person_uid,
                                        "EmployeeID": _to_json_safe(drow.get("EmployeeID")),
                                        "EmployeeName": _to_json_safe(drow.get("EmployeeName")),
                                        "CardNumber": _to_json_safe(drow.get("CardNumber")),
                                        "durations": {d: None for d in dates_iso},
                                        "durations_seconds": {d: None for d in dates_iso},
                                        "total_seconds_present_in_range": 0,
                                        # keep internal First/Last but we'll remove them before returning
                                        "FirstSwipe": None,
                                        "LastSwipe": None,
                                        "FirstDoor": _to_json_safe(drow.get("FirstDoor")),
                                        "LastDoor": _to_json_safe(drow.get("LastDoor")),
                                        "PersonnelType": _to_json_safe(drow.get("PersonnelTypeName") or drow.get("PersonnelType")),
                                        "PartitionName2": _to_json_safe(drow.get("PartitionName2")),
                                        "CompanyName": _to_json_safe(drow.get("CompanyName")),
                                        "PrimaryLocation": _to_json_safe(drow.get("PrimaryLocation")),
                                        "FirstDirection": _to_json_safe(drow.get("FirstDirection")),
                                        "LastDirection": _to_json_safe(drow.get("LastDirection")),
                                    }

                                dur_str = None if pd.isna(drow.get("Duration")) else str(drow.get("Duration"))
                                dur_secs = None
                                try:
                                    v = drow.get("DurationSeconds")
                                    if pd.notna(v):
                                        dur_secs = int(float(v))
                                except Exception:
                                    dur_secs = None

                                employees_map[person_uid]["durations"][iso_d] = dur_str
                                employees_map[person_uid]["durations_seconds"][iso_d] = dur_secs
                                if dur_secs is not None:
                                    employees_map[person_uid]["total_seconds_present_in_range"] += dur_secs

                                # First/Last swipe times: keep earliest first, latest last
                                try:
                                    fs = drow.get("FirstSwipe")
                                    ls = drow.get("LastSwipe")
                                    if pd.notna(fs):
                                        fs_dt = pd.to_datetime(fs)
                                        cur_fs = employees_map[person_uid].get("FirstSwipe")
                                        if cur_fs is None:
                                            employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                        else:
                                            if pd.to_datetime(cur_fs) > fs_dt:
                                                employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                    if pd.notna(ls):
                                        ls_dt = pd.to_datetime(ls)
                                        cur_ls = employees_map[person_uid].get("LastSwipe")
                                        if cur_ls is None:
                                            employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                                        else:
                                            if pd.to_datetime(cur_ls) < ls_dt:
                                                employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                                except Exception:
                                    pass
                            except Exception:
                                logger.exception("Failed processing duration row for region %s date %s", r, iso_d)

                # Build list sorted by name
                emp_list = list(employees_map.values())
                emp_list.sort(key=lambda x: ((x.get("EmployeeName") or "") or "").lower())

                # SHIFT-FIX: attempt reconstruction when days look anomalous
                SHIFT_GAP_SECONDS = 6 * 3600        # gap > 6 hours -> new session
                SHIFT_MIN_FIX_SECONDS = 4 * 3600    # attempt fix if a day duration < 4h
                SHIFT_MAX_FIX_SECONDS = 20 * 3600   # or a day duration > 20h

                def _parse_swipe_ts(swipe_rec):
                    ts = swipe_rec.get("LocaleMessageTime")
                    if not ts:
                        return None
                    try:
                        return pd.to_datetime(ts)
                    except Exception:
                        try:
                            return datetime.fromisoformat(str(ts))
                        except Exception:
                            return None

                # Build swipes_by_person map
                swipes_by_person = {}
                for iso_d in dates_iso:
                    for s in swipes_by_date.get(iso_d, []):
                        pids = []
                        if s.get("PersonGUID"):
                            pids.append(str(s.get("PersonGUID")))
                        if s.get("EmployeeID") is not None:
                            pids.append(str(s.get("EmployeeID")))
                        if s.get("CardNumber") is not None:
                            pids.append(str(s.get("CardNumber")))
                        for pid in pids:
                            swipes_by_person.setdefault(pid, []).append(s)

                # sort each person's swipes by timestamp
                for pid, arr in list(swipes_by_person.items()):
                    arr_ts = []
                    for s in arr:
                        ts = _parse_swipe_ts(s)
                        if ts is not None:
                            arr_ts.append((ts, s))
                    arr_ts.sort(key=lambda x: x[0])
                    swipes_by_person[pid] = [s for _, s in arr_ts]

                def _needs_shift_fix(emp):
                    for v in (emp.get("durations_seconds") or {}).values():
                        if v is None:
                            continue
                        if v < SHIFT_MIN_FIX_SECONDS or v > SHIFT_MAX_FIX_SECONDS:
                            return True
                    return False

                for emp in emp_list:
                    try:
                        # assemble all swipes for this person from swipes_by_person using multiple identifiers
                        person_keys = []
                        if emp.get("person_uid"):
                            person_keys.append(str(emp["person_uid"]))
                        if emp.get("EmployeeID") is not None:
                            person_keys.append(str(emp["EmployeeID"]))
                        if emp.get("CardNumber") is not None:
                            person_keys.append(str(emp.get("CardNumber")))

                        all_swipes = []
                        for k in person_keys:
                            lst = swipes_by_person.get(k) or []
                            for s in lst:
                                ts = _parse_swipe_ts(s)
                                if ts is not None:
                                    all_swipes.append((ts, s))
                        if not all_swipes:
                            continue
                        all_swipes.sort(key=lambda x: x[0])
                        swipe_times = [ts for ts, _ in all_swipes]

                        if not _needs_shift_fix(emp):
                            continue

                        sessions = []
                        cur_start = swipe_times[0]
                        cur_last = swipe_times[0]
                        for ts in swipe_times[1:]:
                            gap = (ts - cur_last).total_seconds()
                            if gap > SHIFT_GAP_SECONDS:
                                sessions.append((cur_start, cur_last))
                                cur_start = ts
                                cur_last = ts
                            else:
                                cur_last = ts
                        sessions.append((cur_start, cur_last))

                        new_durations_seconds = {d: None for d in dates_iso}
                        for s_start, s_end in sessions:
                            session_secs = max(0, int((s_end - s_start).total_seconds()))
                            session_date_iso = s_start.date().isoformat()
                            if session_date_iso not in new_durations_seconds:
                                continue
                            prev = new_durations_seconds.get(session_date_iso)
                            if prev is None:
                                new_durations_seconds[session_date_iso] = session_secs
                            else:
                                new_durations_seconds[session_date_iso] = prev + session_secs

                        # preserve previous per-day values for untouched days
                        for d in dates_iso:
                            if new_durations_seconds.get(d) is None and emp.get("durations_seconds", {}).get(d) is not None:
                                new_durations_seconds[d] = emp["durations_seconds"][d]

                        new_durations_str = {}
                        for d, secs in new_durations_seconds.items():
                            if secs is None:
                                new_durations_str[d] = None
                            else:
                                try:
                                    new_durations_str[d] = str(timedelta(seconds=int(secs)))
                                except Exception:
                                    new_durations_str[d] = None

                        total = 0
                        for v in new_durations_seconds.values():
                            if v is not None:
                                total += int(v)

                        emp["durations_seconds"] = new_durations_seconds
                        emp["durations"] = new_durations_str
                        emp["total_seconds_present_in_range"] = total
                    except Exception:
                        logger.exception("shift-fix reconstruction failed for employee %s", emp.get("person_uid") or emp.get("EmployeeID"))

                # compute per-employee weekly compliance and categories
                for emp in emp_list:
                    try:
                        weeks_info = {}
                        weeks_met = 0
                        weeks_total = 0

                        cat_counts = {"0-30m": 0, "30m-2h": 0, "2h-6h": 0, "6h-8h": 0, "8h+": 0}
                        cat_dates = {k: [] for k in cat_counts.keys()}

                        for ws in week_starts:
                            week_start_iso = ws.isoformat()
                            week_dates = [(ws + timedelta(days=i)).isoformat() for i in range(7)]
                            relevant_dates = [d for d in week_dates if d in dates_iso]
                            if not relevant_dates:
                                continue

                            days_present = 0
                            days_ge8 = 0
                            per_date_durations = {}
                            per_date_compliance = {}

                            for d in relevant_dates:
                                secs = emp["durations_seconds"].get(d)
                                per_date_durations[d] = secs
                                if secs is not None and secs > 0:
                                    days_present += 1
                                is_ge8 = (secs is not None and secs >= 28800)
                                if is_ge8:
                                    days_ge8 += 1
                                per_date_compliance[d] = True if is_ge8 else False

                                if secs is not None and secs > 0:
                                    # use duration_report.categorize_seconds if available
                                    cat = "0-30m"
                                    try:
                                        if hasattr(duration_report, 'categorize_seconds'):
                                            cat = duration_report.categorize_seconds(secs)
                                        else:
                                            # fallback: simple bucket
                                            if secs <= 1800:
                                                cat = "0-30m"
                                            elif secs <= 7200:
                                                cat = "30m-2h"
                                            elif secs <= 21600:
                                                cat = "2h-6h"
                                            elif secs < 28800:
                                                cat = "6h-8h"
                                            else:
                                                cat = "8h+"
                                    except Exception:
                                        cat = "0-30m"
                                    if cat in cat_counts:
                                        cat_counts[cat] += 1
                                        cat_dates[cat].append(d)

                            ct = int(compliance_target or 3)
                            compliant = False
                            # Basic rule: weeks considered only if at least ct days present AND those present days are >=8h
                            if days_present >= ct:
                                if days_present == days_ge8:
                                    compliant = True
                                else:
                                    compliant = False

                            weeks_info[week_start_iso] = {
                                "week_start": week_start_iso,
                                "dates": per_date_durations,
                                "dates_compliance": per_date_compliance,
                                "days_present": days_present,
                                "days_ge8": days_ge8,
                                "compliant": compliant
                            }

                            weeks_total += 1
                            if compliant:
                                weeks_met += 1

                        dominant_category = None
                        max_count = -1
                        for k, v in cat_counts.items():
                            if v > max_count:
                                max_count = v
                                dominant_category = k

                        # cleanup
                        for _k in ("FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor"):
                            if _k in emp:
                                try:
                                    del emp[_k]
                                except Exception:
                                    pass

                        emp["compliance"] = {
                            "weeks": weeks_info,
                            "weeks_met": weeks_met,
                            "weeks_total": weeks_total,
                            "month_summary": f"{weeks_met}/{weeks_total}" if weeks_total > 0 else "0/0",
                            "compliance_target": int(compliance_target or 3)
                        }
                        emp["duration_categories"] = {
                            "counts": cat_counts,
                            "dominant_category": dominant_category,
                            "category_dates": cat_dates,
                            "red_flag": cat_counts.get("2h-6h", 0)
                        }
                    except Exception:
                        logger.exception("Failed computing compliance/categories for employee %s", emp.get("person_uid"))

                durations_sample = emp_list[:sample_rows] if sample_rows and sample_rows > 0 else []

                resp["regions"][r] = {
                    "dates": dates_iso,
                    "employees": emp_list,
                    "durations_sample": durations_sample,
                    "date_rows": date_rows,
                    "swipes_by_date": swipes_by_date
                }
            except Exception:
                logger.exception("Failed to aggregate range results for region %s", r)
                resp["regions"][r] = {"dates": [d.isoformat() for d in date_list], "employees": [], "durations_sample": [], "date_rows": {}, "swipes_by_date": {}}

        # ensure all numpy/pandas types are converted to serializable Python types
        safe_content = jsonable_encoder(resp)
        return JSONResponse(safe_content)

    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("api_duration (range) failed")
        raise HTTPException(status_code=500, detail=f"duration api error: {exc}")












# duration_report.py
"""
duration_report.py

Updated: supports querying multiple monthly ACVSUJournal databases (current + previous N).
Fixes: use pyodbc cursor -> DataFrame to avoid pandas SQLAlchemy warning,
       replace groupby.apply with groupby.agg for better performance and avoid FutureWarning.
"""
import argparse
import logging
import os
import re
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd

# Optional: import pyodbc only when connecting (allows importing this module even without driver)
try:
    import pyodbc
except Exception:
    pyodbc = None

ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# (REGION_CONFIG unchanged - keep your config)
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "partitions": [
            "APAC.Default", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur"
        ]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "database": "ACVSUJournal_00011028",
        "last_n_databases": 2,
        "partitions": [
            "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
            "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
        ]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "partitions": [
            "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
            "PA.Panama City", "PE.Lima"
        ]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
INNER JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
INNER JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
"""

# helper functions (split name, expand DB list, connect master, filter existing dbs)
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# DB connection helper (same as before)
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    """
    Build SQL and fetch using pyodbc cursor -> DataFrame to avoid pandas.read_sql warnings.
    """
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "CardNumber", "PersonnelTypeName", "EmployeeIdentity",
        "PartitionName2", "LocaleMessageTime", "MessageType", "Direction", "CompanyName", "PrimaryLocation"
    ]
    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    cursor = None
    try:
        conn = get_connection(region_key)
        cursor = conn.cursor()
        cursor.execute(sql)
        rows = cursor.fetchall()
        # cursor.description may be None if no rows, but still try to get column names
        descr = cursor.description or []
        colnames = [d[0] for d in descr] if descr else []
        if rows:
            try:
                df = pd.DataFrame.from_records(rows, columns=colnames)
            except Exception:
                # fallback: convert row to tuple list
                df = pd.DataFrame([tuple(r) for r in rows], columns=colnames)
        else:
            # no rows -> empty df with expected columns
            df = pd.DataFrame(columns=colnames)
    except Exception:
        logging.exception("Database fetch failed for region %s; returning empty DataFrame", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if cursor is not None:
                cursor.close()
        except Exception:
            pass
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    # ensure expected columns exist even if SQL returned different set
    for c in cols:
        if c not in df.columns:
            df[c] = None

    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    return df[cols]

def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    if df["LocaleMessageTime"].dtype == object:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    try:
        df = df.drop_duplicates(subset=dedupe_cols, keep="first")
    except Exception:
        # if drop_duplicates fails for unexpected dtypes, ignore and continue
        pass

    df["Date"] = df["LocaleMessageTime"].dt.date

    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    df["person_uid"] = df.apply(make_person_uid, axis=1)
    df = df[df["person_uid"].notna()].copy()
    if df.empty:
        return pd.DataFrame(columns=out_cols)

    # sort then groupby + agg (avoid apply)
    try:
        df_sorted = df.sort_values(["person_uid", "LocaleMessageTime"])
    except Exception:
        df_sorted = df.copy()

    grouped = df_sorted.groupby(["person_uid", "Date"], sort=False).agg(
        EmployeeIdentity=("EmployeeIdentity", "first"),
        EmployeeID=("EmployeeID", "first"),
        EmployeeName=("EmployeeName", "first"),
        CardNumber=("CardNumber", "first"),
        FirstSwipe=("LocaleMessageTime", "min"),
        LastSwipe=("LocaleMessageTime", "max"),
        FirstDoor=("Door", "first"),
        LastDoor=("Door", "last"),
        CountSwipes=("LocaleMessageTime", "count"),
        PersonnelTypeName=("PersonnelTypeName", "first"),
        PartitionName2=("PartitionName2", "first"),
        CompanyName=("CompanyName", "first"),
        PrimaryLocation=("PrimaryLocation", "first"),
        FirstDirection=("Direction", "first"),
        LastDirection=("Direction", "last")
    ).reset_index()

    # compute durations
    try:
        grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    except Exception:
        # if dtype not datetime or subtraction fails
        def _safe_dur(row):
            try:
                if pd.isna(row["FirstSwipe"]) or pd.isna(row["LastSwipe"]):
                    return None
                return max(0, int((row["LastSwipe"] - row["FirstSwipe"]).total_seconds()))
            except Exception:
                return None
        grouped["DurationSeconds"] = grouped.apply(_safe_dur, axis=1)

    grouped["Duration"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    # ensure all output columns present
    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    return grouped[out_cols]

def categorize_seconds(s: Optional[int]) -> str:
    try:
        if s is None or s <= 0:
            return "0-30m"
        s = int(s)
        if s <= 1800:
            return "0-30m"
        if s <= 7200:
            return "30m-2h"
        if s <= 21600:
            return "2h-6h"
        if s < 28800:
            return "6h-8h"
        return "8h+"
    except Exception:
        return "0-30m"

def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        try:
            swipes = fetch_swipes_for_region(r, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", r)
            swipes = pd.DataFrame()

        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                swipes = swipes[combined_mask].copy()
            else:
                logging.debug("City filter provided but no matching columns to filter on for region %s", r)

        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", r)
            durations = pd.DataFrame()

        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
        try:
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", r)
        try:
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", r)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
        results[r] = {"swipes": swipes, "durations": durations}

    return results

# CLI runner (unchanged)
def parse_args():
    p = argparse.ArgumentParser(description="Generate daily duration report from ACVSUJournal swipe logs.")
    p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
    p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
                   default="apac,emea,laca,namer")
    p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
    p.add_argument("--city", help="Optional city/location filter (e.g. Pune)", default=None)
    return p.parse_args()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    args = parse_args()

    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    else:
        tz = ZoneInfo("Asia/Kolkata")
        target_date = datetime.now(tz).date()

    regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
    outdir = args.outdir

    logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
    results = run_for_date(target_date, regions, outdir, city=args.city)

    for r, obj in results.items():
        dur = obj.get("durations")
        logging.info("Region %s: %d persons with computed durations", r, len(dur) if dur is not None else 0)
    logging.info("Completed. CSVs are in %s", Path(outdir).absolute())

















// frontend/src/pages/DurationPage.jsx
import React, { useState, useMemo } from "react";
import axios from "axios";
import {
  Box,
  Grid,
  Paper,
  Typography,
  TextField,
  Button,
  Table,
  TableHead,
  TableRow,
  TableCell,
  TableBody,
  TableContainer,
  CircularProgress,
  IconButton,
  Tooltip,
  Card,
  CardContent,
  Dialog,
  DialogTitle,
  DialogContent,
  Divider,
} from "@mui/material";
import DateRangeIcon from "@mui/icons-material/DateRange";
import SearchIcon from "@mui/icons-material/Search";
import DownloadIcon from "@mui/icons-material/CloudDownload";
import ClearIcon from "@mui/icons-material/Clear";
import VisibilityIcon from "@mui/icons-material/Visibility";

const API_BASE = import.meta.env.VITE_API_BASE || import.meta.env.REACT_APP_API_BASE || "http://localhost:8000";

const REGIONS = [
  { value: "apac", label: "APAC" },
  { value: "emea", label: "EMEA" },
  { value: "laca", label: "LACA" },
  { value: "namer", label: "NAMER" },
];

function secondsToHMS(s) {
  if (s == null) return "";
  const sec = Number(s);
  if (!Number.isFinite(sec)) return "";
  const h = Math.floor(sec / 3600);
  const m = Math.floor((sec % 3600) / 60);
  const r = Math.floor(sec % 60);
  return `${h}:${String(m).padStart(2, "0")}:${String(r).padStart(2, "0")}`;
}

function isoToDDMMYYYY(iso) {
  if (!iso) return iso;
  const dt = new Date(iso.includes("T") ? iso : `${iso}T00:00:00Z`);
  if (Number.isNaN(dt.getTime())) return iso;
  const dd = String(dt.getUTCDate()).padStart(2, "0");
  const mm = String(dt.getUTCMonth() + 1).padStart(2, "0");
  const yyyy = dt.getUTCFullYear();
  return `${dd}-${mm}-${yyyy}`;
}

function isoToLongDateNoCommas(iso) {
  // produce a header string without internal commas to avoid CSV splitting
  if (!iso) return iso;
  const dt = new Date(iso.includes("T") ? iso : `${iso}T00:00:00Z`);
  if (Number.isNaN(dt.getTime())) return iso;
  const weekday = dt.toLocaleDateString(undefined, { weekday: "short" }); // Mon/Tue...
  const day = String(dt.getUTCDate()).padStart(2, "0");
  const month = String(dt.getUTCMonth() + 1).padStart(2, "0");
  const year = dt.getUTCFullYear();
  return `${weekday} ${day}-${month}-${year}`; // no commas
}

export default function DurationPage() {
  const [region, setRegion] = useState("apac");
  const [city, setCity] = useState("");
  const [startDate, setStartDate] = useState("");
  const [endDate, setEndDate] = useState("");
  const [singleDate, setSingleDate] = useState("");
  const [useRange, setUseRange] = useState(true);

  const [data, setData] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState("");

  const [searchEmployeeId, setSearchEmployeeId] = useState("");
  const [searchEmployeeName, setSearchEmployeeName] = useState("");
  const [searchCardNumber, setSearchCardNumber] = useState("");

  const [swipeDialogOpen, setSwipeDialogOpen] = useState(false);
  const [selectedEmployee, setSelectedEmployee] = useState(null);
  const [selectedSwipes, setSelectedSwipes] = useState([]);

  // --- fetch durations ---
  const fetchDurations = async () => {
    setError("");
    setLoading(true);
    setData(null);

    try {
      const params = {};
      if (useRange && startDate && endDate) {
        params.start_date = startDate;
        params.end_date = endDate;
      } else if (!useRange && singleDate) {
        params.date = singleDate;
      } else {
        if (singleDate) params.date = singleDate;
      }

      if (region) params.regions = region;
      if (city) params.city = city;

      const res = await axios.get(`${API_BASE}/duration`, { params, timeout: 12000000 });
      setData(res.data);
    } catch (err) {
      console.error(err);
      setError(err?.response?.data?.detail || err.message || "Failed to fetch duration data");
    } finally {
      setLoading(false);
    }
  };

  const regionObj = useMemo(() => {
    if (!data || !region) return null;
    return data.regions?.[region] || null;
  }, [data, region]);

  // helpers to compute week starts from regionObj.dates
  const computeWeekStarts = (datesIso) => {
    if (!datesIso || datesIso.length === 0) return [];
    const dateObjs = datesIso.map(d => new Date(d + "T00:00:00Z"));
    const weekStartSet = new Set();
    dateObjs.forEach(dt => {
      const day = dt.getUTCDay(); // 0 Sun..6 Sat
      const diff = (day + 6) % 7; // days since Monday
      const monday = new Date(dt);
      monday.setUTCDate(dt.getUTCDate() - diff);
      weekStartSet.add(monday.toISOString().slice(0,10));
    });
    return Array.from(weekStartSet).sort();
  };

  // --- CSV exports ---

  const quote = (s) => `"${String(s ?? "").replace(/"/g, '""')}"`;

  const exportSummaryCsv = () => {
    if (!regionObj) return;
    const dates = regionObj.dates || [];
    const rows = regionObj.employees || [];

    const header = ["EmployeeID", "EmployeeName", "TotalSecondsPresentInRange", "DominantCategory", "ComplianceSummary", ...dates.map(d => d)];
    const csvRows = [header.map(h => quote(h)).join(",")];

    rows.forEach((r) => {
      const complianceText = r.compliance?.month_summary || "";
      const complianceCell = `'${complianceText}`; // force text in Excel
      const row = [
        quote(r.EmployeeID || ""),
        quote(r.EmployeeName || ""),
        r.total_seconds_present_in_range ?? "",
        quote(r.duration_categories?.dominant_category || ""),
        quote(complianceCell),
        ...dates.map(d => quote(r.durations?.[d] ?? ""))
      ];
      csvRows.push(row.join(","));
    });

    const blob = new Blob([csvRows.join("\n")], { type: "text/csv;charset=utf-8;" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    const filename = `duration_summary_${region}_${data?.start_date || ""}_to_${data?.end_date || ""}.csv`;
    a.download = filename;
    document.body.appendChild(a);
    a.click();
    a.remove();
    URL.revokeObjectURL(url);
  };







 const exportReportCsv = () => {
    if (!regionObj) return;
    const datesIso = regionObj.dates || [];
    const weekStarts = computeWeekStarts(datesIso);

    // base header
    const baseHeader = [
      "Sr.No", "EmployeeID", "EmployeeName", "CardNumber", "PersonnelType", "PartitionName2", "TotalSecondsPresentInRange"
    ];

    // per-day headers (Mon..Sun for each week) - use a no-comma format
    const perDayHeaders = [];
    weekStarts.forEach(ws => {
      for (let i = 0; i < 7; i++) {
        const d = new Date(ws + "T00:00:00Z");
        d.setUTCDate(d.getUTCDate() + i);
        const iso = d.toISOString().slice(0,10);
        perDayHeaders.push(isoToLongDateNoCommas(iso));
      }
    });

    // per-week compliance headers
    const perWeekHeaders = weekStarts.map(ws => `Week compliance ${ws}`);

    const header = [...baseHeader, ...perDayHeaders, ...perWeekHeaders, "DominantCategory", "ComplianceSummary"];
    const csvRows = [header.map(h => quote(h)).join(",")];

    const rows = regionObj.employees || [];
    rows.forEach((r, idx) => {
      const srNo = idx + 1;
      const employeeId = r.EmployeeID ?? "";
      const employeeName = r.EmployeeName ?? "";
      const cardNumber = r.CardNumber ?? "";
      const personnelType = r.PersonnelType ?? r.PersonnelTypeName ?? "";
      const partition = r.PartitionName2 ?? "";
      const totalSeconds = r.total_seconds_present_in_range ?? "";

      // per-day durations (string) or "0" when absent
      const perDayVals = [];
      weekStarts.forEach(ws => {
        for (let i = 0; i < 7; i++) {
          const d = new Date(ws + "T00:00:00Z");
          d.setUTCDate(d.getUTCDate() + i);
          const iso = d.toISOString().slice(0,10);

          // prefer API-provided week map if present
          const wk = r.compliance && r.compliance.weeks ? r.compliance.weeks[ws] : null;
          let outVal = "0";

          if (wk && wk.dates && Object.prototype.hasOwnProperty.call(wk.dates, iso)) {
            const secs = wk.dates[iso];
            if (secs !== null && secs !== undefined) {
              // prefer already formatted string if available
              outVal = (r.durations && r.durations[iso]) ? r.durations[iso] : secondsToHMS(secs);
            } else {
              outVal = "0";
            }
          } else if (r.durations && Object.prototype.hasOwnProperty.call(r.durations, iso) && r.durations[iso]) {
            outVal = r.durations[iso];
          } else if (r.durations_seconds && Object.prototype.hasOwnProperty.call(r.durations_seconds, iso) && r.durations_seconds[iso]) {
            outVal = secondsToHMS(r.durations_seconds[iso]);
          } else {
            outVal = "0";
          }

          perDayVals.push(outVal);
        }
      });

      // per-week compliance Yes/No
      const perWeekVals = weekStarts.map(ws => {
        const wk = r.compliance && r.compliance.weeks ? r.compliance.weeks[ws] : null;
        return wk && wk.compliant ? "Yes" : "No";
      });

      const complianceText = r.compliance?.month_summary || "";
      const complianceCell = `'${complianceText}`;

      const row = [
        `${srNo}`,
        quote(employeeId),
        quote(employeeName),
        quote(cardNumber),
        quote(personnelType),
        quote(partition),
        `${totalSeconds}`,
        ...perDayVals.map(v => quote(v)),
        ...perWeekVals.map(v => quote(v)),
        quote(r.duration_categories?.dominant_category || ""),
        quote(complianceCell)
      ];

      csvRows.push(row.join(","));
    });

    const blob = new Blob([csvRows.join("\n")], { type: "text/csv;charset=utf-8;" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    const filename = `duration_report_${region}_${data?.start_date || ""}_to_${data?.end_date || ""}.csv`;
    a.download = filename;
    document.body.appendChild(a);
    a.click();
    a.remove();
    URL.revokeObjectURL(url);
  };





  const exportSwipesCsv = () => {
    if (!regionObj) return;
    const swipesByDate = regionObj.swipes_by_date || {};
    const rows = [];
    Object.entries(swipesByDate).forEach(([d, arr]) => {
      arr.forEach(s => {
        rows.push({
          Date: d,
          LocaleMessageTime: s.LocaleMessageTime || "",
          Swipe_Time: s.Swipe_Time || "",
          EmployeeID: s.EmployeeID || "",
          PersonGUID: s.PersonGUID || "",
          ObjectName1: s.ObjectName1 || "",
          Door: s.Door || "",
          PersonnelType: s.PersonnelType || "",
          CardNumber: s.CardNumber || "",
          PrimaryLocation: s.PrimaryLocation || s.Text5 || "",
          PartitionName2: s.PartitionName2 || "",
          AdmitCode: s.AdmitCode || "",
          Direction: s.Direction || "",
          CompanyName: s.CompanyName || ""
        });
      });
    });

    if (rows.length === 0) {
      alert("No swipe rows available for current selection to export.");
      return;
    }

    const headers = [
      "Date",
      "LocaleMessageTime",
      "Swipe_Time",
      "EmployeeID",
      "PersonGUID",
      "ObjectName1",
      "Door",
      "PersonnelType",
      "CardNumber",
      "PrimaryLocation",
      "PartitionName2",
      "AdmitCode",
      "Direction",
      "CompanyName"
    ];
    const csvRows = [headers.map(h => quote(h)).join(",")];
    rows.forEach(r => {
      const line = headers.map(h => quote(r[h] ?? "")).join(",");
      csvRows.push(line);
    });

    const blob = new Blob([csvRows.join("\n")], { type: "text/csv;charset=utf-8;" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    const filename = `swipes_${region}_${data?.start_date || ""}_to_${data?.end_date || ""}.csv`;
    a.download = filename;
    document.body.appendChild(a);
    a.click();
    a.remove();
    URL.revokeObjectURL(url);
  };

  // --- Render helpers ---
  const getFilteredRows = () => {
    if (!regionObj) return [];
    const rows = regionObj.employees || [];
    return rows.filter(r => {
      const matchesEmployeeId = !searchEmployeeId || String(r.EmployeeID || "").toLowerCase().includes(searchEmployeeId.toLowerCase());
      const matchesName = !searchEmployeeName || String(r.EmployeeName || "").toLowerCase().includes(searchEmployeeName.toLowerCase());
      const matchesCard = !searchCardNumber || String(r.CardNumber || "").toLowerCase().includes(searchCardNumber.toLowerCase());
      return matchesEmployeeId && matchesName && matchesCard;
    });
  };

  const openSwipeDialogFor = (emp) => {
    setSelectedEmployee(emp);
    const swipesByDate = regionObj?.swipes_by_date || {};
    const matches = [];
    Object.entries(swipesByDate).forEach(([d, arr]) => {
      arr.forEach(s => {
        const matchByEmployeeId = emp.EmployeeID && s.EmployeeID && String(s.EmployeeID) === String(emp.EmployeeID);
        const matchByCard = emp.CardNumber && s.CardNumber && String(s.CardNumber) === String(emp.CardNumber);
        const matchByPersonGuid = emp.person_uid && s.PersonGUID && String(s.PersonGUID) === String(emp.person_uid);
        if (matchByEmployeeId || matchByCard || matchByPersonGuid) {
          matches.push({ ...s, Date: d });
        }
      });
    });
    matches.sort((a,b) => {
      const ta = a.LocaleMessageTime ? new Date(a.LocaleMessageTime).getTime() : 0;
      const tb = b.LocaleMessageTime ? new Date(b.LocaleMessageTime).getTime() : 0;
      return ta - tb;
    });

    setSelectedSwipes(matches);
    setSwipeDialogOpen(true);
  };

  const renderTable = () => {
    if (!regionObj) return <Typography>No data for selected region.</Typography>;

    const dates = regionObj.dates || [];
    const rows = getFilteredRows();

    const weekStarts = computeWeekStarts(dates);

    return (
      <TableContainer component={Paper} sx={{ mt: 2, width: "100%", overflowX: "auto" }}>
        <Table size="small" stickyHeader>
          <TableHead>
            <TableRow>
              <TableCell><b>Sr.No</b></TableCell>
              <TableCell><b>EmployeeID</b></TableCell>
              <TableCell><b>EmployeeName</b></TableCell>
              <TableCell><b>CardNumber</b></TableCell>
              <TableCell><b>PersonnelType</b></TableCell>
              <TableCell><b>PartitionName2</b></TableCell>
              <TableCell align="right"><b>Total (hh:mm:ss)</b></TableCell>
              {dates.map((d) => (
                <TableCell key={d} align="center"><b>{isoToDDMMYYYY(d)}</b></TableCell>
              ))}
              {/* add week-level compliance columns */}
              {weekStarts.map(ws => (
                <TableCell key={ws} align="center"><b>{`Week ${ws} compliant`}</b></TableCell>
              ))}
              <TableCell align="center"><b>Dominant Category</b></TableCell>
              <TableCell align="center"><b>Compliance (weeks met/total)</b></TableCell>
              <TableCell align="center"><b>View</b></TableCell>
            </TableRow>
          </TableHead>
          <TableBody>
            {rows.length === 0 ? (
              <TableRow>
                <TableCell colSpan={9 + (regionObj.dates || []).length + weekStarts.length} align="center">No employees in the response.</TableCell>
              </TableRow>
            ) : (
              rows.map((r, idx) => (
                <TableRow key={r.person_uid || `${r.EmployeeID}-${r.EmployeeName}`}>
                  <TableCell>{idx + 1}</TableCell>
                  <TableCell>{r.EmployeeID || "-"}</TableCell>
                  <TableCell>{r.EmployeeName || "-"}</TableCell>
                  <TableCell>{r.CardNumber || "-"}</TableCell>
                  <TableCell>{r.PersonnelType || r.PersonnelTypeName || "-"}</TableCell>
                  <TableCell>{r.PartitionName2 || "-"}</TableCell>
                  <TableCell align="right">{secondsToHMS(r.total_seconds_present_in_range)}</TableCell>
                  {dates.map((d) => (
                    <TableCell key={d} align="center">{r.durations?.[d] ?? "-"}</TableCell>
                  ))}
                  {weekStarts.map(ws => {
                    const wk = r.compliance && r.compliance.weeks ? r.compliance.weeks[ws] : null;
                    return <TableCell key={ws} align="center">{wk && wk.compliant ? "Yes" : "No"}</TableCell>;
                  })}
                  <TableCell align="center">
                    {r.duration_categories?.dominant_category || "-"}
                    {r.duration_categories?.red_flag > 0 ? " " : ""}
                  </TableCell>
                  <TableCell align="center">{r.compliance?.month_summary || "-"}</TableCell>
                  <TableCell align="center">
                    <Tooltip title="View swipe records for this employee">
                      <IconButton size="small" onClick={() => openSwipeDialogFor(r)}>
                        <VisibilityIcon />
                      </IconButton>
                    </Tooltip>
                  </TableCell>
                </TableRow>
              ))
            )}
          </TableBody>
        </Table>
      </TableContainer>
    );
  };

  // --- JSX UI (top controls) ---
  return (
    <Box sx={{ p: 3, width: "100%", maxWidth: "100vw", boxSizing: "border-box" }}>
      <Typography variant="h5" gutterBottom>
        Duration Reports (with Compliance & Category)
      </Typography>

      <Grid container spacing={2}>
        <Grid item xs={12}>
          <Card>
            <CardContent>
              <Grid container spacing={2} alignItems="center">
                <Grid item xs={12} md={2}>
                  <TextField
                    select
                    fullWidth
                    label="Region"
                    value={region}
                    onChange={(e) => setRegion(e.target.value)}
                  >
                    {REGIONS.map((r) => (
                      <option key={r.value} value={r.value}>{r.label}</option>
                    ))}
                  </TextField>
                </Grid>

                <Grid item xs={12} md={3}>
                  <TextField
                    fullWidth
                    label="City / Partition (optional)"
                    placeholder="e.g. CR.Costa Rica Partition"
                    value={city}
                    onChange={(e) => setCity(e.target.value)}
                  />
                </Grid>

                <Grid item xs={12} md={5} sx={{ display: "flex", gap: 1, justifyContent: "flex-end" }}>
                  <Button
                    startIcon={<DateRangeIcon />}
                    variant={useRange ? "contained" : "outlined"}
                    onClick={() => setUseRange(true)}
                  >
                    Range
                  </Button>
                  <Button
                    startIcon={<DateRangeIcon />}
                    variant={!useRange ? "contained" : "outlined"}
                    onClick={() => setUseRange(false)}
                  >
                    Single Day
                  </Button>
                </Grid>

                {useRange ? (
                  <>
                    <Grid item xs={12} md={3}>
                      <TextField
                        label="Start date"
                        type="date"
                        fullWidth
                        InputLabelProps={{ shrink: true }}
                        value={startDate}
                        onChange={(e) => setStartDate(e.target.value)}
                      />
                    </Grid>
                    <Grid item xs={12} md={3}>
                      <TextField
                        label="End date"
                        type="date"
                        fullWidth
                        InputLabelProps={{ shrink: true }}
                        value={endDate}
                        onChange={(e) => setEndDate(e.target.value)}
                      />
                    </Grid>
                  </>
                ) : (
                  <Grid item xs={12} md={3}>
                    <TextField
                      label="Date"
                      type="date"
                      fullWidth
                      InputLabelProps={{ shrink: true }}
                      value={singleDate}
                      onChange={(e) => setSingleDate(e.target.value)}
                    />
                  </Grid>
                )}

                <Grid item xs={12} md={6} sx={{ display: "flex", gap: 1, alignItems: "center" }}>
                  <Button
                    variant="contained"
                    startIcon={<SearchIcon />}
                    onClick={fetchDurations}
                    disabled={loading}
                  >
                    {loading ? "Loading..." : "Run"}
                  </Button>

                  <Button
                    variant="outlined"
                    startIcon={<ClearIcon />}
                    onClick={() => {
                      setStartDate("");
                      setEndDate("");
                      setSingleDate("");
                      setCity("");
                      setData(null);
                      setError("");
                    }}
                  >
                    Clear
                  </Button>

                  <Tooltip title="Export Summary CSV (per-person)">
                    <span>
                      <IconButton onClick={exportSummaryCsv} disabled={!regionObj || (regionObj.employees || []).length === 0}>
                        <DownloadIcon />
                      </IconButton>
                    </span>
                  </Tooltip>

                  <Tooltip title="Export full report (Sr.No, ... Compliance)">
                    <span>
                      <Button variant="contained" onClick={exportReportCsv} disabled={!regionObj || (regionObj.employees || []).length === 0}>
                        Export Report
                      </Button>
                    </span>
                  </Tooltip>

                  <Tooltip title="Export raw swipes for this region/date range">
                    <span>
                      <Button variant="outlined" onClick={exportSwipesCsv} disabled={!regionObj || Object.keys(regionObj.swipes_by_date || {}).length === 0}>
                        Export Swipes
                      </Button>
                    </span>
                  </Tooltip>
                </Grid>

                <Grid item xs={12} md={4}>
                  <TextField
                    fullWidth
                    label="Search Employee ID"
                    value={searchEmployeeId}
                    onChange={(e) => setSearchEmployeeId(e.target.value)}
                    size="small"
                  />
                </Grid>
                <Grid item xs={12} md={4}>
                  <TextField
                    fullWidth
                    label="Search Employee Name"
                    value={searchEmployeeName}
                    onChange={(e) => setSearchEmployeeName(e.target.value)}
                    size="small"
                  />
                </Grid>
                <Grid item xs={12} md={4}>
                  <TextField
                    fullWidth
                    label="Search Card Number"
                    value={searchCardNumber}
                    onChange={(e) => setSearchCardNumber(e.target.value)}
                    size="small"
                  />
                </Grid>

              </Grid>
            </CardContent>
          </Card>
        </Grid>

        <Grid item xs={12}>
          <Paper sx={{ p: 2 }}>
            <Box sx={{ display: "flex", justifyContent: "space-between", alignItems: "center" }}>
              <Typography variant="subtitle1">
                {data ? `Showing ${region.toUpperCase()}  ${data.start_date}  ${data.end_date}` : "No results yet"}
              </Typography>
              <Typography variant="caption" color="text.secondary">
                Tip: use the CSV export for offline analysis
              </Typography>
            </Box>

            {loading && (
              <Box sx={{ display: "flex", justifyContent: "center", py: 4 }}>
                <CircularProgress />
              </Box>
            )}

            {error && (
              <Typography color="error" sx={{ mt: 2 }}>
                {error}
              </Typography>
            )}

            {!loading && !error && (
              <Box sx={{ mt: 2 }}>
                {renderTable()}
              </Box>
            )}
          </Paper>
        </Grid>
      </Grid>

      {/* Swipe dialog */}
      <Dialog open={swipeDialogOpen} onClose={() => setSwipeDialogOpen(false)} fullWidth maxWidth="xl">
        <DialogTitle>
          Swipe records for: {selectedEmployee ? `${selectedEmployee.EmployeeID || ""}  ${selectedEmployee.EmployeeName || ""}` : ""}
        </DialogTitle>
        <DialogContent dividers>
          {selectedSwipes.length === 0 ? (
            <Typography>No swipe records found for this employee in the selected range.</Typography>
          ) : (
            <Table size="small">
              <TableHead>
                <TableRow>
                  <TableCell><b>Date</b></TableCell>
                  <TableCell><b>Time (local)</b></TableCell>
                  <TableCell><b>Door</b></TableCell>
                  <TableCell><b>Direction</b></TableCell>
                  <TableCell><b>CardNumber</b></TableCell>
                  <TableCell><b>PersonnelType</b></TableCell>
                  <TableCell><b>Partition</b></TableCell>
                  <TableCell><b>PrimaryLocation</b></TableCell>
                  <TableCell><b>Company</b></TableCell>
                </TableRow>
              </TableHead>
              <TableBody>
                {selectedSwipes.map((s, i) => (
                  <TableRow key={i}>
                    <TableCell>{s.Date}</TableCell>
                    <TableCell>{s.Swipe_Time ?? (s.LocaleMessageTime ? new Date(s.LocaleMessageTime).toLocaleString() : "-")}</TableCell>
                    <TableCell>{s.Door || s.ObjectName1 || "-"}</TableCell>
                    <TableCell>{s.Direction || "-"}</TableCell>
                    <TableCell>{s.CardNumber || "-"}</TableCell>
                    <TableCell>{s.PersonnelType || "-"}</TableCell>
                    <TableCell>{s.PartitionName2 || "-"}</TableCell>
                    <TableCell>{s.PrimaryLocation || s.Text5 || "-"}</TableCell>
                    <TableCell>{s.CompanyName || "-"}</TableCell>
                  </TableRow>
                ))}
              </TableBody>
            </Table>
          )}
        </DialogContent>
        <Divider />
        <Box sx={{ p: 1, display: "flex", justifyContent: "flex-end" }}>
          <Button onClick={() => setSwipeDialogOpen(false)}>Close</Button>
        </Box>
      </Dialog>
    </Box>
  );
}












