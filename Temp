i Think we need to update as per duration file there are multiple city mention 
Generarte 90 days file for all city 
Note - Currently Build only for November( 23 days) 

for all city 
as per City upadte path for output..
also integrate build_90day_training.py as per mention city name 

then will move once check all file line by line and upadte each file carefully..

Refer Duration file Dont make changes in Duration file..


C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py


# backend/duration_report.py
from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np
import hashlib

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name (override with environment variable if needed)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
try:
    from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    # fallback — keep behaviour if config file unavailable
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

    def map_door_to_zone(door: object, direction: object = None) -> str:
        """
        Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
        """
        try:
            if door is None:
                return None
            s = str(door).strip()
            if not s:
                return None
            s_l = s.lower()
            if direction and isinstance(direction, str):
                d = direction.strip().lower()
                if "out" in d:
                    return OUT_OF_OFFICE_ZONE
                if "in" in d:
                    return "Reception Area"
            if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
                return OUT_OF_OFFICE_ZONE
            return "Working Area"
        except Exception:
            return None

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "SG.Singapore", "PH.Manila","IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023"
        ],
        "partitions": ["LT.Vilnius","IT.Rome","UK.London","IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition","PA.Panama City","PE.Lima", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["US.CO.OBS", "USA/Canada Default", "US.FL.Miami", "US.NYC"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# GENERIC SQL template - we keep AdjustedMessageTime in SELECT for debugging but the Python
# logic in this file no longer depends on it for date assignment.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND t3.[Name] = 'Employee'
  {date_condition}
  {region_filter}
"""

# Helpers
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

# GUID / placeholders helpers
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])



def _strip_person_uid_prefix(token: object) -> Optional[str]:
    """
    If token is like 'emp:123' or 'uid:GUID' or 'name:xxxxx' return the suffix;
    otherwise return plain stripped string.
    Returns None for empty/placeholder tokens.
    """
    if token is None:
        return None
    try:
        s = str(token).strip()
        if not s:
            return None
        # common canonical prefixes used by duration_report: emp:, uid:, name:
        if ':' in s:
            prefix, rest = s.split(':', 1)
            if prefix.lower() in ('emp', 'uid', 'name'):
                rest = rest.strip()
                if rest:
                    return rest
        return s
    except Exception:
        return None

# keep existing _strip_person_uid_prefix (unchanged) and expose a shorter alias
def _strip_person_uid_prefix(token: object) -> Optional[str]:
    """
    Existing behaviour: if token like 'emp:123' or 'uid:GUID' or 'name:xxxxx' return suffix;
    otherwise return plain stripped string. Returns None for empty/placeholder tokens.
    """
    if token is None:
        return None
    try:
        s = str(token).strip()
        if not s:
            return None
        # common canonical prefixes used by duration_report: emp:, uid:, name:
        if ':' in s:
            prefix, rest = s.split(':', 1)
            if prefix.lower() in ('emp', 'uid', 'name'):
                rest = rest.strip()
                if rest:
                    return rest
        return s
    except Exception:
        return None

# Alias for modules that expect `_strip_uid_prefix`
def _strip_uid_prefix(token: object) -> Optional[str]:
    """
    Backwards-compatible alias for _strip_person_uid_prefix.
    """
    return _strip_person_uid_prefix(token)



def _looks_like_guid(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
        except Exception:
            continue
        if not s:
            continue
        if _is_placeholder_str(s):
            continue
        if _looks_like_guid(s):
            continue
        return s
    return None

def _canonical_person_uid_from_row(row):
    """
    Produce canonical person_uid in the form:
      - 'emp:<employeeid>' (if a sensible non-GUID EmployeeID present),
      - 'uid:<EmployeeIdentity>' (if present),
      - 'name:<sha1 10chars>' fallback when name present,
      - otherwise None.
    """
    empid = None
    for cand in ('EmployeeID', 'Int1', 'Text12'):
        if cand in row and row.get(cand) not in (None, '', float('nan')):
            empid = row.get(cand)
            break
    empident = row.get("EmployeeIdentity", None)
    name = row.get("EmployeeName", None)

    # normalize empid numeric floats -> ints
    if empid is not None:
        try:
            s = str(empid).strip()
            if '.' in s:
                f = float(s)
                if f.is_integer():
                    s = str(int(f))
            if s and not _looks_like_guid(s) and not _is_placeholder_str(s):
                return f"emp:{s}"
        except Exception:
            pass

    if empident not in (None, '', float('nan')):
        try:
            si = str(empident).strip()
            if si:
                return f"uid:{si}"
        except Exception:
            pass

    if name not in (None, '', float('nan')):
        try:
            sn = str(name).strip()
            if sn and not _looks_like_guid(sn) and not _is_placeholder_str(sn):
                h = hashlib.sha1(sn.lower().encode('utf8')).hexdigest()[:10]
                return f"name:{h}"
        except Exception:
            pass

    return None

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        partitions = rc.get("partitions", [])
        if partitions:
            parts_sql = ", ".join(f"'{p}'" for p in partitions)
            region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
        else:
            likes = rc.get("logical_like", [])
            if likes:
                like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
                region_filter = f"AND ({like_sql})"
            else:
                region_filter = ""
    else:
        region_filter = ""

    # NOTE: AdjustedMessageTime / 2AM boundary logic removed from date selection.
    date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
        "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
        "Direction", "CompanyName", "PrimaryLocation"
    ]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    # ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Dates parsing
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # Keep AdjustedMessageTime if present for debugging but we do NOT use it for date boundaries anymore.
    try:
        if "AdjustedMessageTime" in df.columns:
            df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
        else:
            df["AdjustedMessageTime"] = pd.NaT
    except Exception:
        df["AdjustedMessageTime"] = pd.NaT

    # defensive: make text fields strings (avoid object type surprises)
    for tcol in ("Door", "PartitionName2", "PersonnelTypeName", "EmployeeName", "CompanyName", "PrimaryLocation"):
        if tcol in df.columns:
            df[tcol] = df[tcol].fillna("").astype(str)

    # Filter: only Employees (defensive; the SQL template already requests t3.Name = 'Employee')
    try:
        if "PersonnelTypeName" in df.columns:
            df = df[df["PersonnelTypeName"].str.strip().str.lower() == "employee"].copy()
    except Exception:
        logging.debug("Could not apply PersonnelTypeName filter for region %s", region_key)

    # canonical person_uid (consistent with trend_runner)
    def make_person_uid(row):
        try:
            return _canonical_person_uid_from_row(row)
        except Exception:
            # fallback: try simple concatenation if canonical fails
            try:
                eid = row.get("EmployeeIdentity")
                if pd.notna(eid) and str(eid).strip() != "":
                    return str(eid).strip()
            except Exception:
                pass
            parts = []
            for c in ("EmployeeID", "CardNumber", "EmployeeName"):
                try:
                    v = row.get(c)
                    if v not in (None, '', float('nan')):
                        parts.append(str(v).strip())
                except Exception:
                    continue
            return "|".join(parts) if parts else None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)

    # APAC partition normalization (more robust)
    if region_key == "apac" and not df.empty:
        def normalize_apac_partition(row):
            """
            Minimal / strict APAC partition mapping (only adjust Taguig / Quezon / KL cases).
            - APAC_PI_*  -> Taguig City
            - APAC_PH_*  -> Quezon City
            - APAC_MY_KL* or APAC_MY + KL -> MY.Kuala Lumpur
            If none match, fall back to previously-known tokens or keep original PartitionName2.
            """
            door = str(row.get("Door") or "") or ""
            part = str(row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()

            # 1) Strict: APAC_PI => Taguig City
            # Matches examples like: "APAC_PI_Manila_DR_MainEntrance"
            if re.search(r'\bAPAC[_\-]?PI\b', d) or re.search(r'\bAPAC[_\-]?PI[_\-]', d):
                return "Taguig City"

            # 2) Strict: APAC_PH => Quezon City
            # Matches examples like: "APAC_PH_Manila_7th Floor_Open Office Door 2-721"
            if re.search(r'\bAPAC[_\-]?PH\b', d) or re.search(r'\bAPAC[_\-]?PH[_\-]', d):
                return "Quezon City"

            # 3) Strict: Kuala Lumpur patterns
            # Accept variants like:
            #   "APAC_MY_KL_MAIN ENTRANCE DOOR", "APAC_MY_KL", "APAC_MY KL MAIN", "APAC_MY_KUALA-LUMPUR"
            # We require APAC + MY + (KL or KUALA) tokens in the door/partition string.
            # if ("APAC" in d) and ("MY" in d) and (
            #     "KL" in d or
            #     "KUALA" in d or
            #     re.search(r'KUALA[^A-Z0-9]*LUMPUR', d)
            # ):
            #     return "MY.Kuala Lumpur"


            # 3) Strict: Kuala Lumpur patterns (robust to underscores, dashes, spaces, extra text)
            # Matches: APAC_MY_KL_MAIN ENTRANCE DOOR, APAC-MY-KL, APAC MY KUALA-LUMPUR, etc.
            if re.search(r'APAC[^A-Z0-9]*MY[^A-Z0-9]*(?:KL\b|KUALA\b|KUALA[^A-Z0-9]*LUMPUR)', d):
                return "MY.Kuala Lumpur"



            # --- Minimal remaining token map (kept small & safe) ---
            token_map = {
                "APAC_IN_PUN": "Pune",
                "APAC_PUN": "Pune",
                "VIS_PUN": "Pune",
                "VIS_PUN_177": "Pune",
                "PUN": "Pune",
                "APAC_IN_HYD": "IN.HYD",
                "APAC_HYD": "IN.HYD",
                "HYD": "IN.HYD",
                "IN.HYD": "IN.HYD",
                "SG": "SG.Singapore",
                "SINGAPORE": "SG.Singapore",
                
            }

            # Prefer explicit tokens already present in PartitionName2
            for key, canonical in token_map.items():
                if key in p:
                    return canonical

            # Check door/partition tokens for any remaining known tokens
            toks = [t for t in re.split(r'[^A-Z0-9]+', d + " " + p) if t]
            for t in toks:
                if t in token_map:
                    return token_map[t]

            # If PartitionName2 already has a meaningful value, keep it
            if p and p.strip():
                return part

            # Unknown -> return empty string (caller applies strict masking)
            return ""

            # helper: split door/partition into alphanumeric tokens (keeps mixed tokens like VIS_PUN)
            def make_tokens(s: str):
                toks = [t for t in re.split(r'[^A-Z0-9\-]+', s or "") if t]
                return toks

            # 1) If PartitionName2 already contains a known canonical token, prefer it.
            for key, canonical in token_map.items():
                if key in p:
                    return canonical

            # 2) Tokenize Door and PartitionName2 and check tokens (exact token match)
            door_tokens = make_tokens(d)
            part_tokens = make_tokens(p)
            all_tokens = door_tokens + part_tokens
            for t in all_tokens:
                if t in token_map:
                    return token_map[t]

            # 3) Substring / regex fallbacks -- more permissive
            # Taguig-specific patterns
            if re.search(r'\bTAGUIG\b', d) or re.search(r'\bTAGUIG\b', p):
                return "Taguig City"
            # Quezon / Manila
            if re.search(r'\bQUEZON\b', d) or re.search(r'\bQUEZON\b', p) or re.search(r'\bMANILA\b', d) or re.search(r'\bMANILA\b', p):
                return "Quezon City"
            # Pune / PUN
            if re.search(r'\bPUN(E)?\b', d) or re.search(r'\bPUN(E)?\b', p):
                return "Pune"
            # Hyderabad
            if re.search(r'\bHYD\b', d) or re.search(r'\bHYD\b', p):
                return "IN.HYD"
            # Singapore
            if re.search(r'\bSINGAPORE\b', d) or re.search(r'\bSG\b', d) or re.search(r'\bSINGAPORE\b', p):
                return "SG.Singapore"
            # Kuala Lumpur / MY
            if re.search(r'\bKUALA\b', d) or re.search(r'\bKUALA\b', p) or re.search(r'\bMY\b', d) or re.search(r'\bMY\b', p) or re.search(r'KUALA.?LUMPUR', d):
                return "MY.Kuala Lumpur"

            # 4) If PartitionName2 is a non-empty meaningful value, keep it (no change)
            if p and p.strip():
                return part

            # 5) Unknown: return empty string (caller can filter strictly if needed)
            return ""

        # apply mapping and log the mapping summary for debugging
        df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)
        try:
            vc = df["PartitionName2"].value_counts(dropna=False).to_dict()
            logging.info("APAC PartitionName2 mapping counts example: %s", {k: vc.get(k, 0) for k in list(vc)[:10]})
        except Exception:
            logging.debug("APAC partition mapping counts unavailable")


    # NAMER: normalize PartitionName2 and add LogicalLocation per previous behaviour
    if region_key == "namer" and not df.empty:
        def namer_partition_and_logical(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()
            normalized = part
            logical = "Other"

            if ("US.CO.HQ" in d) or ("HQ" in d and "HQ" in d[:20]) or ("DENVER" in d) or (p == "US.CO.OBS"):
                normalized = "US.CO.OBS"
                logical = "Denver-HQ"
            elif "AUSTIN" in d or "AUSTIN TX" in d or p == "USA/CANADA DEFAULT":
                normalized = "USA/Canada Default"
                logical = "Austin Texas"
            elif "MIAMI" in d or p == "US.FL.MIAMI":
                normalized = "US.FL.Miami"
                logical = "Miami"
            elif "NYC" in d or "NEW YORK" in d or p == "US.NYC":
                normalized = "US.NYC"
                logical = "New York"
            else:
                if p == "US.CO.OBS":
                    normalized = "US.CO.OBS"; logical = "Denver-HQ"
                elif p == "USA/CANADA DEFAULT":
                    normalized = "USA/Canada Default"; logical = "Austin Texas"
                elif p == "US.FL.MIAMI":
                    normalized = "US.FL.Miami"; logical = "Miami"
                elif p == "US.NYC":
                    normalized = "US.NYC"; logical = "New York"
                else:
                    normalized = part
                    logical = "Other"
            return pd.Series({"PartitionName2": normalized, "LogicalLocation": logical})

        mapped = df.apply(namer_partition_and_logical, axis=1)
        df["PartitionName2"] = mapped["PartitionName2"].astype(str)
        df["LogicalLocation"] = mapped["LogicalLocation"].astype(str)

    # ensure PartitionName2 column exists as string
    if "PartitionName2" not in df.columns:
        df["PartitionName2"] = ""

    # ensure LogicalLocation exists
    if "LogicalLocation" not in df.columns:
        df["LogicalLocation"] = ""

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]

# ---------------------------------------------------------------------
# compute_daily_durations (single robust implementation)
# ---------------------------------------------------------------------
# def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
#     out_cols = [
#         "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
#         "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
#         "DurationSeconds", "Duration", "DurationMinutes", "DurationDisplay", "DurationHMS",
#         "PersonnelTypeName", "PartitionName2",
#         "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
#     ]

#     if swipes_df is None or swipes_df.empty:
#         return pd.DataFrame(columns=out_cols)

#     df = swipes_df.copy()
#     expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
#                 "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
#     for col in expected:
#         if col not in df.columns:
#             df[col] = None

#     # parse datetimes if present
#     try:
#         if df["LocaleMessageTime"].dtype == object:
#             df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
#     except Exception:
#         df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

#     # drop near-duplicate swipes (round to seconds)
#     try:
#         df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
#         dedupe_subset = ["person_uid", "_lts_rounded", "CardNumber", "Door"]
#         df = df.drop_duplicates(subset=dedupe_subset, keep="first").copy()
#     except Exception:
#         dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
#         try:
#             df = df.drop_duplicates(subset=dedupe_cols, keep="first")
#         except Exception:
#             pass

#     # Date assignment (STRICT: only use LocaleMessageTime.date())
#     try:
#         df["Date"] = df["LocaleMessageTime"].dt.date
#     except Exception:
#         try:
#             df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
#         except Exception:
#             df["Date"] = None

#     # ensure person_uid (keep existing canonicalization where present)
#     df["person_uid"] = df.apply(
#         lambda row: row.get("person_uid")
#         if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
#         else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
#         axis=1
#     )
#     df = df[df["person_uid"].notna()].copy()

#     # Group and aggregate — pick first non-empty EmployeeName and EmployeeID
#     try:
#         df = df.sort_values("LocaleMessageTime")
#         grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
#             FirstSwipe=("LocaleMessageTime", "first"),
#             LastSwipe=("LocaleMessageTime", "last"),
#             FirstDoor=("Door", "first"),
#             LastDoor=("Door", "last"),
#             CountSwipes=("LocaleMessageTime", "count"),
#             EmployeeIdentity=("EmployeeIdentity", "first"),
#             EmployeeID=("EmployeeID", lambda s: _pick_first_non_guid_value(s)),
#             EmployeeName=("EmployeeName", lambda s: _pick_first_non_guid_value(s)),
#             CardNumber=("CardNumber", lambda s: _pick_first_non_guid_value(s)),
#             PersonnelTypeName=("PersonnelTypeName", "first"),
#             PartitionName2=("PartitionName2", "first"),
#             CompanyName=("CompanyName", "first"),
#             PrimaryLocation=("PrimaryLocation", "first"),
#             FirstDirection=("Direction", "first"),
#             LastDirection=("Direction", "last")
#         ).reset_index()
#     except Exception:
#         def agg_for_group(g):
#             g_sorted = g.sort_values("LocaleMessageTime")
#             first = g_sorted.iloc[0]
#             last = g_sorted.iloc[-1]
#             empid = _pick_first_non_guid_value(g_sorted["EmployeeID"]) if "EmployeeID" in g_sorted else first.get("EmployeeID")
#             ename = _pick_first_non_guid_value(g_sorted["EmployeeName"]) if "EmployeeName" in g_sorted else first.get("EmployeeName")
#             cnum = _pick_first_non_guid_value(g_sorted["CardNumber"]) if "CardNumber" in g_sorted else first.get("CardNumber")
#             return pd.Series({
#                 "person_uid": first["person_uid"],
#                 "EmployeeIdentity": first.get("EmployeeIdentity"),
#                 "EmployeeID": empid,
#                 "EmployeeName": ename,
#                 "CardNumber": cnum,
#                 "Date": first["Date"],
#                 "FirstSwipe": first["LocaleMessageTime"],
#                 "LastSwipe": last["LocaleMessageTime"],
#                 "FirstDoor": first.get("Door"),
#                 "LastDoor": last.get("Door"),
#                 "CountSwipes": int(len(g_sorted)),
#                 "PersonnelTypeName": first.get("PersonnelTypeName"),
#                 "PartitionName2": first.get("PartitionName2"),
#                 "CompanyName": first.get("CompanyName"),
#                 "PrimaryLocation": first.get("PrimaryLocation"),
#                 "FirstDirection": first.get("Direction"),
#                 "LastDirection": last.get("Direction")
#             })
#         grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

#     # # compute duration using LocaleMessageTime first/last (wall-clock)
#     # grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
#     # grouped["Duration"] = grouped["DurationSeconds"].apply(
#     #     lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
#     # )



#         # compute duration using LocaleMessageTime first/last (wall-clock)
#     grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)

#     # Format Duration as total hours:minutes (e.g. "02:13" or "26:05")
#     def _seconds_to_hhmm(seconds_val):
#         try:
#             if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
#                 return None
#             total = int(round(float(seconds_val)))
#             hours = total // 3600
#             minutes = (total % 3600) // 60
#             # hours may be >24; keep full hours
#             return f"{hours}:{minutes:02d}"
#         except Exception:
#             return None

#     grouped["Duration"] = grouped["DurationSeconds"].apply(_seconds_to_hhmm)


#     # helper: format minutes to "Hh Mm"
#     def _format_minutes_to_hhmm(seconds_val):
#         try:
#             if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
#                 return None
#             total_minutes = int(round(float(seconds_val) / 60.0))
#             h = total_minutes // 60
#             m = total_minutes % 60
#             return f"{h}h {m}m"
#         except Exception:
#             return None

#     # Duration in integer minutes (useful where UI currently shows '689 min')
#     # grouped["DurationMinutes"] = grouped["DurationSeconds"].apply(lambda s: int(round(s/60)) if pd.notna(s) else None)

#     # Human readable hours display, e.g. "11h 29m"
#     grouped["DurationDisplay"] = grouped["DurationSeconds"].apply(_format_minutes_to_hhmm)

#     # Also provide a standard H:MM:SS string
#     grouped["DurationHMS"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

#     # ensure all output cols exist
#     for c in out_cols:
#         if c not in grouped.columns:
#             grouped[c] = None

#     # drop helper column if present
#     if "_lts_rounded" in grouped.columns:
#         try:
#             grouped = grouped.drop(columns=["_lts_rounded"])
#         except Exception:
#             pass
#     if "_prev_ts" in grouped.columns:
#         try:
#             grouped = grouped.drop(columns=["_prev_ts"])
#         except Exception:
#             pass

#     return grouped[out_cols]





def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    """
    Robust, defensive implementation that:
     - accepts a swipes dataframe (may be empty)
     - ensures expected columns exist
     - deduplicates near-duplicate swipes (rounded to seconds)
     - assigns Date from LocaleMessageTime (strict local wall-time date)
     - groups by person_uid + Date and computes first/last, counts and durations
     - returns dataframe with stable output columns (same order as earlier code)
    """
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "DurationMinutes", "DurationDisplay", "DurationHMS",
        "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    # quick return for empty input
    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    # work on a copy
    df = swipes_df.copy()

    # ensure expected columns exist so later code can always reference them
    expected = [
        "EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
        "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"
    ]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes robustly
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # drop near-duplicate swipes (round to seconds)
    try:
        df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
        dedupe_subset = ["person_uid", "_lts_rounded", "CardNumber", "Door"]
        # If person_uid missing, fallback to EmployeeIdentity + LocaleMessageTime
        if df["person_uid"].isnull().all():
            dedupe_subset = ["EmployeeIdentity", "_lts_rounded", "CardNumber", "Door"]
        df = df.drop_duplicates(subset=dedupe_subset, keep="first").copy()
    except Exception:
        # best-effort fallback
        try:
            df = df.drop_duplicates(subset=["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"], keep="first")
        except Exception:
            pass

    # Date assignment (strict local date)
    try:
        df["Date"] = df["LocaleMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure person_uid column exists; fallback to a stable join key if missing
    if "person_uid" not in df.columns or df["person_uid"].isnull().all():
        def make_person_uid(row):
            try:
                # prefer canonical EmployeeID-like tokens
                for cand in ("EmployeeID", "Int1", "Text12"):
                    if cand in row and row.get(cand) not in (None, '', float('nan')):
                        s = str(row.get(cand)).strip()
                        if s:
                            return s
                # fallback to EmployeeIdentity
                if row.get("EmployeeIdentity") not in (None, '', float('nan')):
                    return str(row.get("EmployeeIdentity")).strip()
                # fallback to name
                if row.get("EmployeeName") not in (None, '', float('nan')):
                    return str(row.get("EmployeeName")).strip()
            except Exception:
                pass
            return None
        df["person_uid"] = df.apply(make_person_uid, axis=1)

    # drop rows with no person_uid or no Date (they are not groupable)
    df = df[df["person_uid"].notna() & df["Date"].notna()].copy()
    if df.empty:
        return pd.DataFrame(columns=out_cols)

    # sort then group to pick first/last and other aggregations
    try:
        df = df.sort_values(["person_uid", "Date", "LocaleMessageTime"])
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", lambda s: _pick_first_non_guid_value(s) if not s.empty else None),
            EmployeeName=("EmployeeName", lambda s: _pick_first_non_guid_value(s) if not s.empty else None),
            CardNumber=("CardNumber", lambda s: _pick_first_non_guid_value(s) if not s.empty else None),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        # fallback single-group aggregator (safer for unexpected frames)
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            empid = _pick_first_non_guid_value(g_sorted["EmployeeID"]) if "EmployeeID" in g_sorted else first.get("EmployeeID")
            ename = _pick_first_non_guid_value(g_sorted["EmployeeName"]) if "EmployeeName" in g_sorted else first.get("EmployeeName")
            cnum = _pick_first_non_guid_value(g_sorted["CardNumber"]) if "CardNumber" in g_sorted else first.get("CardNumber")
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": empid,
                "EmployeeName": ename,
                "CardNumber": cnum,
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    # compute duration seconds using first/last swipes (wall-clock)
    grouped["DurationSeconds"] = (pd.to_datetime(grouped["LastSwipe"]) - pd.to_datetime(grouped["FirstSwipe"])).dt.total_seconds().clip(lower=0)

    # format Duration as H:MM string (hours can exceed 24)
    def _seconds_to_hhmm(seconds_val):
        try:
            if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
                return None
            total = int(round(float(seconds_val)))
            hours = total // 3600
            minutes = (total % 3600) // 60
            return f"{hours}:{minutes:02d}"
        except Exception:
            return None

    grouped["Duration"] = grouped["DurationSeconds"].apply(_seconds_to_hhmm)

    # human readable minutes and hms representations
    def _format_minutes_to_hhmm(seconds_val):
        try:
            if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
                return None
            total_minutes = int(round(float(seconds_val) / 60.0))
            h = total_minutes // 60
            m = total_minutes % 60
            return f"{h}h {m}m"
        except Exception:
            return None

    grouped["DurationMinutes"] = grouped["DurationSeconds"].apply(lambda s: int(round(float(s) / 60.0)) if pd.notna(s) else None)
    grouped["DurationDisplay"] = grouped["DurationSeconds"].apply(_format_minutes_to_hhmm)
    grouped["DurationHMS"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    # ensure all requested out_cols exist (stable ordering)
    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    # drop helper column if present
    if "_lts_rounded" in grouped.columns:
        try:
            grouped = grouped.drop(columns=["_lts_rounded"])
        except Exception:
            pass

    # final column ordering and return
    return grouped[out_cols]



def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    # Defensive: normalize incoming regions list
    try:
        requested_regions = [r.lower() for r in (regions or []) if r]
    except Exception:
        requested_regions = []

    # If user provided a city/site, try to map that city to one or more region keys
    def _normalize_token(s: str) -> str:
        return re.sub(r'[^a-z0-9]', '', str(s or '').strip().lower())

    if city:
        city_raw = str(city).strip()
        city_norm = _normalize_token(city_raw)

        # find regions whose partitions or logical_like look like this city
        matched_regions = []
        for rkey, rc in (REGION_CONFIG or {}).items():
            parts = rc.get("partitions", []) or []
            likes = rc.get("logical_like", []) or []
            tokens = set()
            for p in parts:
                if not p:
                    continue
                tokens.add(_normalize_token(p))
                # also split on punctuation/dot and add parts (e.g. "LT.Vilnius" -> "vilnius")
                for part_piece in re.split(r'[.\-/\s]', str(p)):
                    if part_piece:
                        tokens.add(_normalize_token(part_piece))
            for lk in likes:
                tokens.add(_normalize_token(lk))
            # also include server/database names as a fallback
            if city_norm and city_norm in tokens:
                matched_regions.append(rkey)

        # If we could map city to specific region(s), only run those
        if matched_regions:
            requested_regions = [m for m in matched_regions]

    # fallback to all regions in config if none requested
    try:
        if not requested_regions:
            requested_regions = [k.lower() for k in list(REGION_CONFIG.keys())]
    except Exception:
        requested_regions = ['apac']

    results: Dict[str, Any] = {}
    for r in requested_regions:
        if not r:
            continue
        rkey = r.lower()
        if rkey not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", rkey, target_date)
        try:
            swipes = fetch_swipes_for_region(rkey, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", rkey)
            swipes = pd.DataFrame()

        # If a city was requested, apply a strict (but defensive) city filter
        if city and not swipes.empty:
            city_raw = str(city).strip()
            city_norm = _normalize_token(city_raw)

            alt_tokens = set()
            alt_tokens.add(city_raw)
            alt_tokens.add(city_raw.replace('-', ' '))
            alt_tokens.add(city_raw.replace('_', ' '))
            alt_tokens.add(city_raw.replace('.', ' '))
            alt_tokens.add(city_raw.replace(' ', '-'))
            alt_tokens.update({t.title() for t in list(alt_tokens)})
            if city_norm:
                alt_tokens.add(city_norm)

            def _norm_for_cmp(s):
                try:
                    if s is None:
                        return ''
                    return re.sub(r'[^a-z0-9]', '', str(s).strip().lower())
                except Exception:
                    return ''

            # precompute normalized columns (safe defaults)
            try:
                part_norm = swipes["PartitionName2"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "PartitionName2" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
                door_norm = swipes["Door"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "Door" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
                pl_norm = swipes["PrimaryLocation"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "PrimaryLocation" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
            except Exception:
                part_norm = pd.Series([''] * len(swipes), index=swipes.index)
                door_norm = pd.Series([''] * len(swipes), index=swipes.index)
                pl_norm = pd.Series([''] * len(swipes), index=swipes.index)

            # Build mask: start False
            mask = pd.Series(False, index=swipes.index)



            # 1) Strict: prefer PartitionName2 exact matches (canonical names from normalize)
            for t in alt_tokens:
                t_norm = _norm_for_cmp(t)
                if not t_norm:
                    continue
                try:
                    mask = mask | (part_norm == t_norm)
                except Exception:
                    continue

            # 2) For rows where PartitionName2 is empty / unknown, allow Door token contains match (permissive)
            try:
                no_part_mask = part_norm.fillna('').astype(str) == ''
                if no_part_mask.any():
                    door_mask = pd.Series(False, index=swipes.index)
                    for t in alt_tokens:
                        t_norm = _norm_for_cmp(t)
                        if not t_norm:
                            continue
                        door_mask = door_mask | door_norm.str.contains(t_norm, na=False)
                    # only accept door matches when partition is empty
                    mask = mask | (door_mask & no_part_mask)
            except Exception:
                logging.debug("Door-based fallback match failed for city filter in region %s", rkey)

            # 3) Allow PrimaryLocation match ONLY for rows with empty PartitionName2 and not already matched
            try:
                remaining_mask = ~mask
                pl_mask = pd.Series(False, index=swipes.index)
                for t in alt_tokens:
                    t_norm = _norm_for_cmp(t)
                    if not t_norm:
                        continue
                    pl_mask = pl_mask | pl_norm.str.contains(t_norm, na=False)
                mask = mask | (pl_mask & (part_norm.fillna('') == '') & remaining_mask)
            except Exception:
                logging.debug("PrimaryLocation fallback match failed for city filter in region %s", rkey)



            # --- Special-case matching for MY.Kuala Lumpur requested by user ---
            # Many KL door names are like "APAC_MY_KL_MAIN ENTRANCE DOOR" which normalize to
            # 'apacmyklmainentrancedoor' and won't contain 'mykualalumpur' as a contiguous token.
            # So when user asked for MY.Kuala Lumpur, accept any door that contains APAC + MY + (KL | KUALA | KUALALUMPUR).
            try:
                # city_norm is already normalized (non-alphanum removed). compare against expected KL token
                if city_norm in ("mykualalumpur", "mykuala", "kualalumpur", "kuala"):
                    kl_mask = (
                        door_norm.str.contains("apac", na=False)
                        & door_norm.str.contains("my", na=False)
                        & (
                            door_norm.str.contains("kl", na=False)
                            | door_norm.str.contains("kuala", na=False)
                            | door_norm.str.contains("kualalumpur", na=False)
                        )
                    )
                    # Accept KL door matches even if PartitionName2 is empty (safe & strict)
                    mask = mask | kl_mask
            except Exception:
                logging.debug("KL special-case matching failed for city filter")



            # 4) Finally, also check Door and EmployeeName contains across all rows (additional permissive matches)
            try:
                for col in ("Door", "EmployeeName"):
                    if col in swipes.columns:
                        col_norm = swipes[col].fillna("").astype(str).str.lower().apply(_norm_for_cmp)
                        col_mask = pd.Series(False, index=swipes.index)
                        for t in alt_tokens:
                            t_norm = _norm_for_cmp(t)
                            if not t_norm:
                                continue
                            col_mask = col_mask | col_norm.str.contains(t_norm, na=False)
                        mask = mask | col_mask
            except Exception:
                logging.debug("Door/EmployeeName contains-match fallback failed for city filter in region %s", rkey)

            # Apply the mask strictly
            before = len(swipes)
            swipes = swipes[mask].copy()
            logging.info("City filter '%s' applied for region %s: rows before=%d after=%d", city_raw, rkey, before, len(swipes))

        # compute durations for this region
        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", rkey)
            durations = pd.DataFrame()

        # frontend-friendly display columns for swipes
        try:
            if "LocaleMessageTime" in swipes.columns:
                swipes["LocaleMessageTime"] = pd.to_datetime(swipes["LocaleMessageTime"], errors="coerce")
                swipes["DateOnly"] = swipes["LocaleMessageTime"].dt.date
                swipes["Time"] = swipes["LocaleMessageTime"].dt.strftime("%H:%M:%S")
            else:
                if "Date" in swipes.columns and "Time" in swipes.columns:
                    swipes["DateOnly"] = swipes["Date"]
        except Exception:
            logging.debug("Frontend display enrichment failed for region %s", rkey)

        if "AdjustedMessageTime" not in swipes.columns:
            swipes["AdjustedMessageTime"] = pd.NaT

        # write outputs
        try:
            csv_path = outdir_path / f"{rkey}_duration_{target_date.strftime('%Y%m%d')}.csv"
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", rkey)
        try:
            swipes_csv_path = outdir_path / f"{rkey}_swipes_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", rkey)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", rkey, csv_path if 'csv_path' in locals() else '<unknown>', len(durations) if durations is not None else 0)
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", rkey, swipes_csv_path if 'swipes_csv_path' in locals() else '<unknown>', len(swipes) if swipes is not None else 0)

        results[rkey] = {"swipes": swipes, "durations": durations}

    return results


# end of file











# C:\Users\W0024618\Desktop\Trend Analysis\backend\ml_training.py
"""
Train one binary classifier per scenario using the training CSV produced by trend_runner.build_monthly_training.
Usage:
    python ml_training.py --input outputs/training_person_month.csv --models_dir models/
Outputs:
    models/<scenario>.joblib
Requirements:
    scikit-learn, joblib, pandas, numpy
"""
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)

try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report
    import joblib
except Exception as e:
    logging.error("Required ML packages missing: %s", e)
    logging.error("Install scikit-learn and joblib: pip install scikit-learn joblib")
    raise

DEFAULT_FEATURE_COLS = [
    'CountSwipes_median', 'CountSwipes_mean', 'CountSwipes_sum',
    'DurationMinutes_median', 'DurationMinutes_mean', 'DurationMinutes_sum',
    'MaxSwipeGapSeconds_max', 'MaxSwipeGapSeconds_median',
    'ShortGapCount_sum', 'UniqueDoors_median', 'UniqueLocations_median', 'RejectionCount_sum',
    'days_present'
]

def auto_detect_scenarios(df):
    scenario_labels = [c for c in df.columns if c.endswith('_label')]
    scenarios = [c[:-6] for c in scenario_labels]
    return scenarios

def prepare_features(df, features=None):
    if features is None:
        features = DEFAULT_FEATURE_COLS
    # ensure columns exist, fill missing with 0/median
    X = df.copy()
    for f in features:
        if f not in X.columns:
            X[f] = 0.0
    X = X[features].fillna(0.0)
    return X

def train_one(df, scenario, features):
    label_col = f"{scenario}_label"
    if label_col not in df.columns:
        logging.warning("Label %s not in dataframe, skipping", label_col)
        return None
    y = df[label_col].astype(int)
    if y.sum() == 0:
        logging.warning("No positive examples for %s; skipping model training", scenario)
        return None
    X = prepare_features(df, features)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    logging.info("Classification report for %s:\n%s", scenario, classification_report(y_test, y_pred, zero_division=0))
    return clf

def main(input_csv: Path, models_dir: Path, feature_cols=None):
    if not input_csv.exists():
        raise FileNotFoundError(f"input CSV not found: {input_csv}")
    df = pd.read_csv(input_csv)
    scenarios = auto_detect_scenarios(df)
    if not scenarios:
        logging.error("No scenarios ( *_label ) columns found in %s", input_csv)
        return
    models_dir.mkdir(parents=True, exist_ok=True)
    for s in scenarios:
        logging.info("Training for scenario: %s", s)
        clf = train_one(df, s, feature_cols)
        if clf is not None:
            outp = models_dir / f"{s}.joblib"
            joblib.dump({"model": clf, "features": (feature_cols or DEFAULT_FEATURE_COLS)}, outp)
            logging.info("Saved model to %s", outp)
        else:
            logging.info("Skipped training for %s", s)




if __name__ == "__main__":
    import os
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="training CSV (person-month) created by /train endpoint")
    parser.add_argument("--models_dir", default="models", help="folder to save models")
    parser.add_argument("--features", default=None, help="comma separated feature columns (optional)")
    args = parser.parse_args()

    input_csv = Path(args.input)
    # If path not found, try relative to backend project root (defensive)
    if not input_csv.exists():
        # assume script is inside backend/ or backend/scripts/
        candidate = Path(__file__).resolve().parents[1] / args.input
        if candidate.exists():
            input_csv = candidate
        else:
            # last attempt: try cwd + args.input
            candidate2 = Path(os.getcwd()) / args.input
            if candidate2.exists():
                input_csv = candidate2

    models_dir = Path(args.models_dir)
    feature_cols = args.features.split(",") if args.features else None

    if not input_csv.exists():
        raise FileNotFoundError(f"input CSV not found: {input_csv}. Run build_90day_training first to create it.")
    main(input_csv, models_dir, feature_cols)















#C:\Users\W0024618\Desktop\Trend Analysis\backend\scripts\build_90day_training.py
"""
Create a 90-day training CSV by calling into trend_runner.

Usage:
    python build_90day_training.py --end 2025-11-09 --outdir ./outputs --window 90 --min_unique_employees 1000 --city Pune --force

Behavior:
 - Prefers trend_runner.build_90day_training if available.
 - Falls back to trend_runner.build_monthly_training (approximate months) if not.
 - If --force is provided, it will remove the common 90-day cache file (trend_pune_90day_cache.csv) in outdir before building.
"""
from datetime import date
from pathlib import Path
import sys
import argparse
import os
import math
import shutil
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

# Defensive: make backend project root importable
HERE = Path(__file__).resolve()
PROJECT_ROOT = HERE.parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
os.chdir(str(PROJECT_ROOT))

# try to import helpers from trend_runner
try:
    import trend_runner as tr_mod
    # functions we may call
    build_90day_fn = getattr(tr_mod, "build_90day_training", None)
    build_monthly_fn = getattr(tr_mod, "build_monthly_training", None)
    # possible OUTDIR constant inside trend_runner
    TREND_OUTDIR = getattr(tr_mod, "OUTDIR", None)
except Exception:
    tr_mod = None
    build_90day_fn = None
    build_monthly_fn = None
    TREND_OUTDIR = None

def remove_cache_file(outdir: Path):
    # best-effort remove of common cache filename used by the app (fallback names)
    candidates = [
        outdir / "trend_pune_90day_cache.csv",
        outdir / "trend_pune_cache.csv",
        outdir / "trend_90day_cache.csv"
    ]
    removed = []
    for p in candidates:
        try:
            if p.exists():
                p.unlink()
                removed.append(str(p.name))
        except Exception:
            logging.exception("Failed removing cache file %s", p)
    if removed:
        logging.info("Removed cache files: %s", ", ".join(removed))
    else:
        logging.debug("No cache files removed from %s", outdir)

def main(end_date_str=None, outdir="./outputs", window_days=90,  force=False):
    if end_date_str:
        end_date = date.fromisoformat(end_date_str)
    else:
        end_date = date.today()

    out_path_dir = Path(outdir)
    # if trend_runner provides OUTDIR, prefer that unless user explicitly passed --outdir
    if TREND_OUTDIR and (outdir in (None, "./outputs", "") or str(out_path_dir) == "./outputs"):
        try:
            out_path_dir = Path(TREND_OUTDIR)
        except Exception:
            pass

    out_path_dir.mkdir(parents=True, exist_ok=True)

    if force:
        remove_cache_file(out_path_dir)

    # call the appropriate builder
    if build_90day_fn:
        logging.info("Calling trend_runner.build_90day_training(end_date=%s, window_days=%s, outdir=%s, min_unique_employees=%s, city=%s)",
                     end_date, window_days, out_path_dir, min_unique_employees, city)
        try:
            res = build_90day_fn(end_date=end_date, window_days=window_days, outdir=str(out_path_dir),
                                 min_unique_employees=min_unique_employees, city=city)
            if res:
                print("Training CSV created:", res)
                return res
            else:
                print("Training CSV not created. Check logs and ensure trend CSVs exist in", out_path_dir)
                return None
        except Exception as e:
            logging.exception("build_90day_training raised an exception")
            raise

    # fallback: if a monthly builder exists, call it with approximate months
    if build_monthly_fn:
        approx_months = max(1, int(round(float(window_days) / 30.0)))
        logging.info("trend_runner.build_90day_training not found; falling back to build_monthly_training(months=%s)", approx_months)
        try:
            res = build_monthly_fn(end_date=end_date, months=approx_months, min_unique_employees=min_unique_employees, outdir=str(out_path_dir))
            if res:
                print("Training CSV created (via monthly builder):", res)
                return res
            else:
                print("Training CSV not created (monthly builder). Check logs and ensure trend CSVs exist in", out_path_dir)
                return None
        except Exception:
            logging.exception("build_monthly_training failed")
            raise

    logging.error("No suitable build function found in trend_runner (neither build_90day_training nor build_monthly_training).")
    raise RuntimeError("trend_runner missing required build function")

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--end", help="end date (YYYY-MM-DD). default = today", default=None)
    p.add_argument("--outdir", help="outputs dir", default="./outputs")
    p.add_argument("--min_unique_employees", type=int, default=1000)
    p.add_argument("--window", type=int, default=90, help="window days (default 90)")
    p.add_argument("--city", default="Pune")
    p.add_argument("--force", action="store_true", help="force refresh (remove cache files before building)")
    args = p.parse_args()
    main(end_date_str=args.end, outdir=args.outdir, window_days=args.window,
         min_unique_employees=args.min_unique_employees, city=args.city, force=args.force)












#C:\Users\W0024618\Desktop\Trend Analysis\backend\scripts\generate_90_days.py
"""
Generate trend CSVs for a sliding window of days by calling trend_runner.run_trend_for_date.

Usage:
    python generate_90_days.py --end 2025-11-09 --window 90 --city Pune --outdir ./outputs

Notes:
 - Will create outputs directory if missing.
 - Retries a single date once on failure (useful for flaky raw ingestion).
"""
from datetime import date, timedelta
from pathlib import Path
import sys
import os
import argparse
import logging
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

# Make the backend project root importable when running the script directly.
HERE = Path(__file__).resolve()
PROJECT_ROOT = HERE.parents[1]

if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# Ensure we run with backend as CWD so relative paths like "./outputs" behave predictably
os.chdir(str(PROJECT_ROOT))

# Now imports should work
try:
    from trend_runner import run_trend_for_date, OUTDIR as TREND_OUTDIR
except Exception:
    # if import fails, let the script still try and fail later with clearer message
    run_trend_for_date = None
    TREND_OUTDIR = None

def process_dates(start_date: date, end_date: date, outdir: Path, city: str, max_retries: int = 1, pause_seconds: float = 1.0):
    d = start_date
    while d <= end_date:
        attempt = 0
        success = False
        while attempt <= max_retries and not success:
            try:
                logging.info("Processing %s (attempt %s)", d.isoformat(), attempt + 1)
                if run_trend_for_date is None:
                    raise RuntimeError("trend_runner.run_trend_for_date not importable. Ensure trend_runner is on PYTHONPATH.")
                # call with both outdir and city if supported
                # many implementations accept (date, outdir=str(...), city=...) — passing both is safe for kwargs
                run_trend_for_date(d, outdir=str(outdir), city=city)
                success = True
            except TypeError as te:
                # maybe signature doesn't accept city/outdir as kwargs; try positional fallback
                logging.warning("run_trend_for_date signature mismatch (%s). Trying positional call.", te)
                try:
                    run_trend_for_date(d)
                    success = True
                except Exception as e2:
                    logging.exception("Positional run_trend_for_date call also failed: %s", e2)
            except Exception as e:
                logging.exception("Failed for %s: %s", d.isoformat(), e)
            attempt += 1
            if not success and attempt <= max_retries:
                time.sleep(pause_seconds)
        if not success:
            logging.error("Giving up for %s after %s attempts", d.isoformat(), max_retries + 1)
        d = d + timedelta(days=1)

def main(end_date=None, window_days=90, outdir="./outputs", city="Pune", start_date_override=None):
    if end_date:
        end_dt = date.fromisoformat(end_date)
    else:
        end_dt = date.today()

    if start_date_override:
        start_dt = date.fromisoformat(start_date_override)
    else:
        start_dt = end_dt - timedelta(days=window_days - 1)

    out_path = Path(outdir)
    # prefer TREND_OUTDIR constant from trend_runner if user did not supply explicit outdir
    if TREND_OUTDIR and outdir in ("./outputs", None, ""):
        try:
            out_path = Path(TREND_OUTDIR)
        except Exception:
            pass

    out_path.mkdir(parents=True, exist_ok=True)

    logging.info("Generating trend CSVs for %s -> %s into %s (city=%s)", start_dt.isoformat(), end_dt.isoformat(), out_path, city)
    process_dates(start_dt, end_dt, out_path, city, max_retries=1, pause_seconds=1.0)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--end", help="end date (YYYY-MM-DD). default = today", default=None)
    parser.add_argument("--start", help="start date (YYYY-MM-DD). optional, overrides window", default=None)
    parser.add_argument("--outdir", help="outputs dir", default="./outputs")
    parser.add_argument("--window", type=int, default=90, help="window days (default 90)")
    parser.add_argument("--city", default="Pune")
    args = parser.parse_args()

    main(end_date=args.end, window_days=args.window, outdir=args.outdir, city=args.city, start_date_override=args.start)




