read below trend runner.py file line by line and 
Updgrade long gap logic like 
where Where upgrade long gap like greater than eeual to 4 hr  
4.5  to 4 hr 
only and 
count Violation if there is gap 

# backend/trend_runner.py
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re
import os
import calendar
import json
from collections import defaultdict
from datetime import datetime as _datetime
from datetime import timedelta as _timedelta
from typing import Optional, List

# ------------------ personnel enrichment (lazy) ------------------
def _get_personnel_funcs_lazy():
    """
    Try to import personnel helpers from (preferably) the app module at call time (avoids circular imports).
    Fallbacks:
      - try 'app'
      - try 'backend.app'
      - try direct 'employeeimage' module
    Returns tuple (get_personnel_info_fn_or_None, get_person_image_bytes_fn_or_None).
    """
    import importlib, logging
    mod_names = ['app', 'backend.app', 'employeeimage', 'backend.employeeimage']
    for mn in mod_names:
        try:
            mod = importlib.import_module(mn)
            gpi = getattr(mod, 'get_personnel_info', None)
            gpib = getattr(mod, 'get_person_image_bytes', None)
            if gpi or gpib:
                logging.debug("_get_personnel_funcs_lazy: using module %s (gpi=%s gpib=%s)", mn, bool(gpi), bool(gpib))
                return gpi, gpib
        except Exception:
            continue
    logging.debug("_get_personnel_funcs_lazy: no personnel helpers found in tried modules")
    return None, None


def _normalize_for_lookup(val):
    if val is None:
        return None
    s = str(val).strip()
    if not s:
        return None
    if len(s) > 36 and '-' in s:
        return s
    return s


def _enrich_with_personnel_info(df, image_endpoint_template="/employee/{}/image"):
    if df is None or df.empty:
        return df
    get_personnel_info, get_person_image_bytes = _get_personnel_funcs_lazy()
    emails = []
    image_urls = []
    for _, row in df.iterrows():
        email = None
        image_url = None
        cand_empid = None
        # prefer non-guid employee tokens
        for tok in ('EmployeeID', 'EmployeeID_feat', 'EmployeeID_dur', 'Int1', 'Text12'):
            if tok in row and row.get(tok) not in (None, '', float('nan')):
                cand_empid = _normalize_for_lookup(row.get(tok))
                break
        cand_uid = row.get('EmployeeIdentity') or row.get('person_uid') or None

        # 1) try the personnel helper if available
        if get_personnel_info:
            try:
                lookup = cand_empid or cand_uid
                if lookup:
                    pi = get_personnel_info(lookup)
                    if pi and isinstance(pi, dict):
                        # normalize email keys from returned dict
                        email = email or pi.get('EmployeeEmail') or pi.get('EmailAddress') or pi.get('Email') or pi.get('WorkEmail') or None
                        # prefer objectid/guid as image parent
                        parent = pi.get('ObjectID') or pi.get('GUID') or None
                        if parent:
                            image_url = image_endpoint_template.format(str(parent))
            except Exception:
                # non-fatal - continue to other fallbacks
                pass

        # 2) fallback: use any email present in the row itself
        if email is None:
            for fld in ('EmployeeEmail', 'Email', 'EmailAddress', 'ManagerEmail', 'WorkEmail', 'EMail'):
                if fld in row and row.get(fld) not in (None, '', float('nan')):
                    val = row.get(fld)
                    try:
                        if isinstance(val, str) and val.strip():
                            email = val.strip()
                            break
                    except Exception:
                        email = val
                        break

        # 3) build image url from best candidate id if not set already
        if image_url is None:
            if cand_empid:
                image_url = image_endpoint_template.format(cand_empid)
            elif cand_uid:
                image_url = image_endpoint_template.format(cand_uid)

        emails.append(email)
        image_urls.append(image_url)
    out = df.copy()
    out['EmployeeEmail'] = emails
    out['imageUrl'] = image_urls
    return out


# ---------------------------------------------------------------------------

# ------------------ duration_report imports (robust) ------------------
try:
    from duration_report import run_for_date, compute_daily_durations, REGION_CONFIG
except Exception:
    try:
        from duration_report import run_for_date, compute_daily_durations
        REGION_CONFIG = {}
    except Exception:
        try:
            from duration_report import run_for_date
            compute_daily_durations = None
            REGION_CONFIG = {}
        except Exception:
            run_for_date = None
            compute_daily_durations = None
            REGION_CONFIG = {}

# ------------------ optional config door_zone mapping ------------------
try:
    from config.door_zone import map_door_to_zone as config_map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    config_map_door_to_zone = None
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

# History profile (optional)
CANDIDATE_HISTORY = [
    Path(__file__).parent / "config" / "current_analysis.csv",
    Path(__file__).parent.parent / "config" / "current_analysis.csv",
    Path.cwd() / "current_analysis.csv",
    Path(__file__).parent / "current_analysis.csv"
]
HIST_PATH = None
for p in CANDIDATE_HISTORY:
    if p.exists():
        HIST_PATH = p
        break

if HIST_PATH:
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception:
        logging.warning("Failed to load historical profile from %s", HIST_PATH)
        HIST_DF = pd.DataFrame()
else:
    HIST_DF = pd.DataFrame()

# Outdir
OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)

# Small helpers
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')


# ---------------------------------------------------------------------------
# small time formatting helper used for raw_swipes_all
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return "-"
        s = int(seconds)
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return "-"


def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _normalize_id_val(v):
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s

def _looks_like_guid(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        if _looks_like_guid(st):
            return False
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None

# Short card xml extractor
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None

# door -> zone mapping fallback
try:
    _BREAK_ZONES = BREAK_ZONES
    _OUT_OF_OFFICE_ZONE = OUT_OF_OFFICE_ZONE
except Exception:
    _BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    _OUT_OF_OFFICE_ZONE = "Out of office"

def map_door_to_zone(door: object, direction: object = None) -> str:
    try:
        if config_map_door_to_zone is not None:
            return config_map_door_to_zone(door, direction)
    except Exception:
        pass
    try:
        if door is None:
            return None
        s = str(door).strip()
        if not s:
            return None
        s_l = s.lower()
        if direction and isinstance(direction, str):
            d = direction.strip().lower()
            if "out" in d:
                return _OUT_OF_OFFICE_ZONE
            if "in" in d:
                return "Reception Area"
        if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
            return _OUT_OF_OFFICE_ZONE
        return "Working Area"
    except Exception:
        return None

# ----- Config and scenarios -----
VIOLATION_WINDOW_DAYS = 90
RISK_THRESHOLDS = [
    (0.5, "Low"),
    (1.5, "Low Medium"),
    (2.5, "Medium"),
    (4.0, "Medium High"),
    (float("inf"), "High"),
]

def map_score_to_label(score: float) -> (int, str):
    try:
        if score is None:
            score = 0.0
        s = float(score)
    except Exception:
        s = 0.0
    bucket = 1
    label = "Low"
    for i, (threshold, lbl) in enumerate(RISK_THRESHOLDS, start=1):
        if s <= threshold:
            bucket = i
            label = lbl
            break
    return bucket, label



# scenario functions (kept from your improved version)
def scenario_long_gap(row):
    try:
        gap = int(row.get('MaxSwipeGapSeconds') or 0)
        return gap >= int(4.5 * 3600)
    except Exception:
        return False

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 5 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 5

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 14 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60


# def scenario_zero_swipes(row):
    """
    Return True if CountSwipes is zero (or effectively zero/empty).
    Be defensive: handle None, NaN, numeric strings, floats etc.
    """
    try:
        v = row.get('CountSwipes', 0)
        # handle pandas NaN
        try:
            import pandas as _pd
            if _pd.isna(v):
                v = 0
        except Exception:
            pass
        # convert strings/numeric-like to float -> int
        if v is None:
            return True  # treat missing as zero-swipes for scenario detection
        try:
            # float handles "0.0", "0", "0.00" etc
            num = float(v)
            return int(num) == 0
        except Exception:
            # non-numeric values - be conservative: treat as not zero (avoid false positives)
            return False
    except Exception:
        return False


# def scenario_unusually_high_swipes(row):
#     cur = int(row.get('CountSwipes') or 0)
#     dur = float(row.get('DurationMinutes') or 0.0)
#     empid = row.get('EmployeeID')
#     try:
#         if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
#             rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
#             median = rec.get('TotalSwipes_median', np.nan)
#             if pd.notna(median) and median > 0:
#                 return (cur > 3 * float(median)) and (dur < 60)
#     except Exception:
#         pass
#     try:
#         if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
#             global_med = HIST_DF['TotalSwipes_median'].median()
#             if pd.notna(global_med) and global_med > 0:
#                 return (cur > 3 * float(global_med)) and (dur < 60)
#     except Exception:
#         pass
#     return (cur > 50) and (dur < 60)


# def scenario_high_swipes_benign(row):
#     cur = int(row.get('CountSwipes') or 0)
#     dur = float(row.get('DurationMinutes') or 0.0)
#     empid = row.get('EmployeeID')
#     try:
#         if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
#             rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
#             median = rec.get('TotalSwipes_median', np.nan)
#             if pd.notna(median) and median > 0:
#                 return (cur > 3 * float(median)) and (dur >= 60)
#     except Exception:
#         pass
#     try:
#         if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
#             global_med = HIST_DF['TotalSwipes_median'].median()
#             if pd.notna(global_med) and global_med > 0:
#                 return (cur > 3 * float(global_med)) and (dur >= 60)
#     except Exception:
#         pass
#     return (cur > 50) and (dur >= 60)

def scenario_behaviour_shift(row, hist_df=None, minutes_threshold=180):
    try:
        if pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None:
            return False
        first_ts = pd.to_datetime(row.get('FirstSwipe'))
        today_minutes = first_ts.hour * 60 + first_ts.minute
        empid = row.get('EmployeeID')
        hist = hist_df if hist_df is not None else (HIST_DF if (HIST_DF is not None and not HIST_DF.empty) else None)
        if hist is None or hist.empty or empid is None:
            return False
        try:
            rec = hist[hist['EmployeeID'] == empid]
            if rec.empty:
                return False
            if 'FirstSwipeMinutes_median' in rec.columns:
                median_min = rec.iloc[0].get('FirstSwipeMinutes_median')
            else:
                median_min = rec.iloc[0].get('AvgFirstSwipeMins_median', None)
            if pd.isna(median_min) or median_min is None:
                return False
            diff = abs(today_minutes - float(median_min))
            return diff >= int(minutes_threshold)
        except Exception:
            return False
    except Exception:
        return False


# def scenario_repeated_short_breaks(row):
#     """
#     New logic:
#       - Flag if total break minutes > total work minutes (break-dominant).
#       - Flag if total break minutes >= 4 hours (240 minutes).
#       - Keep a fallback: if many very short gaps exist (ShortGapCount >= 5) flag as before.
#     This removes brittle "BreakCount == 2" style checks and relies on durations/ratios instead.
#     """
#     try:
#         total_break_mins = float(row.get('TotalBreakMinutes') or 0.0)
#         total_work_mins  = float(row.get('TotalWorkMinutes') or 0.0)
#         longest_break_mins = float(row.get('LongestBreakMinutes') or 0.0)
#         short_gap_count = int(row.get('ShortGapCount') or 0)

#         # Primary rule: break duration dominates working duration
#         if total_break_mins > total_work_mins and total_break_mins > 0:
#             return True

#         # Secondary rule: any break-zone total >= 4 hours => flag
#         if total_break_mins >= 240:
#             return True

#         # Fallback / traditional indicator: lots of very short repeated gaps
#         if short_gap_count >= 5:
#             return True

#         return False
#     except Exception:
#         return False


def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False



def scenario_late_exit_after_23(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        # treat last-swipe at or after 23:30 as late exit
        return t >= time(hour=23, minute=30)
    except Exception:
        return False


def scenario_first_swipe_after_20(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        # first swipe between 20:00 and 23:59:59 (inclusive)
        return t >= time(hour=20, minute=0) and t <= time(hour=23, minute=59, second=59)
    except Exception:
        return False



def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map

# def scenario_shortstay_longout_repeat(row):
#     return bool(row.get('PatternShortLongRepeat', False))



SCENARIOS = [
    ("long_gap_>=4.0h", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=5", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=14h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    # ("zero_swipes", scenario_zero_swipes),
    # ("unusually_high_swipes", scenario_unusually_high_swipes),
    # ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("first_swipe_after_20", scenario_first_swipe_after_20),
    ("late_exit_after_23", scenario_late_exit_after_23),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap),
    # ("high_swipes_benign", scenario_high_swipes_benign),
    ("behaviour_shift", scenario_behaviour_shift),
    # ("shortstay_longout_repeat", scenario_shortstay_longout_repeat)
]

# --- improved human-readable scenario explanations (use hours for duration/gaps) ---
def _hrs_from_minutes(mins):
    try:
        m = float(mins or 0.0)
        return round(m / 60.0, 1)
    except Exception:
        return None

def _hrs_from_seconds(sec):
    try:
        s = float(sec or 0.0)
        return round(s / 3600.0, 1)
    except Exception:
        return None

SCENARIO_EXPLANATIONS = {
    "long_gap_>=4.0h": lambda r: (
        (lambda h: f"Long gap between swipes (~{h} h)." if h is not None else "Long gap between swipes.")
        (_hrs_from_seconds(r.get('MaxSwipeGapSeconds')))
    ),
    "short_duration_<4h": lambda r: (
        # if duration is zero but we only saw only_in/only_out, be explicit
        "Only 'IN' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyIn', 0)) == 1 else
        "Only 'OUT' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyOut', 0)) == 1 else
        (lambda h: f"Short total presence (~{h} h)." if h is not None else "Short total presence.")(_hrs_from_minutes(r.get('DurationMinutes')))
    ),
    "coffee_badging": lambda r: "Multiple quick swipes in short time.",
    "low_swipe_count_<=5": lambda r: "Very few swipes on day.",
    "single_door": lambda r: "Only a single door used during the day.",
    "only_in": lambda r: "Only 'IN' events recorded.",
    "only_out": lambda r: "Only 'OUT' events recorded.",
    "overtime_>=14h": lambda r: "Overtime detected (>=14 hours).",
    "very_long_duration_>=16h": lambda r: "Very long presence (>=16 hours).",
    "zero_swipes": lambda r: "No swipes recorded on this day.",
    "unusually_high_swipes": lambda r: "Unusually high number of swipes compared to peers/history.",
    "repeated_short_breaks": lambda r: "Many short gaps between swipes.",
    "multiple_location_same_day": lambda r: "Multiple locations/partitions used in same day.",
    "weekend_activity": lambda r: "Activity recorded on weekend day.",
    "repeated_rejection_count": lambda r: "Multiple rejection events recorded.",
    "badge_sharing_suspected": lambda r: "Same card used by multiple users on same day — possible badge sharing.",
    "early_arrival_before_06": lambda r: "First swipe earlier than 06:00.",
    "late_exit_after_23": lambda r: "Last swipe after 23:30.",
    "first_swipe_after_20": lambda r: "First swipe after 20:00 (night arrival).",
    "shift_inconsistency": lambda r: "Duration deviates from historical shift patterns.",
    "trending_decline": lambda r: "Employee shows trending decline in presence.",
    "consecutive_absent_days": lambda r: "Consecutive absent days observed historically.",
    "high_variance_duration": lambda r: "High variance in daily durations historically.",
    "short_duration_on_high_presence_days": lambda r: "Short duration despite normally high presence days.",
    "swipe_overlap": lambda r: "Overlap in swipe times with other persons on same door.",
    "behaviour_shift": lambda r: "Significant change in arrival time compared to historical baseline.",
    "shortstay_longout_repeat": lambda r: "Repeated pattern: short in → long out → short return."
}


def _explain_scenarios_detected(row, detected_list):
    pieces = []
    # derive a human display label consisting of Name and EmployeeID where possible
    try:
        empid = None
        # prefer explicit EmployeeID (non-GUID) from various tokens
        for tok in ('EmployeeID', 'EmployeeID_feat', 'EmployeeID_dur', 'Int1', 'Text12'):
            if tok in row and row.get(tok) not in (None, '', 'nan'):
                val = _normalize_id_val(row.get(tok))
                if val and not _looks_like_guid(val):
                    empid = str(val)
                    break
        # if empid still None, allow non-GUID EmployeeIdentity
        if not empid and row.get('EmployeeIdentity') not in (None, '', 'nan'):
            tmp = _normalize_id_val(row.get('EmployeeIdentity'))
            if tmp and not _looks_like_guid(tmp):
                empid = str(tmp)

        name = None
        try:
            nm = row.get('EmployeeName')
            if nm and _looks_like_name(nm) and not _is_placeholder_str(nm):
                name = str(nm).strip()
            else:
                # fallback: pick first non-guid textual name from common tokens
                for cand in ('EmployeeName', 'EmployeeName_feat', 'EmployeeName_dur', 'ObjectName1'):
                    if cand in row and row.get(cand) not in (None, '', 'nan'):
                        v = row.get(cand)
                        if v and not _looks_like_guid(v) and _looks_like_name(v):
                            name = str(v).strip()
                            break
                if not name:
                    # try to strip person_uid prefixes if present
                    pu = row.get('person_uid')
                    if isinstance(pu, str) and (pu.startswith('emp:') or pu.startswith('uid:') or pu.startswith('name:')):
                        try:
                            stripped = _strip_uid_prefix(pu)
                            if stripped and not _looks_like_guid(stripped):
                                name = str(stripped)
                        except Exception:
                            pass
        except Exception:
            name = None

        # build prefix
        prefix = ""
        if name and empid:
            prefix = f"{name} ({empid}) - "
        elif name:
            prefix = f"{name} - "
        elif empid:
            prefix = f"{empid} - "
        else:
            prefix = ""
    except Exception:
        prefix = ""

    for sc in detected_list:
        sc = sc.strip()
        fn = SCENARIO_EXPLANATIONS.get(sc)
        try:
            if fn:
                pieces.append(fn(row))
            else:
                pieces.append(sc.replace("_", " ").replace(">=", "≥"))
        except Exception:
            pieces.append(sc)
    if not pieces:
        return None
    explanation = " ".join([p if p.endswith('.') else p + '.' for p in pieces])

    # Replace any GUID that accidentally remained inside explanation with the chosen human identifier (without duplicating parentheses)
    try:
        GUID_IN_TEXT_RE = re.compile(r'[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}')
        if prefix and isinstance(explanation, str) and GUID_IN_TEXT_RE.search(explanation):
            # replace GUIDs inside with the prefix label (strip trailing ' - ' from prefix)
            label = prefix.rstrip(' - ')
            explanation = GUID_IN_TEXT_RE.sub(str(label), explanation)
    except Exception:
        pass

    return prefix + explanation


# ---------------- compute_features (robust merged version) ----------------
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:

    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)

    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
    else:
        if 'Date' in sw.columns:
            sw['LocaleMessageTime'] = None
            try:
                sw['LocaleMessageTime'] = pd.to_datetime(sw['Date'], errors='coerce')
            except Exception:
                sw['LocaleMessageTime'] = None

    # By default Date comes from LocaleMessageTime (local, human timestamps).
    # However if an AdjustedMessageTime column exists (the Pune 2AM boundary) prefer that
    # for date assignment so trend grouping matches compute_daily_durations().
    if 'AdjustedMessageTime' in sw.columns and sw['AdjustedMessageTime'].notna().any():
        try:
            sw['AdjustedMessageTime'] = pd.to_datetime(sw['AdjustedMessageTime'], errors='coerce')
            # Prefer adjusted date for rows where it exists (this mirrors duration_report logic).
            mask_adj = sw['AdjustedMessageTime'].notna()
            # Ensure LocaleMessageTime parsed for those not adjusted
            sw.loc[~mask_adj, 'Date'] = pd.to_datetime(sw.loc[~mask_adj, 'LocaleMessageTime'], errors='coerce').dt.date
            sw.loc[mask_adj, 'Date']  = sw.loc[mask_adj,  'AdjustedMessageTime'].dt.date
        except Exception:
            # fallback to LocaleMessageTime date
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
    else:
        # normal path
        sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date

    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    try:
        if dir_col and dir_col in sw.columns:
            sw['Direction'] = sw[dir_col]
        if door_col and door_col in sw.columns:
            sw['Door'] = sw[door_col]
        if empid_col and empid_col in sw.columns:
            sw['EmployeeID'] = sw[empid_col]
        if name_col and name_col in sw.columns:
            sw['EmployeeName'] = sw[name_col]
        if card_col and card_col in sw.columns:
            sw['CardNumber'] = sw[card_col]
        if 'LocaleMessageTime' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
        elif 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
    except Exception:
        logging.exception("Normalization of swipe columns failed.")

    # PersonnelType filtering (tolerant) - avoid dropping if column absent
    if 'PersonnelTypeName' in sw.columns:
        sw['PersonnelTypeName'] = sw['PersonnelTypeName'].astype(str).str.strip()
        mask = sw['PersonnelTypeName'].str.lower().str.contains(r'employee|terminated', na=False)
        logging.info("PersonnelTypeName values example: %s", list(sw['PersonnelTypeName'].dropna().unique()[:6]))
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelTypeName filter applied: before=%d after=%d", before, len(sw))
    elif 'PersonnelType' in sw.columns:
        sw['PersonnelType'] = sw['PersonnelType'].astype(str).str.strip()
        mask = sw['PersonnelType'].str.lower().str.contains(r'employee|terminated', na=False)
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelType filter applied: before=%d after=%d", before, len(sw))

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()



    # ensure person_uid and Date exist
    if 'person_uid' not in sw.columns:
        def _make_uid_row(r):
            return _canonical_person_uid({
                'EmployeeID': r.get(empid_col) if empid_col in r else r.get('EmployeeID'),
                'EmployeeIdentity': r.get('EmployeeIdentity', None),
                'EmployeeName': r.get(name_col) if name_col in r else r.get('EmployeeName', None)
            })
        sw['person_uid'] = sw.apply(_make_uid_row, axis=1)

    # ensure Date parsed
    if 'Date' not in sw.columns or sw['Date'].isnull().all():
        try:
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
        except Exception:
            sw['Date'] = None


    # ---- prepare list of extra columns we may want to include when grouping ----
    sel_cols_candidates = [
        'LocaleMessageTime', 'Direction', 'Door', 'CardNumber', 'EmployeeID',
        'EmployeeName', 'PartitionName2', 'Rejection_Type', 'PersonnelTypeName',
        'EmployeeIdentity', 'Int1', 'Text12', 'ObjectName1'
    ]
    # keep only those actually present in the current swipe frame
    sel_cols_present = [c for c in sel_cols_candidates if c in sw.columns]
    # selection slice we will use for grouping (always include person_uid and Date)
    selection_for_group = ['person_uid', 'Date'] + sel_cols_present

    # final safe selection for grouping (only columns that actually exist)
    available_cols = [c for c in selection_for_group if c in sw.columns]
    # must have both person_uid and Date to group
    if 'person_uid' not in available_cols or 'Date' not in available_cols:
        # nothing to group on -> return empty
        return pd.DataFrame()

    try:
        # group on the safe selection and apply aggregator
        grouped = sw[available_cols].groupby(['person_uid', 'Date'])
        features = grouped.apply(agg_swipe_group).reset_index()
    except Exception:
        logging.exception("compute_features: groupby/apply failed")
        # try a defensive fallback grouping without selecting columns slice
        try:
            grouped = sw.groupby(['person_uid', 'Date'])
            features = grouped.apply(agg_swipe_group).reset_index()
        except Exception:
            logging.exception("compute_features: fallback groupby failed")
            return pd.DataFrame()

    # normalize column names (keep compatibility)
    if 'person_uid' not in features.columns and 'level_0' in features.columns:
        features.rename(columns={'level_0': 'person_uid', 'level_1': 'Date'}, inplace=True)
    return features




# ---------- robust agg_swipe_group (replace existing definition) ----------
def agg_swipe_group(g: pd.DataFrame) -> pd.Series:
    # times sorted
    times = []
    if isinstance(g, pd.DataFrame) and 'LocaleMessageTime' in g.columns:
        try:
            times = sorted(pd.to_datetime(g['LocaleMessageTime'].dropna().tolist()))
        except Exception:
            # fallback: attempt to coerce individually
            try:
                times = sorted([pd.to_datetime(x) for x in list(g['LocaleMessageTime'].dropna())])
            except Exception:
                times = []
    gaps = []
    short_gap_count = 0
    for i in range(1, len(times)):
        s = (times[i] - times[i-1]).total_seconds()
        gaps.append(s)
        if s <= 5 * 60:
            short_gap_count += 1
    max_gap = int(max(gaps)) if gaps else 0

    # Direction detection: be tolerant (substring match, case-insensitive)
    in_count = 0
    out_count = 0
    if 'Direction' in g.columns:
        try:
            sers = g['Direction'].astype(str).fillna('').str.lower()
            in_count = int(sers.str.contains(r'\bin\b|\bin-direction\b|in\$|in$|in\W', regex=True).sum())
            out_count = int(sers.str.contains(r'\bout\b|\bout-direction\b|out\$|out$|out\W|exit', regex=True).sum())
            # fallback: simple containment
            if in_count == 0:
                in_count = int(sers.str.contains('in', na=False).sum())
            if out_count == 0:
                out_count = int(sers.str.contains('out|exit', na=False).sum())
        except Exception:
            try:
                in_count = int((g['Direction'] == 'IN').sum())
                out_count = int((g['Direction'] == 'OUT').sum())
            except Exception:
                in_count = 0
                out_count = 0

    unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
    unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
    rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

    # card extraction: tolerant
    card_numbers = []
    try:
        if 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna().astype(str)))
        if not card_numbers:
            for c in g.columns:
                lc = c.lower()
                if 'card' in lc or 'chuid' in lc or 'value' == lc or 'xml' in lc or 'msg' in lc:
                    try:
                        vals = list(pd.unique(g[c].dropna().astype(str)))
                        if vals:
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        # xml extraction fallback
        if not card_numbers:
            for c in g.columns:
                if 'xml' in c.lower():
                    for raw in g[c].dropna().astype(str):
                        x = _extract_card_from_xml(raw)
                        if x:
                            card_numbers.append(x)
    except Exception:
        card_numbers = []
    card_numbers = list(dict.fromkeys([_normalize_id_val(x) for x in card_numbers if _normalize_id_val(x)]))
    card_number = card_numbers[0] if card_numbers else None

    # stable id/name extraction (kept as before but robust)
    employee_id = None
    if 'EmployeeID' in g.columns:
        try:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        except Exception:
            employee_id = None

    employee_identity = None
    if 'EmployeeIdentity' in g.columns:
        try:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]
        except Exception:
            employee_identity = None

    employee_name = None
    candidate_name_vals = None
    if 'EmployeeName' in g.columns:
        candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
    elif 'ObjectName1' in g.columns:
        candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())
    if candidate_name_vals is not None and not candidate_name_vals.empty:
        employee_name = _pick_first_non_guid_value(candidate_name_vals)
        if employee_name is None:
            for v in candidate_name_vals:
                if _looks_like_name(v) and not _is_placeholder_str(v):
                    employee_name = str(v).strip()
                    break

    personnel_type = None
    if 'PersonnelTypeName' in g.columns:
        try:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        except Exception:
            personnel_type = None
    elif 'PersonnelType' in g.columns:
        try:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        except Exception:
            personnel_type = None

    first_swipe = times[0] if times else None
    last_swipe = times[-1] if times else None

    # build timeline -> segments (zone aware)
    timeline = []
    # SAFE sort/iterate: only sort when column exists, otherwise iterate rows as-is
    if 'LocaleMessageTime' in g.columns:
        try:
            rows_to_iter = g.sort_values('LocaleMessageTime')
        except Exception:
            # older pandas or unexpected errors -> fall back to unsorted frame
            rows_to_iter = g
    else:
        rows_to_iter = g

    for _, row in rows_to_iter.iterrows():
        t = row.get('LocaleMessageTime')
        dname = row.get('Door') if 'Door' in row else None
        direction = row.get('Direction') if 'Direction' in row else None
        zone = map_door_to_zone(dname, direction)
        timeline.append((t, dname, direction, zone))

    segments = []
    if timeline:
        cur_zone = None
        seg_start = timeline[0][0]
        seg_label = None
        for (t, dname, direction, zone) in timeline:
            lbl = 'work'
            if zone is not None and zone in _BREAK_ZONES:
                lbl = 'break'
            elif zone == _OUT_OF_OFFICE_ZONE:
                lbl = 'out_of_office'
            else:
                lbl = 'work'
            if cur_zone is None:
                cur_zone = zone
                seg_label = lbl
                seg_start = t
            else:
                if lbl != seg_label:
                    segments.append({'label': seg_label, 'start': seg_start, 'end': t, 'start_zone': cur_zone})
                    seg_start = t
                    seg_label = lbl
                    cur_zone = zone
                else:
                    cur_zone = cur_zone or zone
        if seg_label is not None:
            segments.append({'label': seg_label, 'start': seg_start, 'end': timeline[-1][0], 'start_zone': cur_zone})

    # compute durations
    total_work_minutes = 0.0
    total_break_minutes = 0.0
    longest_break_minutes = 0.0
    break_segments_count = 0
    for s in segments:
        lbl = s.get('label')
        start = s.get('start')
        end = s.get('end')
        try:
            dur_mins = ((end - start).total_seconds() / 60.0) if (start is not None and end is not None) else 0.0
        except Exception:
            dur_mins = 0.0
        if lbl == 'work':
            total_work_minutes += dur_mins
        elif lbl in ('break', 'out_of_office'):
            break_segments_count += 1
            total_break_minutes += dur_mins
            if dur_mins > longest_break_minutes:
                longest_break_minutes = dur_mins

    break_dominant = bool(total_break_minutes > total_work_minutes and total_break_minutes > 0)
    break_more_than_4h = bool(total_break_minutes >= 240)

    break_count = int(break_segments_count)
    long_break_count = int(sum(1 for s in segments
                               if s.get('label') in ('break','out_of_office') and
                                  (s.get('start') is not None and s.get('end') is not None and ((s['end'] - s['start']).total_seconds()/60.0) >= 120)))

    # pattern detection (unchanged)
    pattern_flag = False
    pattern_sequence_readable = None
    try:
        seq = []
        for s in segments:
            dur_mins = (s['end'] - s['start']).total_seconds() / 60.0 if (s['end'] and s['start']) else 0
            seq.append((s['label'], int(round(dur_mins))))
        for i in range(len(seq)-2):
            a = seq[i]; b = seq[i+1]; c = seq[i+2]
            if (a[0] == 'work' and a[1] < 60) and (b[0] in ('out_of_office','break') and b[1] >= 120) and (c[0] == 'work' and c[1] < 60):
                pattern_flag = True
                seq_fragment = [a,b,c]
                pattern_sequence_readable = " -> ".join([f"{lbl} ({mins}m)" for lbl, mins in seq_fragment])
                break
    except Exception:
        pattern_flag = False
        pattern_sequence_readable = None

    return pd.Series({
        'CountSwipes': int(len(g)) if hasattr(g, '__len__') else 0,
        'MaxSwipeGapSeconds': int(max_gap),
        'ShortGapCount': int(short_gap_count),
        'InCount': int(in_count),
        'OutCount': int(out_count),
        'UniqueDoors': int(unique_doors),
        'UniqueLocations': int(unique_locations),
        'RejectionCount': int(rejection_count),
        'CardNumber': card_number,
        'EmployeeID': employee_id,
        'EmployeeIdentity': employee_identity,
        'EmployeeName': employee_name,
        'PersonnelType': personnel_type,
        'FirstSwipe': first_swipe,
        'LastSwipe': last_swipe,
        'BreakCount': int(break_count),
        'LongBreakCount': int(long_break_count),
        'TotalBreakMinutes': float(round(total_break_minutes, 1)),
        'TotalWorkMinutes': float(round(total_work_minutes, 1)),
        'LongestBreakMinutes': float(round(longest_break_minutes, 1)),
        'BreakSegmentsCount': int(break_segments_count),
        'BreakDominant': bool(break_dominant),
        'BreakMoreThan4h': bool(break_more_than_4h),
        'PatternShortLongRepeat': bool(pattern_flag),
        'PatternSequenceReadable': pattern_sequence_readable,
        'PatternSequence': None
    })



# ---------------- SCENARIO WEIGHTS ----------------
WEIGHTS = {
    "long_gap_>=4.0h": 1.3,
    "short_duration_<4h": 1.0,
    "coffee_badging": 1.0,
    "low_swipe_count_<=5": 0.5,
    "single_door": 0.25,
    "only_in": 0.8,
    "only_out": 0.8,
    "overtime_>=14h": 0.2,
    "very_long_duration_>=16h": 1.5,
    "zero_swipes": 0.4,
    "unusually_high_swipes": 1.5,
    "repeated_short_breaks": 0.5,
    "multiple_location_same_day": 0.6,
    "weekend_activity": 0.6,
    "repeated_rejection_count": 0.8,
    "badge_sharing_suspected": 2.0,
    "early_arrival_before_06": 0.4,
    "late_exit_after_23": 0.4,
    "first_swipe_after_20": 0.6,
    "shift_inconsistency": 1.2,
    "trending_decline": 0.7,
    "consecutive_absent_days": 1.2,
    "high_variance_duration": 0.8,
    "short_duration_on_high_presence_days": 1.1,
    "swipe_overlap": 2.0,
    "high_swipes_benign": 0.1,
    "shortstay_longout_repeat": 2.0
}
ANOMALY_THRESHOLD = 1.5




# ---------- small improvement in _read_past_trend_csvs: normalize 'date' column name ----------
def _read_past_trend_csvs(outdir: str, window_days: int, target_date: date):
    p = Path(outdir)
    csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return pd.DataFrame()
    dfs = []
    cutoff = target_date - timedelta(days=window_days)
    for fp in csvs:
        try:
            df = pd.read_csv(fp, parse_dates=['Date'])
        except Exception:
            try:
                df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

        # normalize case-insensitive 'date' -> 'Date' so subsequent logic is consistent
        if 'Date' not in df.columns:
            for c in df.columns:
                if c.lower() == 'date':
                    df.rename(columns={c: 'Date'}, inplace=True)
                    break

        if 'Date' in df.columns:
            try:
                df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            except Exception:
                pass

            def _date_in_window(d):
                try:
                    return d is not None and (d >= cutoff and d <= target_date)
                except Exception:
                    return False

            try:
                df = df[df['Date'].apply(_date_in_window)]
            except Exception:
                # if filtering fails, keep as-is (we'll handle later)
                pass

        dfs.append(df)
    if not dfs:
        return pd.DataFrame()
    try:
        out = pd.concat(dfs, ignore_index=True)
        return out
    except Exception:
        return pd.DataFrame()



# ---------- consolidated compute_violation_days_map (robust, improved) ----------
def compute_violation_days_map(outdir: str, window_days: int, target_date: date):
    """
    Build a map {normalized_id_string -> number_of_distinct_flagged_days}
    from trend_*.csv files in outdir within the window_days before target_date (inclusive).

    Robust to:
      - case/format differences in 'Date' column
      - historic files missing 'IsFlagged' but having 'AnomalyScore'
      - various identifier columns (person_uid, EmployeeID, CardNumber, Int1, Text12, ...)
      - storing both prefixed ids (emp:..., uid:...) and stripped ids
      - uses canonical person uid when possible to match different identifier schemes
    """
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty:
        logging.info("compute_violation_days_map: no historic trend rows found in %s", outdir)
        return {}

    # normalize Date column if present (try multiple common names)
    date_col = None
    for c in df.columns:
        if c.lower() == 'date':
            date_col = c
            break
        if c.lower() in ('displaydate', 'display_date', 'displaydatekey', 'displaydate_key'):
            date_col = c
    if date_col:
        try:
            df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.date
            df.rename(columns={date_col: 'Date'}, inplace=True)
        except Exception:
            pass

    # ensure Date is date-typed where possible
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass

    # determine ID columns to consider (broad coverage)
    candidate_id_cols = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12',
                         'EmployeeID_feat', 'EmployeeID_dur']
    id_cols = [c for c in candidate_id_cols if c in df.columns]

    # Build IsFlagged robustly if missing
    if 'IsFlagged' not in df.columns:
        if 'AnomalyScore' in df.columns:
            try:
                df['IsFlagged'] = pd.to_numeric(df['AnomalyScore'], errors='coerce').fillna(0).astype(float) >= ANOMALY_THRESHOLD
            except Exception:
                df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: (float(s) >= ANOMALY_THRESHOLD) if (s is not None and str(s).strip()!='') else False)
        else:
            df['IsFlagged'] = False
            for cand in ('Reasons', 'DetectedScenarios', 'IsAnomaly', 'Flagged'):
                if cand in df.columns:
                    try:
                        df['IsFlagged'] = df[cand].astype(str).str.strip().str.lower().isin({'true','1','yes','y','t'}) | df['IsFlagged']
                    except Exception:
                        pass

    flagged = df[df.get('IsFlagged') == True].copy()
    if flagged.empty:
        return {}

    ident_dates = defaultdict(set)

    def _add_ident(id_value, the_date):
        if the_date is None:
            return
        try:
            norm = _normalize_id_val(id_value)
            if not norm:
                return
            # primary exact form
            keys = set()
            keys.add(str(norm))
            # lowercase variant
            try:
                keys.add(str(norm).lower())
            except Exception:
                pass
            # stripped uid prefix if present (emp:, uid:, name:)
            try:
                stripped = _strip_uid_prefix(str(norm))
                if stripped and stripped != str(norm):
                    keys.add(str(stripped))
                    keys.add(str(stripped).lower())
            except Exception:
                pass
            # numeric-only variant (useful when old files store plain numbers)
            try:
                numeric = re.sub(r'\D', '', str(norm))
                if numeric:
                    keys.add(str(numeric))
            except Exception:
                pass
            for k in keys:
                ident_dates[k].add(the_date)
        except Exception:
            pass

    # iterate flagged rows and collect ids + date, but also try canonical uid
    for _, r in flagged.iterrows():
        d = r.get('Date')
        if d is None:
            for c in ('DisplayDate', 'DisplayDateKey'):
                if c in r and pd.notna(r.get(c)):
                    try:
                        d = pd.to_datetime(r.get(c), errors='coerce').date()
                        break
                    except Exception:
                        d = None
        if d is None:
            continue

        # collect ids from known columns
        for col in id_cols:
            try:
                raw = r.get(col)
                if raw in (None, '', float('nan')):
                    continue
                _add_ident(raw, d)
            except Exception:
                continue

        # also attempt to capture IDs from fallback fields
        for fallback in ('Int1', 'Text12', 'CardNumber', 'EmployeeID_feat', 'EmployeeID_dur'):
            try:
                if fallback in r and r.get(fallback) not in (None, '', float('nan')):
                    _add_ident(r.get(fallback), d)
            except Exception:
                continue

        # attempt weak-extraction from other columns (EmployeeName maybe encoded as 'uid:...' or 'emp:123')
        for cand in ('person_uid', 'EmployeeIdentity', 'personid', 'EmployeeID', 'EmployeeName'):
            try:
                if cand in r and r.get(cand) not in (None, '', float('nan')):
                    _add_ident(r.get(cand), d)
            except Exception:
                pass

        # --- add canonical person uid (best-effort) for the historic row to help cross-matching ---
        try:
            # build a small dict like compute_features expects
            tmp_row = {
                'EmployeeID': r.get('EmployeeID', None),
                'EmployeeIdentity': r.get('EmployeeIdentity', None),
                'EmployeeName': r.get('EmployeeName', None)
            }
            can = _canonical_person_uid(tmp_row)
            if can:
                _add_ident(can, d)
                # stripped of prefix too
                try:
                    _add_ident(_strip_uid_prefix(can), d)
                except Exception:
                    pass
            # also add lower-case canonical
            if can:
                try:
                    ident_dates[str(can).lower()].add(d)
                except Exception:
                    pass
        except Exception:
            pass

    out = {k: int(len(v)) for k, v in ident_dates.items()}
    logging.info("compute_violation_days_map: produced map with %d ids", len(out))
    return out



def _read_scenario_counts_by_person(outdir: str, window_days: int, target_date: date, scenario_col: str):
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty or scenario_col not in df.columns:
        return {}
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass
    id_cols = [c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in df.columns]
    out = defaultdict(int)
    q = df[df[scenario_col] == True] if df[scenario_col].dtype == bool else df[df[scenario_col].astype(str).str.lower() == 'true']
    for _, r in q.iterrows():
        for col in id_cols:
            try:
                raw = r.get(col)
                if raw in (None, '', float('nan')):
                    continue
                norm = _normalize_id_val(raw)
                if norm:
                    out[str(norm)] += 1
                    stripped = _strip_uid_prefix(str(norm))
                    if stripped != str(norm):
                        out[str(stripped)] += 1
            except Exception:
                continue
        for fallback in ('Int1', 'Text12'):
            if fallback in r and r.get(fallback) not in (None, '', 'nan'):
                try:
                    norm = _normalize_id_val(r.get(fallback))
                    if norm:
                        out[str(norm)] += 1
                except Exception:
                    continue
    return dict(out)

def _compute_weeks_with_threshold(past_df: pd.DataFrame,
                                  person_col: str = 'person_uid',
                                  date_col: str = 'Date',
                                  scenario_col: str = 'short_duration_<4h',
                                  threshold_days: int = 3) -> dict:
    if past_df is None or past_df.empty:
        return {}
    df = past_df.copy()
    if date_col not in df.columns:
        return {}
    try:
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.date
    except Exception:
        pass
    if scenario_col not in df.columns:
        return {}
    try:
        if df[scenario_col].dtype == bool:
            df['__scenario_flag__'] = df[scenario_col].astype(bool)
        else:
            df['__scenario_flag__'] = df[scenario_col].astype(str).str.strip().str.lower().isin({'true', '1', 'yes', 'y', 't'})
    except Exception:
        df['__scenario_flag__'] = df[scenario_col].apply(lambda v: str(v).strip().lower() in ('true','1','yes','y','t') if v is not None else False)
    df = df[df['__scenario_flag__'] == True].copy()
    if df.empty:
        return {}
    if person_col not in df.columns:
        fallback = next((c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in past_df.columns), None)
        if fallback is None:
            return {}
        person_col = fallback
    def _week_monday(d):
        try:
            if d is None or (isinstance(d, float) and np.isnan(d)):
                return None
            iso = d.isocalendar()
            return date.fromisocalendar(iso[0], iso[1], 1)
        except Exception:
            return None
    df['__week_monday__'] = df[date_col].apply(_week_monday)
    df = df.dropna(subset=['__week_monday__', person_col])
    if df.empty:
        return {}
    week_counts = (df.groupby([person_col, '__week_monday__'])
                     .size()
                     .reset_index(name='days_flagged'))
    valid_weeks = week_counts[week_counts['days_flagged'] >= int(threshold_days)].copy()
    if valid_weeks.empty:
        return {}
    person_weeks = {}
    for person, grp in valid_weeks.groupby(person_col):
        wlist = sorted(pd.to_datetime(grp['__week_monday__']).dt.date.unique(), reverse=True)
        person_weeks[str(person)] = wlist
    def _consecutive_week_count(week_dates_desc):
        if not week_dates_desc:
            return 0
        count = 1
        prev = week_dates_desc[0]
        for cur in week_dates_desc[1:]:
            try:
                if (prev - cur).days == 7:
                    count += 1
                    prev = cur
                else:
                    break
            except Exception:
                break
        return count
    out = {}
    for pid, weeks in person_weeks.items():
        c = _consecutive_week_count(weeks)
        out[str(pid)] = int(c)
        try:
            stripped = _strip_uid_prefix(str(pid))
            if stripped and stripped != str(pid):
                out[str(stripped)] = int(c)
        except Exception:
            pass
    return out

def _strip_uid_prefix(s):
    try:
        if s is None:
            return s
        st = str(s)
        for p in ('emp:', 'uid:', 'name:'):
            if st.startswith(p):
                return st[len(p):]
        return st
    except Exception:
        return s






#update




def score_trends_from_durations(combined_df: pd.DataFrame,
                                swipes_df: Optional[pd.DataFrame] = None,
                                outdir: Optional[str] = None,
                                target_date: Optional[date] = None,
                                window_days: Optional[int] = None) -> pd.DataFrame:
    """
    window_days: number of days (inclusive) prior to target_date to read historical trend CSVs for
                 scenario counts and violation-days. If None, falls back to VIOLATION_WINDOW_DAYS.
    """
    # normalize window
    if window_days is None:
        window_days = VIOLATION_WINDOW_DAYS
 
    if combined_df is None or combined_df.empty:
        return pd.DataFrame()

    df = combined_df.copy()
    # Ensure person_uid exists
    if 'person_uid' not in df.columns:
        df['person_uid'] = df.apply(lambda r: _canonical_person_uid(r), axis=1)

    # Ensure key columns exist
    for c in ['FirstSwipe','LastSwipe','CountSwipes','DurationMinutes','MaxSwipeGapSeconds','EmployeeID','CardNumber','person_uid','Date']:
        if c not in df.columns:
            df[c] = None

    # reconcile zero CountSwipes with raw swipes (if provided)
    if swipes_df is not None and not swipes_df.empty and 'person_uid' in swipes_df.columns:
        tsw = swipes_df.copy()
        # ensure LocaleMessageTime parsed
        if 'LocaleMessageTime' in tsw.columns:
            tsw['LocaleMessageTime'] = pd.to_datetime(tsw[found_time_col] if 'found_time_col' in globals() and found_time_col in tsw.columns else tsw['LocaleMessageTime'], errors='coerce')
        else:
            for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                if cand in tsw.columns:
                    tsw['LocaleMessageTime'] = pd.to_datetime(tsw[cand], errors='coerce')
                    break
        if 'Date' not in tsw.columns:
            if 'LocaleMessageTime' in tsw.columns:
                tsw['Date'] = tsw['LocaleMessageTime'].dt.date
            else:
                for cand in ('date','Date'):
                    if cand in tsw.columns:
                        try:
                            tsw['Date'] = pd.to_datetime(tsw[cand], errors='coerce').dt.date
                        except Exception:
                            tsw['Date'] = None
                        break

        try:
            grp = tsw.dropna(subset=['person_uid', 'Date']).groupby(['person_uid', 'Date'])
            counts = grp.size().to_dict()
            firsts = grp['LocaleMessageTime'].min().to_dict()
            lasts = grp['LocaleMessageTime'].max().to_dict()
        except Exception:
            counts = {}
            firsts = {}
            lasts = {}

        def _fix_row_by_raw(idx, row):
            key = (row.get('person_uid'), row.get('Date'))
            if key in counts and (int(row.get('CountSwipes') or 0) == 0 or pd.isna(row.get('CountSwipes'))):
                try:
                    c = int(counts.get(key, 0))
                    df.at[idx, 'CountSwipes'] = c
                    f = firsts.get(key)
                    l = lasts.get(key)
                    if pd.notna(f) and (pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None):
                        df.at[idx, 'FirstSwipe'] = pd.to_datetime(f)
                    if pd.notna(l) and (pd.isna(row.get('LastSwipe')) or row.get('LastSwipe') is None):
                        df.at[idx, 'LastSwipe'] = pd.to_datetime(l)
                    try:
                        fs = df.at[idx, 'FirstSwipe']
                        ls = df.at[idx, 'LastSwipe']
                        if pd.notna(fs) and pd.notna(ls):
                            dursec = (pd.to_datetime(ls) - pd.to_datetime(fs)).total_seconds()
                            dursec = max(0, dursec)
                            df.at[idx, 'DurationSeconds'] = float(dursec)
                            df.at[idx, 'DurationMinutes'] = float(dursec / 60.0)
                    except Exception:
                        pass
                except Exception:
                    pass

        for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
            try:
                _fix_row_by_raw(ix, r)
            except Exception:
                logging.debug("Failed to reconcile row %s with raw swipes", ix)

    # Build badge map and swipe overlap maps for higher-severity scenarios
    badge_map = {}
    swipe_overlap_map = {}
    if swipes_df is not None and not swipes_df.empty:
        try:
            tmp = swipes_df[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
            if not tmp.empty:
                grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
                badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}
        except Exception:
            badge_map = {}

        overlap_window_seconds = 2
        if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes_df.columns):
            try:
                tmp2 = swipes_df[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
                if not tmp2.empty:
                    tmp2 = tmp2.sort_values(['Door', 'LocaleMessageTime'])
                    for (d, door), g in tmp2.groupby(['Date', 'Door']):
                        items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                        n = len(items)
                        for i in range(n):
                            t_i, uid_i = items[i]
                            j = i+1
                            while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                                uid_j = items[j][1]
                                if uid_i != uid_j:
                                    swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                                    swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                                j += 1
            except Exception:
                swipe_overlap_map = {}


    # Evaluate scenarios (use weighting to compute anomaly score)
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            df[name] = df.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
        elif name == "swipe_overlap":
            df[name] = df.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map=swipe_overlap_map), axis=1)
        else:
            df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)






    # --- Weekly suppression for short-duration / repeated-break / shortstay-longout / long-gap scenarios ---
    try:
        # include the extra scenario names you requested
        _weekly_special = {"long_gap_>=4.0h", "short_duration_<4h", "coffee_badging",
                           "low_swipe_count_<=5", "single_door", "only_in","only_out","overtime_>=14h",
                           "very_long_duration_>=16h","unusually_high_swipes","repeated_short_breaks",
                           "late_exit_after_23","first_swipe_after_20","shift_inconsistency",
                           "high_swipes_benign","shortstay_longout_repeat"}

        # read up to 7 days (inclusive) of past trend csvs so we can compute week counts for escalation rule
        week_df = _read_past_trend_csvs(str(outdir) if outdir else str(OUTDIR), 7, target_date if target_date else date.today())
        if week_df is not None and not week_df.empty:
            # normalise Date if present
            if 'Date' in week_df.columns:
                try:
                    week_df['Date'] = pd.to_datetime(week_df['Date'], errors='coerce').dt.date
                except Exception:
                    pass

            # build per-person summary maps for this week (present days and scenario-days)
            present_map = defaultdict(int)  # pid -> number of days present in week
            scen_map = {s: defaultdict(int) for s in _weekly_special}  # scenario -> (pid -> days flagged)

            # iterate week history and populate maps
            for _, wr in week_df.iterrows():
                # determine if that historic row counts as present
                present_flag = False
                try:
                    cs = wr.get('CountSwipes')
                    if cs is not None:
                        try:
                            present_flag = int(cs) > 0
                        except Exception:
                            present_flag = str(cs).strip() not in ('0', 'nan', '')
                except Exception:
                    present_flag = False

                # collect normalized identifiers for this historic row
                ids = set()
                for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
                    try:
                        v = wr.get(k)
                        if v not in (None, '', float('nan')):
                            nv = _normalize_id_val(v)
                            if nv:
                                ids.add(str(nv))
                                stripped = _strip_uid_prefix(str(nv))
                                if stripped != str(nv):
                                    ids.add(str(stripped))
                    except Exception:
                        continue
                if not ids:
                    continue

                # update maps
                for pid in ids:
                    if present_flag:
                        present_map[pid] += 1
                    for scen in _weekly_special:
                        try:
                            val = wr.get(scen)
                            flag = False
                            if val is not None:
                                if isinstance(val, bool):
                                    flag = val
                                else:
                                    flag = str(val).strip().lower() in ('true', '1', 'yes', 'y', 't')
                            if flag:
                                scen_map[scen][pid] += 1
                        except Exception:
                            continue

            # Now apply suppression to today's df rows:
            for idx, row in df.iterrows():
                # gather normalized ids for current row
                ids_curr = set()
                for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
                    try:
                        v = row.get(k)
                        if v not in (None, '', float('nan')):
                            nv = _normalize_id_val(v)
                            if nv:
                                ids_curr.add(str(nv))
                                stripped = _strip_uid_prefix(str(nv))
                                if stripped != str(nv):
                                    ids_curr.add(str(stripped))
                    except Exception:
                        continue
                if not ids_curr:
                    continue

                # aggregate hist counts for this week for all matching ids
                hist_present = 0
                hist_scen_counts = {s: 0 for s in _weekly_special}
                for pid in ids_curr:
                    hist_present += int(present_map.get(pid, 0))
                    for s in _weekly_special:
                        hist_scen_counts[s] += int(scen_map[s].get(pid, 0))

                # include today's presence and today's scenario flag in counts
                present_today = int(row.get('CountSwipes') or 0) > 0
                for s in _weekly_special:
                    current_flag = bool(row.get(s))
                    present_days = int(hist_present) + (1 if present_today else 0)
                    scenario_days = int(hist_scen_counts.get(s, 0)) + (1 if current_flag else 0)

                    # Compute the minimum scenario-days required to keep the flag, per your rules:
                    # required = max(1, present_days - 2)
                    try:
                        required = max(1, int(present_days) - 2)
                    except Exception:
                        required = 1

                    # If scenario_days is less than required -> suppress today's flag
                    if scenario_days < required:
                        if s in df.columns and bool(df.at[idx, s]):
                            df.at[idx, s] = False

    except Exception:
        logging.exception("Weekly suppression check failed (non-fatal).")

    def compute_score(r):
        score = 0.0
        detected = []
        for name, _ in SCENARIOS:
            val = bool(r.get(name))
            w = WEIGHTS.get(name, 0.0)
            if val and w > 0:
                score += float(w)
                detected.append(name)
        return score, detected

    scores = df.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
    df['AnomalyScore'] = scores['AnomalyScore'].astype(float)
    df['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
    df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))



    # PresentToday flag, ViolationDays from history, and weekly adjustments
    try:
        df['PresentToday'] = df['CountSwipes'].fillna(0).astype(int) > 0

     
        # historical scenario counts (for escalation) using the configured window_days
        hist_pattern_counts = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), int(window_days), target_date if target_date else date.today(), 'shortstay_longout_repeat')
        hist_rep_breaks = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), int(window_days), target_date if target_date else date.today(), 'repeated_short_breaks')
        hist_short_duration = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), int(window_days), target_date if target_date else date.today(), 'short_duration_<4h')


        def get_hist_count_for_row(row, hist_map):
            for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
                if k in row and row.get(k) not in (None, '', float('nan')):
                    try:
                        norm = _normalize_id_val(row.get(k))
                        if norm and str(norm) in hist_map:
                            return int(hist_map.get(str(norm), 0))
                        stripped = _strip_uid_prefix(str(norm)) if norm else None
                        if stripped and str(stripped) in hist_map:
                            return int(hist_map.get(str(stripped), 0))
                    except Exception:
                        continue
            return 0



        # historical aggregated counts (use window_days)
        df['HistPatternShortLongCount'] = df.apply(lambda r: get_hist_count_for_row(r, hist_pattern_counts), axis=1)
        df['HistRepeatedShortBreakCount'] = df.apply(lambda r: get_hist_count_for_row(r, hist_rep_breaks), axis=1)
        df['HistShortDurationCount'] = df.apply(lambda r: get_hist_count_for_row(r, hist_short_duration), axis=1)

        # ViolationDays computed from history window_days (read from past trend CSVs)
        # ViolationDays computed from history window_days (read from past trend CSVs)
        vmap = compute_violation_days_map(str(outdir) if outdir else str(OUTDIR), int(window_days), target_date if target_date else date.today())

        # lookup helper must be defined inside this try so it can see vmap
     
     
     
        # lookup helper must be defined inside this try so it can see vmap
        def lookup_violation_days(row):
            try:
                # gather candidate id strings from many columns
                candidates = []
                for k in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
                    v = row.get(k)
                    if v not in (None, '', float('nan')):
                        nv = _normalize_id_val(v)
                        if nv:
                            candidates.append(str(nv))
                            candidates.append(str(nv).lower())
                            try:
                                candidates.append(_strip_uid_prefix(str(nv)))
                            except Exception:
                                pass
                # also try canonical person uid for this current row (best-effort)
                try:
                    can = _canonical_person_uid(row)
                    if can:
                        candidates.append(can)
                        candidates.append(str(can).lower())
                        try:
                            candidates.append(_strip_uid_prefix(str(can)))
                        except Exception:
                            pass
                except Exception:
                    pass

                # dedupe preserve order
                seen = set()
                for c in candidates:
                    if not c or c in (None, '', 'nan'):
                        continue
                    if c in seen:
                        continue
                    seen.add(c)
                    # direct match
                    if c in vmap:
                        return int(vmap.get(c, 0))
                    # lowercase match
                    if str(c).lower() in vmap:
                        return int(vmap.get(str(c).lower(), 0))
                    # stripped prefix match
                    stripped = _strip_uid_prefix(str(c))
                    if stripped != str(c) and stripped in vmap:
                        return int(vmap.get(stripped, 0))
                    if stripped.lower() in vmap:
                        return int(vmap.get(stripped.lower(), 0))

                # nothing in history — if the current day is flagged, count it as 1
                try:
                    if bool(row.get('IsFlagged')):
                        return 1
                except Exception:
                    pass

                return 0
            except Exception:
                return 0

        # now apply function to dataframe (still inside same try)
        df['ViolationDays'] = df.apply(lookup_violation_days, axis=1)
        # backward-compatible alias expected by some frontends
        df['ViolationDaysLast90'] = df['ViolationDays']
        # expose the window used so front-end can render "Violation (N days)"
        df['ViolationWindowDays'] = int(window_days)

        # Append monitoring note for persons who have past violations and are present today.
        def _append_monitor_note(idx, row):
            try:
                vd = int(row.get('ViolationDays') or 0)
            except Exception:
                vd = 0
            if vd <= 0:
                return row.get('ViolationExplanation') or row.get('Explanation')
            if not row.get('PresentToday', False):
                return row.get('ViolationExplanation') or row.get('Explanation')
            note = f"Note: Previously flagged {vd} time{'s' if vd!=1 else ''} in the last {int(window_days)} days — monitor when present today."
            ex = row.get('ViolationExplanation') or row.get('Explanation') or ''
            if ex and not ex.strip().endswith('.'):
                ex = ex.strip() + '.'
            if note in ex:
                return ex
            return (ex + ' ' + note).strip()
        df['ViolationExplanation'] = df.apply(lambda r: _append_monitor_note(r.name, r), axis=1)

        df['MonitorFlag'] = df.apply(lambda r: (int(r.get('ViolationDays') or 0) > 0) and bool(r.get('PresentToday')), axis=1)

        # OVERRIDE: force High risk when ViolationDays >= 4
        try:
            high_violation_mask = df['ViolationDays'] >= 4
            if high_violation_mask.any():
                df.loc[high_violation_mask, 'RiskScore'] = 5
                df.loc[high_violation_mask, 'RiskLevel'] = 'High'
        except Exception:
            pass

        # Ensure default RiskScore / RiskLevel are present (map from AnomalyScore if not set)
        try:
            if 'RiskLevel' not in df.columns or df['RiskLevel'].isnull().all():
                df['RiskScore'] = None
                df['RiskLevel'] = None
            # fill per-row if missing using map_score_to_label
            def _fill_risk(r):
                try:
                    if r.get('RiskLevel') not in (None, '', float('nan')):
                        return r.get('RiskScore'), r.get('RiskLevel')
                    sc = float(r.get('AnomalyScore') or 0.0)
                    bucket, label = map_score_to_label(sc)
                    return int(bucket), label
                except Exception:
                    return (None, None)
            risk_pairs = df.apply(lambda r: pd.Series(_fill_risk(r), index=['RiskScore_temp','RiskLevel_temp']), axis=1)
            # only set where missing (don't override forced High)
            df['RiskScore'] = df['RiskScore'].combine_first(risk_pairs['RiskScore_temp'])
            df['RiskLevel'] = df['RiskLevel'].combine_first(risk_pairs['RiskLevel_temp'])
            # cleanup temps
            if 'RiskScore_temp' in df.columns:
                try:
                    df.drop(columns=['RiskScore_temp','RiskLevel_temp'], inplace=True)
                except Exception:
                    pass
        except Exception:
            logging.exception("Failed to populate default RiskScore/RiskLevel")

    except Exception:
        logging.exception("Failed post-scoring weekly-run / monitoring augmentation.")

    # -------------------------
    # Scenario-based Risk Overrides (per requested rules)
    # -------------------------
    try:
        # scenarios that follow the special escalation-by-ViolationDays rule
        escalation_set = {"long_gap_>=4.0h", "coffee_badging", "single_door", "only_in", "only_out"}
        # scenarios that when alone should be considered Low
        low_only_set = {"short_duration_<4h", "overtime_>=14h", "early_arrival_before_06"}
        label_to_score = {"Low": 1, "Low Medium": 2, "Medium": 3, "Medium High": 4, "High": 5}

        def _extract_detected_set(r):
            ds = r.get('DetectedScenarios') or r.get('Reasons') or ''
            out = set()
            try:
                if isinstance(ds, (list, tuple)):
                    for x in ds:
                        if x:
                            out.add(str(x).strip())
                elif isinstance(ds, str):
                    for p in re.split(r'[;,\|]', ds):
                        p = (p or '').strip()
                        if p:
                            out.add(p)
            except Exception:
                pass
            return out

        def _compute_override_for_row(r):
            try:
                # only consider overriding flagged rows
                if not bool(r.get('IsFlagged')):
                    return (r.get('RiskScore'), r.get('RiskLevel'))

                detected = _extract_detected_set(r)
                vd = int(r.get('ViolationDays') or 0)

                # 1) If any of escalation_set present -> escalate using ViolationDays
                if detected & escalation_set:
                    if vd <= 1:
                        lab = "Low Medium"
                    elif vd == 2:
                        lab = "Medium"
                    elif vd == 3:
                        lab = "Medium High"
                    else:
                        lab = "High"
                    return (label_to_score[lab], lab)

                # 2) If exactly one detected scenario and it is in low_only_set -> Low
                if len(detected) == 1 and next(iter(detected)) in low_only_set:
                    return (label_to_score["Low"], "Low")

                # 3) For other flagged rows, escalate by ViolationDays if > 0, else keep existing
                if vd == 1:
                    return (label_to_score["Low Medium"], "Low Medium")
                if vd == 2:
                    return (label_to_score["Medium"], "Medium")
                if vd == 3:
                    return (label_to_score["Medium High"], "Medium High")
                if vd >= 4:
                    return (label_to_score["High"], "High")

                # 4) fallback — keep whatever was computed earlier
                cur_label = r.get('RiskLevel') or None
                cur_score = r.get('RiskScore') if r.get('RiskScore') is not None else (label_to_score.get(cur_label) if cur_label else None)
                return (int(cur_score) if cur_score is not None else None, cur_label)
            except Exception:
                return (r.get('RiskScore'), r.get('RiskLevel'))

        # apply override and set columns (only where override returns non-null)
        overrides = df.apply(lambda r: pd.Series(_compute_override_for_row(r), index=['__rs_override', '__rl_override']), axis=1)
        # prefer override values where provided (non-null)
        def _coerce_int(v):
            try:
                if v is None:
                    return None
                return int(v)
            except Exception:
                return None

        df['RiskScore'] = overrides['__rs_override'].combine_first(df.get('RiskScore'))
        df['RiskLevel'] = overrides['__rl_override'].combine_first(df.get('RiskLevel'))

        # ensure RiskScore is integer where possible
        try:
            df['RiskScore'] = df['RiskScore'].apply(lambda v: _coerce_int(v) if v is not None else None)
        except Exception:
            pass

    except Exception:
        logging.exception("Scenario-based risk override failed (non-fatal).")


    # Build textual Reasons and Explanation (if not already)
    def reasons_for_row(r):
        if not bool(r.get('IsFlagged')):
            return None, None
        ds_raw = r.get('DetectedScenarios')
        if ds_raw:
            ds = [s.strip() for s in ds_raw.split(";") if s and s.strip()]
            explanation = _explain_scenarios_detected(r, ds)
            reasons_codes = "; ".join(ds) if ds else None
            return reasons_codes, explanation
        return None, None




    reason_tuples = df.apply(lambda r: pd.Series(reasons_for_row(r), index=['Reasons', 'ViolationExplanation']), axis=1)
    def _sanitize_reason_val(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == "" or _is_placeholder_str(s):
                return None
            return s
        except Exception:
            return None

    df['Reasons'] = reason_tuples['Reasons'].apply(_sanitize_reason_val)
    df['ViolationExplanation'] = reason_tuples['ViolationExplanation'].apply(lambda v: None if _is_placeholder_str(v) else (str(v).strip() if v is not None else None))

    # If flagged but no Reasons, ensure fallback
    def _ensure_reason_for_flagged(row):
        if bool(row.get('IsFlagged')) and (row.get('Reasons') is None or row.get('Reasons') == ''):
            ds = row.get('DetectedScenarios')
            if ds and not _is_placeholder_str(ds):
                parts = [p.strip() for p in re.split(r'[;,\|]', str(ds)) if p and not _is_placeholder_str(p)]
                if parts:
                    return "; ".join(parts)
            if int(row.get('ConsecWeeksShort4hrs') or 0) >= 1:
                return "consecutive_short_weeks"
            if int(row.get('ViolationDaysLast90') or 0) > 0:
                return "historical_monitoring"
            return None
        return row.get('Reasons')

    if 'IsFlagged' in df.columns:
        df['Reasons'] = df.apply(_ensure_reason_for_flagged, axis=1)
    else:
        df['Reasons'] = df['Reasons'].apply(_sanitize_reason_val)

    if 'OverlapWith' not in df.columns:
        def overlap_with_fn(r):
            d = r.get('Date')
            uid = r.get('person_uid')
            if (d, uid) in swipe_overlap_map:
                return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
            return None
        df['OverlapWith'] = df.apply(overlap_with_fn, axis=1)

    # ensure booleans native
    for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
        if col in df.columns:
            df[col] = df[col].astype(bool)

    # return DataFrame
    return df



#end 

# ---------------- run_trend_for_date ----------------
def _slug_city(city: str) -> str:
    if not city:
        return "pune"
    return str(city).strip().lower().replace(" ", "_")


def run_trend_for_date(target_date: date,
                       regions: Optional[List[str]] = None,
                       outdir: str = None,
                       city: str = "pune",
                       window_days: Optional[int] = None,
                       as_dict: bool = False) -> pd.DataFrame:
    city_slug = _slug_city(city)
    if regions is None:
        regions = []
        try:
            rcfg = REGION_CONFIG if isinstance(REGION_CONFIG, dict) else {}
            city_norm = None
            if city:
                city_norm = re.sub(r'[^a-z0-9]', '', str(city).strip().lower())
            # build list of candidate region keys
            all_region_keys = list(rcfg.keys()) if rcfg else []
            matched = []
            if city_norm and rcfg:
                for rkey, rc in rcfg.items():
                    parts = rc.get("partitions", []) or []
                    likes = rc.get("logical_like", []) or []
                    tokens = set()
                    for p in parts:
                        if not p:
                            continue
                        tokens.add(re.sub(r'[^a-z0-9]', '', str(p).strip().lower()))
                        # also split on punctuation/dot and add pieces
                        for piece in re.split(r'[.\-/\s]', str(p)):
                            if piece:
                                tokens.add(re.sub(r'[^a-z0-9]', '', piece.strip().lower()))
                    for lk in likes:
                        tokens.add(re.sub(r'[^a-z0-9]', '', str(lk).strip().lower()))
                    # also include servers/databases if present (safe fallback)
                    serv = rc.get("server") or ""
                    tokens.add(re.sub(r'[^a-z0-9]', '', str(serv).strip().lower()))
                    # match
                    if city_norm in tokens:
                        matched.append(rkey)
                # if matched regions found, use them
                if matched:
                    regions = matched
                else:
                    # fallback: if no match, default to all region keys
                    regions = all_region_keys
            else:
                regions = all_region_keys
        except Exception:
            regions = list(REGION_CONFIG.keys()) if isinstance(REGION_CONFIG, dict) and REGION_CONFIG else []
    # final normalization
    regions = [r.lower() for r in regions if r]

    outdir_path = Path(outdir) if outdir else OUTDIR
    if run_for_date is None:
        raise RuntimeError("duration_report.run_for_date is not available in this environment.")
    # call run_for_date defensively
    try:
        results = run_for_date(target_date, regions, str(outdir_path), city)
    except TypeError:
        try:
            results = run_for_date(target_date, regions, str(outdir_path))
        except Exception:
            try:
                results = run_for_date(target_date)
            except Exception as e:
                logging.exception("run_for_date failed entirely.")
                raise
    # combine durations and swipes across regions
    dur_list = []
    swipe_list = []
    try:
        for rkey, rr in (results or {}).items():
            try:
                dfdur = rr.get('durations')
                if dfdur is not None and not dfdur.empty:
                    dfdur = dfdur.copy()
                    dfdur['region'] = rkey
                    dur_list.append(dfdur)
            except Exception:
                pass
            try:
                dfsw = rr.get('swipes')
                if dfsw is not None and not dfsw.empty:
                    dfcopy = dfsw.copy()
                    dfcopy['region'] = rkey
                    swipe_list.append(dfcopy)
            except Exception:
                pass
    except Exception:
        logging.exception("Failed to iterate results returned by run_for_date.")
    combined = pd.concat(dur_list, ignore_index=True) if dur_list else pd.DataFrame()
    sw_combined = pd.concat(swipe_list, ignore_index=True) if swipe_list else pd.DataFrame()

    # Decide Pune 2AM boundary
    use_pune_2am_boundary = False
    try:
        if city and isinstance(city, str) and 'pun' in city.strip().lower():
            use_pune_2am_boundary = True
        else:
            if os.getenv("PUNE_2AM_BOUNDARY", "0") == "1":
                use_pune_2am_boundary = True
    except Exception:
        use_pune_2am_boundary = False


 # Prepare for features; possibly shift times for Pune 02:00 grouping
    sw_for_features = sw_combined.copy() if sw_combined is not None else pd.DataFrame()
    durations_for_features = combined.copy() if combined is not None else pd.DataFrame()

    if use_pune_2am_boundary and (sw_for_features is not None) and (not sw_for_features.empty):
        try:
            if 'LocaleMessageTime' in sw_for_features.columns:
                sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features['LocaleMessageTime'], errors='coerce')
            else:
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                    if cand in sw_for_features.columns:
                        sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features[cand], errors='coerce')
                        break
            sw_for_features['OriginalLocaleMessageTime'] = sw_for_features['LocaleMessageTime']
            sw_for_features['LocaleMessageTime'] = sw_for_features['LocaleMessageTime'] - pd.Timedelta(hours=2)
            # recompute durations if compute_daily_durations is available
            if callable(compute_daily_durations):
                try:
                    durations_for_features = compute_daily_durations(sw_for_features)
                except Exception:
                    logging.exception("compute_daily_durations failed for shifted swipes; falling back to original durations.")
                    durations_for_features = combined.copy()
            # save shifted raw optionally
            try:
                sw_shifted_out = Path(outdir_path) / f"swipes_{city_slug}_{target_date.strftime('%Y%m%d')}_shifted.csv"
                cols_keep = [c for c in sw_for_features.columns if c in ('person_uid','EmployeeID','CardNumber','LocaleMessageTime','OriginalLocaleMessageTime','Door','Direction','PartitionName2')]
                sw_for_features[cols_keep].to_csv(sw_shifted_out, index=False)
            except Exception:
                logging.debug("Could not write shifted swipes file.")
        except Exception:
            logging.exception("Failed to prepare shifted swipes for Pune 2AM logic.")
            sw_for_features = sw_combined.copy()
            durations_for_features = combined.copy()


    # compute features once (use possibly-shifted data so grouping uses 02:00 boundary for Pune)
    features = compute_features(sw_for_features, durations_for_features)
    if features is None:
        features = pd.DataFrame()
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        if as_dict:
            return {'rows': 0, 'flagged_rows': 0, 'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': 0}
        return pd.DataFrame()
    # restore FirstSwipe/LastSwipe to original timeline if shifted (only once)
    if use_pune_2am_boundary:
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in features.columns:
                try:
                    features[dtcol] = pd.to_datetime(features[dtcol], errors='coerce') + pd.Timedelta(hours=2)
                except Exception:
                    pass

    # Save raw swipes for evidence
    try:
        if sw_combined is not None and not sw_combined.empty:
            sw_out = Path(outdir_path) / f"swipes_{city_slug}_{target_date.strftime('%Y%m%d')}.csv"
            sw_combined.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception:
        logging.warning("Failed to save raw swipes")

    # Recompute per-row metrics from raw swipes and merge into features
    try:
        if sw_combined is not None and not sw_combined.empty:
            if 'LocaleMessageTime' not in sw_combined.columns:
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                    if cand in sw_combined.columns:
                        sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined[cand], errors='coerce')
                        break
            else:
                sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined['LocaleMessageTime'], errors='coerce')
            if use_pune_2am_boundary:
                sw_combined['DisplayDateKey'] = (sw_combined['LocaleMessageTime'] - pd.Timedelta(hours=2)).dt.date
            else:
                sw_combined['DisplayDateKey'] = sw_combined['LocaleMessageTime'].dt.date

            def _agg_metrics(g):
                times_sorted = sorted(list(pd.to_datetime(g['LocaleMessageTime'].dropna())))
                count_swipes = len(times_sorted)
                max_gap = 0
                short_gap_count = 0
                if len(times_sorted) >= 2:
                    gaps = []
                    for i in range(1, len(times_sorted)):
                        s = (times_sorted[i] - times_sorted[i-1]).total_seconds()
                        gaps.append(s)
                        if s <= 5*60:
                            short_gap_count += 1
                    max_gap = int(max(gaps)) if gaps else 0
                first_ts = times_sorted[0] if times_sorted else None
                last_ts = times_sorted[-1] if times_sorted else None
                unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
                unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
                def _pick_non_guid(colname):
                    if colname in g.columns:
                        for v in pd.unique(g[colname].dropna().astype(str).map(lambda x: x.strip())):
                            if v and (not _GUID_RE.match(v)) and v.lower() not in _PLACEHOLDER_STRS:
                                return v
                    return None
                card = _pick_non_guid('CardNumber')
                empid = _pick_non_guid('EmployeeID') or _pick_non_guid('Int1') or _pick_non_guid('Text12')
                empname = _pick_non_guid('EmployeeName') or _pick_non_guid('ObjectName1')
                duration_sec = 0.0
                if first_ts is not None and last_ts is not None:
                    try:
                        duration_sec = float((pd.to_datetime(last_ts) - pd.to_datetime(first_ts)).total_seconds())
                        if duration_sec < 0:
                            duration_sec = 0.0
                    except Exception:
                        duration_sec = 0.0
                return pd.Series({
                    'FirstSwipe_raw': first_ts,
                    'LastSwipe_raw': last_ts,
                    'CountSwipes_raw': int(count_swipes),
                    'DurationSeconds_raw': float(duration_sec),
                    'DurationMinutes_raw': float(duration_sec/60.0),
                    'MaxSwipeGapSeconds_raw': int(max_gap),
                    'ShortGapCount_raw': int(short_gap_count),
                    'UniqueDoors_raw': int(unique_doors),
                    'UniqueLocations_raw': int(unique_locations),
                    'CardNumber_raw': card,
                    'EmployeeID_raw': empid,
                    'EmployeeName_raw': empname
                })

            grouped_raw = sw_combined.dropna(subset=['person_uid', 'DisplayDateKey'], how='any').groupby(['person_uid', 'DisplayDateKey'])
            if not grouped_raw.ngroups:
                raw_metrics_df = pd.DataFrame(columns=[
                    'person_uid','DisplayDate','FirstSwipe_raw','LastSwipe_raw','CountSwipes_raw','DurationSeconds_raw',
                    'DurationMinutes_raw','MaxSwipeGapSeconds_raw','ShortGapCount_raw','UniqueDoors_raw','UniqueLocations_raw',
                    'CardNumber_raw','EmployeeID_raw','EmployeeName_raw'
                ])
            else:
                raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
                raw_metrics_df.rename(columns={'DisplayDateKey':'DisplayDate'}, inplace=True)

            # --- robust creation of merge keys for DisplayDate ---
            # We used to assume 'DisplayDate' exists; sometimes it doesn't which caused KeyError/AttributeError.
            # Create two helper columns that are safe for joining: a normalized Timestamp and a safe string.
            try:
                if 'DisplayDate' in features.columns:
                    try:
                        features['_DisplayDate_for_merge'] = pd.to_datetime(features['DisplayDate'], errors='coerce').dt.normalize()
                    except Exception:
                        features['_DisplayDate_for_merge'] = pd.NaT
                    try:
                        features['_DisplayDate_for_merge_str'] = pd.to_datetime(features['DisplayDate'], errors='coerce').astype(str).fillna('')
                    except Exception:
                        # fallback to stringification of the original series
                        try:
                            features['_DisplayDate_for_merge_str'] = features['DisplayDate'].astype(str).fillna('')
                        except Exception:
                            features['_DisplayDate_for_merge_str'] = ''
                else:
                    features['_DisplayDate_for_merge'] = pd.NaT
                    features['_DisplayDate_for_merge_str'] = ''
            except Exception:
                logging.exception("Failed to build feature merge keys for DisplayDate; proceeding without them")
                features['_DisplayDate_for_merge'] = pd.NaT
                features['_DisplayDate_for_merge_str'] = ''

            try:
                if 'DisplayDate' in raw_metrics_df.columns:
                    raw_metrics_df['_DisplayDate_for_merge'] = pd.to_datetime(raw_metrics_df['DisplayDate'], errors='coerce').dt.normalize()
                    raw_metrics_df['_DisplayDate_for_merge_str'] = pd.to_datetime(raw_metrics_df['DisplayDate'], errors='coerce').astype(str).fillna('')
                else:
                    raw_metrics_df['_DisplayDate_for_merge'] = pd.NaT
                    raw_metrics_df['_DisplayDate_for_merge_str'] = ''
            except Exception:
                logging.exception("Failed to build raw_metrics merge keys; falling back to string keys")
                raw_metrics_df['_DisplayDate_for_merge'] = pd.NaT
                raw_metrics_df['_DisplayDate_for_merge_str'] = ''

            # Prefer the datetime normalized join if available, else fall back to string join
            merged_metrics = None
            try:
                merged_metrics = pd.merge(features, raw_metrics_df, how='left',
                                          left_on=['person_uid', '_DisplayDate_for_merge'],
                                          right_on=['person_uid', '_DisplayDate_for_merge'],
                                          suffixes=('','_rawagg'))
            except Exception:
                try:
                    merged_metrics = pd.merge(features, raw_metrics_df, how='left',
                                              left_on=['person_uid', '_DisplayDate_for_merge_str'],
                                              right_on=['person_uid', '_DisplayDate_for_merge_str'],
                                              suffixes=('','_rawagg'))
                except Exception:
                    logging.exception("Both merge attempts failed; continuing without raw-agg merge")
                    merged_metrics = features.copy()

            # coalesce raw columns back into features (if present)
            try:
                for base_col, raw_col in [
                    ('FirstSwipe','FirstSwipe_raw'),
                    ('LastSwipe','LastSwipe_raw'),
                    ('CountSwipes','CountSwipes_raw'),
                    ('DurationSeconds','DurationSeconds_raw'),
                    ('DurationMinutes','DurationMinutes_raw'),
                    ('MaxSwipeGapSeconds','MaxSwipeGapSeconds_raw'),
                    ('ShortGapCount','ShortGapCount_raw'),
                    ('UniqueDoors','UniqueDoors_raw'),
                    ('UniqueLocations','UniqueLocations_raw'),
                    ('CardNumber','CardNumber_raw'),
                    ('EmployeeID','EmployeeID_raw'),
                    ('EmployeeName','EmployeeName_raw')
                ]:
                    if raw_col in merged_metrics.columns:
                        try:
                            merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
                        except Exception:
                            # best-effort: if combine_first fails, keep original
                            pass
                feature_cols = list(features.columns)
                if all(c in merged_metrics.columns for c in feature_cols):
                    features = merged_metrics[feature_cols].copy()
                else:
                    features = merged_metrics.copy()
                for helper_col in ['_DisplayDate_for_merge', '_DisplayDate_for_merge_str']:
                    if helper_col in features.columns:
                        try:
                            features.drop(columns=[helper_col], inplace=True)
                        except Exception:
                            pass
            except Exception:
                logging.exception("Post-merge coalescing failed; leaving features as-is.")



    except Exception:
        logging.exception("Failed recomputing raw metrics (non-fatal)")




    # If we used shifted timeline restore displayed times (safety)
    if use_pune_2am_boundary:
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in features.columns:
                try:
                    features[dtcol] = pd.to_datetime(features[dtcol], errors='coerce') + pd.Timedelta(hours=2)
                except Exception:
                    pass

    # Merge features with durations (prefer features)
    try:
        merged = pd.merge(features, combined, how='left', on=['person_uid', 'Date'], suffixes=('_feat', '_dur'))
    except Exception:
        merged = features

    #trend_df = score_trends_from_durations(merged, swipes_df=sw_combined, outdir=str(outdir_path), target_date=target_date)

    trend_df = score_trends_from_durations(merged, swipes_df=sw_combined, outdir=str(outdir_path), target_date=target_date, window_days=window_days)

    # write csv (use city_slug, not hard-coded 'pune')
    try:
        write_df = trend_df.copy()
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        write_df = write_df.where(pd.notnull(write_df), None)
        # IMPORTANT: write with city_slug so app.py can find the file
        out_csv = Path(outdir_path) / f"trend_{city_slug}_{target_date.strftime('%Y%m%d')}.csv"
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception:
        logging.exception("Failed to write trend CSV")

    # Format DisplayDate
    try:
        if 'DisplayDate' in trend_df.columns:
            trend_df['DisplayDate'] = pd.to_datetime(trend_df['DisplayDate'], errors='coerce').dt.date
            trend_df['DisplayDate'] = trend_df['DisplayDate'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
    except Exception:
        pass

    if as_dict:
        # ------------------ Ensure sample/aggregated rows contain enrichment (email/image) ------------------
        try:
            # make a copy we will return
            rec_df = trend_df.copy()

            # add friendly string times for First/Last for JSON output
            for dtcol in ('FirstSwipe', 'LastSwipe'):
                if dtcol in rec_df.columns:
                    rec_df[dtcol] = pd.to_datetime(rec_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')

            # add Date ISO strings
            if 'Date' in rec_df.columns:
                try:
                    rec_df['Date'] = pd.to_datetime(rec_df['Date'], errors='coerce').dt.date
                    rec_df['Date'] = rec_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
                except Exception:
                    pass

            # Enrich with personnel info (email, imageUrl) using the helper already defined
            try:
                # use endpoint template that the frontend expects (it will call /employee/<id>/image)
                rec_df = _enrich_with_personnel_info(rec_df, image_endpoint_template="/employee/{}/image")
            except Exception:
                # non-fatal - if enrichment fails, continue without email/image
                logging.exception("Personnel enrichment failed (non-fatal).")

            # Build files list (raw swipe files written earlier)
            files_list = []
            try:
                # looks for swipes file for this city/date naming conventions saved earlier
                # collect any swipes_*.csv in OUTDIR for this run date
                globp = list(Path(outdir_path).glob("swipes_*_*.csv"))
                files_list = [p.name for p in globp]
            except Exception:
                files_list = []

            # Optionally build a raw_swipes map from sw_combined (if large, this can be trimmed later)
            raw_swipes_all = []
            try:
                if sw_combined is not None and not sw_combined.empty:
                    # ensure LocaleMessageTime parsed
                    if 'LocaleMessageTime' in sw_combined.columns:
                        sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined['LocaleMessageTime'], errors='coerce')
                    else:
                        # try common candidates
                        for cand in ('MessageUTC','MessageTime','Timestamp','timestamp','Date'):
                            if cand in sw_combined.columns:
                                sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined[cand], errors='coerce')
                                break
                    # minimal projection fields used by frontend screenshot timeline
                    proj_cols = []
                    for c in ('EmployeeName','EmployeeID','person_uid','CardNumber','Door','Direction','Zone','PartitionName2'):
                        if c in sw_combined.columns:
                            proj_cols.append(c)
                    # add a safe time/date/time-string, and compute gap per door/person grouping if possible
                    tmp = sw_combined.copy()
                    tmp['Date'] = tmp['LocaleMessageTime'].dt.date.astype(str)
                    tmp['Time'] = tmp['LocaleMessageTime'].dt.time.astype(str)
                    # sort so gap calc is consistent
                    tmp = tmp.sort_values(['person_uid','LocaleMessageTime'])
                    tmp['SwipeGapSeconds'] = tmp.groupby(['person_uid','Date'])['LocaleMessageTime'].diff().dt.total_seconds().fillna(0).astype(int)
                    tmp['SwipeGap'] = tmp['SwipeGapSeconds'].apply(lambda s: format_seconds_to_hms(s) if s is not None else "-")
                    # include Zone column if map_door_to_zone produced it earlier - otherwise attempt mapping by door/direction
                    if 'Zone' not in tmp.columns and 'Door' in tmp.columns:
                        tmp['Zone'] = tmp.apply(lambda r: map_door_to_zone(r.get('Door'), r.get('Direction')), axis=1)
                    raw_swipes_all = tmp.to_dict(orient='records')
            except Exception:
                logging.exception("Building raw_swipes list failed (non-fatal).")
                raw_swipes_all = []

            # Build reason counts and risk counts (existing logic)
            reasons_count = {}
            risk_counts = {}
            try:
                if 'Reasons' in rec_df.columns:
                    for v in rec_df['Reasons'].dropna().astype(str):
                        for part in re.split(r'[;,\|]', v):
                            key = part.strip()
                            if key:
                                reasons_count[key] = reasons_count.get(key, 0) + 1
                if 'RiskLevel' in rec_df.columns:
                    for v in rec_df['RiskLevel'].fillna('').astype(str):
                        if v:
                            risk_counts[v] = risk_counts.get(v, 0) + 1
            except Exception:
                pass

            # sample records (top 20)
            sample_records = rec_df.head(20).to_dict(orient='records') if not rec_df.empty else []

            return {
                'rows': int(len(rec_df)),
                'flagged_rows': int(rec_df['IsFlagged'].sum()) if 'IsFlagged' in rec_df.columns else 0,
                'aggregated_unique_persons': int(len(rec_df)),
                'sample': sample_records,
                'reasons_count': reasons_count,
                'risk_counts': risk_counts,
                'files': files_list,
                # convenience additions used by the frontend record endpoint for quick lookup
                'raw_swipes_all': raw_swipes_all
            }
        except Exception:
            logging.exception("Failed to build as_dict output for run_trend_for_date")
            # fallback minimal structure
            return {'rows': len(trend_df), 'flagged_rows': int(trend_df['IsFlagged'].sum() if 'IsFlagged' in trend_df.columns else 0),
                    'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': len(trend_df), 'files': []}

    return trend_df




# ---------------- helper wrappers ----------------
from datetime import datetime as _datetime
from datetime import timedelta as _timedelta
def _ensure_date_obj(d):
    if d is None:
        return None
    if isinstance(d, date):
        return d
    if isinstance(d, _datetime):
        return d.date()
    if isinstance(d, str):
        try:
            return _datetime.strptime(d, "%Y-%m-%d").date()
        except Exception:
            try:
                return _datetime.fromisoformat(d).date()
            except Exception:
                raise ValueError(f"Unsupported date string: {d}")
    raise ValueError(f"Unsupported date type: {type(d)}")

def build_monthly_training(start_date=None, end_date=None, outdir: str = None, city: str = 'Pune', as_dict: bool = False):
    od = Path(outdir) if outdir else OUTDIR
    od.mkdir(parents=True, exist_ok=True)
    if start_date is None and end_date is None:
        today = date.today()
        first = date(today.year, today.month, 1)
        last = date(today.year, today.month, calendar.monthrange(today.year, today.month)[1])
    else:
        if start_date is None:
            raise ValueError("start_date must be provided when end_date is provided")
        first = _ensure_date_obj(start_date)
        if end_date is None:
            last = date(first.year, first.month, calendar.monthrange(first.year, first.month)[1])
        else:
            last = _ensure_date_obj(end_date)
    if last < first:
        raise ValueError("end_date must be >= start_date")
    cur = first
    ran = []
    errors = {}
    total_flagged = 0
    total_rows = 0
    while cur <= last:
        try:
            logging.info("build_monthly_training: running for %s (city=%s)", cur.isoformat(), city)
            res = run_trend_for_date(cur, outdir=str(od), city=city, as_dict=as_dict)
            ran.append({'date': cur.isoformat(), 'result': res})
            if isinstance(res, dict):
                total_flagged += int(res.get('flagged_rows', 0) or 0)
                total_rows += int(res.get('rows', 0) or 0)
        except KeyboardInterrupt:
            raise
        except Exception as e:
            logging.exception("build_monthly_training: failed for %s", cur)
            errors[cur.isoformat()] = str(e)
        cur = cur + _timedelta(days=1)
    summary = {
        'start_date': first.isoformat(),
        'end_date': last.isoformat(),
        'dates_attempted': (last - first).days + 1,
        'dates_succeeded': len([r for r in ran if r.get('result') is not None]),
        'dates_failed': len(errors),
        'errors': errors,
        'total_rows': total_rows,
        'total_flagged': total_flagged
    }
    if as_dict:
        return summary
    return ran


def read_90day_cache(outdir: str = None):
    od = Path(outdir) if outdir else OUTDIR
    fp = od / "90day_cache.json"
    if not fp.exists():
        return {}
    try:
        with fp.open("r", encoding="utf8") as fh:
            return json.load(fh)
    except Exception:
        logging.exception("read_90day_cache: failed to read %s", str(fp))
        return {}

if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today, as_dict=False)
    print("Completed; rows:", len(df) if df is not None else 0)
