# ccure_compare_service.py
"""
Compare CCURE active lists (employees & contractors) with uploaded Active sheets (DB).
Provides compare_ccure_vs_sheets(mode='full', stats_detail='ActiveProfiles', limit_list=200, export=False)

Saves an Excel report to OUTPUT_DIR when export=True.
"""

from datetime import datetime
import os
import re
import uuid
import logging
import pandas as pd

logger = logging.getLogger("ccure_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    import sys
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Settings fallback (matches app.py pattern)
try:
    from settings import OUTPUT_DIR
except Exception:
    OUTPUT_DIR = "./output"

# DB / models
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor

# ccure client helper (already present in project)
try:
    import ccure_client
except Exception:
    ccure_client = None
    logger.warning("ccure_client not importable; CCURE calls will return None")

# ---------- small normalizers (kept local to avoid circular imports) ----------
def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

def _make_w_variant(s):
    if s is None:
        return None
    ss = str(s).strip()
    if ss.upper().startswith('W'):
        return ss
    return 'W' + ss

# numeric variants (e.g. strip non-digits, add/remove W)
def _numeric_variants(s):
    out = set()
    if s is None:
        return out
    try:
        s = str(s)
        clean = re.sub(r'\D', '', s)
        if clean:
            out.add(clean)
            out.add(clean.lstrip('0') or clean)
            out.add('W' + clean)
    except Exception:
        pass
    return out

# ---------- CCURE fetch helpers ----------
def _fetch_ccure_list(detail_name):
    """
    Uses ccure_client.fetch_all_stats(detail_name) if available, otherwise tries ccure_client.fetch_stats_page(detail_name).
    Returns list of dicts or [].
    """
    if ccure_client is None:
        logger.warning("ccure_client missing - cannot fetch CCURE lists")
        return []
    try:
        if hasattr(ccure_client, "fetch_all_stats"):
            res = ccure_client.fetch_all_stats(detail_name, limit=500)
            return res or []
    except Exception:
        logger.exception("fetch_all_stats failed")
    # fallback to multiple page fetch
    try:
        data = []
        page = 1
        limit = 500
        while True:
            page_res = ccure_client.fetch_stats_page(detail_name, page=page, limit=limit)
            if not page_res:
                break
            part = page_res.get("data") or []
            if not part:
                break
            data.extend(part)
            total = int(page_res.get("total") or len(data) or 0)
            if len(data) >= total:
                break
            page += 1
            if page > 1000:
                break
        return data
    except Exception:
        logger.exception("per-page fetch failed for %s", detail_name)
        return []

# ---------- DB loaders ----------
def _load_active_employees_db():
    """Return set of normalized employee_ids and a dict mapping id -> row for samples"""
    with SessionLocal() as db:
        rows = db.query(ActiveEmployee).all()
        ids = set()
        mapping = {}
        for r in rows:
            nid = _normalize_employee_key(r.employee_id)
            if nid:
                ids.add(nid)
                mapping[nid] = {
                    "employee_id": nid,
                    "full_name": r.full_name,
                    "location_city": r.location_city,
                    "status": r.current_status
                }
        return ids, mapping

def _load_active_contractors_db():
    """Return set of candidate contractor ids (worker_system_id, ipass, W-ipass) and mapping"""
    with SessionLocal() as db:
        rows = db.query(ActiveContractor).all()
        ids = set()
        mapping = {}
        for r in rows:
            wsid = _normalize_employee_key(getattr(r, "worker_system_id", None))
            ipass = _normalize_employee_key(getattr(r, "ipass_id", None))
            if wsid:
                ids.add(wsid)
                mapping[wsid] = {"worker_system_id": wsid, "full_name": r.full_name, "vendor": r.vendor, "location": r.location}
            if ipass:
                ids.add(ipass)
                mapping[ipass] = {"ipass_id": ipass, "full_name": r.full_name, "vendor": r.vendor, "location": r.location}
                wvar = _make_w_variant(ipass)
                ids.add(wvar)
                mapping[wvar] = {"ipass_id": ipass, "w_ipass": wvar, "full_name": r.full_name, "vendor": r.vendor, "location": r.location}
            # also add numeric variants if worker id numeric-ish
            for cand in (wsid, ipass):
                for v in _numeric_variants(cand):
                    ids.add(v)
                    if v not in mapping:
                        mapping[v] = {"derived_id": v, "full_name": r.full_name}
        return ids, mapping

# ---------- core compare function ----------
def compare_ccure_vs_sheets(mode="full", stats_detail="ActiveProfiles", limit_list=200, export=False):
    """
    mode: 'full' or 'stats' (keeps signature compatible with app.py)
    stats_detail: used when mode='stats' (ignored here)
    limit_list: sample limit for returned lists
    export: if True, writes an Excel report to OUTPUT_DIR and returns 'report_path' in result
    """
    result = {
        "ccure_active_employees_count": None,
        "ccure_active_contractors_count": None,
        "active_sheet_employee_count": None,
        "active_sheet_contractor_count": None,
        "missing_employees_count": None,
        "missing_contractors_count": None,
        "missing_employees_sample": [],
        "missing_contractors_sample": [],
        "report_path": None
    }

    # 1) fetch CCURE lists
    ccure_emps = _fetch_ccure_list("ActiveEmployees")
    ccure_contrs = _fetch_ccure_list("ActiveContractors")

    result["ccure_active_employees_count"] = len(ccure_emps)
    result["ccure_active_contractors_count"] = len(ccure_contrs)

    # 2) load DB sheets
    emp_ids_db, emp_map_db = _load_active_employees_db()
    contr_ids_db, contr_map_db = _load_active_contractors_db()

    result["active_sheet_employee_count"] = len(emp_ids_db)
    result["active_sheet_contractor_count"] = len(contr_ids_db)

    # 3) build ccure id sets for employees
    ccure_emp_id_set = set()
    ccure_emp_rows_by_id = {}
    for row in ccure_emps:
        try:
            # common CCURE key is "EmployeeID" (from samples)
            eid = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("EmpID") or row.get("Employee Id"))
            if not eid:
                # try other keys / card-like
                eid = _normalize_card_like(row.get("CardNumber") or row.get("iPass ID") or row.get("IPassID") or row.get("Card"))
            if not eid:
                # try name-based fallback: not ideal for id set but include a synthetic key
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    eid = f"name::{fname}"
            if eid:
                ccure_emp_id_set.add(eid)
                ccure_emp_rows_by_id[eid] = row
        except Exception:
            continue

    # 4) employees missing = ccure_emp_id_set - emp_ids_db (but consider numeric variants)
    # expand DB set with numeric variants for better match
    expanded_emp_db_ids = set(emp_ids_db)
    for v in list(emp_ids_db):
        for nv in _numeric_variants(v):
            expanded_emp_db_ids.add(nv)
    # now compute missing
    missing_emp_ids = []
    for cid in ccure_emp_id_set:
        # if synthetic name key, do name match against DB names
        if str(cid).startswith("name::"):
            name = cid.split("::", 1)[1]
            found = False
            # try match by normalized name in DB names
            for dbk, dbv in emp_map_db.items():
                n = _normalize_name(dbv.get("full_name"))
                if n and n == name:
                    found = True
                    break
            if not found:
                missing_emp_ids.append(cid)
            continue
        # direct equality or numeric variants
        if cid in expanded_emp_db_ids:
            continue
        # check numeric variants of cid
        found = False
        for v in _numeric_variants(cid):
            if v in expanded_emp_db_ids:
                found = True
                break
        if not found:
            # not found -> missing
            missing_emp_ids.append(cid)

    result["missing_employees_count"] = len(missing_emp_ids)
    # sample
    samp_emp = []
    for mid in missing_emp_ids[:limit_list]:
        r = ccure_emp_rows_by_id.get(mid) or {}
        samp_emp.append({
            "ccure_key": mid,
            "EmployeeID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "PersonnelType": r.get("PersonnelType"),
            "raw": r
        })
    result["missing_employees_sample"] = samp_emp

    # 5) contractors: build ccure contractor id set and rows
    ccure_contr_id_set = set()
    ccure_contr_rows_by_id = {}
    for row in ccure_contrs:
        try:
            # try EmployeeID (samples show W0026455), also try IPass fields
            cand_ids = []
            e1 = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("Employee Id"))
            if e1:
                cand_ids.append(e1)
            ip = _normalize_employee_key(row.get("IPassID") or row.get("iPass ID") or row.get("iPass") or row.get("IPASSID"))
            if ip:
                cand_ids.append(ip)
                cand_ids.append(_make_w_variant(ip))
            # card-like variants
            cardlike = _normalize_card_like(row.get("CardNumber") or row.get("card_number") or row.get("Badge") or row.get("BadgeNo"))
            if cardlike:
                cand_ids.append(cardlike)
                cand_ids.extend(list(_numeric_variants(cardlike)))
            # name key fallback
            if not cand_ids:
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    cand_ids.append(f"name::{fname}")
            # add all candidates for this row
            added_key = None
            for cid in cand_ids:
                if cid:
                    ccure_contr_id_set.add(cid)
                    ccure_contr_rows_by_id[cid] = row
                    if not added_key:
                        added_key = cid
            # if nothing, make synthetic entry
            if not cand_ids:
                key = f"unknown::{uuid.uuid4().hex[:8]}"
                ccure_contr_id_set.add(key)
                ccure_contr_rows_by_id[key] = row
        except Exception:
            continue

    # expand DB contractor ids (contr_ids_db already includes W-variants and numeric variants)
    expanded_contr_db_ids = set(contr_ids_db)
    for v in list(contr_ids_db):
        for nv in _numeric_variants(v):
            expanded_contr_db_ids.add(nv)
    # compute missing
    missing_contr_ids = []
    for cid in ccure_contr_id_set:
        if str(cid).startswith("name::"):
            name = cid.split("::", 1)[1]
            found = False
            for dbk, dbv in contr_map_db.items():
                n = _normalize_name(dbv.get("full_name"))
                if n and n == name:
                    found = True
                    break
            if not found:
                missing_contr_ids.append(cid)
            continue
        if cid in expanded_contr_db_ids:
            continue
        found = False
        for v in _numeric_variants(cid):
            if v in expanded_contr_db_ids:
                found = True
                break
        if not found:
            missing_contr_ids.append(cid)

    result["missing_contractors_count"] = len(missing_contr_ids)
    samp_contr = []
    for mid in missing_contr_ids[:limit_list]:
        r = ccure_contr_rows_by_id.get(mid) or {}
        samp_contr.append({
            "ccure_key": mid,
            "EmployeeID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "VendorCompany": r.get("Vendor Company Name") or r.get("Vendor"),
            "raw": r
        })
    result["missing_contractors_sample"] = samp_contr

    # 6) optionally export report
    if export:
        try:
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            fname = f"missing_vs_ccure_{ts}.xlsx"
            fullpath = os.path.join(OUTPUT_DIR, fname)
            # build DataFrames
            df_emp = pd.DataFrame(samp_emp) if samp_emp else pd.DataFrame(columns=["ccure_key","EmployeeID","EmpName","PersonnelType"])
            df_con = pd.DataFrame(samp_contr) if samp_contr else pd.DataFrame(columns=["ccure_key","EmployeeID","EmpName","VendorCompany"])
            # write
            try:
                with pd.ExcelWriter(fullpath, engine="openpyxl") as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
            except Exception:
                # fallback: use default engine
                with pd.ExcelWriter(fullpath) as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
        except Exception:
            logger.exception("Failed to export report")
            result["report_path"] = None

    return result









# data_compare_service.py
"""
Compare CCURE active lists (employees & contractors) with uploaded Active sheets (DB).
Provides compare_ccure_vs_sheets(mode='full', stats_detail='ActiveProfiles', limit_list=200, export=False)

Saves an Excel report to OUTPUT_DIR when export=True.
"""

import logging
logger = logging.getLogger("data_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    import sys
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# ... rest of file (unchanged) ...































Compare CCURE active lists (employees & contractors) with uploaded Active sheets (DB).
Provides compare_ccure_vs_sheets(mode='full', stats_detail='ActiveProfiles', limit_list=200, export=False)

Saves an Excel report to OUTPUT_DIR when export=True.
"""

from datetime import datetime
import os
import re
import uuid
import logging
import pandas as pd

logger = logging.getLogger("ccure_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    import sys
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Settings fallback (matches app.py pattern)
try:
    from settings import OUTPUT_DIR
except Exception:
    OUTPUT_DIR = "./output"

# DB / models
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor

# ccure client helper (already present in project)
try:
    import ccure_client
except Exception:
    ccure_client = None
    logger.warning("ccure_client not importable; CCURE calls will return None")

# ---------- small normalizers (kept local to avoid circular imports) ----------
def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

def _make_w_variant(s):
    if s is None:
        return None
    ss = str(s).strip()
    if ss.upper().startswith('W'):
        return ss
    return 'W' + ss

# numeric variants (e.g. strip non-digits, add/remove W)
def _numeric_variants(s):
    out = set()
    if s is None:
        return out
    try:
        s = str(s)
        clean = re.sub(r'\D', '', s)
        if clean:
            out.add(clean)
            out.add(clean.lstrip('0') or clean)
            out.add('W' + clean)
    except Exception:
        pass
    return out

# ---------- CCURE fetch helpers ----------
def _fetch_ccure_list(detail_name):
    """
    Uses ccure_client.fetch_all_stats(detail_name) if available, otherwise tries ccure_client.fetch_stats_page(detail_name).
    Returns list of dicts or [].
    """
    if ccure_client is None:
        logger.warning("ccure_client missing - cannot fetch CCURE lists")
        return []
    try:
        if hasattr(ccure_client, "fetch_all_stats"):
            res = ccure_client.fetch_all_stats(detail_name, limit=500)
            return res or []
    except Exception:
        logger.exception("fetch_all_stats failed")
    # fallback to multiple page fetch
    try:
        data = []
        page = 1
        limit = 500
        while True:
            page_res = ccure_client.fetch_stats_page(detail_name, page=page, limit=limit)
            if not page_res:
                break
            part = page_res.get("data") or []
            if not part:
                break
            data.extend(part)
            total = int(page_res.get("total") or len(data) or 0)
            if len(data) >= total:
                break
            page += 1
            if page > 1000:
                break
        return data
    except Exception:
        logger.exception("per-page fetch failed for %s", detail_name)
        return []

# ---------- DB loaders ----------
def _load_active_employees_db():
    """Return set of normalized employee_ids and a dict mapping id -> row for samples"""
    with SessionLocal() as db:
        rows = db.query(ActiveEmployee).all()
        ids = set()
        mapping = {}
        for r in rows:
            nid = _normalize_employee_key(r.employee_id)
            if nid:
                ids.add(nid)
                mapping[nid] = {
                    "employee_id": nid,
                    "full_name": r.full_name,
                    "location_city": r.location_city,
                    "status": r.current_status
                }
        return ids, mapping

def _load_active_contractors_db():
    """Return set of candidate contractor ids (worker_system_id, ipass, W-ipass) and mapping"""
    with SessionLocal() as db:
        rows = db.query(ActiveContractor).all()
        ids = set()
        mapping = {}
        for r in rows:
            wsid = _normalize_employee_key(getattr(r, "worker_system_id", None))
            ipass = _normalize_employee_key(getattr(r, "ipass_id", None))
            if wsid:
                ids.add(wsid)
                mapping[wsid] = {"worker_system_id": wsid, "full_name": r.full_name, "vendor": r.vendor, "location": r.location}
            if ipass:
                ids.add(ipass)
                mapping[ipass] = {"ipass_id": ipass, "full_name": r.full_name, "vendor": r.vendor, "location": r.location}
                wvar = _make_w_variant(ipass)
                ids.add(wvar)
                mapping[wvar] = {"ipass_id": ipass, "w_ipass": wvar, "full_name": r.full_name, "vendor": r.vendor, "location": r.location}
            # also add numeric variants if worker id numeric-ish
            for cand in (wsid, ipass):
                for v in _numeric_variants(cand):
                    ids.add(v)
                    if v not in mapping:
                        mapping[v] = {"derived_id": v, "full_name": r.full_name}
        return ids, mapping

# ---------- core compare function ----------
def compare_ccure_vs_sheets(mode="full", stats_detail="ActiveProfiles", limit_list=200, export=False):
    """
    mode: 'full' or 'stats' (keeps signature compatible with app.py)
    stats_detail: used when mode='stats' (ignored here)
    limit_list: sample limit for returned lists
    export: if True, writes an Excel report to OUTPUT_DIR and returns 'report_path' in result
    """
    result = {
        "ccure_active_employees_count": None,
        "ccure_active_contractors_count": None,
        "active_sheet_employee_count": None,
        "active_sheet_contractor_count": None,
        "missing_employees_count": None,
        "missing_contractors_count": None,
        "missing_employees_sample": [],
        "missing_contractors_sample": [],
        "report_path": None
    }

    # 1) fetch CCURE lists
    ccure_emps = _fetch_ccure_list("ActiveEmployees")
    ccure_contrs = _fetch_ccure_list("ActiveContractors")

    result["ccure_active_employees_count"] = len(ccure_emps)
    result["ccure_active_contractors_count"] = len(ccure_contrs)

    # 2) load DB sheets
    emp_ids_db, emp_map_db = _load_active_employees_db()
    contr_ids_db, contr_map_db = _load_active_contractors_db()

    result["active_sheet_employee_count"] = len(emp_ids_db)
    result["active_sheet_contractor_count"] = len(contr_ids_db)

    # 3) build ccure id sets for employees
    ccure_emp_id_set = set()
    ccure_emp_rows_by_id = {}
    for row in ccure_emps:
        try:
            # common CCURE key is "EmployeeID" (from samples)
            eid = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("EmpID") or row.get("Employee Id"))
            if not eid:
                # try other keys / card-like
                eid = _normalize_card_like(row.get("CardNumber") or row.get("iPass ID") or row.get("IPassID") or row.get("Card"))
            if not eid:
                # try name-based fallback: not ideal for id set but include a synthetic key
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    eid = f"name::{fname}"
            if eid:
                ccure_emp_id_set.add(eid)
                ccure_emp_rows_by_id[eid] = row
        except Exception:
            continue

    # 4) employees missing = ccure_emp_id_set - emp_ids_db (but consider numeric variants)
    # expand DB set with numeric variants for better match
    expanded_emp_db_ids = set(emp_ids_db)
    for v in list(emp_ids_db):
        for nv in _numeric_variants(v):
            expanded_emp_db_ids.add(nv)
    # now compute missing
    missing_emp_ids = []
    for cid in ccure_emp_id_set:
        # if synthetic name key, do name match against DB names
        if str(cid).startswith("name::"):
            name = cid.split("::", 1)[1]
            found = False
            # try match by normalized name in DB names
            for dbk, dbv in emp_map_db.items():
                n = _normalize_name(dbv.get("full_name"))
                if n and n == name:
                    found = True
                    break
            if not found:
                missing_emp_ids.append(cid)
            continue
        # direct equality or numeric variants
        if cid in expanded_emp_db_ids:
            continue
        # check numeric variants of cid
        found = False
        for v in _numeric_variants(cid):
            if v in expanded_emp_db_ids:
                found = True
                break
        if not found:
            # not found -> missing
            missing_emp_ids.append(cid)

    result["missing_employees_count"] = len(missing_emp_ids)
    # sample
    samp_emp = []
    for mid in missing_emp_ids[:limit_list]:
        r = ccure_emp_rows_by_id.get(mid) or {}
        samp_emp.append({
            "ccure_key": mid,
            "EmployeeID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "PersonnelType": r.get("PersonnelType"),
            "raw": r
        })
    result["missing_employees_sample"] = samp_emp

    # 5) contractors: build ccure contractor id set and rows
    ccure_contr_id_set = set()
    ccure_contr_rows_by_id = {}
    for row in ccure_contrs:
        try:
            # try EmployeeID (samples show W0026455), also try IPass fields
            cand_ids = []
            e1 = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("Employee Id"))
            if e1:
                cand_ids.append(e1)
            ip = _normalize_employee_key(row.get("IPassID") or row.get("iPass ID") or row.get("iPass") or row.get("IPASSID"))
            if ip:
                cand_ids.append(ip)
                cand_ids.append(_make_w_variant(ip))
            # card-like variants
            cardlike = _normalize_card_like(row.get("CardNumber") or row.get("card_number") or row.get("Badge") or row.get("BadgeNo"))
            if cardlike:
                cand_ids.append(cardlike)
                cand_ids.extend(list(_numeric_variants(cardlike)))
            # name key fallback
            if not cand_ids:
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    cand_ids.append(f"name::{fname}")
            # add all candidates for this row
            added_key = None
            for cid in cand_ids:
                if cid:
                    ccure_contr_id_set.add(cid)
                    ccure_contr_rows_by_id[cid] = row
                    if not added_key:
                        added_key = cid
            # if nothing, make synthetic entry
            if not cand_ids:
                key = f"unknown::{uuid.uuid4().hex[:8]}"
                ccure_contr_id_set.add(key)
                ccure_contr_rows_by_id[key] = row
        except Exception:
            continue

    # expand DB contractor ids (contr_ids_db already includes W-variants and numeric variants)
    expanded_contr_db_ids = set(contr_ids_db)
    for v in list(contr_ids_db):
        for nv in _numeric_variants(v):
            expanded_contr_db_ids.add(nv)
    # compute missing
    missing_contr_ids = []
    for cid in ccure_contr_id_set:
        if str(cid).startswith("name::"):
            name = cid.split("::", 1)[1]
            found = False
            for dbk, dbv in contr_map_db.items():
                n = _normalize_name(dbv.get("full_name"))
                if n and n == name:
                    found = True
                    break
            if not found:
                missing_contr_ids.append(cid)
            continue
        if cid in expanded_contr_db_ids:
            continue
        found = False
        for v in _numeric_variants(cid):
            if v in expanded_contr_db_ids:
                found = True
                break
        if not found:
            missing_contr_ids.append(cid)

    result["missing_contractors_count"] = len(missing_contr_ids)
    samp_contr = []
    for mid in missing_contr_ids[:limit_list]:
        r = ccure_contr_rows_by_id.get(mid) or {}
        samp_contr.append({
            "ccure_key": mid,
            "EmployeeID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "VendorCompany": r.get("Vendor Company Name") or r.get("Vendor"),
            "raw": r
        })
    result["missing_contractors_sample"] = samp_contr

    # 6) optionally export report
    if export:
        try:
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            fname = f"missing_vs_ccure_{ts}.xlsx"
            fullpath = os.path.join(OUTPUT_DIR, fname)
            # build DataFrames
            df_emp = pd.DataFrame(samp_emp) if samp_emp else pd.DataFrame(columns=["ccure_key","EmployeeID","EmpName","PersonnelType"])
            df_con = pd.DataFrame(samp_contr) if samp_contr else pd.DataFrame(columns=["ccure_key","EmployeeID","EmpName","VendorCompany"])
            # write
            try:
                with pd.ExcelWriter(fullpath, engine="openpyxl") as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
            except Exception:
                # fallback: use default engine
                with pd.ExcelWriter(fullpath) as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
        except Exception:
            logger.exception("Failed to export report")
            result["report_path"] = None

    return result














http://10.199.22.57:5001/api/stats
  "TotalProfiles": 21150,
  "ActiveProfiles": 10173,
  "ActiveEmployees": 8584,
  "ActiveContractors": 652,
  "TerminatedProfiles": 10977,
  "TerminatedEmployees": 148,
  "TerminatedContractors": 145
}

here focus only 
 "ActiveEmployees": 8584,
  "ActiveContractors": 652,

if want to details 

http://10.199.22.57:5001/api/stats?details=ActiveEmployees&page=1&limit=50
{
  "total": 8584,
  "page": 1,
  "limit": 50,
  "data": [
    {
      "id": 2097197204,
      "EmpName": "., Anushka",
      "EmployeeID": "319473",
      "PersonnelType": "Employee",
      "Manager_Name": "Rawat, Mayank",
      "Manager_WU_ID": "0",
      "Profile_Disabled": false,
      "Total_Cards": 1,
      "Active_Cards": 1,
      "Employee_Status": "Active",
      "imageUrl": "/api/employees/2097197204/image"
    },
    {
      "id": 2097203526,
      "EmpName": "., Diwakar",
      "EmployeeID": "324002",
      "PersonnelType": "Employee",
      "Manager_Name": "Charkha, Bhakti",
      "Manager_WU_ID": "0",
      "Profile_Disabled": false,
      "Total_Cards": 1,
      "Active_Cards": 1,
      "Employee_Status": "Active",
      "imageUrl": "/api/employees/2097203526/image"
    },
    {
      "id": 2097203363,
      "EmpName": "., Vikas",
      "EmployeeID": "323879",
      "PersonnelType": "Employee",
      "Manager_Name": "Gupta, Abhishek",
      "Manager_WU_ID": "0",
      "Profile_Disabled": false,
      "Total_Cards": 1,
      "Active_Cards": 1,
      "Employee_Status": "Active",
      "imageUrl": "/api/employees/2097203363/image"
    },


above API provide all details about Active Employee so we need to compare all  "ActiveEmployees": 8584, with Active Sheet 
our Flow is - CCure with Active Employee 
Note- Dont compare Sheet with ccure 

Same Critirea for Contractor, For Contractor refer below API .

http://10.199.22.57:5001/api/stats?details=ActiveContractors&page=1&limit=50
{
  "total": 652,
  "page": 1,
  "limit": 50,
  "data": [
    {
      "id": 2097209941,
      "EmpName": "Abdalla, Meira",
      "EmployeeID": "W0026455",
      "PersonnelType": "Contractor",
      "Manager_Name": "Luis Rodriguez ",
      "Manager_WU_ID": "0",
      "Profile_Disabled": false,
      "Total_Cards": 2,
      "Active_Cards": 2,
      "Employee_Status": "Active",
      "imageUrl": "/api/employees/2097209941/image"
    },
    {
      "id": 2097202322,
      "EmpName": "Acosta, Victor",
      "EmployeeID": "W0013788",
      "PersonnelType": "Contractor",
      "Manager_Name": "Annie Vassallo",
      "Manager_WU_ID": "315081",
      "Profile_Disabled": false,
      "Total_Cards": 2,
      "Active_Cards": 1,
      "Employee_Status": "Active",
      "imageUrl": "/api/employees/2097202322/image"
    },
    {
      "id": 2097207926,
      "EmpName": "Acuna, Javier",
      "EmployeeID": "W0023773",
      "PersonnelType": "Contractor",
      "Manager_Name": "Gregory Madriz",
      "Manager_WU_ID": "320079",
      "Profile_Disabled": false,
      "Total_Cards": 2,
      "Active_Cards": 1,
      "Employee_Status": "Active",
      "imageUrl": "/api/employees/2097207926/image"
    },



Will share you Active Employee sheet row details 

Employee ID	 Last Name 	First Name 	Preferred First Name 	Middle  Name	Full Name	Worker has Photo	Current Status	Employee Type	Hire Date	Original Hire Date	Continuous Service Date	End Employment Date	Job Code	Position ID	Business Title	Job Family Group Code	Job Family Group	Job Family Code	Job Family	FLA Y/N	Department Code	Department Name	Oracle Account	WUIB Cost Center	Company Code	Company Name	Work Country	Location Code	Location Description	Location City	Location State / Province	Region Code	Talent Center Region Code	Business Region	People Manager (Y/N)	Management Level	Manager ID	Manager Name	Manager Country	Reporting Level 6 Name	Reporting Level 5 Name	Reporting Level 4 Name	Reporting Level 3 Name	EVP / Business Leader	Employee's Email	Manager's Email	Work Phone	Standard Hours	Scheduled Hours	FTE	Time Type	Reporting Level from CEO (N-level)	WUPSIL Cost Center	Time in Position	Tenure	Tenure Category	Supervisory Organization	ET Supervisory Org	Years of Service	Length of Service in Months	Time in Position (Days)	Time in Position (Months)

Employee ID	Last Name	First Name	Preferred First Name	Middle Name	Full Name	Worker has Photo	Current Status	Employee Type	Hire Date	Original Hire Date	Continuous Service Date	End Employment Date	Job Code	Position ID	Business Title	Job Family Group Code	Job Family Group	Job Family Code	Job Family	FLA Y/N	Department Code	Department Name	Oracle Account	WUIB Cost Center	Company Code	Company Name	Work Country	Location Code	Location Description	Location City	Location State / Province	Region Code	Talent Center Region Code	Business Region	People Manager (Y/N)	Management Level	Manager ID	Manager Name	Manager Country	Reporting Level 6 Name	Reporting Level 5 Name	Reporting Level 4 Name	Reporting Level 3 Name	EVP / Business Leader	Employee's Email	Manager's Email	Work Phone	Standard Hours	Scheduled Hours	FTE	Time Type	Reporting Level from CEO (N-level)	WUPSIL Cost Center	Time in Position	Tenure	Tenure Category	Supervisory Organization	ET Supervisory Org	Years of Service	Length of Service in Months	Time in Position (Days)	Time in Position (Months)
072072	Galligan	Michelle	Michelle	L	Galligan, Michelle L	Yes		Regular	30-12-1996	30-12-1996	30-12-1996		1262	P100027	Vice President, Executive Finance	CFO	Finance	FMA	Finance Management	No	00273190	CFO Exec/Admin	2020-9000-4098-9999		312	Western Union, LLC	United States of America	7001	Denver - WU HQ	Denver	Colorado	NAMER	Denver	NAMER	N	Vice President	328913	Oberoi, Aditya	United States of America				Oberoi, Aditya	Cagwin, Matthew L	Michelle.Galligan@westernunion.com	Aditya.Oberoi@westernunion.com	+1 (720) 3325428	40	40	1.000	Full time	3	No		28 year(s), 7 month(s), 20 day(s)	9. 10 years or more	Financial Planning and Analysis (Aditya Oberoi (328913))	Global Finance (Matt Cagwin (322360))	28	343	0	0
072315	Mackintosh	William	William	A	Mackintosh, William A	Yes		Regular	19-05-1992	19-05-1992	19-05-1992		2708	P1077345	Manager, AML Compliance	CPCE	Compliance	AMG	AML/CPCE Compliance Global Operations	No	00211395	Corporate Security - Governmnt	2020-9000-9425-9999		312	Western Union, LLC	United States of America	7001	Denver - WU HQ	Denver	Colorado	NAMER	Denver	NAMER	Y	Supervisory / Professional	324635	Lord Eisert, Stephenie	United States of America		Lord Eisert, Stephenie	Apodaca, Scott M	Axelrod, Cherie Z	Axelrod, Cherie Z	William.Mackintosh@westernunion.com	Stephenie.Eisert@westernunion.com	"+1 (303) 7253673

+1 (303) 7253673"	40	40	1.000	Full time	5	No		33 year(s), 3 month(s), 0 day(s)	9. 10 years or more	LEEP Americas (Stephenie Eisert (324635))	Enterprise Risk and Compliance (Cherie Axelrod (248793))	33	399	0	0
072526	Luft	Mathias	Mathias		Luft, Mathias	Yes		Regular	28-10-1996	28-10-1996	28-10-1996		1336	P100065	Regional Vice President	OPS	Operations	GOM	General Operations: Management	No	33235880	NW Channel Post 2	3348-7001-9608-1100	FR520	I60	WUPSIL Office France	France	I4426	Paris - Tour W, 102 Terrasses Boieldieu	Puteaux	Paris	EMEA	EMEA	UKEA	Y	Vice President	244691	Alvisini, Massimiliano	Ireland				Alvisini, Massimiliano	Angelini, Giovanni	Mathias.Luft@westernunion.com	Massimiliano.Alvisini@westernunion.com	"+33 (6) 72495411

+33 (6) 72495411"	35	35	1.000	Full time	3	Yes		28 year(s), 9 month(s), 22 day(s)	9. 10 years or more	Europe/CIS (Massimiliano Alvisini (244691))	EMEA (Giovanni Angelini (244783))	28	345	0	0
072690	Ragnone-Biesiada	Charlene	Charlene		Ragnone-Biesiada, Charlene	Yes		Regular	05-11-2012	10-06-1996	10-06-1996		2725	P1010307	Analyst, AML Compliance	CPCE	Compliance	AMG	AML/CPCE Compliance Global Operations	No	00273098	N.A Geo Matching / SAR	2020-9000-7201-9999		312	Western Union, LLC	United States of America	7001	Denver - WU HQ	Denver	Colorado	NAMER	Denver	NAMER	N	All Other Positions	301585	Nieto, Felicia F	United States of America	Nieto, Felicia F	Marx, Mary Kristine	Apodaca, Scott M	Axelrod, Cherie Z	Axelrod, Cherie Z	Charlene.Ragnone-Biesiada@westernunion.com	Felicia.Nieto@westernunion.com	+1 (720) 3325759	40	40	1.000	Full time	6	No		29 year(s), 2 month(s), 9 day(s)	9. 10 years or more	FIU-OCMI TM (Felicia Nieto (301585))	Enterprise Risk and Compliance (Cherie Axelrod (248793))	29	350	0	0
073134	Lupo	Wendy	Wendy	S	Lupo, Wendy S	Yes		Regular	27-10-2014	28-08-1995	10-12-1997		2725	P1011199	Analyst, AML Compliance	CPCE	Compliance	AMG	AML/CPCE Compliance Global Operations	No	00273098	N.A Geo Matching / SAR	2020-9000-7201-9999		312	Western Union, LLC	United States of America	7001	Denver - WU HQ	Denver	Colorado	NAMER	Denver	NAMER	N	All Other Positions	074480	Schloeman, William T	United States of America	Schloeman, William T	Valaikiene, Rusne	Apodaca, Scott M	Axelrod, Cherie Z	Axelrod, Cherie Z	Wendy.Lupo@westernunion.com	William.Schloeman@westernunion.com		40	40	1.000	Full time	6	No		27 year(s), 8 month(s), 9 day(s)	9. 10 years or more	OCMI TM (William Schloeman (074480))	Enterprise Risk and Compliance (Cherie Axelrod (248793))	27	332	0	0
073376	Sherman	Lisa	Lisa	R	Sherman, Lisa R	Yes		Regular	05-12-1988	05-12-1988	05-12-1988		7081	P100139	Senior Manager, Vendor Relations	OPS	Operations	VEN	Vendor Relations	No	00288950	Operations Staffing	2020-9000-1770-9999		312	Western Union, LLC	United States of America	7001	Denver - WU HQ	Denver	Colorado	NAMER	Denver	NAMER	Y	Middle Mgmt / Sr. Professional	238686	Rivera Azofeifa, Luis Francisco	Costa Rica			Rivera Azofeifa, Luis Francisco	Formento, Dennis W	Hawksworth, Benjamin Scott	Lisa.Sherman@westernunion.com	luis.rivera@westernunion.com	"+1 (720) 2422489

+1 (720) 3323010"	40	40	1.000	Full time	4	No		36 year(s), 8 month(s), 14 day(s)	9. 10 years or more	AGENT OPERATIONS - Agent Experience & Global Supply Chain (Luis Francisco Rivera Azofeifa (238686))	Technology (Ben Hawksworth (325209))	36	440	0	0
073479	Morales	John	John M	M	Morales, John M	Yes		Regular	01-04-2024	28-05-1991	01-04-2024		2772	P1087358	Senior Manager, AML Compliance	CPCE	Compliance	AMG	AML/CPCE Compliance Global Operations	No	00295390	Compliance Systems & Analytics	2020-9000-9091-9999		312	Western Union, LLC	United States of America	7001	Denver - WU HQ	Denver	Colorado	NAMER	Denver	NAMER	Y	Middle Mgmt / Sr. Professional	328802	Simpson, Adam	United States of America			Simpson, Adam	Coffelt, Ty	Axelrod, Cherie Z	JohnM.Morales@westernunion.com	Adam.Simpson@westernunion.com	+1 (720) 3322706	40	40	1.000	Full time	4	No		1 year(s), 4 month(s), 18 day(s)	5. 1-2 years	CS&A Product - Investigation Platforms (Adam Simpson (328802))	Enterprise Risk and Compliance (Cherie Axelrod (248793))	1	16	0	0
073521	Borinshteyn	Dmitriy	Dmitriy		Borinshteyn, Dmitriy	Yes		Regular	15-06-1998	15-06-1998	15-06-1998		330043	P100149	Principal Software Engineering Architect	MIS	IT, Management Information Systems	SFE	Software Engineering	No	00250088	Retail Front End	2020-9000-1416-9999		312	Western Union, LLC	United States of America	99NJ	Work From Home - USA  NJ	Montvale	New Jersey	NAMER	NAMER	NAMER	Y	Upper Mid Mgmt / Director	326451	Danielewski, Dariusz	United States of America			Danielewski, Dariusz	Shah, Swati	Hawksworth, Benjamin Scott	Dmitriy.Borinshteyn@westernunion.com	Dariusz.Danielewski@westernunion.com	+1 (201) 9826401	40	40	1.000	Full time	4	No		27 year(s), 2 month(s), 4 day(s)	9. 10 years or more	Retail Engineering (Dariusz Danielewski (326451))	Technology (Ben Hawksworth (325209))	27	326	0	0
073674	Park	Leslie	Leslie	E	Park, Leslie E	Yes		Regular	10-11-2014	27-07-1998	24-03-2002		2725	P102326	Analyst, AML Compliance	CPCE	Compliance	AMG	AML/CPCE Compliance Global Operations	No	00273098	N.A Geo Matching / SAR	2020-9000-7201-9999		312	Western Union, LLC	United States of America	7001	Denver - WU HQ	Denver	Colorado	NAMER	Denver	NAMER	N	All Other Positions	074480	Schloeman, William T	United States of America	Schloeman, William T	Valaikiene, Rusne	Apodaca, Scott M	Axelrod, Cherie Z	Axelrod, Cherie Z	Leslie.Park@westernunion.com	William.Schloeman@westernunion.com		40	40	1.000	Full time	6	No		23 year(s), 4 month(s), 26 day(s)	9. 10 years or more	OCMI TM (William Schloeman (074480))	Enterprise Risk and Compliance (Cherie Axelrod (248793))	23	280	0	0



also i have attached excel file so read excel file carefully and compare data 

Compare data and Find how many enty are not present in Active sheet  but present in ccure data .
same for contractor 


so refer below Contractor sheet .


Worker System Id	iPass ID	"W" iPass ID	Full Name	Vendor Company Name	Resource Manager Full Name	Worker Location	Status	Work Order End Date	Work Order Start Date
C0000012	88525764	W0000012	VIMAL  JAIN	HCL America	Chris Lunstra	External Office Location, Denver, CO 80237 (Contractor Location)	Active	08-31-2025	01-02-2023
C0000022	84912332	W0000022	Vikram  Thakur	Quant Systems	Manish Paripagar	External Office Location,  Bengaluru, IN, 560100 (Contractor Location)	Active	08-31-2025	01-01-2024
C0000025	85818153	W0000025	Praneeth  Chigurupati	Quant Systems	Manish Paripagar	Cognizant Technology Solutions	Active	08-31-2025	01-01-2024


For Contractor Compare Employee ID with Worker System Id  	iPass ID	 "W" iPass ID  alos name ...



also chek below file from backend also check frontend 
in python create new file in backend and update it carefully...






// frontend/src/pages/GlobalPage.jsx
import React, { useState, useEffect, useRef } from 'react';
import {
  Box, Typography, CircularProgress, IconButton, Button, Paper, Divider,
  LinearProgress, Snackbar, Alert, List, ListItem, ListItemText
} from '@mui/material';
import HomeIcon from '@mui/icons-material/Home';
import DescriptionIcon from '@mui/icons-material/Description';
import UploadFileIcon from '@mui/icons-material/UploadFile';
import MapChart from '../components/MapChart.jsx';
import api from '../api';
import { useNavigate, Link } from 'react-router-dom';

/*
  Important:
  - Do NOT mix /api/headcount and /api/ccure/verify.
  - Region cards (APAC/EMEA/LACA/NAMER) come only from /api/headcount.
  - Live vs CCURE Summary now comes from /api/ccure/verify?raw=true.
  - Initial region totals are zero (keeps previous UI behaviour).
  - We implement polling for headcount and SSE for ccure/stream (realtime via SSE).
*/

export default function GlobalPage() {
  const navigate = useNavigate();

  // Region totals (headcount) - default to zeros so UI shows 0 immediately (preserve previous behaviour)
  const [counts, setCounts] = useState({ apac: 0, emea: 0, laca: 0, namer: 0 });
  const [selected, setSelected] = useState('global');

  // Averages/ccure state (left panel)
  const [averages, setAverages] = useState(null);
  const [loadingAverages, setLoadingAverages] = useState(true);
  const [averagesError, setAveragesError] = useState(null);

  // upload state
  const [uploading, setUploading] = useState(false);
  const [uploadResult, setUploadResult] = useState(null);
  const [uploadError, setUploadError] = useState(null);

  // top-row file inputs
  const fileInputEmpRef = useRef();
  const fileInputContrRef = useRef();
  const [snack, setSnack] = useState({ open: false, severity: 'info', message: '' });

  // date-range state for top-right controls
  const [startDate, setStartDate] = useState('');
  const [endDate, setEndDate] = useState('');

  // Polling refs for safe scheduling and backoff
  const headcountRef = useRef({ timerId: null, failureCount: 0, isFetching: false });
  const averagesRef = useRef({ timerId: null, failureCount: 0, isFetching: false });

  // -----------------------
  // HEADCOUNT POLLING ONLY (unchanged)
  // -----------------------
  useEffect(() => {
    let mounted = true;

    const fetchHeadcount = async () => {
      if (!mounted) return;
      if (headcountRef.current.isFetching) return;
      headcountRef.current.isFetching = true;

      try {
        // Important: only call /headcount here (proxy rewrites /api -> backend)
        const res = await api.get('/headcount');
        if (!mounted) return;
        const d = res.data;
        // We expect an object with keys apac/emea/laca/namer (defensive)
        if (d && typeof d === 'object') {
          const newCounts = {
            apac: Number(d.apac || 0),
            emea: Number(d.emea || 0),
            laca: Number(d.laca || 0),
            namer: Number(d.namer || 0),
          };
          setCounts(prev => {
            if (
              prev.apac === newCounts.apac &&
              prev.emea === newCounts.emea &&
              prev.laca === newCounts.laca &&
              prev.namer === newCounts.namer
            ) {
              return prev;
            }
            return newCounts;
          });
        } else {
          console.warn('[headcount] unexpected response shape - ignoring', d);
        }
        headcountRef.current.failureCount = 0;
      } catch (err) {
        headcountRef.current.failureCount = (headcountRef.current.failureCount || 0) + 1;
        console.warn('[headcount] fetch failed:', err?.message || err);
      } finally {
        headcountRef.current.isFetching = false;
        const f = headcountRef.current.failureCount || 0;
        const backoffMs = 15000 * Math.pow(2, Math.min(Math.max(f - 1, 0), 4)); // 15s..240s
        headcountRef.current.timerId = setTimeout(fetchHeadcount, backoffMs);
      }
    };

    fetchHeadcount();

    return () => {
      mounted = false;
      if (headcountRef.current.timerId) clearTimeout(headcountRef.current.timerId);
      headcountRef.current.isFetching = false;
    };
  }, []); // run once



  // AVERAGES: use SSE (direct to Python backend) with fallback initial fetch
  useEffect(() => {
    let stopped = false;
    let es = null;
    let backoff = 1000;

    // Allow override via VITE_PY_BACKEND; otherwise assume python at :8000
    const PY_BACKEND = (import.meta.env.VITE_PY_BACKEND || `${window.location.protocol}//${window.location.hostname}:8000`).replace(/\/$/, '');

    const connect = () => {
      if (stopped) return;
      try {
        // Directly connect to Python SSE endpoint (bypasses Vite proxy for streaming)
        es = new EventSource(`${PY_BACKEND}/ccure/stream`);
      } catch (err) {
        console.warn('SSE creation failed', err);
        es = null;
      }

      if (!es) {
        // fallback to polling if EventSource not supported or creation failed
        initialFetch();
        return;
      }

      es.onopen = () => {
        console.info('[SSE] connected to', `${PY_BACKEND}/ccure/stream`);
        backoff = 1000;
        setAveragesError(null);
      };

      es.onmessage = (evt) => {
        try {
          const payload = JSON.parse(evt.data);
          setAverages(payload);
          setLoadingAverages(false);
          setAveragesError(null);
        } catch (e) {
          console.warn('Failed to parse SSE message', e);
        }
      };

      es.onerror = (err) => {
        console.warn('[SSE] error/closed, attempting reconnect', err);
        try { es.close(); } catch (e) {}
        es = null;
        if (stopped) return;
        // exponential backoff reconnect (capped)
        setTimeout(() => {
          backoff = Math.min(backoff * 2, 30000);
          connect();
        }, backoff);
      };
    };

    const initialFetch = async () => {
      setLoadingAverages(true);
      setAveragesError(null);
      try {
        // <-- changed: call verify endpoint (raw=true) instead of /ccure/averages
        const res = await api.get('/ccure/verify?raw=true');
        setAverages(res.data);
        setLoadingAverages(false);
        setAveragesError(null);
      } catch (err) {
        console.warn('initial /ccure/verify?raw=true fetch failed', err);
        setLoadingAverages(false);
        setAveragesError(err);
      }
    };

    // Start with initial fetch so UI is populated quickly, then open SSE
    initialFetch();
    connect();

    return () => {
      stopped = true;
      if (es) {
        try { es.close(); } catch (e) {}
        es = null;
      }
    };
  }, []);



  // -----------------------
  // Upload helper (unchanged)
  // -----------------------
  const handleUpload = async (file, type) => {
    if (!file) return;
    const endpoint = type === 'employee' ? '/upload/active-employees' : '/upload/active-contractors';
    const fd = new FormData();
    fd.append('file', file, file.name);

    setUploading(true);
    setUploadResult(null);
    setUploadError(null);

    try {
      const res = await api.post(endpoint, fd, {
        headers: { 'Content-Type': 'multipart/form-data' },
        timeout: 120000
      });
      setUploadResult(res.data);
      setSnack({ open: true, severity: 'success', message: `Upload successful: ${file.name}` });
      // Optionally re-fetch averages/headcount after successful upload:
      try { await api.get('/ccure/verify?raw=true').then(r => setAverages(r.data)); } catch (_) {}
      try { await api.get('/headcount').then(r => {
        const d = r.data;
        if (d && typeof d === 'object') {
          setCounts(prev => ({
            apac: Number(d.apac || prev.apac || 0),
            emea: Number(d.emea || prev.emea || 0),
            laca: Number(d.laca || prev.laca || 0),
            namer: Number(d.namer || prev.namer || 0)
          }));
        }
      }) } catch (_) {}
    } catch (err) {
      console.error('Upload failed', err);
      setUploadError(err);
      setSnack({ open: true, severity: 'error', message: `Upload failed: ${file.name}` });
    } finally {
      setUploading(false);
    }
  };

  const onChooseEmployeeFile = (e) => { const f = e.target.files && e.target.files[0]; if (f) handleUpload(f, 'employee'); e.target.value = null; };
  const onChooseContractorFile = (e) => { const f = e.target.files && e.target.files[0]; if (f) handleUpload(f, 'contractor'); e.target.value = null; };

  // apply date range to re-fetch /ccure/verify
  const applyDateRange = async () => {
    if (!startDate || !endDate) {
      setSnack({ open: true, severity: 'warning', message: 'Please select start and end dates' });
      return;
    }
    setLoadingAverages(true);
    setAveragesError(null);
    try {
      const res = await api.get(`/ccure/verify?raw=true&start_date=${startDate}&end_date=${endDate}`);
      setAverages(res.data);
      setLoadingAverages(false);
      setSnack({ open: true, severity: 'success', message: 'Averages updated' });
    } catch (err) {
      console.warn('applyDateRange failed', err);
      setLoadingAverages(false);
      setAveragesError(err);
      setSnack({ open: true, severity: 'error', message: 'Failed to update averages' });
    }
  };

  // safe helper for nested averages paths
  const safe = (path, fallback = null) => {
    if (!averages) return fallback;
    try {
      return path.split('.').reduce((a, k) => (a && a[k] !== undefined ? a[k] : fallback), averages);
    } catch {
      return fallback;
    }
  };

  // Derived values (updated to match /ccure/verify response shape)
 
  const ccureActiveEmployees = safe('ccure_reported.employees',
    safe('ccure_active.active_employees',
      safe('ccure_active.ccure_active_employees_reported', null)
    )
  );
  const ccureActiveContractors = safe('ccure_reported.contractors',
    safe('ccure_active.active_contractors',
      safe('ccure_active.ccure_active_contractors_reported', null)
    )
  );

  // headcount attendance summary (from AttendanceSummary / union)
  const headTotalVisited = safe('headcount_attendance_summary.total_visited_today',
    safe('headcount_details.total_visited_today', null)
  );
  const headEmployee = safe('headcount_attendance_summary.employee',
    safe('headcount_details.employee', null)
  );
  const headContractor = safe('headcount_attendance_summary.contractor',
    safe('headcount_details.contractor', null)
  );

  // live headcount from region clients
  const liveCurrentTotal = safe('live_headcount_region_clients.currently_present_total',
    safe('live_headcount_details.currently_present_total',
      null
    )
  );
  const liveEmp = safe('live_headcount_region_clients.employee',
    safe('live_headcount_details.employee', null)
  );
  const liveContr = safe('live_headcount_region_clients.contractor',
    safe('live_headcount_details.contractor', null)
  );

  // percentages (prefer top-level percentages_vs_ccure, fallback into averages compatibility keys)
  const empPct = safe('percentages_vs_ccure.head_employee_pct_vs_ccure_today',
    safe('averages.head_emp_pct_vs_ccure_today', null)
  );
  const conPct = safe('percentages_vs_ccure.head_contractor_pct_vs_ccure_today',
    safe('averages.head_contractor_pct_vs_ccure_today', null)
  );
  const overallPct = safe('percentages_vs_ccure.head_overall_pct_vs_ccure_today',
    safe('averages.headcount_overall_pct_vs_ccure_today', null)
  );

  // averages / 7-day
  const avg7 = safe('averages.history_avg_overall_last_7_days',
    safe('averages.avg_headcount_last_7_days',
      safe('averages.avg_headcount_last_7_days_db', null)
    )
  );

  // date
  const respDate = safe('date', null);

  // location-wise averages map (history_avg_by_location_last_7_days)
  const locationAvgsObj = safe('averages.history_avg_by_location_last_7_days',
    safe('history_avg_by_location_last_7_days',
      safe('raw.averages.history_avg_by_location_last_7_days', {})
    )
  );

  // convert to array for rendering (sort by avg_overall desc if present)
  const locationAvgsList = React.useMemo(() => {
    if (!locationAvgsObj || typeof locationAvgsObj !== 'object') return [];
    const arr = Object.entries(locationAvgsObj).map(([loc, vals]) => {
      return {
        location: loc,
        avg_employee_last_7_days: vals.avg_employee_last_7_days ?? vals.history_avg_employee_last_7_days ?? vals.avg_employee ?? null,
        avg_contractor_last_7_days: vals.avg_contractor_last_7_days ?? vals.history_avg_contractor_last_7_days ?? vals.avg_contractor ?? null,
        avg_overall_last_7_days: vals.avg_overall_last_7_days ?? vals.history_avg_overall_last_7_days ?? vals.avg_overall ?? null,
        history_days_counted: vals.history_days_counted ?? null
      };
    });
    // sort descending by overall avg (nulls to end)
    arr.sort((a, b) => (b.avg_overall_last_7_days ?? -Infinity) - (a.avg_overall_last_7_days ?? -Infinity));
    return arr;
  }, [locationAvgsObj]);

  // ------------------------------------------------------------
  // derive global total from headcount regions (APAC+EMEA+LACA+NAMER)
  // ------------------------------------------------------------
  const globalCount = Number((counts.apac || 0)) + Number((counts.emea || 0)) + Number((counts.laca || 0)) + Number((counts.namer || 0));

  // Helper style used to hide scrollbars but allow scrolling
  const hideScrollbarSx = {
    overflowY: 'auto',
    // hide scrollbar visual (webkit)
    '&::-webkit-scrollbar': { width: 0, height: 0 },
    // firefox
    scrollbarWidth: 'none',
    // ie
    msOverflowStyle: 'none',
  };

  // Render
  return (
    <Box sx={{ display: 'flex', flexDirection: 'column', height: '100vh', overflow: 'hidden', bgcolor: 'background.default' }}>
      {/* Header */}
      <Box px={2} py={1} sx={{ backgroundColor: 'black', color: '#fff', borderBottom: '4px solid #FFD700', display: 'flex', alignItems: 'center', justifyContent: 'space-between' }}>
        <Box>
          <IconButton component={Link} to="/" sx={{ color: '#FFC72C' }}><HomeIcon fontSize="medium" /></IconButton>
          <IconButton component={Link} to="/reports" sx={{ color: '#FFC72C', ml: 1 }}><DescriptionIcon fontSize="medium" /></IconButton>
        </Box>

        <Box sx={{ flexGrow: 1, display: 'flex', alignItems: 'center', justifyContent: 'center' }}>
          <Box component="img" src="/wu-head-logo.png" alt="WU Logo" sx={{ height: { xs: 30, md: 55 }, mr: 2 }} />
          <Typography variant="h5" sx={{ fontWeight: 'bold', color: 'primary.main' }}>Global Headcount Dashboard</Typography>
        </Box>

        <Box sx={{ width: 120 }} />
      </Box>

      {/* Top row: Uploads | GLOBAL + Region Cards | Date selectors */}
      <Box sx={{ display: 'flex', alignItems: 'center', gap: 5, p: 1, flexWrap: 'wrap' }}>
        {/* Left: two small Upload boxes stacked (Employee above Contractor) */}
        <Box sx={{ display: 'flex', flexDirection: 'column', gap: 1 }}>
          <Paper sx={{ p: 1, width: 200, boxShadow: 1 }}>
            {/* <Typography variant="caption" color="text.secondary">Upload Active Employee Sheet</Typography> */}
            <input type="file" accept=".xls,.xlsx" style={{ display: 'none' }} ref={fileInputEmpRef} onChange={onChooseEmployeeFile} />
            <Button variant="contained" size="small" startIcon={<UploadFileIcon />} sx={{ mt: 1, width: '100%' }} onClick={() => fileInputEmpRef.current && fileInputEmpRef.current.click()} disabled={uploading}>
              Upload Employee
            </Button>
          </Paper>

          <Paper sx={{ p: 0, width: 200, boxShadow: 1 }}>
            {/* <Typography variant="caption" color="text.secondary">Upload Active Contractor Sheet</Typography> */}
            <input type="file" accept=".xls,.xlsx" style={{ display: 'none' }} ref={fileInputContrRef} onChange={onChooseContractorFile} />
            <Button variant="outlined" size="small" startIcon={<UploadFileIcon />} sx={{ mt: 1, width: '100%' }} onClick={() => fileInputContrRef.current && fileInputContrRef.current.click()} disabled={uploading}>
              Upload Contractor
            </Button>
          </Paper>
        </Box>

        {/* Middle: region cards */}
        <Box sx={{ display: 'flex', gap: 2, alignItems: 'center', flexWrap: 'wrap', justifyContent: 'center' }}>
          {[
            { key: 'global', label: 'GLOBAL', count: globalCount, url: null, textColor: '#ffff' },
            { key: 'apac', label: 'APAC', count: counts.apac, url: 'http://10.199.22.57:3000/', textColor: '#ffff' },
            { key: 'emea', label: 'EMEA', count: counts.emea, url: 'http://10.199.22.57:3001/', textColor: '#ffff' },
            { key: 'laca', label: 'LACA', count: counts.laca, url: 'http://10.199.22.57:3003/', textColor: '#ffff' },
            { key: 'namer', label: 'NAMER', count: counts.namer, url:'http://10.199.22.57:3002/', textColor: '#ffff' },
          ].map(region => (
            <Box
              key={region.key}
              onClick={() => {
                if (region.key === 'global') {
                  const el = document.querySelector('[data-global-left-panel]');
                  if (el) el.scrollIntoView({ behavior: 'smooth', block: 'start' });
                  setSelected('global');
                  return;
                }
                // external region dashboards for others
                if (region.url) window.location.href = region.url;
              }}
              sx={{
                cursor: 'pointer',
                width: 200,
                height: 80,
                display: 'flex',
                flexDirection: 'column',
                justifyContent: 'center',
                alignItems: 'center',
                border: '4px solid rgba(255, 204, 0, 0.89)',
                borderRadius: 2,
                boxShadow: 3,
                color: region.textColor,
                '&:hover': { opacity: 0.95 },
              }}
            >
              <Typography variant="subtitle1" sx={{ fontWeight: 'bold', color: region.textColor, fontSize: { xs: '1.1rem' } }}>{region.label}</Typography>
              <Typography variant="h4" sx={{ fontWeight: 800, fontSize: { xs: '1.2rem', sm: '1.6rem' }, color: region.textColor }}>
                {region.count ?? 0}
              </Typography>
            </Box>
          ))}
        </Box>

        {/* Right: date selectors (Start & End side-by-side; Apply below right-aligned) */}
        <Box sx={{ marginLeft: 'auto', display: 'flex', flexDirection: 'column', alignItems: 'flex-end', gap: 1 }}>
          <Paper sx={{ p: 1, display: 'flex', gap: 1, alignItems: 'center', boxShadow: 1 }}>
            <Box sx={{ display: 'flex', flexDirection: 'column', width: 160 }}>
              <Typography variant="caption" color="text.secondary">Select Start date</Typography>
              <input
                type="date"
                value={startDate}
                onChange={(e) => setStartDate(e.target.value)}
                style={{ width: '100%', height: 32, borderRadius: 4, border: '1px solid rgba(255,255,255,0.06)', padding: 4, background: 'transparent', color: 'inherit' }}
              />
            </Box>

            <Box sx={{ display: 'flex', flexDirection: 'column', width: 160 }}>
              <Typography variant="caption" color="text.secondary">Select End date</Typography>
              <input
                type="date"
                value={endDate}
                onChange={(e) => setEndDate(e.target.value)}
                style={{ width: '100%', height: 32, borderRadius: 4, border: '1px solid rgba(255,255,255,0.06)', padding: 4, background: 'transparent', color: 'inherit' }}
              />
            </Box>
          </Paper>

          <Box sx={{ width: '100%', display: 'flex', justifyContent: 'space-between', mt: 0, alignItems: 'center' }}>
            <Button size="small" variant="contained" onClick={applyDateRange}>Apply</Button>
          </Box>
        </Box>
      </Box>

      {/* Main: left summary | center map | right averages */}
      <Box sx={{ display: 'flex', flex: 1, overflow: 'hidden' }}>
        {/* Left detail panel */}
        <Box
          data-global-left-panel
          sx={{
            width: { xs: 320, md: 360 },
            minWidth: { md: 320 },
            p: 2,
            bgcolor: 'background.paper',
            borderRight: '1px solid rgba(255,255,255,0.06)',
            display: 'flex',
            flexDirection: 'column',
            ...hideScrollbarSx,
            // ensure left panel takes full height and internal content can scroll (hidden scrollbar)
            height: '100%',
          }}
        >
          <Typography variant="h6" sx={{ mb: 1, color: 'primary.main' }}>Live vs CCURE Summary</Typography>

          {loadingAverages ? (
            <Box sx={{ py: 2 }}><LinearProgress /></Box>
          ) : averagesError ? (
            <Alert severity="error">Failed to load CCURE averages</Alert>
          ) : averages ? (
            <>
              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.02)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary">CCURE Active (reported)</Typography>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1, alignItems: 'center' }}>
                  <Box>
                    <Typography variant="h4" sx={{ fontWeight: 800 }}>{ccureActiveEmployees ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Active Employees</Typography>
                  </Box>
                  <Box sx={{ textAlign: 'right' }}>
                    <Typography variant="h5" sx={{ fontWeight: 800 }}>{ccureActiveContractors ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Active Contractors</Typography>
                  </Box>
                </Box>
              </Paper>

              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.02)' }} elevation={0}>
                <Box sx={{ display: 'flex', justifyContent: 'space-between' }}>
                  <Typography variant="subtitle2" color="text.secondary">Live Today</Typography>
                  <Typography variant="caption" color="text.secondary">{respDate ?? ''}</Typography>
                </Box>

                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Box>
                    <Typography variant="h5" sx={{ fontWeight: 800 }}>{headEmployee ?? liveEmp ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Employee</Typography>
                  </Box>
                  <Box>
                    <Typography variant="h5" sx={{ fontWeight: 800 }}>{headContractor ?? liveContr ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Contractor</Typography>
                  </Box>
                </Box>

                <Divider sx={{ my: 1 }} />

                <Box>
                  <Typography variant="caption" color="text.secondary">Totals</Typography>
                  <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.75 }}>
                    <Typography variant="body2">Attendance total (today)</Typography>
                    <Typography variant="body2" sx={{ fontWeight: 700 }}>{headTotalVisited ?? '—'}</Typography>
                  </Box>

                  <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                    <Typography variant="body2">Live region total</Typography>
                    <Typography variant="body2" sx={{ fontWeight: 700 }}>{liveCurrentTotal ?? '—'}</Typography>
                  </Box>

                  {/** Provide a detail rows total if available in response */}
                  {safe('headcount_details.total_visited_today', null) != null && (
                    <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                      <Typography variant="body2">Detail rows total</Typography>
                      <Typography variant="body2" sx={{ fontWeight: 700 }}>{safe('headcount_details.total_visited_today', '—')}</Typography>
                    </Box>
                  )}
                </Box>
              </Paper>

              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.02)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary">Percentages vs CCURE</Typography>

                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Typography variant="body2">Employees</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{empPct != null ? `${empPct}%` : '—'}</Typography>
                </Box>
                <Box sx={{ display: 'flex', justifyContent: 'space-between' }}>
                  <Typography variant="body2">Contractors</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{conPct != null ? `${conPct}%` : '—'}</Typography>
                </Box>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                  <Typography variant="body2">Overall</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{overallPct != null ? `${overallPct}%` : '—'}</Typography>
                </Box>

                <Divider sx={{ my: 1 }} />
                <Typography variant="caption" color="text.secondary">Averages</Typography>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Typography variant="body2">7-day avg headcount</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{avg7 ?? '—'}</Typography>
                </Box>
              </Paper>

              {/* Upload controls intentionally removed from this left panel (moved to top-row) */}

              {averages.notes && (
                <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.01)' }}>
                  <Typography variant="body2" sx={{ mt: 1 }}>{averages.notes}</Typography>
                </Paper>
              )}
            </>
          ) : (
            <Typography variant="body2" color="text.secondary">No data</Typography>
          )}
        </Box>

        {/* Center: map (flex) */}
        <Box sx={{ flex: 1, minWidth: 0, position: 'relative', display: 'flex', flexDirection: 'column' }}>
          <Box sx={{ flex: 1, minHeight: 0 }}>
            <MapChart selected={selected} onClickSite={r => setSelected(r)} initialZoom={1.8} />
          </Box>
        </Box>

        {/* Right side: Location averages panel */}
        <Box
          sx={{
            width: { xs: 320, md: 360 },
            minWidth: { md: 320 },
            borderLeft: '1px solid rgba(255,255,255,0.06)',
            bgcolor: 'background.paper',
            p: 2,
            display: 'flex',
            flexDirection: 'column',
            ...hideScrollbarSx,
            height: '100%',
          }}
        >
          <Typography variant="h6" sx={{ mb: 1, color: 'primary.main' }}>Location Averages</Typography>

          {loadingAverages ? (
            <Box sx={{ display: 'flex', alignItems: 'center', justifyContent: 'center', py: 4 }}>
              <CircularProgress />
            </Box>
          ) : averagesError ? (
            <Alert severity="error">Failed to load location averages</Alert>
          ) : locationAvgsList.length === 0 ? (
            <Typography variant="body2" color="text.secondary">No location averages available</Typography>
          ) : (
            <List dense disablePadding sx={{ flex: 1 }}>
              {locationAvgsList.map(item => (
                <ListItem key={item.location} sx={{ alignItems: 'flex-start', py: 1.25, borderBottom: '1px solid rgba(255,255,255,0.03)' }}>
                  <ListItemText
                    primary={
                      <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
                        <Typography sx={{ fontWeight: 800 }}>{item.location}</Typography>
                        <Typography variant="body2" sx={{ fontWeight: 800 }}>{item.avg_overall_last_7_days != null ? Math.round(item.avg_overall_last_7_days) : '—'}</Typography>
                      </Box>
                    }
                    secondary={
                      <Box sx={{ display: 'flex', gap: 2, mt: 0.5, flexWrap: 'wrap' }}>
                        <Typography variant="caption" color="text.secondary">Emp: <strong>{item.avg_employee_last_7_days != null ? Math.round(item.avg_employee_last_7_days) : '—'}</strong></Typography>
                        <Typography variant="caption" color="text.secondary">Contr: <strong>{item.avg_contractor_last_7_days != null ? Math.round(item.avg_contractor_last_7_days) : '—'}</strong></Typography>
                        {item.history_days_counted != null && <Typography variant="caption" color="text.secondary">Days: {item.history_days_counted}</Typography>}
                      </Box>
                    }
                  />
                </ListItem>
              ))}
            </List>
          )}
        </Box>
      </Box>

      <Snackbar open={snack.open} autoHideDuration={3500} onClose={() => setSnack(prev => ({ ...prev, open: false }))}>
        <Alert severity={snack.severity} onClose={() => setSnack(prev => ({ ...prev, open: false }))}>{snack.message}</Alert>
      </Snackbar>
    </Box>
  );
}






# app.py (keep only /ccure/verify, removed /ccure/averages)
from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Query
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import shutil
import uuid
import json
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
import re
import asyncio
from typing import Optional, Dict, Any

# --- DB / models imports (your existing project modules) ---
from db import SessionLocal
from models import LiveSwipe, AttendanceSummary  # removed ActiveEmployee/ActiveContractor dependence for averages

# --- settings (assumes these exist in your project) ---
try:
    from settings import UPLOAD_DIR, OUTPUT_DIR
except Exception:
    UPLOAD_DIR = "./uploads"
    OUTPUT_DIR = "./output"

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# ----------------- GLOBAL TIMEOUTS (UNIFY) -----------------
REGION_TIMEOUT_SECONDS = 20
COMPUTE_WAIT_TIMEOUT_SECONDS = 30
COMPUTE_SYNC_TIMEOUT_SECONDS = 60
# ----------------------------------------------------------

_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

_broadcaster_clients = set()

def broadcast_ccure_update(payload: dict):
    if not _broadcaster_clients:
        return
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = None
    for q in list(_broadcaster_clients):
        try:
            if loop and loop.is_running():
                loop.call_soon_threadsafe(q.put_nowait, payload)
            else:
                q.put_nowait(payload)
        except Exception:
            logger.exception("Failed to push payload to SSE client (will remove client)")
            try:
                _broadcaster_clients.discard(q)
            except Exception:
                pass

async def _sse_event_generator(client_queue: asyncio.Queue):
    try:
        while True:
            payload = await client_queue.get()
            try:
                data = json.dumps(payload, default=str)
            except Exception:
                data = json.dumps({"error": "serialization error", "payload": str(payload)})
            yield f"data: {data}\n\n"
    finally:
        try:
            _broadcaster_clients.discard(client_queue)
        except Exception:
            pass
        return

@app.get("/ccure/stream")
async def ccure_stream():
    q = asyncio.Queue()
    _broadcaster_clients.add(q)
    generator = _sse_event_generator(q)
    headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    return StreamingResponse(generator, media_type="text/event-stream", headers=headers)

def _guess_region_from_text(txt: str) -> str:
    if not txt:
        return "unknown"
    s = str(txt).strip().lower()
    s = re.sub(r"[,\-/()]", " ", s)
    if any(k in s for k in ("pune","quezon city","taguig city","bengaluru","hyderabad","chennai","manila","singapore","hong kong","beijing","shanghai","jakarta","kuala","osaka","tokyo","seoul","bangkok")):
        return "apac"
    if any(k in s for k in ("london","dublin","paris","frankfurt","amsterdam","stockholm","cape town","johannesburg","berlin","brussels","madrid","rome","milan")):
        return "emea"
    if any(k in s for k in ("mexico","bogota","buenos","santiago","sao","salvador","lima","caracas")):
        return "laca"
    if any(k in s for k in ("denver","new york","ny","chicago","toronto","vancouver","los angeles","san francisco","boston","houston","atlanta","miami")):
        return "namer"
    return "unknown"

@app.get("/headcount")
def api_headcount():
    try:
        totals = {"apac": 0, "emea": 0, "laca": 0, "namer": 0, "unknown": 0}
        with SessionLocal() as db:
            try:
                today = date.today()
                rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                if rows:
                    for r in rows:
                        try:
                            partition = None
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                            loc = partition or "unknown"
                            region = _guess_region_from_text(loc)
                            totals[region] = totals.get(region, 0) + 1
                        except Exception:
                            totals["unknown"] += 1
                else:
                    start = datetime.combine(today, datetime.min.time())
                    end = datetime.combine(today, datetime.max.time())
                    swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
                    for s in swipes:
                        loc = s.partition or "unknown"
                        region = _guess_region_from_text(loc)
                        totals[region] = totals.get(region, 0) + 1
            except Exception:
                logger.exception("Failed to compute headcount regions")
        out = {
            "apac": int(totals.get("apac", 0)),
            "emea": int(totals.get("emea", 0)),
            "laca": int(totals.get("laca", 0)),
            "namer": int(totals.get("namer", 0))
        }
        return JSONResponse(out)
    except Exception as exc:
        logger.exception("api_headcount failed")
        raise HTTPException(status_code=500, detail=f"headcount error: {exc}")

# ---------- Helpers retained (normalize / safe conversions) -------------
def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- build_ccure_averages (fallback) ------------------------------
def build_ccure_averages(start_date: Optional[str] = None, end_date: Optional[str] = None):
    """
    Fallback averages computation using AttendanceSummary only.
    Returns a compact shape (live_today, ccure_active, averages, notes).
    """
    try:
        def _parse_date_param(s):
            if not s:
                return None
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                try:
                    return date.fromisoformat(s)
                except Exception:
                    return None

        today = date.today()
        start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
        end_obj = _parse_date_param(end_date) if end_date else today
        if start_obj is None or end_obj is None or start_obj > end_obj:
            start_obj = today - timedelta(days=6)
            end_obj = today

        with SessionLocal() as db:
            try:
                att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
            except Exception:
                logger.exception("Failed to query AttendanceSummary")
                att_rows = []

            live_emp = 0
            live_contr = 0
            unknown_count = 0
            seen_keys = set()

            def classify_from_derived(derived):
                try:
                    if not derived or not isinstance(derived, dict):
                        return "contractor"
                    for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                        v = derived.get(k)
                        if v and "employee" in str(v).strip().lower():
                            return "employee"
                    for k in ("Employee_Status","Employee Status","Status"):
                        v = derived.get(k)
                        if v and "terminated" in str(v).strip().lower():
                            return "employee"
                    return "contractor"
                except Exception:
                    return "contractor"

            if att_rows:
                for a in att_rows:
                    key = None
                    try:
                        key = _normalize_employee_key(a.employee_id)
                    except Exception:
                        key = None
                    if not key:
                        try:
                            key = _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None)
                        except Exception:
                            key = None
                    if not key:
                        unknown_count += 1
                        continue
                    if key in seen_keys:
                        continue
                    seen_keys.add(key)
                    cls = classify_from_derived(a.derived)
                    if cls == "employee":
                        live_emp += 1
                    elif cls == "contractor":
                        live_contr += 1
                    else:
                        unknown_count += 1

                live_total_reported = live_emp + live_contr + unknown_count
                live_total_details = len(att_rows)
            else:
                live_total_reported = 0
                live_total_details = 0
                try:
                    import region_clients
                    details = []
                    try:
                        details = region_clients.fetch_all_details(timeout=REGION_TIMEOUT_SECONDS) or []
                    except Exception:
                        logger.exception("region_clients.fetch_all_details failed in build_ccure_averages()")
                        details = []
                    if details:
                        for d in details:
                            try:
                                cls = "contractor"
                                for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                                    v = d.get(k)
                                    if v and "employee" in str(v).strip().lower():
                                        cls = "employee"
                                        break
                                if cls == "employee":
                                    live_emp += 1
                                else:
                                    live_contr += 1
                                live_total_details += 1
                            except Exception:
                                continue
                        live_total_reported = live_emp + live_contr
                    else:
                        try:
                            regions = region_clients.fetch_all_regions(timeout=REGION_TIMEOUT_SECONDS) or []
                            for r in regions:
                                try:
                                    c = r.get("count")
                                    if isinstance(c, (int, float)):
                                        live_total_reported += int(c)
                                except Exception:
                                    continue
                        except Exception:
                            logger.exception("region_clients.fetch_all_regions failed in build_ccure_averages()")
                except Exception:
                    logger.exception("region_clients not importable in build_ccure_averages()")

            # compute avg range using AttendanceSummary if possible
            avg_range = None
            try:
                q = db.query(AttendanceSummary.date, AttendanceSummary.employee_id, AttendanceSummary.presence_count)\
                      .filter(AttendanceSummary.date >= start_obj, AttendanceSummary.date <= end_obj).all()
                by_date = {}
                for row in q:
                    d = row[0]
                    key = (row[1] or "").strip() if row[1] else None
                    if not key:
                        continue
                    if d not in by_date:
                        by_date[d] = set()
                    try:
                        presence_val = getattr(row, 'presence_count', row[2])
                        if int(presence_val) > 0:
                            by_date[d].add(key)
                    except Exception:
                        by_date[d].add(key)
                days_count = (end_obj - start_obj).days + 1
                daily_counts = [len(by_date.get(start_obj + timedelta(days=i), set())) for i in range(days_count)]
                if days_count and any(daily_counts):
                    avg_range = int(round(sum(daily_counts) / float(days_count)))
                else:
                    avg_range = None
            except Exception:
                logger.exception("Failed computing range average from AttendanceSummary")
                avg_range = None

        # fallback: use region history to compute avg_range if still None
        if avg_range is None:
            try:
                import region_clients
                entries = region_clients.fetch_all_history(timeout=REGION_TIMEOUT_SECONDS) or []
                agg = {}
                for e in entries:
                    try:
                        dstr = e.get("date")
                        if not dstr:
                            continue
                        region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                        emp = None
                        con = None
                        tot = None
                        if region_obj:
                            emp = _safe_int(region_obj.get("Employee"))
                            con = _safe_int(region_obj.get("Contractor"))
                            tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                        else:
                            emp = _safe_int(e.get("Employee"))
                            con = _safe_int(e.get("Contractor"))
                            tot = _safe_int(e.get("total"))
                        if emp is None and con is None:
                            continue
                        if tot is None:
                            tot = (emp or 0) + (con or 0)
                        if dstr not in agg:
                            agg[dstr] = {"total": 0, "count": 0}
                        agg[dstr]["total"] += tot or 0
                        agg[dstr]["count"] += 1
                    except Exception:
                        continue
                per_date_totals = []
                days_count = (end_obj - start_obj).days + 1
                for i in range(days_count):
                    dcheck = (start_obj + timedelta(days=i)).isoformat()
                    if dcheck in agg and agg[dcheck]["count"] > 0:
                        per_day_avg = float(agg[dcheck]["total"]) / float(agg[dcheck]["count"])
                        per_date_totals.append(per_day_avg)
                if per_date_totals:
                    avg_range = int(round(sum(per_date_totals) / float(len(per_date_totals))))
            except Exception:
                logger.exception("Failed computing avg_range from region history in fallback")

        # get ccure stats if available
        ccure_stats = {}
        try:
            import ccure_client
            if hasattr(ccure_client, "get_global_stats"):
                ccure_stats = ccure_client.get_global_stats() or {}
        except Exception:
            logger.debug("ccure_client.get_global_stats not available", exc_info=True)

        cc_active_emps = None
        cc_active_contractors = None
        try:
            if isinstance(ccure_stats, dict):
                a = ccure_stats.get("ActiveEmployees") or ccure_stats.get("active_employees") or None
                b = ccure_stats.get("ActiveContractors") or ccure_stats.get("active_contractors") or None
                if a is not None and str(a).strip() != "":
                    cc_active_emps = int(a)
                if b is not None and str(b).strip() != "":
                    cc_active_contractors = int(b)
        except Exception:
            cc_active_emps = cc_active_contractors = None

        emp_pct = None
        contr_pct = None
        overall_pct = None
        try:
            if isinstance(cc_active_emps, int) and cc_active_emps > 0:
                emp_pct = round((live_emp / float(cc_active_emps)) * 100.0, 2)
            if isinstance(cc_active_contractors, int) and cc_active_contractors > 0:
                contr_pct = round((live_contr / float(cc_active_contractors)) * 100.0, 2)
            if isinstance(cc_active_emps, int) and isinstance(cc_active_contractors, int) and (cc_active_emps + cc_active_contractors) > 0:
                overall_pct = round(((live_emp + live_contr) / float(cc_active_emps + cc_active_contractors)) * 100.0, 2)
        except Exception:
            emp_pct = contr_pct = overall_pct = None

        resp = {
            "date": today.isoformat(),
            "notes": f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}" if (start_date or end_date) else None,
            "live_today": {
                "employee": live_emp,
                "contractor": live_contr,
                "total_reported": live_total_reported,
                "total_from_details": live_total_details
            },
            "ccure_active": {
                "active_employees": cc_active_emps,
                "active_contractors": cc_active_contractors,
                "ccure_active_employees_reported": cc_active_emps,
                "ccure_active_contractors_reported": cc_active_contractors
            },
            "averages": {
                "employee_pct": emp_pct,
                "contractor_pct": contr_pct,
                "overall_pct": overall_pct,
                "avg_headcount_last_7_days": avg_range,
                "head_emp_pct_vs_ccure_today": emp_pct,
                "head_contractor_pct_vs_ccure_today": contr_pct,
                "headcount_overall_pct_vs_ccure_today": overall_pct,
                "history_avg_overall_last_7_days": avg_range
            }
        }

        return resp
    except Exception:
        logger.exception("build_ccure_averages failed")
        raise

# ---------- map detailed -> compact (used when compute returns detailed) ----
def _map_detailed_to_resp(detailed: Dict[str, Any]) -> Dict[str, Any]:
    live_h = detailed.get("live_headcount", {}) or {}
    head_h = detailed.get("headcount", {}) or {}
    ccure_active_obj = detailed.get("ccure_active", {}) or {}
    averages_obj = detailed.get("averages", {}) or {}

    live_employee = int(live_h.get("employee") or head_h.get("employee") or 0)
    live_contractor = int(live_h.get("contractor") or head_h.get("contractor") or 0)
    total_reported = int(
        live_h.get("currently_present_total")
        or head_h.get("total_visited_today")
        or (live_employee + live_contractor)
        or 0
    )
    total_from_details = int(head_h.get("total_visited_today") or 0)

    mapped_headcount = {
        "total_visited_today": int(head_h.get("total_visited_today") or 0),
        "employee": int(head_h.get("employee") or 0),
        "contractor": int(head_h.get("contractor") or 0),
        "by_location": head_h.get("by_location") or {}
    }

    resp = {
        "date": detailed.get("date"),
        "notes": detailed.get("notes"),
        "live_today": {
            "employee": live_employee,
            "contractor": live_contractor,
            "total_reported": total_reported,
            "total_from_details": total_from_details
        },
        "headcount": mapped_headcount,
        "live_headcount": live_h,
        "ccure_active": {
            "active_employees": ccure_active_obj.get("ccure_active_employees_reported")
                             or ccure_active_obj.get("active_employees"),
            "active_contractors": ccure_active_obj.get("ccure_active_contractors_reported")
                               or ccure_active_obj.get("active_contractors"),
            "ccure_active_employees_reported": ccure_active_obj.get("ccure_active_employees_reported"),
            "ccure_active_contractors_reported": ccure_active_obj.get("ccure_active_contractors_reported")
        },
        "averages": averages_obj
    }
    return resp

# ---------- build a verify-compatible summary from mapped payload -----------
def _build_verify_like_summary_from_mapped(mapped: Dict[str, Any], include_raw: bool = False) -> Dict[str, Any]:
    """
    Return object shaped exactly like /ccure/verify response so frontend can use a single endpoint.
    Produces:
      - date
      - ccure_reported (employees, contractors, total_reported)
      - headcount_attendance_summary
      - live_headcount_region_clients
      - percentages_vs_ccure
      - averages (merged)
      - notes
      - optionally raw = mapped (detailed debug)
      - headcount_details, live_headcount_details, ccure_active for convenience
    """
    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    cc = mapped.get("ccure_active", {}) or {}
    head = mapped.get("headcount", {}) or {}
    live_head = mapped.get("live_headcount", {}) or {}
    averages = mapped.get("averages", {}) or {}

    # ccure_reported block (employees/contractors/total_reported)
    cc_emp = to_int(cc.get("ccure_active_employees_reported") or cc.get("active_employees"))
    cc_con = to_int(cc.get("ccure_active_contractors_reported") or cc.get("active_contractors"))

    # headcount attendance summary (from AttendanceSummary / union)
    head_total = to_int(head.get("total_visited_today") or mapped.get("live_today", {}).get("total_from_details"))
    head_emp = to_int(head.get("employee") or mapped.get("live_today", {}).get("employee"))
    head_con = to_int(head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    # live headcount region clients (prefer live_headcount -> live_today)
    live_total = to_int(live_head.get("currently_present_total") or mapped.get("live_today", {}).get("total_reported"))
    live_emp = to_int(live_head.get("employee") or mapped.get("live_today", {}).get("employee"))
    live_con = to_int(live_head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    history_emp_avg = averages.get("history_avg_employee_last_7_days")
    history_con_avg = averages.get("history_avg_contractor_last_7_days")
    history_overall_avg = averages.get("history_avg_overall_last_7_days")

    def pct(n, d):
        try:
            if n is None or d is None:
                return None
            if float(d) == 0:
                return None
            return round((float(n) / float(d)) * 100.0, 2)
        except Exception:
            return None

    summary = {
        "date": mapped.get("date"),
        "ccure_reported": {
            "employees": cc_emp,
            "contractors": cc_con,
            "total_reported": (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None
        },
        "headcount_attendance_summary": {
            "total_visited_today": head_total,
            "employee": head_emp,
            "contractor": head_con,
            # by_location intentionally omitted here to mirror /ccure/verify top-level shape; details in 'raw' or headcount_details
        },
        "live_headcount_region_clients": {
            "currently_present_total": live_total,
            "employee": live_emp,
            "contractor": live_con,
        },
        "percentages_vs_ccure": {
            "head_employee_pct_vs_ccure_today": pct(head_emp, cc_emp),
            "head_contractor_pct_vs_ccure_today": pct(head_con, cc_con),
            "head_overall_pct_vs_ccure_today": pct(head_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "live_employee_pct_vs_ccure_today": pct(live_emp, cc_emp),
            "live_contractor_pct_vs_ccure_today": pct(live_con, cc_con),
            "live_overall_pct_vs_ccure_today": pct(live_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "history_employee_pct_vs_ccure": pct(history_emp_avg, cc_emp),
            "history_contractor_pct_vs_ccure": pct(history_con_avg, cc_con),
            "history_overall_pct_vs_ccure": pct(history_overall_avg, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None)
        },
        "averages": {
            # Surface the core expected average fields and keep the rest verbatim
            "history_avg_employee_last_7_days": history_emp_avg,
            "history_avg_contractor_last_7_days": history_con_avg,
            "history_avg_overall_last_7_days": history_overall_avg,
            "avg_headcount_last_7_days_db": averages.get("avg_headcount_last_7_days") or averages.get("avg_headcount_last_7_days_db"),
            "avg_headcount_per_site_last_7_days": averages.get("avg_headcount_per_site_last_7_days"),
            # include compatibility keys (employee_pct, contractor_pct, overall_pct)
            "employee_pct": averages.get("employee_pct"),
            "contractor_pct": averages.get("contractor_pct"),
            "overall_pct": averages.get("overall_pct"),
            # include the rest of averages payload so frontend can use any additional keys
            **({k: v for k, v in averages.items() if k not in (
                "history_avg_employee_last_7_days",
                "history_avg_contractor_last_7_days",
                "history_avg_overall_last_7_days",
                "avg_headcount_last_7_days",
                "avg_headcount_last_7_days_db",
                "avg_headcount_per_site_last_7_days",
                "employee_pct","contractor_pct","overall_pct"
            )})
        },
        "notes": mapped.get("notes")
    }

    # additional convenience blocks for frontend parity with the 'raw' verify response
    summary["headcount_details"] = {
        "total_visited_today": head_total,
        "employee": head_emp,
        "contractor": head_con,
        "by_location": head.get("by_location") if isinstance(head.get("by_location"), dict) else {}
    }
    summary["live_headcount_details"] = {
        "currently_present_total": live_total,
        "employee": live_emp,
        "contractor": live_con,
        "by_location": live_head.get("by_location") if isinstance(live_head.get("by_location"), dict) else {}
    }

    summary["ccure_active"] = {
        "active_employees": cc.get("active_employees") or cc.get("ccure_active_employees_reported"),
        "active_contractors": cc.get("active_contractors") or cc.get("ccure_active_contractors_reported"),
        "ccure_active_employees_reported": cc.get("ccure_active_employees_reported"),
        "ccure_active_contractors_reported": cc.get("ccure_active_contractors_reported")
    }

    if include_raw:
        summary["raw"] = mapped

    return summary

# ---------- /ccure/verify (single canonical endpoint frontend will use) -----
@app.get("/ccure/verify")
def ccure_verify(
    raw: bool = Query(False, description="if true, include the raw compute payload for debugging"),
    start_date: Optional[str] = Query(None, description="YYYY-MM-DD start date (inclusive)"),
    end_date: Optional[str] = Query(None, description="YYYY-MM-DD end date (inclusive)")
):
    """
    Synchronous verification endpoint. Prefer compute_visit_averages() (synchronous call).
    If compute raises or fails, fall back to build_ccure_averages() so output shape remains consistent.
    """
    try:
        detailed = None
        try:
            from ccure_compare_service import compute_visit_averages
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
        except Exception:
            logger.exception("compute_visit_averages() failed inside /ccure/verify; falling back")
            detailed = None

        if isinstance(detailed, dict):
            mapped = _map_detailed_to_resp(detailed)
            summary = _build_verify_like_summary_from_mapped(mapped, include_raw=raw)
            if raw and isinstance(detailed, dict):
                # For verify endpoint the raw block might be the more detailed 'detailed' payload
                summary["raw"] = detailed
            return JSONResponse(summary)
        else:
            fallback = build_ccure_averages(start_date, end_date)
            mapped_fallback = {
                "date": fallback.get("date"),
                "notes": fallback.get("notes"),
                "live_today": fallback.get("live_today", {}),
                "headcount": {
                    "total_visited_today": fallback.get("live_today", {}).get("total_from_details") or fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "live_headcount": {
                    "currently_present_total": fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "ccure_active": fallback.get("ccure_active", {}),
                "averages": fallback.get("averages", {})
            }



            summary = _build_verify_like_summary_from_mapped(mapped_fallback, include_raw=raw)
            if raw:
                summary["raw"] = mapped_fallback
       
            return JSONResponse(summary)
    except Exception as e:
        logger.exception("ccure_verify failed")
        raise HTTPException(status_code=500, detail=f"ccure verify error: {e}")
    
    

# ---------- unchanged remaining endpoints (compare, report) -----------------
@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from ccure_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("ccure_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")
    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)

@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = Path(OUTPUT_DIR) / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(str(full),
                            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            filename=safe_name)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")

# Upload / ingest endpoints left unchanged; they call build_ccure_averages which now uses AttendanceSummary only
# End of file








# ccure_client.py
"""
Lightweight CCURE client wrappers used by compare service.
This file is defensive: missing 'requests' or network failures return None instead of raising.
"""

import math
import logging
from requests.exceptions import RequestException

logger = logging.getLogger("ccure_client")
logger.setLevel(logging.INFO)
if not logger.handlers:
    import sys
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Base URL for CCURE API - adjust if necessary
BASE = "http://10.199.22.57:5001"
DEFAULT_TIMEOUT = 10

HEADERS = {
    "Accept": "application/json"
}

# Defensive import of requests
try:
    import requests
except Exception:
    requests = None
    logger.warning("requests module not available; ccure_client will return None for HTTP calls")

def _safe_get(path, params=None, timeout=DEFAULT_TIMEOUT):
    """
    Safe GET wrapper. Returns parsed JSON on success or None on failure.
    path may include leading slash or not; we join safely.
    """
    if requests is None:
        logger.debug("_safe_get: requests not available")
        return None
    # ensure path begins with '/'
    if not path.startswith("/"):
        path = "/" + path
    url = BASE.rstrip("/") + path
    try:
        r = requests.get(url, params=params, headers=HEADERS, timeout=timeout)
        r.raise_for_status()
        return r.json()
    except RequestException as e:
        logger.warning(f"[ccure_client] request failed {url} params={params} -> {e}")
        return None
    except ValueError:
        logger.warning(f"[ccure_client] response JSON decode error for {url}")
        return None

def fetch_all_employees_full():
    """Try to fetch a full dump from /api/employees (may return list or None)."""
    return _safe_get("/api/employees")

def fetch_stats_page(detail, page=1, limit=500):
    """
    One page of /api/stats?details=detail&page=page&limit=limit
    Returns page dict or None.
    """
    params = {"details": detail, "page": page, "limit": limit}
    return _safe_get("/api/stats", params=params)

def fetch_all_stats(detail, limit=1000):
    """
    Iterate pages for /api/stats detail and return combined data list.
    Returns list or None.
    """
    first = fetch_stats_page(detail, page=1, limit=limit)
    if not first:
        return None
    data = first.get("data") or []
    total = int(first.get("total") or len(data) or 0)
    if total <= len(data):
        return data
    pages = int(math.ceil(total / float(limit)))
    for p in range(2, pages + 1):
        page_res = fetch_stats_page(detail, page=p, limit=limit)
        if not page_res:
            # stop early on error
            break
        data.extend(page_res.get("data") or [])
    return data

def get_global_stats():
    """
    Best-effort summary using /api/stats (preferred) or /api/employees (fallback).
    Returns dict or None.
    """
    # First: try to call /api/stats endpoints for canonical totals (preferred).
    details = ["TotalProfiles", "ActiveProfiles", "ActiveEmployees", "ActiveContractors",
               "TerminatedProfiles", "TerminatedEmployees", "TerminatedContractors"]
    out = {}

    # Try a single call to /api/stats with no detail (some CCURE deployments return a summary dict)
    try:
        summary = _safe_get("/api/stats")
        if isinstance(summary, dict) and any(k in summary for k in details):
            # normalize keys to expected names
            for k in details:
                # attempt case-insensitive lookup
                for key in summary.keys():
                    if key.lower() == k.lower():
                        out[k] = summary.get(key)
                        break
            if out:
                # convert numeric-like to int where possible
                safe_out = {}
                for k, v in out.items():
                    try:
                        safe_out[k] = int(v) if v is not None and str(v).strip() != "" else None
                    except Exception:
                        safe_out[k] = v
                return safe_out
    except Exception:
        pass

    # If that didn't work, try per-detail endpoints (some setups expose /api/stats?details=...)
    try:
        any_found = False
        for d in details:
            resp = fetch_stats_page(d, page=1, limit=1)
            if isinstance(resp, dict):
                # common patterns:
                # - { "total": 123, "data": [...] }
                # - { "TotalProfiles": 123, ... } (summary response)
                if 'total' in resp and isinstance(resp['total'], (int, float, str)):
                    out[d] = int(resp['total'])
                    any_found = True
                elif d in resp:
                    out[d] = resp.get(d)
                    any_found = True
                else:
                    # try case-insensitive key match
                    for key in resp.keys():
                        if key.lower() == d.lower() and isinstance(resp.get(key), (int, float, str)):
                            try:
                                out[d] = int(resp.get(key))
                                any_found = True
                            except Exception:
                                out[d] = resp.get(key)
                                any_found = True
                            break
        if any_found:
            return {k: (int(v) if (v is not None and str(v).strip() != "") else None) for k, v in out.items()}
    except Exception:
        logger.exception("fetch per-detail stats failed")

    # Fallback: try /api/employees full dump and compute counts locally.
    try:
        full = fetch_all_employees_full()
        if isinstance(full, list):
            total = len(full)
            active_profiles = sum(1 for r in full if (r.get("Employee_Status") or "").lower() == "active")
            active_emps = sum(1 for r in full if (r.get("PersonnelType") or "").lower().startswith("employee") and (r.get("Employee_Status") or "").lower() == "active")
            active_contractors = sum(1 for r in full if (r.get("PersonnelType") or "").lower().startswith("contractor") and (r.get("Employee_Status") or "").lower() == "active")
            terminated = sum(1 for r in full if (r.get("Employee_Status") or "").lower() in ("deactive", "deactivated", "inactive", "terminated"))
            return {
                "TotalProfiles": total,
                "ActiveProfiles": active_profiles,
                "ActiveEmployees": active_emps,
                "ActiveContractors": active_contractors,
                "TerminatedProfiles": terminated
            }
    except Exception:
        logger.exception("Error calculating global stats from full dump fallback")

    # nothing available
    return None









# C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\compare_service.py
import pandas as pd
import numpy as np
from datetime import datetime, date, timezone
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary
import re
from dateutil import parser as dateutil_parser
import traceback
import logging

logger = logging.getLogger("compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# --- Helpers -----------------------------------------------------------------
def _to_native(value):
    if value is None:
        return None
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    if isinstance(value, (np.integer,)):
        return int(value)
    if isinstance(value, (np.floating,)):
        return float(value)
    if isinstance(value, (np.bool_, bool)):
        return bool(value)
    try:
        import datetime as _dt
        if isinstance(value, _dt.datetime):
            try:
                if value.tzinfo is not None:
                    utc = value.astimezone(timezone.utc)
                    return utc.replace(tzinfo=None).isoformat() + "Z"
                else:
                    return value.isoformat()
            except Exception:
                return str(value)
        if hasattr(value, 'isoformat'):
            try:
                return value.isoformat()
            except Exception:
                return str(value)
    except Exception:
        pass
    return value

def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

# timestamp parsing helpers (unchanged)
def _parse_timestamp_from_value(val):
    if val is None:
        return None
    import datetime as _dt
    if isinstance(val, _dt.datetime):
        dt = val
        try:
            if dt.tzinfo is not None:
                return dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            return dt
    try:
        import numpy as _np
        if isinstance(val, (int, float, _np.integer, _np.floating)):
            v = int(val)
            if v > 1e12:
                return _dt.fromtimestamp(v / 1000.0, tz=timezone.utc).replace(tzinfo=None)
            if v > 1e9:
                return _dt.fromtimestamp(v, tz=timezone.utc).replace(tzinfo=None)
    except Exception:
        pass
    if isinstance(val, str):
        s = val.strip()
        if s == "":
            return None
        try:
            dt = dateutil_parser.parse(s)
            if dt.tzinfo is not None:
                dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            fmts = ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M:%S.%f",
                    "%d/%m/%Y %H:%M:%S", "%d-%m-%Y %H:%M:%S",
                    "%Y-%m-%dT%H:%M:%S")
            for fmt in fmts:
                try:
                    return _dt.strptime(s, fmt)
                except Exception:
                    pass
    return None

def _extract_timestamp_from_detail(detail):
    fields = [
        "LocaleMessageDateTime", "LocalMessageDateTime", "LocaleMessageTime", "LocalMessageTime",
        "LocaleMessageDate", "Timestamp", "timestamp", "Time", "LocaleTime", "LocalTime",
        "time", "date", "LocaleMessageDateTimeUtc", "LocalMessageDateTimeUtc",
        "Swipe_Time", "SwipeTime", "SwipeTimeLocal", "SwipeTimestamp", "SwipeDateTime"
    ]
    if isinstance(detail, dict):
        for k in fields:
            if k in detail:
                dt = _parse_timestamp_from_value(detail.get(k))
                if dt is not None:
                    return dt
        for v in detail.values():
            dt = _parse_timestamp_from_value(v)
            if dt is not None:
                return dt
    else:
        return _parse_timestamp_from_value(detail)
    return None



# --- Main functions ----------------------------------------------------------

def ingest_live_details_list(details_list):
    """Persist details_list into LiveSwipe. returns counts."""
    from db import SessionLocal as _SessionLocal
    inserted = 0
    skipped = 0
    with _SessionLocal() as db:
        for d in details_list:
            try:
                ts_parsed = _extract_timestamp_from_detail(d)
            except Exception:
                ts_parsed = None
            if ts_parsed is None:
                # skip rows without parseable timestamp
                skipped += 1
                continue

            # robust extraction of employee id and card fields (many alias names)
            emp = None
            for k in ("EmployeeID", "employee_id", "employeeId", "Employee Id", "EmpID", "Emp Id"):
                if isinstance(d, dict) and k in d:
                    emp = d.get(k)
                    break
            emp = _normalize_employee_key(emp)

            card = None
            for k in ("CardNumber", "card_number", "Card", "Card No", "CardNo", "Badge", "BadgeNo", "badge_number", "IPassID", "iPass ID"):
                if isinstance(d, dict) and k in d:
                    card = d.get(k)
                    break
            card = _normalize_card_like(card)

            full_name = None
            for k in ("ObjectName1", "FullName", "full_name", "EmpName", "Name"):
                if isinstance(d, dict) and k in d:
                    full_name = d.get(k)
                    break

            partition = None
            for k in ("PartitionName2", "PartitionName1", "Partition", "PartitionName", "Region"):
                if isinstance(d, dict) and k in d:
                    partition = d.get(k)
                    break

            floor = d.get("Floor") if isinstance(d, dict) else None
            door = None
            for k in ("Door", "DoorName", "door"):
                if isinstance(d, dict) and k in d:
                    door = d.get(k)
                    break

            region = d.get("__region") if isinstance(d, dict) and "__region" in d else d.get("Region") if isinstance(d, dict) else None

            try:
                rec = LiveSwipe(
                    timestamp=ts_parsed,
                    employee_id=emp,
                    card_number=card,
                    full_name=full_name,
                    partition=partition,
                    floor=floor,
                    door=door,
                    region=region,
                    raw=d
                )
                db.add(rec)
                inserted += 1
            except Exception:
                # skip insertion errors but continue
                db.rollback()
                skipped += 1
                continue
        db.commit()
    print(f"[ingest_live_details_list] inserted={inserted} skipped={skipped}")
    return {"inserted": inserted, "skipped_invalid_timestamp": skipped}


def compute_daily_attendance(target_date: date):
    """Build AttendanceSummary rows for target_date (upserts)."""
    with SessionLocal() as db:
        start = datetime.combine(target_date, datetime.min.time())
        end = datetime.combine(target_date, datetime.max.time())
        swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
        if not swipes:
            print(f"[compute_daily_attendance] no swipes for {target_date}")
            return []

        rows = []
        for s in swipes:
            rows.append({
                "id": s.id,
                "timestamp": s.timestamp,
                "employee_id": _normalize_employee_key(s.employee_id),
                "card_number": _normalize_card_like(s.card_number),
                "full_name": s.full_name,
                "partition": s.partition,
                "floor": s.floor,
                "door": s.door
            })
        df = pd.DataFrame(rows)
        if df.empty:
            print(f"[compute_daily_attendance] dataframe empty after rows -> {target_date}")
            return []

        # create grouping key: prefer employee_id, otherwise card_number
        df['key'] = df['employee_id'].fillna(df['card_number'])
        df = df[df['key'].notna()]
        if df.empty:
            print("[compute_daily_attendance] no usable keys after filling employee_id/card")
            return []

        grouped = df.groupby('key', dropna=False).agg(
            presence_count=('id', 'count'),
            first_seen=('timestamp', 'min'),
            last_seen=('timestamp', 'max'),
            full_name=('full_name', 'first'),
            partition=('partition', 'first'),
            card_number=('card_number', 'first')
        ).reset_index().rename(columns={'key': 'employee_id'})

        # upsert AttendanceSummary rows (merge)
        for _, row in grouped.iterrows():
            try:
                derived_obj = {
                    "partition": (row.get('partition') or None),
                    "full_name": (row.get('full_name') or None),
                    "card_number": (row.get('card_number') or None)
                }
                rec = AttendanceSummary(
                    employee_id=str(row['employee_id']) if pd.notna(row['employee_id']) else None,
                    date=target_date,
                    presence_count=int(row['presence_count']),
                    first_seen=row['first_seen'],
                    last_seen=row['last_seen'],
                    derived=derived_obj
                )
                db.merge(rec)
            except Exception as e:
                print("[compute_daily_attendance] upsert error:", e)
                continue
        db.commit()
        print(f"[compute_daily_attendance] built {len(grouped)} attendance keys for {target_date}")
        return grouped.to_dict(orient='records')


def compare_with_active(target_date: date):
    """Compare AttendanceSummary for date with ActiveEmployee & ActiveContractor and return json-safe dict."""
    # NOTE: we intentionally do NOT import get_global_stats_or_none from ccure_client;
    # a local helper wrapper below will call ccure_client.get_global_stats() safely.
    with SessionLocal() as db:
        att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == target_date).all()
        if not att_rows:
            att_df = pd.DataFrame(columns=["employee_id", "presence_count", "first_seen", "last_seen", "card_number", "partition", "full_name"])
        else:
            att_df = pd.DataFrame([{
                "employee_id": _normalize_employee_key(a.employee_id),
                "presence_count": a.presence_count,
                "first_seen": a.first_seen,
                "last_seen": a.last_seen,
                "card_number": _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None),
                "partition": (a.derived.get('partition') if (a.derived and isinstance(a.derived, dict)) else None),
                "full_name": (a.derived.get('full_name') if (a.derived and isinstance(a.derived, dict)) else None)
            } for a in att_rows])

        act_rows = db.query(ActiveEmployee).all()
        contractor_rows = db.query(ActiveContractor).all()

        # Build maps & active list
        act_list = []
        card_to_emp = {}
        name_to_emp = {}

        # Employees
        for e in act_rows:
            emp_id_norm = _normalize_employee_key(e.employee_id)
            # extract card-like from raw if present
            card_from_raw = None
            try:
                rr = e.raw_row or {}
                if isinstance(rr, dict):
                    ck_list = [
                        "CardNumber","card_number","Card","Card No","CardNo","IPassID","IpassID","iPass ID","IPASSID",
                        "Badge Number","BadgeNo","Badge"
                    ]
                    for ck in ck_list:
                        v = rr.get(ck)
                        if v:
                            ckey = _normalize_card_like(v)
                            if ckey:
                                card_from_raw = ckey
                                break
                    # fallback: scan all values for numeric candidate
                    if not card_from_raw:
                        for v in rr.values():
                            try:
                                tmp = _normalize_card_like(v)
                                if tmp and 3 <= len(tmp) <= 12:
                                    card_from_raw = tmp
                                    break
                            except Exception:
                                pass
            except Exception:
                card_from_raw = None

            act_list.append({
                "employee_id": emp_id_norm,
                "full_name": e.full_name,
                "location_city": e.location_city,
                "status": e.current_status,
                "card_number": card_from_raw
            })
            if emp_id_norm:
                card_to_emp[emp_id_norm] = emp_id_norm
            if card_from_raw:
                card_to_emp[card_from_raw] = emp_id_norm
            n = _normalize_name(e.full_name)
            if n:
                name_to_emp[n] = emp_id_norm

        # Contractors
        for c in contractor_rows:
            worker_id = _normalize_employee_key(c.worker_system_id)
            ipass = _normalize_employee_key(c.ipass_id)
            w_ipass = ("W" + ipass) if ipass and not str(ipass).startswith("W") else ipass
            primary_id = worker_id or ipass or None
            act_list.append({
                "employee_id": primary_id,
                "full_name": c.full_name,
                "location_city": c.location,
                "status": c.status,
                "card_number": None
            })
            if primary_id:
                card_to_emp[primary_id] = primary_id
            if ipass:
                card_to_emp[ipass] = primary_id
            if w_ipass:
                card_to_emp[w_ipass] = primary_id
            try:
                rr = c.raw_row or {}
                if isinstance(rr, dict):
                    for ck in ("Worker System Id","Worker System ID","iPass ID","IPASSID","CardNumber","card_number"):
                        if ck in rr and rr.get(ck):
                            key = _normalize_card_like(rr.get(ck))
                            if key:
                                card_to_emp[key] = primary_id
            except Exception:
                pass
            n = _normalize_name(c.full_name)
            if n:
                name_to_emp[n] = primary_id

        act_df = pd.DataFrame(act_list)

        # If no active rows, return attendance-only view
        if act_df.empty:
            if att_df.empty:
                return {"by_location": [], "merged": [], "ccure": get_global_stats_or_none()}
            att_df['partition'] = att_df.get('partition').fillna('Unknown')
            att_df['presence_count'] = att_df['presence_count'].fillna(0)
            att_df['present_today'] = att_df['presence_count'].apply(lambda x: bool(x and x != 0))
            loc_group = att_df.groupby('partition', dropna=False).agg(
                total_n=('employee_id', 'count'),
                present_n=('present_today', 'sum')
            ).reset_index().rename(columns={'partition':'location_city'})
            loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
            by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]
            merged_list = []
            for r in att_df.to_dict(orient='records'):
                merged_list.append({
                    "employee_id": _to_native(r.get('employee_id')),
                    "presence_count": _to_native(r.get('presence_count')),
                    "first_seen": _to_native(r.get('first_seen')),
                    "last_seen": _to_native(r.get('last_seen')),
                    "full_name": _to_native(r.get('full_name')),
                    "location_city": _to_native(r.get('partition')),
                    "present_today": _to_native(r.get('present_today'))
                })
            return {"by_location": by_location, "merged": merged_list, "ccure": get_global_stats_or_none()}

        # normalize columns
        act_df['employee_id'] = act_df['employee_id'].astype(object).apply(_normalize_employee_key)
        att_df['employee_id'] = att_df['employee_id'].astype(object).apply(_normalize_employee_key)
        act_df['card_number'] = act_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in act_df.columns else pd.Series([pd.NA]*len(act_df))
        att_df['card_number'] = att_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in att_df.columns else pd.Series([pd.NA]*len(att_df))

        # ensure card_to_emp includes act_df card_numbers
        for r in act_df.to_dict(orient='records'):
            c = r.get('card_number')
            eid = r.get('employee_id')
            if c and eid:
                card_to_emp[c] = eid
            if eid:
                # also map numeric-only forms of eid
                n = re.sub(r'\D','', str(eid))
                if n:
                    card_to_emp[n.lstrip('0') or n] = eid

        # mapping function tries multiple strategies
        emp_set = set([x for x in act_df['employee_id'].dropna().astype(str)])

        def numeric_variants(s):
            s = str(s)
            clean = re.sub(r'\D','', s)
            variants = set()
            if clean:
                variants.add(clean)
                variants.add(clean.lstrip('0') or clean)
                if not s.startswith('W'):
                    variants.add('W' + clean)
            return list(variants)

        def remap_att_key(row):
            primary = row.get('employee_id') or None
            card = row.get('card_number') or None

            primary_norm = _normalize_employee_key(primary)
            card_norm = _normalize_card_like(card)

            # 1) exact employee id exists in active list
            if primary_norm and primary_norm in emp_set:
                return primary_norm

            # 2) numeric-variants of primary may map to card_to_emp
            if primary_norm:
                for v in numeric_variants(primary_norm):
                    if v in card_to_emp:
                        return card_to_emp[v]
                if primary_norm in card_to_emp:
                    return card_to_emp[primary_norm]

            # 3) direct card mapping
            if card_norm:
                if card_norm in card_to_emp:
                    return card_to_emp[card_norm]
                if (card_norm.lstrip('0') or card_norm) in card_to_emp:
                    return card_to_emp[card_norm.lstrip('0') or card_norm]
                if ('W' + card_norm) in card_to_emp:
                    return card_to_emp['W' + card_norm]

            # 4) name matching fallback
            fname = _normalize_name(row.get('full_name') or row.get('full_name_att') or None)
            if fname and fname in name_to_emp:
                return name_to_emp[fname]

            # 5) last resort - return primary_norm (maybe non-mapped) so it still shows up
            return primary_norm or card_norm or None

        att_df['mapped_employee_id'] = att_df.apply(remap_att_key, axis=1)

        # drop original employee_id column to avoid duplicate label conflict
        att_merge_df = att_df.drop(columns=['employee_id'], errors='ignore').copy()

        # merge left: act_df left_on employee_id, right_on mapped_employee_id
        merged = pd.merge(
            act_df,
            att_merge_df,
            left_on='employee_id',
            right_on='mapped_employee_id',
            how='left',
            suffixes=('', '_att')
        )

        # fill and finalize
        merged['presence_count'] = merged.get('presence_count', pd.Series([0]*len(merged))).fillna(0)
        # ensure ints when possible
        def safe_int(v):
            try:
                if pd.isna(v):
                    return 0
                iv = int(float(v))
                return iv
            except Exception:
                return v
        merged['presence_count'] = merged['presence_count'].apply(safe_int)
        merged['present_today'] = merged['presence_count'].apply(lambda x: bool(x and x != 0))
        merged['location_city'] = merged.get('location_city').fillna('Unknown')

        # by_location
        loc_group = merged.groupby('location_city', dropna=False).agg(
            total_n=('employee_id', 'count'),
            present_n=('present_today', 'sum')
        ).reset_index()
        loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
        by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]

        merged_list = []
        for r in merged.to_dict(orient='records'):
            clean = {k:_to_native(v) for k,v in r.items()}
            # unify keys for clarity in API response
            clean['mapped_employee_id'] = clean.get('mapped_employee_id')
            clean['card_number_att'] = clean.get('card_number') or clean.get('card_number_att') or None
            # include status if present
            if 'status' not in clean:
                clean['status'] = None
            # ensure employee_id key exists
            if 'employee_id' not in clean:
                clean['employee_id'] = None
            merged_list.append(clean)

        # CCURE stats fetch (best-effort)
        ccure_stats = get_global_stats_or_none()

        # compare counts summary between CCure and Active sheets
        try:
            ccure_summary = ccure_stats or {}
            cc_total_profiles = ccure_summary.get('TotalProfiles')
            cc_active_profiles = ccure_summary.get('ActiveProfiles')
            cc_active_emps = ccure_summary.get('ActiveEmployees')
            cc_active_contractors = ccure_summary.get('ActiveContractors')
        except Exception:
            cc_total_profiles = cc_active_profiles = cc_active_emps = cc_active_contractors = None

        # local sheet counts
        active_emp_count = len(act_rows)
        active_contract_count = len(contractor_rows)

        diff = {
            "active_sheet_employee_count": active_emp_count,
            "active_sheet_contractor_count": active_contract_count,
            "ccure_active_employees": cc_active_emps,
            "ccure_active_contractors": cc_active_contractors,
            "delta_employees": (cc_active_emps - active_emp_count) if (isinstance(cc_active_emps, int) and isinstance(active_emp_count, int)) else None,
            "delta_contractors": (cc_active_contractors - active_contract_count) if (isinstance(cc_active_contractors, int) and isinstance(active_contract_count, int)) else None
        }

        result = {
            "by_location": by_location,
            "merged": merged_list,
            "ccure": ccure_stats,
            "count_comparison": diff
        }
        return result

# Helper wrapper
def get_global_stats_or_none():
    try:
        from ccure_client import get_global_stats
        return get_global_stats()
    except Exception:
        logger.debug("ccure_client.get_global_stats not available", exc_info=True)
        return None







# C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\ingest_excel.py
import pandas as pd
from datetime import datetime
from sqlalchemy.exc import IntegrityError
from db import SessionLocal, engine
from models import Base, ActiveEmployee, ActiveContractor
from settings import UPLOAD_DIR
import uuid, os

# --- database setup: do NOT run create_all at import time ---
def init_db():
    """
    Create DB tables if they do not exist.
    Call this manually only when you want to initialize/repair the DB:
      python -c "from ingest_excel import init_db; init_db()"
    """
    from db import engine
    from models import Base
    Base.metadata.create_all(bind=engine)

def _first_present(row, candidates):
    for c in candidates:
        v = row.get(c)
        if v is not None and str(v).strip() != "":
            return v
    return None

def ingest_employee_excel(path, uploaded_by="system"):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    # robust mapping keys
    with SessionLocal() as db:
        for _, row in df.iterrows():
            emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id'])
            if emp_id:
                emp_id = str(emp_id).strip()
            if not emp_id:
                # skip rows without an employee id
                continue
            full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
            # robust current_status detection
            status_candidates = ['Current Status','Status','Employee Status','Employee_Status','Status (Current)','CurrentStatus']
            current_status = _first_present(row, status_candidates)
            email = _first_present(row, ["Employee's Email",'Email','Email Address'])
            location_city = _first_present(row, ['Location City','Location','Location Description','City'])
            rec = ActiveEmployee(
                employee_id=emp_id,
                full_name=full_name,
                email=email,
                location_city=location_city,
                location_desc=row.get('Location Description'),
                current_status=current_status,
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)  # upsert
                db.commit()
            except IntegrityError:
                db.rollback()
            except Exception:
                db.rollback()

def ingest_contractor_excel(path):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    with SessionLocal() as db:
        for _, row in df.iterrows():
            wsid = _first_present(row, ['Worker System Id','Worker System ID','Worker ID','WorkerSystemId'])
            if wsid:
                wsid = str(wsid).strip()
            if not wsid:
                continue
            ipass = _first_present(row, ['iPass ID','"W" iPass ID','IPassID','iPassID','Ipass ID'])
            full_name = _first_present(row, ['Full Name','FullName','Name'])
            rec = ActiveContractor(
                worker_system_id=wsid,
                ipass_id=ipass,
                full_name=full_name,
                vendor=_first_present(row, ['Vendor Company Name','Vendor']),
                location=_first_present(row, ['Worker Location','Location']),
                status=_first_present(row, ['Status','Current Status']),
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)
                db.commit()
            except IntegrityError:
                db.rollback()
            except Exception:
                db.rollback()

if __name__ == "__main__":
    # ingestion convenience: read all uploaded files
    for f in os.listdir(UPLOAD_DIR):
        p = UPLOAD_DIR / f
        if 'contractor' in f.lower() or 'contractor' in str(p).lower():
            ingest_contractor_excel(p)
        else:
            ingest_employee_excel(p)
    print("Ingestion completed.")









# C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\reports.py
import csv
from compare_service import compare_with_active
from datetime import date

def write_daily_location_csv(target_date: date, out_path: str):
    summary = compare_with_active(target_date)
    rows = summary.get("by_location", [])
    # write to CSV
    with open(out_path, "w", newline="", encoding="utf-8") as fh:
        writer = csv.DictWriter(fh, fieldnames=["location_city", "total_n", "present_n", "percent_present"])
        writer.writeheader()
        for r in rows:
            writer.writerow({
                "location_city": r.get("location_city"),
                "total_n": r.get("total_n"),
                "present_n": r.get("present_n"),
                "percent_present": r.get("percent_present")
            })
    return out_path







#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\db.py
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from settings import DB_URL

engine = create_engine(DB_URL, connect_args={"check_same_thread": False} if DB_URL.startswith("sqlite") else {})
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
Base = declarative_base()






# ml/predict.py
from joblib import load
import os
import pandas as pd

MODEL_PATH = os.path.join(os.path.dirname(__file__), "isojob.joblib")

def score_features(df_features: pd.DataFrame):
    """
    Returns anomaly scores if model exists, otherwise returns None.
    df_features: DataFrame with same columns used during training (e.g. days_present, presence_rate)
    """
    if not os.path.exists(MODEL_PATH):
        return None
    clf = load(MODEL_PATH)
    preds = clf.predict(df_features)        # -1 anomaly, 1 normal
    scores = clf.decision_function(df_features)
    df = df_features.copy()
    df["pred"] = preds
    df["score"] = scores
    return df






#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\ml\train_model.py
import pandas as pd
from sklearn.ensemble import IsolationForest
from joblib import dump, load
from db import SessionLocal
from models import AttendanceSummary
from datetime import date, timedelta

def build_feature_table(last_n_days=30):
    with SessionLocal() as db:
        # pull attendance summary for last N days
        end = date.today()
        start = end - timedelta(days=last_n_days)
        q = db.query(AttendanceSummary).filter(AttendanceSummary.date >= start, AttendanceSummary.date <= end)
        df = pd.read_sql(q.statement, q.session.bind)
    if df.empty:
        return None
    # pivot: rows=employee_id, cols=date, values=presence_count>0
    df['present'] = df['presence_count'] > 0
    pivot = df.pivot_table(index='employee_id', columns='date', values='present', aggfunc='max', fill_value=0)
    pivot['days_present'] = pivot.sum(axis=1)
    pivot['presence_rate'] = pivot['days_present'] / last_n_days
    features = pivot[['days_present','presence_rate']].fillna(0)
    return features

def train_isolationforest(save_path="models/isojob.joblib"):
    features = build_feature_table()
    if features is None:
        raise RuntimeError("No data")
    clf = IsolationForest(contamination=0.05, random_state=42)
    clf.fit(features)
    dump(clf, save_path)
    return save_path

if __name__ == "__main__":
    print("Training...")
    print(train_isolationforest())

