Check Below API end point and their OutPut format alos check backend File for more information then Update Frontend page ...


1)http://localhost:8000/duration?start_date=2025-08-25&end_date=2025-09-05&regions=namer
{
  "start_date": "2025-08-25",
  "end_date": "2025-09-05",
  "regions": {
    "namer": {
      "dates": [
        "2025-08-25",
        "2025-08-26",
        "2025-08-27",
        "2025-08-28",
        "2025-08-29",
        "2025-08-30",
        "2025-08-31",
        "2025-09-01",
        "2025-09-02",
        "2025-09-03",
        "2025-09-04",
        "2025-09-05"
      ],
      "employees": [
        {
          "person_uid": "FEE70D0B-BBF1-42E0-A595-310E8C09AA1A",
          "EmployeeID": "W0026455",
          "EmployeeName": "Abdalla, Meira",
          "durations": {
            "2025-08-25": "6:24:25",
            "2025-08-26": "7:31:27",
            "2025-08-27": "23:56:01",
            "2025-08-28": null,
            "2025-08-29": null,
            "2025-08-30": "11:32:13",
            "2025-08-31": "11:41:16",
            "2025-09-01": null,
            "2025-09-02": null,
            "2025-09-03": null,
            "2025-09-04": null,
            "2025-09-05": null
          },
          "durations_seconds": {
            "2025-08-25": 23065,
            "2025-08-26": 27087,
            "2025-08-27": 86161,
            "2025-08-28": null,
            "2025-08-29": null,
            "2025-08-30": 41533,
            "2025-08-31": 42076,
            "2025-09-01": null,
            "2025-09-02": null,
            "2025-09-03": null,
            "2025-09-04": null,
            "2025-09-05": null
          },
          "total_seconds_present_in_range": 219922
        },
        {
          "person_uid": "87B09761-5280-4C43-ABB7-99B65AC9ACA0",
          "EmployeeID": "326269",
          "EmployeeName": "Acevedo, Alondra",
          "durations": {
            "2025-08-25": "8:01:02",
            "2025-08-26": "8:23:19",
            "2025-08-27": "4:52:05",
            "2025-08-28": null,
            "2025-08-29": null,
            "2025-08-30": null,
            "2025-08-31": null,
            "2025-09-01": null,
            "2025-09-02": null,
            "2025-09-03": "6:06:09",
            "2025-09-04": "8:38:13",
            "2025-09-05": null
          },
          "durations_seconds": {
            "2025-08-25": 28862,
            "2025-08-26": 30199,
            "2025-08-27": 17525,
            "2025-08-28": null,
            "2025-08-29": null,
            "2025-08-30": null,
            "2025-08-31": null,
            "2025-09-01": null,
            "2025-09-02": null,
            "2025-09-03": 21969,
            "2025-09-04": 31093,
            "2025-09-05": null
          },
          "total_seconds_present_in_range": 129648
        },





2)http://localhost:8000/duration?start_date=2025-08-25&end_date=2025-09-05&regions=laca&city=CR.Costa%20Rica%20Partition
{
  "start_date": "2025-08-25",
  "end_date": "2025-09-05",
  "regions": {
    "laca": {
      "dates": [
        "2025-08-25",
        "2025-08-26",
        "2025-08-27",
        "2025-08-28",
        "2025-08-29",
        "2025-08-30",
        "2025-08-31",
        "2025-09-01",
        "2025-09-02",
        "2025-09-03",
        "2025-09-04",
        "2025-09-05"
      ],
      "employees": [
        {
          "person_uid": "3DED6F95-AAB7-4CCB-8C70-DD8770ABAEB0",
          "EmployeeID": "322554",
          "EmployeeName": "Abarca, Joshua",
          "durations": {
            "2025-08-25": null,
            "2025-08-26": null,
            "2025-08-27": "10:04:42",
            "2025-08-28": "10:08:45",
            "2025-08-29": "10:08:32",
            "2025-08-30": null,
            "2025-08-31": null,
            "2025-09-01": null,
            "2025-09-02": null,
            "2025-09-03": null,
            "2025-09-04": "9:19:37",
            "2025-09-05": "10:10:52"
          },
          "durations_seconds": {
            "2025-08-25": null,
            "2025-08-26": null,
            "2025-08-27": 36282,
            "2025-08-28": 36525,
            "2025-08-29": 36512,
            "2025-08-30": null,
            "2025-08-31": null,
            "2025-09-01": null,
            "2025-09-02": null,
            "2025-09-03": null,
            "2025-09-04": 33577,
            "2025-09-05": 36652
          },
          "total_seconds_present_in_range": 179548
        },






same APi endPoint for Specific date or range date...

3) http://localhost:8000/duration?date=2025-09-04&regions=apac

{
  "start_date": "2025-09-04",
  "end_date": "2025-09-04",
  "regions": {
    "apac": {
      "dates": [
        "2025-09-04"
      ],
      "employees": [
        {
          "person_uid": "4EB3B79A-791A-4471-B004-00E0B7C23C88",
          "EmployeeID": "328996",
          "EmployeeName": "Abalos, Larry",
          "durations": {
            "2025-09-04": "10:07:32"
          },
          "durations_seconds": {
            "2025-09-04": 36452
          },
          "total_seconds_present_in_range": 36452
        },
        {
          "person_uid": "E68943D7-23BD-49DF-A158-5E3BD8473A59",
          "EmployeeID": "326980",
          "EmployeeName": "Abancio, Mechel",
          "durations": {
            "2025-09-04": "1:57:44"
          },
          "durations_seconds": {
            "2025-09-04": 7064
          },
          "total_seconds_present_in_range": 7064
        },
        {
          "person_uid": "273ACECE-91D8-4F13-88BF-3EDE8AE9ADDA",
          "EmployeeID": "309123",
          "EmployeeName": "Abarzosa, Rowegine Divina",
          "durations": {
            "2025-09-04": "9:37:10"
          },
          "durations_seconds": {
            "2025-09-04": 34630
          },
          "total_seconds_present_in_range": 34630
        },
        {
          "person_uid": "AB116345-9B98-4C3C-81A9-48D10B2A08EC",
          "EmployeeID": "311840",
          "EmployeeName": "Abhale, Satish Sampat",
          "durations": {
            "2025-09-04": "8:05:16"
          },
          "durations_seconds": {
            "2025-09-04": 29116
          },
          "total_seconds_present_in_range": 29116
        },
        {
          "person_uid": "4CF8948E-4FCD-4937-822E-F14B073906C5",
          "EmployeeID": "325241",
          "EmployeeName": "Abidi, Syed Farqaleeta Asghar",
          "durations": {
            "2025-09-04": "8:08:06"
          },
          "durations_seconds": {
            "2025-09-04": 29286
          },
          "total_seconds_present_in_range": 29286
        },
        {
          "person_uid": "5BB74526-F9CE-48FF-816E-86BD269CC43D",
          "EmployeeID": "312458",
          "EmployeeName": "Abrea, Jem Bulos",
          "durations": {
            "2025-09-04": "23:31:40"
          },
          "durations_seconds": {
            "2025-09-04": 84700
          },
          "total_seconds_present_in_range": 84700
        },
        {
          "person_uid": "09A36BB9-F472-44CC-A7E9-E5EEF0389A15",
          "EmployeeID": "309875",
          "EmployeeName": "Abrea, Jerry Rivero",
          "durations": {
            "2025-09-04": "3:19:19"
          },
          "durations_seconds": {
            "2025-09-04": 11959
          },
          "total_seconds_present_in_range": 11959
        },
        {
          "person_uid": "5ADF8D42-840C-4F76-94A9-D25800FDFF54",
          "EmployeeID": "306181",
          "EmployeeName": "Academia, Angelo Alberto Hernandez",
          "durations": {
            "2025-09-04": "6:24:11"
          },
          "durations_seconds": {
            "2025-09-04": 23051
          },
          "total_seconds_present_in_range": 23051
        },
        {
          "person_uid": "BB0F5E1C-3B7D-42AA-B18C-7CCDFF1443CF",
          "EmployeeID": "239360",
          "EmployeeName": "Acance, Maria Cecilia Viloria",
          "durations": {
            "2025-09-04": "20:47:39"
          },
          "durations_seconds": {
            "2025-09-04": 74859
          },
          "total_seconds_present_in_range": 74859
        },
        {
          "person_uid": "58D6D6B3-E8AA-4AC7-BB61-23C55DE6FD14",
          "EmployeeID": "",
          "EmployeeName": "Aclag, Jay",
          "durations": {
            "2025-09-04": "8:10:47"
          },
          "durations_seconds": {
            "2025-09-04": 29447
          },
          "total_seconds_present_in_range": 29447
        },
        {
          "person_uid": "14723C20-4898-4788-904F-EF18407E7562",
          "EmployeeID": "250505",
          "EmployeeName": "Acuba, Juliet Cortan",
          "durations": {
            "2025-09-04": "0:47:05"
          },
          "durations_seconds": {
            "2025-09-04": 2825
          },
          "total_seconds_present_in_range": 2825
        },
        {
          "person_uid": "E0D866B5-1714-4C98-A25A-5D1BCD5D4012",
          "EmployeeID": "",
          "EmployeeName": "Acujedo, Cherry Mae",
          "durations": {
            "2025-09-04": "12:56:34"
          },
          "durations_seconds": {
            "2025-09-04": 46594
          },
          "total_seconds_present_in_range": 46594
        },
        {
          "person_uid": "D1AF8483-0519-488E-AD9B-323CF6565513",
          "EmployeeID": "327185",
          "EmployeeName": "Adda, Rohith Kumar",
          "durations": {
            "2025-09-04": "8:00:01"
          },
          "durations_seconds": {
            "2025-09-04": 28801
          },
          "total_seconds_present_in_range": 28801
        },
        {
          "person_uid": "D088B0A4-AC13-43D6-8BD3-AF839999EB82",
          "EmployeeID": "326419",
          "EmployeeName": "Adoni, Sureshkumar Adoni",
          "durations": {
            "2025-09-04": "8:25:15"
          },
          "durations_seconds": {
            "2025-09-04": 30315
          },
          "total_seconds_present_in_range": 30315
        },
        {
          "person_uid": "218425C7-A37B-4219-A912-07D75A3FA3D7",
          "EmployeeID": "",
          "EmployeeName": "Adovino, Genevette Ann",
          "durations": {
            "2025-09-04": "8:25:14"
          },
          "durations_seconds": {
            "2025-09-04": 30314
          },
          "total_seconds_present_in_range": 30314
        },






Now Check below APi backenbd and Frontend Details ...



#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py

"""
duration_report.py

Updated: includes Door, EmployeeName, CardNumber extraction (XML fallback),
CompanyName, PrimaryLocation, Direction, and per-swipe output alongside per-person durations.

Place into your backend directory and restart the server.
"""
import argparse
import logging
import os
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd

# Optional: import pyodbc only when connecting (allows importing this module even without driver)
try:
    import pyodbc
except Exception:
    pyodbc = None

# --------------------- Configuration ---------------------
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "database": "ACVSUJournal_00010029",
        "partitions": [
            "APAC.Default", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur"
        ]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "database": "ACVSUJournal_00011028",
        "partitions": [
            "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
            "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
        ]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "database": "ACVSUJournal_00010029",
        "partitions": [
            "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
            "PA.Panama City", "PE.Lima"
        ]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "database": "ACVSUJournal_00010029",
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# --------------------- SQL Builder ---------------------
# This SQL attempts to extract Card from XML and falls back to t2.Text12.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,                       -- Person name from ACVS
    t1.[ObjectName2] AS Door,                               -- Door / reader name
    CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    -- try extract card from xml CHUID/Card or CHUID element, then fallback to shred 'sc' and finally t2.Text12
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
INNER JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
INNER JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
;
"""


def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    return GENERIC_SQL_TEMPLATE.format(db=rc["database"], date=date_str, region_filter=region_filter)


# --------------------- DB Utilities ---------------------
def get_connection(region_key: str):
    """Create and return a pyodbc connection for the region configuration."""
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)


def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    """
    Fetch CardAdmitted swipes for the given region and target_date.
    Returns a pandas DataFrame with normalized column names:
      ['EmployeeName','Door','EmployeeID','CardNumber','PersonnelTypeName','EmployeeIdentity',
       'PartitionName2','LocaleMessageTime','MessageType','Direction','CompanyName','PrimaryLocation']
    If pyodbc is not available, returns an empty skeleton DataFrame with those columns.
    """
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "CardNumber", "PersonnelTypeName", "EmployeeIdentity",
        "PartitionName2", "LocaleMessageTime", "MessageType", "Direction", "CompanyName", "PrimaryLocation"
    ]
    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = get_connection(region_key)
    try:
        df = pd.read_sql(sql, conn)
    finally:
        conn.close()

    # Ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Parse datetime
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    return df[cols]


# --------------------- Duration Calculation ---------------------
def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute first and last swipe per person per day and duration.
    - uses EmployeeIdentity (GUID) as primary dedupe key
    - falls back to EmployeeID|CardNumber|EmployeeName when GUID missing
    - returns DataFrame with columns:
      person_uid, EmployeeIdentity, EmployeeID, EmployeeName, CardNumber,
      Date, FirstSwipe, LastSwipe, FirstDoor, LastDoor, CountSwipes,
      DurationSeconds, Duration, PersonnelTypeName, PartitionName2,
      CompanyName, PrimaryLocation, FirstDirection, LastDirection
    """
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()

    # Ensure expected columns present (avoid KeyError)
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # Parse datetime if required
    if df["LocaleMessageTime"].dtype == object:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    # Drop exact duplicate swipe records (same GUID, same timestamp, same card, same door)
    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    df = df.drop_duplicates(subset=dedupe_cols, keep="first")

    # Create Date column (uses local date from LocaleMessageTime)
    df["Date"] = df["LocaleMessageTime"].dt.date

    # Build person_uid: prefer EmployeeIdentity (GUID); otherwise concat EmployeeID|CardNumber|EmployeeName
    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    df["person_uid"] = df.apply(make_person_uid, axis=1)
    df = df[df["person_uid"].notna()].copy()

    # Group by person_uid + Date and compute aggregates
    def agg_for_group(g):
        g_sorted = g.sort_values("LocaleMessageTime")
        first = g_sorted.iloc[0]
        last = g_sorted.iloc[-1]

        # First/last directions might be missing or identical; preserve both
        first_dir = first.get("Direction")
        last_dir = last.get("Direction")

        return pd.Series({
            "person_uid": first["person_uid"],
            "EmployeeIdentity": first.get("EmployeeIdentity"),
            "EmployeeID": first.get("EmployeeID"),
            "EmployeeName": first.get("EmployeeName"),
            "CardNumber": first.get("CardNumber"),
            "Date": first["Date"],
            "FirstSwipe": first["LocaleMessageTime"],
            "LastSwipe": last["LocaleMessageTime"],
            "FirstDoor": first.get("Door"),
            "LastDoor": last.get("Door"),
            "CountSwipes": int(len(g_sorted)),
            "PersonnelTypeName": first.get("PersonnelTypeName"),
            "PartitionName2": first.get("PartitionName2"),
            "CompanyName": first.get("CompanyName"),
            "PrimaryLocation": first.get("PrimaryLocation"),
            "FirstDirection": first_dir,
            "LastDirection": last_dir
        })

    grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    # compute duration fields
    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    # Reorder columns and return
    return grouped[out_cols]


# --------------------- Main Runner ---------------------
def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    """
    Run duration reports for the requested regions on target_date.
    Returns a dict: region -> {"swipes": DataFrame, "durations": DataFrame}
    """
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        try:
            swipes = fetch_swipes_for_region(r, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", r)
            swipes = pd.DataFrame()

        # optional city filter: match PartitionName2 OR PrimaryLocation OR Door OR EmployeeName (case-insensitive)
        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                swipes = swipes[combined_mask].copy()
            else:
                # no matching columns -> keep original
                logging.debug("City filter provided but no matching columns to filter on for region %s", r)

        # compute durations (de-duplicated, per-person)
        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", r)
            durations = pd.DataFrame()

        # save CSVs
        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
        try:
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", r)
        try:
            # write swipes with a stable column order if possible
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", r)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
        results[r] = {"swipes": swipes, "durations": durations}

    return results


def parse_args():
    p = argparse.ArgumentParser(description="Generate daily duration report from ACVSUJournal swipe logs.")
    p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
    p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
                   default="apac,emea,laca,namer")
    p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
    p.add_argument("--city", help="Optional city/location filter (e.g. Pune)", default=None)
    return p.parse_args()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    args = parse_args()

    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    else:
        tz = ZoneInfo("Asia/Kolkata")
        target_date = datetime.now(tz).date()

    regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
    outdir = args.outdir

    logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
    results = run_for_date(target_date, regions, outdir, city=args.city)

    for r, obj in results.items():
        dur = obj.get("durations")
        logging.info("Region %s: %d persons with computed durations", r, len(dur) if dur is not None else 0)
    logging.info("Completed. CSVs are in %s", Path(outdir).absolute())











C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py
@app.get("/duration")
async def api_duration(
    date_param: Optional[str] = Query(None, alias="date", description="Target date YYYY-MM-DD. Defaults to today in Asia/Kolkata"),
    start_date: Optional[str] = Query(None, description="Start date for a range (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date for a range (YYYY-MM-DD)"),
    regions: Optional[str] = Query(None, description="Comma-separated list: apac,emea,laca,namer. Default: all"),
    city: Optional[str] = Query(None, description="Optional city/location filter (e.g. Pune). Case-insensitive, matches PartitionName2/PrimaryLocation/Door/EmployeeName"),
    outdir: Optional[str] = Query(None, description="Output directory for CSVs. Defaults to OUTPUT_DIR/duration_reports"),
    sample_rows: int = Query(10, ge=0, le=100, description="How many sample rows to include per region in response")
):
    try:
        # --- parse region list
        if regions:
            regions_list = [r.strip().lower() for r in regions.split(",") if r.strip()]
        else:
            regions_list = ["apac", "emea", "laca", "namer"]

        # --- parse output dir
        if outdir:
            outdir_path = Path(outdir)
        else:
            outdir_path = OUTPUT_DIR / "duration_reports"
        outdir_path.mkdir(parents=True, exist_ok=True)

        # --- determine date(s)
        def _parse_date(s: str) -> date:
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                return date.fromisoformat(s)

        if start_date and end_date:
            try:
                start_obj = _parse_date(start_date)
                end_obj = _parse_date(end_date)
            except Exception:
                raise HTTPException(status_code=400, detail="Invalid start_date or end_date format. Use YYYY-MM-DD.")
            if start_obj > end_obj:
                raise HTTPException(status_code=400, detail="start_date must be <= end_date")
            # safety: limit max days to avoid overloading
            max_days = 31
            days_count = (end_obj - start_obj).days + 1
            if days_count > max_days:
                raise HTTPException(status_code=400, detail=f"Date range too large (> {max_days} days). Please request a smaller range.")
            date_list = [start_obj + timedelta(days=i) for i in range(days_count)]
            range_mode = True
        else:
            # single-date: prefer explicit `date` query param, otherwise today's Asia/Kolkata
            if date_param:
                try:
                    target_date = _parse_date(date_param)
                except Exception:
                    raise HTTPException(status_code=400, detail="Invalid date format. Use YYYY-MM-DD.")
            else:
                tz = ZoneInfo("Asia/Kolkata")
                target_date = datetime.now(tz).date()
            date_list = [target_date]
            start_obj = end_obj = date_list[0]
            range_mode = False

        # --- import duration_report lazily
        try:
            import duration_report
        except Exception as e:
            logger.exception("Failed importing duration_report module")
            raise HTTPException(status_code=500, detail=f"duration module import failed: {e}")

        loop = asyncio.get_running_loop()

        # Helper serializer
        def _to_json_safe(v):
            try:
                if pd.isna(v):
                    return None
            except Exception:
                pass
            if isinstance(v, (datetime, date)):
                return v.isoformat()
            if hasattr(v, "isoformat") and not isinstance(v, str):
                try:
                    return v.isoformat()
                except Exception:
                    pass
            if isinstance(v, (int, float, bool)):
                return v
            return v

        # For each date, call run_for_date and collect results
        per_date_results = {}  # iso_date -> results dict returned by duration_report.run_for_date
        for single_date in date_list:
            try:
                task = loop.run_in_executor(None, duration_report.run_for_date, single_date, regions_list, str(outdir_path), city)
                per_date_results[single_date.isoformat()] = await asyncio.wait_for(task, timeout=COMPUTE_WAIT_TIMEOUT_SECONDS)
            except asyncio.TimeoutError:
                raise HTTPException(status_code=504, detail=f"Duration computation timed out for date {single_date.isoformat()}")
            except Exception as e:
                logger.exception("duration run_for_date failed for date %s", single_date)
                raise HTTPException(status_code=500, detail=f"duration run failed for {single_date.isoformat()}: {e}")

        # Aggregate results per region and per employee across dates
        resp = {
            "start_date": start_obj.isoformat(),
            "end_date": end_obj.isoformat(),
            "regions": {}
        }

        for r in regions_list:
            try:
                # Prepare date list strings
                dates_iso = [d.isoformat() for d in date_list]

                # employee_map keyed by person_uid (prefer) else by EmployeeID|EmployeeName
                employees: Dict[str, Dict[str, Any]] = {}
                date_rows = {}

                for iso_d, day_res in per_date_results.items():
                    # day_res is a dict: region -> {"swipes": df, "durations": df}
                    region_obj = day_res.get(r) if isinstance(day_res, dict) else None
                    durations_df = None
                    swipes_df = None
                    if isinstance(region_obj, dict):
                        swipes_df = region_obj.get("swipes")
                        durations_df = region_obj.get("durations")
                    elif isinstance(region_obj, pd.DataFrame):
                        durations_df = region_obj

                    # count rows for this date / region
                    rows_count = int(len(durations_df)) if durations_df is not None else 0
                    swipe_count = int(len(swipes_df)) if swipes_df is not None else 0
                    date_rows[iso_d] = {"rows": rows_count, "swipe_rows": swipe_count}

                    if durations_df is None or durations_df.empty:
                        continue

                    # normalize columns safety
                    for col in ("person_uid", "EmployeeID", "EmployeeName", "Duration", "DurationSeconds"):
                        if col not in durations_df.columns:
                            durations_df[col] = None

                    # iterate rows and populate map
                    for _, row in durations_df.iterrows():
                        person_uid = row.get("person_uid") or None
                        # fallback key
                        if not person_uid or pd.isna(person_uid):
                            key = f"{str(row.get('EmployeeID') or '').strip()}|{str(row.get('EmployeeName') or '').strip()}"
                            person_uid = key

                        if person_uid not in employees:
                            employees[person_uid] = {
                                "person_uid": person_uid,
                                "EmployeeID": None if pd.isna(row.get("EmployeeID")) else str(row.get("EmployeeID")),
                                "EmployeeName": None if pd.isna(row.get("EmployeeName")) else str(row.get("EmployeeName")),
                                "durations": {d: None for d in dates_iso},
                                "durations_seconds": {d: None for d in dates_iso},
                                "total_seconds_present_in_range": 0
                            }
                        # fill this date
                        dur_str = None if pd.isna(row.get("Duration")) else str(row.get("Duration"))
                        dur_secs = None
                        try:
                            v = row.get("DurationSeconds")
                            if pd.notna(v):
                                dur_secs = int(float(v))
                        except Exception:
                            dur_secs = None

                        employees[person_uid]["durations"][iso_d] = dur_str
                        employees[person_uid]["durations_seconds"][iso_d] = dur_secs
                        if dur_secs is not None:
                            employees[person_uid]["total_seconds_present_in_range"] += dur_secs

                # convert employees map to sorted list (sort by EmployeeName then EmployeeID)
                emp_list = list(employees.values())
                emp_list.sort(key=lambda x: (x.get("EmployeeName") or "").lower(), reverse=False)

                # Keep only top sample_rows in durations_sample (if requested) -- still return full employees list
                durations_sample = emp_list[:sample_rows] if sample_rows and sample_rows > 0 else []

                resp["regions"][r] = {
                    "dates": dates_iso,
                    "employees": emp_list,
                    "durations_sample": durations_sample,
                    "date_rows": date_rows
                }
            except Exception:
                logger.exception("Failed to aggregate range results for region %s", r)
                resp["regions"][r] = {"dates": [d.isoformat() for d in date_list], "employees": [], "durations_sample": [], "date_rows": {}}

        return JSONResponse(resp)
    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("api_duration (range) failed")
        raise HTTPException(status_code=500, detail=f"duration api error: {exc}")





so Basically check below frontend File ..


In Global Page add one icon after Assocoate Verification Tool 10.199.22.57:3004 
add one icon .
when click this icon navigate new Duration page
In duration page make make proper duation table ...
add start date . end date for range duration .. , also for specific day add calendar ...
add Region dropdown ..(APAC, EMEA, LACA, NAMER )
then add city filter.. ( APAC.Default , CR.Costa Rica, HQ...like ..

make a page more attractive carefully 




