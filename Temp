# combined_app_with_duration.py
# Single-file: app + duration_report (updated shift/session logic + overrides)
from fastapi import FastAPI, UploadFile, File, HTTPException, Query, Body
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.encoders import jsonable_encoder
import shutil
import uuid
import json
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
import re
import asyncio
from typing import Optional, Dict, Any, List
import hashlib
import time
import os

# NEW imports required to fix NameError and duration timezone usage
import pandas as pd
from zoneinfo import ZoneInfo
import warnings
import functools

# --- DB / models imports (kept for endpoints that still use DB like headcount/attendance fallback) ---
try:
    from db import SessionLocal
    from models import LiveSwipe, AttendanceSummary
except Exception:
    # If you're running without DB for testing, these imports may fail.
    SessionLocal = None
    LiveSwipe = None
    AttendanceSummary = None

# --- settings (optional override) ---
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"

import sys  # ensure logging stream available
RAW_UPLOADS_DIR = DATA_DIR / "raw_uploads"
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# ----------------- GLOBAL TIMEOUTS (UNIFY) -----------------
REGION_TIMEOUT_SECONDS = 300
COMPUTE_WAIT_TIMEOUT_SECONDS = 300
COMPUTE_SYNC_TIMEOUT_SECONDS = 300
# ----------------------------------------------------------

_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

_broadcaster_clients = set()

def broadcast_ccure_update(payload: dict):
    """
    Broadcast a ccure summary to all SSE clients.
    Accepts either:
      - the payload directly (dict with keys like ccure_reported, headcount_attendance_summary, ...)
      - or a cache-wrapper dict: {"cached_at": ..., "payload": <payload>}
    This function will unwrap the wrapper automatically.
    """
    # unwrap possible cache wrapper
    try:
        if isinstance(payload, dict) and "payload" in payload and isinstance(payload["payload"], dict):
            payload_to_send = payload["payload"]
        else:
            payload_to_send = payload
    except Exception:
        payload_to_send = payload

    if not _broadcaster_clients:
        return
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = None
    for q in list(_broadcaster_clients):
        try:
            # put the payload (not the wrapper) into client queue
            if loop and loop.is_running():
                loop.call_soon_threadsafe(q.put_nowait, payload_to_send)
            else:
                q.put_nowait(payload_to_send)
        except Exception:
            logger.exception("Failed to push payload to SSE client (will remove client)")
            try:
                _broadcaster_clients.discard(q)
            except Exception:
                pass

async def _sse_event_generator(client_queue: asyncio.Queue):
    try:
        while True:
            payload = await client_queue.get()
            try:
                data = json.dumps(payload, default=str)
            except Exception:
                data = json.dumps({"error": "serialization error", "payload": str(payload)})
            yield f"data: {data}\n\n"
    finally:
        try:
            _broadcaster_clients.discard(client_queue)
        except Exception:
            pass
        return

@app.get("/ccure/stream")
async def ccure_stream():
    q = asyncio.Queue()
    _broadcaster_clients.add(q)
    # Immediately push latest cache if present (and ensure we push only the payload)
    try:
        cached = _load_ccure_cache_any()
        if cached:
            try:
                # cached might be wrapper {"cached_at":..., "payload": ...}
                payload = cached.get("payload") if (isinstance(cached, dict) and "payload" in cached) else cached
                q.put_nowait(payload)
            except Exception:
                pass
    except Exception:
        pass
    generator = _sse_event_generator(q)
    headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    return StreamingResponse(generator, media_type="text/event-stream", headers=headers)

# provide API-prefixed alias for Vite /api proxy issues
@app.get("/api/ccure/stream")
async def api_ccure_stream():
    return await ccure_stream()


def _guess_region_from_text(txt: str) -> str:
    if not txt:
        return "unknown"
    s = str(txt).strip().lower()
    s = re.sub(r"[,\-/()]", " ", s)
    if any(k in s for k in ("pune","quezon city","taguig city","bengaluru","hyderabad","chennai","manila","singapore","hong kong","beijing","shanghai","jakarta","kuala","osaka","tokyo","seoul","bangkok")):
        return "apac"
    if any(k in s for k in ("london","dublin","paris","frankfurt","amsterdam","stockholm","cape town","johannesburg","berlin","brussels","madrid","rome","milan")):
        return "emea"
    if any(k in s for k in ("mexico","bogota","buenos","santiago","sao","salvador","lima","caracas")):
        return "laca"
    if any(k in s for k in ("denver","new york","ny","chicago","toronto","vancouver","los angeles","san francisco","boston","houston","atlanta","miami")):
        return "namer"
    return "unknown"

@app.get("/headcount")
def api_headcount():
    try:
        totals = {"apac": 0, "emea": 0, "laca": 0, "namer": 0, "unknown": 0}
        if SessionLocal is None:
            return JSONResponse({"apac": 0, "emea": 0, "laca": 0, "namer": 0})
        with SessionLocal() as db:
            try:
                today = date.today()
                rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                if rows:
                    for r in rows:
                        try:
                            partition = None
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                            loc = partition or "unknown"
                            region = _guess_region_from_text(loc)
                            totals[region] = totals.get(region, 0) + 1
                        except Exception:
                            totals["unknown"] += 1
                else:
                    start = datetime.combine(today, datetime.min.time())
                    end = datetime.combine(today, datetime.max.time())
                    swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
                    for s in swipes:
                        loc = s.partition or "unknown"
                        region = _guess_region_from_text(loc)
                        totals[region] = totals.get(region, 0) + 1
            except Exception:
                logger.exception("Failed to compute headcount regions")
        out = {
            "apac": int(totals.get("apac", 0)),
            "emea": int(totals.get("emea", 0)),
            "laca": int(totals.get("laca", 0)),
            "namer": int(totals.get("namer", 0))
        }
        return JSONResponse(out)
    except Exception as exc:
        logger.exception("api_headcount failed")
        raise HTTPException(status_code=500, detail=f"headcount error: {exc}")

# ---------- Helpers retained (normalize / safe conversions) -------------
def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- build_ccure_averages (fallback) ------------------------------
def build_ccure_averages(start_date: Optional[str] = None, end_date: Optional[str] = None):
    """
    Fallback averages computation using AttendanceSummary only.
    Returns a compact shape (live_today, ccure_active, averages, notes).
    """
    try:
        def _parse_date_param(s):
            if not s:
                return None
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                try:
                    return date.fromisoformat(s)
                except Exception:
                    return None

        today = date.today()
        start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
        end_obj = _parse_date_param(end_date) if end_date else today
        if start_obj is None or end_obj is None or start_obj > end_obj:
            start_obj = today - timedelta(days=6)
            end_obj = today

        if SessionLocal is None:
            return {
                "date": today.isoformat(),
                "notes": None,
                "live_today": {"employee": 0, "contractor": 0, "total_reported": 0, "total_from_details": 0},
                "ccure_active": {},
                "averages": {}
            }

        with SessionLocal() as db:
            try:
                att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
            except Exception:
                logger.exception("Failed to query AttendanceSummary")
                att_rows = []

            live_emp = 0
            live_contr = 0
            unknown_count = 0
            seen_keys = set()

            def classify_from_derived(derived):
                try:
                    if not derived or not isinstance(derived, dict):
                        return "contractor"
                    for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                        v = derived.get(k)
                        if v and "employee" in str(v).strip().lower():
                            return "employee"
                    for k in ("Employee_Status","Employee Status","Status"):
                        v = derived.get(k)
                        if v and "terminated" in str(v).strip().lower():
                            return "employee"
                    return "contractor"
                except Exception:
                    return "contractor"

            if att_rows:
                for a in att_rows:
                    key = None
                    try:
                        key = _normalize_employee_key(a.employee_id)
                    except Exception:
                        key = None
                    if not key:
                        try:
                            key = _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None)
                        except Exception:
                            key = None
                    if not key:
                        unknown_count += 1
                        continue
                    if key in seen_keys:
                        continue
                    seen_keys.add(key)
                    cls = classify_from_derived(a.derived)
                    if cls == "employee":
                        live_emp += 1
                    elif cls == "contractor":
                        live_contr += 1
                    else:
                        unknown_count += 1

                live_total_reported = live_emp + live_contr + unknown_count
                live_total_details = len(att_rows)
            else:
                live_total_reported = 0
                live_total_details = 0
                try:
                    import region_clients
                    details = []
                    try:
                        details = region_clients.fetch_all_details(timeout=REGION_TIMEOUT_SECONDS) or []
                    except Exception:
                        logger.exception("region_clients.fetch_all_details failed in build_ccure_averages()")
                        details = []
                    if details:
                        for d in details:
                            try:
                                cls = "contractor"
                                for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                                    v = d.get(k)
                                    if v and "employee" in str(v).strip().lower():
                                        cls = "employee"
                                        break
                                if cls == "employee":
                                    live_emp += 1
                                else:
                                    live_contr += 1
                                live_total_details += 1
                            except Exception:
                                continue
                        live_total_reported = live_emp + live_contr
                    else:
                        try:
                            regions = region_clients.fetch_all_regions(timeout=REGION_TIMEOUT_SECONDS) or []
                            for r in regions:
                                try:
                                    c = r.get("count")
                                    if isinstance(c, (int, float)):
                                        live_total_reported += int(c)
                                except Exception:
                                    continue
                        except Exception:
                            logger.exception("region_clients.fetch_all_regions failed in build_ccure_averages()")
                except Exception:
                    logger.exception("region_clients not importable in build_ccure_averages()")

            # compute avg range using AttendanceSummary if possible
            avg_range = None
            try:
                q = db.query(AttendanceSummary.date, AttendanceSummary.employee_id, AttendanceSummary.presence_count)\
                      .filter(AttendanceSummary.date >= start_obj, AttendanceSummary.date <= end_obj).all()
                by_date = {}
                for row in q:
                    d = row[0]
                    key = (row[1] or "").strip() if row[1] else None
                    if not key:
                        continue
                    if d not in by_date:
                        by_date[d] = set()
                    try:
                        presence_val = getattr(row, 'presence_count', row[2])
                        if int(presence_val) > 0:
                            by_date[d].add(key)
                    except Exception:
                        by_date[d].add(key)
                days_count = (end_obj - start_obj).days + 1
                daily_counts = [len(by_date.get(start_obj + timedelta(days=i), set())) for i in range(days_count)]
                if days_count and any(daily_counts):
                    avg_range = int(round(sum(daily_counts) / float(days_count)))
                else:
                    avg_range = None
            except Exception:
                logger.exception("Failed computing range average from AttendanceSummary")
                avg_range = None

        # fallback: use region history to compute avg_range if still None
        if avg_range is None:
            try:
                import region_clients
                entries = region_clients.fetch_all_history(timeout=REGION_TIMEOUT_SECONDS) or []
                agg = {}
                for e in entries:
                    try:
                        dstr = e.get("date")
                        if not dstr:
                            continue
                        region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                        emp = None
                        con = None
                        tot = None
                        if region_obj:
                            emp = _safe_int(region_obj.get("Employee"))
                            con = _safe_int(region_obj.get("Contractor"))
                            tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                        else:
                            emp = _safe_int(e.get("Employee"))
                            con = _safe_int(e.get("Contractor"))
                            tot = _safe_int(e.get("total"))
                        if emp is None and con is None:
                            continue
                        if tot is None:
                            tot = (emp or 0) + (con or 0)
                        if dstr not in agg:
                            agg[dstr] = {"total": 0, "count": 0}
                        agg[dstr]["total"] += tot or 0
                        agg[dstr]["count"] += 1
                    except Exception:
                        continue
                per_date_totals = []
                days_count = (end_obj - start_obj).days + 1
                for i in range(days_count):
                    dcheck = (start_obj + timedelta(days=i)).isoformat()
                    if dcheck in agg and agg[dcheck]["count"] > 0:
                        per_day_avg = float(agg[dcheck]["total"]) / float(agg[dcheck]["count"])
                        per_date_totals.append(per_day_avg)
                if per_date_totals:
                    avg_range = int(round(sum(per_date_totals) / float(len(per_date_totals))))
            except Exception:
                logger.exception("Failed computing avg_range from region history in fallback")

        # get ccure stats if available
        ccure_stats = {}
        try:
            import ccure_client
            if hasattr(ccure_client, "get_global_stats"):
                ccure_stats = ccure_client.get_global_stats() or {}
        except Exception:
            logger.debug("ccure_client.get_global_stats not available", exc_info=True)

        cc_active_emps = None
        cc_active_contractors = None
        try:
            if isinstance(ccure_stats, dict):
                a = ccure_stats.get("ActiveEmployees") or ccure_stats.get("active_employees") or None
                b = ccure_stats.get("ActiveContractors") or ccure_stats.get("active_contractors") or None
                if a is not None and str(a).strip() != "":
                    cc_active_emps = int(a)
                if b is not None and str(b).strip() != "":
                    cc_active_contractors = int(b)
        except Exception:
            cc_active_emps = cc_active_contractors = None

        emp_pct = None
        contr_pct = None
        overall_pct = None
        try:
            if isinstance(cc_active_emps, int) and cc_active_emps > 0:
                emp_pct = round((live_emp / float(cc_active_emps)) * 100.0, 2)
            if isinstance(cc_active_contractors, int) and cc_active_contractors > 0:
                contr_pct = round((live_contr / float(cc_active_contractors)) * 100.0, 2)
            if isinstance(cc_active_emps, int) and isinstance(cc_active_contractors, int) and (cc_active_emps + cc_active_contractors) > 0:
                overall_pct = round(((live_emp + live_contr) / float(cc_active_emps + cc_active_contractors)) * 100.0, 2)
        except Exception:
            emp_pct = contr_pct = overall_pct = None

        resp = {
            "date": today.isoformat(),
            "notes": f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}" if (start_date or end_date) else None,
            "live_today": {
                "employee": live_emp,
                "contractor": live_contr,
                "total_reported": live_total_reported,
                "total_from_details": live_total_details
            },
            "ccure_active": {
                "active_employees": cc_active_emps,
                "active_contractors": cc_active_contractors,
                "ccure_active_employees_reported": cc_active_emps,
                "ccure_active_contractors_reported": cc_active_contractors
            },
            "averages": {
                "employee_pct": emp_pct,
                "contractor_pct": contr_pct,
                "overall_pct": overall_pct,
                "avg_headcount_last_7_days": avg_range,
                "head_emp_pct_vs_ccure_today": emp_pct,
                "head_contractor_pct_vs_ccure_today": contr_pct,
                "headcount_overall_pct_vs_ccure_today": overall_pct,
                "history_avg_overall_last_7_days": avg_range
            }
        }

        return resp
    except Exception:
        logger.exception("build_ccure_averages failed")
        raise

# -------------------------
# Upload endpoints (store uploads under data/ and data/raw_uploads/ and rotate)
# -------------------------
ALLOWED_EXT = (".xls", ".xlsx", ".csv")

def _remove_old_files_for_kind(kind: str):
    """
    Remove previous canonical files and previous raw uploads that include kind in filename.
    """
    try:
        # canonical in DATA_DIR
        for p in DATA_DIR.iterdir():
            if p.is_file() and f"active_{kind}" in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old canonical file %s", p)
        # remove raw uploads for same kind to keep only latest raw (user requested)
        for p in RAW_UPLOADS_DIR.iterdir():
            if p.is_file() and kind in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old raw file %s", p)
    except Exception:
        logger.exception("error while removing old files for kind=%s", kind)

def _save_upload_and_rotate(upload_file: UploadFile, kind: str) -> dict:
    """
    Save the uploaded file as:
      - data/active_<kind>.<ext>     (canonical)
      - data/raw_uploads/<timestamp>_<kind>_<origname>  (raw trace)
    Remove previous files for same kind (both canonical & raw).
    Returns metadata dict.
    """
    fname = Path(upload_file.filename).name
    ext = Path(fname).suffix.lower()
    if ext not in ALLOWED_EXT:
        raise HTTPException(status_code=400, detail="Only .xls .xlsx .csv allowed")

    # rotate old files
    _remove_old_files_for_kind(kind)

    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    raw_name = f"{ts}_{kind}_{fname}"
    raw_path = RAW_UPLOADS_DIR / raw_name

    # Save raw
    try:
        with raw_path.open("wb") as out_f:
            shutil.copyfileobj(upload_file.file, out_f)
    finally:
        try:
            upload_file.file.close()
        except Exception:
            pass

    # Write canonical: keep same extension as original (simpler)
    canonical_path = DATA_DIR / f"active_{kind}{ext}"
    try:
        shutil.copy(raw_path, canonical_path)
    except Exception:
        logger.exception("Failed to write canonical copy for %s", canonical_path)

    info = {
        "raw_saved": str(raw_path),
        "canonical_saved": str(canonical_path),
        "original_filename": fname,
        "size_bytes": raw_path.stat().st_size if raw_path.exists() else None
    }
    logger.info("Uploaded %s: %s", kind, info)
    return info

@app.post("/api/upload/active-employees")
async def upload_active_employees_api(file: UploadFile = File(...)):
    return await upload_active_employees(file)

@app.post("/api/upload/active-contractors")
async def upload_active_contractors_api(file: UploadFile = File(...)):
    return await upload_active_contractors(file)

@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    """
    Upload Active Employee sheet:
      - stores raw to data/raw_uploads and canonical to data/active_employee.*
      - removes previous uploaded employee sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="employee")
        # best-effort broadcast to SSE clients to re-request if they want (UI refresh already attempts fetch)
        try:
            cached = _load_ccure_cache_any()
            if cached:
                # broadcast the unwrapped payload if cache wrapper present
                payload = cached.get("payload") if (isinstance(cached, dict) and "payload" in cached) else cached
                broadcast_ccure_update(payload)
        except Exception:
            pass
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("employee upload failed")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    """
    Upload Active Contractor sheet:
      - stores raw to data/raw_uploads and canonical to data/active_contractor.*
      - removes previous uploaded contractor sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="contractor")
        try:
            cached = _load_ccure_cache_any()
            if cached:
                payload = cached.get("payload") if (isinstance(cached, dict) and "payload" in cached) else cached
                broadcast_ccure_update(payload)
        except Exception:
            pass
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("contractor upload failed")
        raise HTTPException(status_code=500, detail=str(e))

# ---------- map detailed -> resp (unchanged) ----------
def _map_detailed_to_resp(detailed: Dict[str, Any]) -> Dict[str, Any]:
    # unchanged mapping from earlier implementation (kept identical to previous)
    live_h = detailed.get("live_headcount", {}) or {}
    head_h = detailed.get("headcount", {}) or {}
    ccure_active_obj = detailed.get("ccure_active", {}) or {}
    averages_obj = detailed.get("averages", {}) or {}

    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    live_employee = int(live_h.get("employee") or head_h.get("employee") or 0)
    live_contractor = int(live_h.get("contractor") or head_h.get("contractor") or 0)
    total_reported = int(
        live_h.get("currently_present_total")
        or head_h.get("total_visited_today")
        or (live_employee + live_contractor)
        or 0
    )
    total_from_details = int(head_h.get("total_visited_today") or 0)

    mapped_headcount = {
        "total_visited_today": int(head_h.get("total_visited_today") or 0),
        "employee": int(head_h.get("employee") or 0),
        "contractor": int(head_h.get("contractor") or 0),
        "by_location": head_h.get("by_location") or {}
    }

    resp = {
        "date": detailed.get("date"),
        "notes": detailed.get("notes"),
        "live_today": {
            "employee": live_employee,
            "contractor": live_contractor,
            "total_reported": total_reported,
            "total_from_details": total_from_details
        },
        "headcount": mapped_headcount,
        "live_headcount": live_h,
        "ccure_active": {
            "active_employees": ccure_active_obj.get("ccure_active_employees_reported")
                             or ccure_active_obj.get("active_employees"),
            "active_contractors": ccure_active_obj.get("ccure_active_contractors_reported")
                               or ccure_active_obj.get("active_contractors"),
            "ccure_active_employees_reported": ccure_active_obj.get("ccure_active_employees_reported"),
            "ccure_active_contractors_reported": ccure_active_obj.get("ccure_active_contractors_reported")
        },
        "averages": averages_obj
    }
    return resp

# ---------- build verify-like summary (unchanged) ----------
def _build_verify_like_summary_from_mapped(mapped: Dict[str, Any], include_raw: bool = False) -> Dict[str, Any]:
    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    cc = mapped.get("ccure_active", {}) or {}
    head = mapped.get("headcount", {}) or {}
    live_head = mapped.get("live_headcount", {}) or {}
    averages = mapped.get("averages", {}) or {}

    cc_emp = to_int(cc.get("ccure_active_employees_reported") or cc.get("active_employees"))
    cc_con = to_int(cc.get("ccure_active_contractors_reported") or cc.get("active_contractors"))

    head_total = to_int(head.get("total_visited_today") or mapped.get("live_today", {}).get("total_from_details"))
    head_emp = to_int(head.get("employee") or mapped.get("live_today", {}).get("employee"))
    head_con = to_int(head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    live_total = to_int(live_head.get("currently_present_total") or mapped.get("live_today", {}).get("total_reported"))
    live_emp = to_int(live_head.get("employee") or mapped.get("live_today", {}).get("employee"))
    live_con = to_int(live_head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    history_emp_avg = averages.get("history_avg_employee_last_7_days")
    history_con_avg = averages.get("history_avg_contractor_last_7_days")
    history_overall_avg = averages.get("history_avg_overall_last_7_days")

    def pct(n, d):
        try:
            if n is None or d is None:
                return None
            if float(d) == 0:
                return None
            return round((float(n) / float(d)) * 100.0, 2)
        except Exception:
            return None

    summary = {
        "date": mapped.get("date"),
        "ccure_reported": {
            "employees": cc_emp,
            "contractors": cc_con,
            "total_reported": (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None
        },
        "headcount_attendance_summary": {
            "total_visited_today": head_total,
            "employee": head_emp,
            "contractor": head_con,
        },
        "live_headcount_region_clients": {
            "currently_present_total": live_total,
            "employee": live_emp,
            "contractor": live_con,
        },
        "percentages_vs_ccure": {
            "head_employee_pct_vs_ccure_today": pct(head_emp, cc_emp),
            "head_contractor_pct_vs_ccure_today": pct(head_con, cc_con),
            "head_overall_pct_vs_ccure_today": pct(head_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "live_employee_pct_vs_ccure_today": pct(live_emp, cc_emp),
            "live_contractor_pct_vs_ccure_today": pct(live_con, cc_con),
            "live_overall_pct_vs_ccure_today": pct(live_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "history_employee_pct_vs_ccure": pct(history_emp_avg, cc_emp),
            "history_contractor_pct_vs_ccure": pct(history_con_avg, cc_con),
            "history_overall_pct_vs_ccure": pct(history_overall_avg, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None)
        },
        "averages": {
            "history_avg_employee_last_7_days": history_emp_avg,
            "history_avg_contractor_last_7_days": history_con_avg,
            "history_avg_overall_last_7_days": history_overall_avg,
            "avg_headcount_last_7_days_db": averages.get("avg_headcount_last_7_days") or averages.get("avg_headcount_last_7_days_db"),
            "avg_headcount_per_site_last_7_days": averages.get("avg_headcount_per_site_last_7_days"),
            "employee_pct": averages.get("employee_pct"),
            "contractor_pct": averages.get("contractor_pct"),
            "overall_pct": averages.get("overall_pct"),
            **({k: v for k, v in averages.items() if k not in (
                "history_avg_employee_last_7_days",
                "history_avg_contractor_last_7_days",
                "history_avg_overall_last_7_days",
                "avg_headcount_last_7_days",
                "avg_headcount_last_7_days_db",
                "avg_headcount_per_site_last_7_days",
                "employee_pct","contractor_pct","overall_pct"
            )})
        },
        "notes": mapped.get("notes")
    }

    summary["headcount_details"] = {
        "total_visited_today": head_total,
        "employee": head_emp,
        "contractor": head_con,
        "by_location": head.get("by_location") if isinstance(head.get("by_location"), dict) else {}
    }
    summary["live_headcount_details"] = {
        "currently_present_total": live_total,
        "employee": live_emp,
        "contractor": live_con,
        "by_location": live_head.get("by_location") if isinstance(live_head.get("by_location"), dict) else {}
    }

    summary["ccure_active"] = {
        "active_employees": cc.get("active_employees") or cc.get("ccure_active_employees_reported"),
        "active_contractors": cc.get("active_contractors") or cc.get("ccure_active_contractors_reported"),
        "ccure_active_employees_reported": cc.get("ccure_active_employees_reported"),
        "ccure_active_contractors_reported": cc.get("ccure_active_contractors_reported")
    }

    return summary

# -------------------------
# CACHING HELPERS
# -------------------------
def _sha_for_parts(*parts: str):
    s = "|".join([str(p or "") for p in parts])
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

_CCURE_CACHE_DIR = OUTPUT_DIR / "ccure_cache"
_CCURE_CACHE_DIR.mkdir(parents=True, exist_ok=True)
_DURATION_CACHE_DIR = OUTPUT_DIR / "duration_cache"
_DURATION_CACHE_DIR.mkdir(parents=True, exist_ok=True)

def _ccure_cache_path(start_date: Optional[str], end_date: Optional[str]):
    key = _sha_for_parts(start_date or "", end_date or "")
    return _CCURE_CACHE_DIR / f"ccure_verify_cache_{key}.json"

def _duration_cache_path(key: str):
    safe = hashlib.sha256(key.encode("utf-8")).hexdigest()
    return _DURATION_CACHE_DIR / f"duration_cache_{safe}.json"

def _load_ccure_cache(start_date: Optional[str], end_date: Optional[str], max_age_seconds: int):
    p = _ccure_cache_path(start_date, end_date)
    if not p.exists():
        return None
    try:
        st = p.stat()
        age = time.time() - st.st_mtime
        if age > max_age_seconds:
            return None
        with p.open("r", encoding="utf-8") as fh:
            return json.load(fh)
    except Exception:
        logger.exception("Failed reading ccure cache at %s", p)
        return None

def _load_ccure_cache_any():
    # return the newest cache wrapper (useful for SSE initial push)
    try:
        files = sorted(_CCURE_CACHE_DIR.glob("ccure_verify_cache_*.json"), key=lambda p: p.stat().st_mtime, reverse=True)
        if not files:
            return None
        with files[0].open("r", encoding="utf-8") as fh:
            return json.load(fh)
    except Exception:
        return None

def _save_ccure_cache(start_date: Optional[str], end_date: Optional[str], payload: dict):
    p = _ccure_cache_path(start_date, end_date)
    try:
        enc = jsonable_encoder(payload)
        with p.open("w", encoding="utf-8") as fh:
            json.dump({"cached_at": datetime.utcnow().isoformat(), "payload": enc}, fh)
    except Exception:
        logger.exception("Failed writing ccure cache to %s", p)

def _load_duration_cache_for_key(cache_path: Path, max_age_seconds: int):
    if not cache_path.exists():
        return None
    try:
        age = time.time() - cache_path.stat().st_mtime
        if age > max_age_seconds:
            return None
        with cache_path.open("r", encoding="utf-8") as fh:
            return json.load(fh)
    except Exception:
        logger.exception("Failed reading duration cache at %s", cache_path)
        return None

def _save_duration_cache(cache_path: Path, payload: dict):
    try:
        enc = jsonable_encoder(payload)
        with cache_path.open("w", encoding="utf-8") as fh:
            json.dump({"cached_at": datetime.utcnow().isoformat(), "payload": enc}, fh)
    except Exception:
        logger.exception("Failed writing duration cache to %s", cache_path)

# ---------- /ccure/verify (updated with caching + robust return) -----
@app.get("/ccure/verify")
async def ccure_verify(
    raw: bool = Query(False, description="if true, include the raw compute payload for debugging"),
    start_date: Optional[str] = Query(None, description="YYYY-MM-DD start date (inclusive)"),
    end_date: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    refresh: bool = Query(False, description="If true, force recompute and update cache")
):
    """
    Synchronous verification endpoint. Compute is executed in thread with a timeout.
    If compute_visit_averages fails or times out, fall back to build_ccure_averages().
    Caching: cached results are stored on disk and returned quickly unless refresh=True.
    """
    try:
        # caching TTL: once per day (86400 seconds)
        CCURE_CACHE_TTL = 86400  # 24 hours
        if not refresh:
            cached = _load_ccure_cache(start_date, end_date, CCURE_CACHE_TTL)
            if cached and isinstance(cached, dict) and "payload" in cached:
                payload = cached["payload"]
                # If frontend asks for raw and cached doesn't have it, we still return cached payload (raw optional).
                return JSONResponse(payload)

        detailed = None
        try:
            # import safely
            from ccure_compare_service import compute_visit_averages
            loop = asyncio.get_running_loop()
            # run compute_visit_averages in thread with timeout (use REGION_TIMEOUT_SECONDS as base)
            compute_fn = functools.partial(compute_visit_averages, start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
            try:
                # allow a small buffer above REGION_TIMEOUT_SECONDS
                timeout_seconds = max(5, REGION_TIMEOUT_SECONDS + 5)
                detailed = await asyncio.wait_for(loop.run_in_executor(None, compute_fn), timeout=timeout_seconds)
            except asyncio.TimeoutError:
                logger.warning("compute_visit_averages timed out after %s seconds; falling back", timeout_seconds)
                detailed = None
            except Exception:
                logger.exception("compute_visit_averages raised; falling back to build_ccure_averages()")
                detailed = None
        except Exception:
            logger.exception("compute_visit_averages import or invocation failed; falling back")
            detailed = None

        if isinstance(detailed, dict):
            mapped = _map_detailed_to_resp(detailed)
            summary = _build_verify_like_summary_from_mapped(mapped, include_raw=raw)
            if raw:
                summary["raw"] = detailed
            # cache and broadcast
            try:
                _save_ccure_cache(start_date, end_date, summary)
                broadcast_ccure_update(summary)
            except Exception:
                logger.exception("Failed to cache/broadcast compute result")
            return JSONResponse(summary)
        else:
            # fallback to DB/attendance summary based computation (safe)
            fallback = build_ccure_averages(start_date, end_date)
            mapped_fallback = {
                "date": fallback.get("date"),
                "notes": fallback.get("notes"),
                "live_today": fallback.get("live_today", {}),
                "headcount": {
                    "total_visited_today": fallback.get("live_today", {}).get("total_from_details") or fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": fallback.get("by_location") or {}
                },
                "live_headcount": {
                    "currently_present_total": fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": fallback.get("by_location") or {}
                },
                "ccure_active": fallback.get("ccure_active", {}),
                "averages": fallback.get("averages", {})
            }

            summary = _build_verify_like_summary_from_mapped(mapped_fallback, include_raw=raw)
            if raw:
                summary["raw"] = mapped_fallback

            # cache and broadcast
            try:
                _save_ccure_cache(start_date, end_date, summary)
                broadcast_ccure_update(summary)
            except Exception:
                logger.exception("Failed to cache/broadcast fallback result")

            return JSONResponse(summary)
    except Exception as e:
        logger.exception("ccure_verify top-level failure")
        # Return a 500 with JSON detail (avoid empty response)
        return JSONResponse({"detail": f"ccure verify error: {e}"}, status_code=500)

# provide API-prefixed alias for Vite /api proxy issues
@app.get("/api/ccure/verify")
async def api_ccure_verify(
    raw: bool = Query(False),
    start_date: Optional[str] = Query(None),
    end_date: Optional[str] = Query(None),
    refresh: bool = Query(False)
):
    return await ccure_verify(raw=raw, start_date=start_date, end_date=end_date, refresh=refresh)

# ---------- /ccure/compare -> uses data_compare_service.compare_ccure_vs_sheets ----------
@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from data_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)

@app.get("/ccure/export_uploaded")
def ccure_export_uploaded():
    """
    Export the currently uploaded canonical sheets into one workbook.
    Returns JSON: { status: "ok", report_path: "<filename>" }
    Downloadable at /ccure/report/{filename}
    """
    try:
        from data_compare_service import export_uploaded_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed for export_uploaded")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    fname = export_uploaded_sheets()
    if not fname:
        raise HTTPException(status_code=500, detail="failed to export uploaded sheets")
    return JSONResponse({"status": "ok", "report_path": fname})

@app.get("/ccure/compare_v2")
def ccure_compare_v2(
    mode: str = Query("full"),
    stats_detail: str = Query("ActiveProfiles"),
    limit_list: int = Query(200, ge=1, le=5000),
    export: bool = Query(False),
    region_filter: Optional[str] = Query(None, description="Region code e.g. APAC"),
    location_city: Optional[str] = Query(None, description="Location city e.g. Pune"),
    location_state: Optional[str] = Query(None, description="Location state/province"),
    location_description: Optional[str] = Query(None, description="Location Description"),
    week_ref_date: Optional[str] = Query(None, description="YYYY-MM-DD for week (Mon-Fri)")
):
    try:
        from data_compare_service_v2 import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service_v2 import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(
        mode=mode,
        stats_detail=stats_detail,
        limit_list=limit_list,
        export=export,
        region_filter=region_filter,
        location_city=location_city,
        location_state=location_state,
        location_description=location_description,
        week_ref_date=week_ref_date
    )
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)

@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = OUTPUT_DIR / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(str(full),
                            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            filename=safe_name)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")

# -------------------------------------------------------------------------------
# DURATION endpoint (with updated, stricter shift/sessionization rules + overrides)
# -------------------------------------------------------------------------------

# Overrides storage (simple JSON file)
_OVERRIDES_PATH = OUTPUT_DIR / "duration_overrides.json"
def _load_overrides() -> Dict[str, Any]:
    try:
        if not _OVERRIDES_PATH.exists():
            return {}
        with _OVERRIDES_PATH.open("r", encoding="utf-8") as fh:
            return json.load(fh)
    except Exception:
        logger.exception("Failed to load overrides file; returning empty")
        return {}

def _save_overrides(overrides: Dict[str, Any]) -> None:
    try:
        with _OVERRIDES_PATH.open("w", encoding="utf-8") as fh:
            json.dump(overrides, fh, indent=2, default=str)
    except Exception:
        logger.exception("Failed to persist overrides")

def _override_key(region: str, person_uid: str, date_iso: str) -> str:
    return f"{(region or '').lower()}|{(person_uid or '').strip()}|{date_iso}"

@app.post("/duration/override")
def duration_override(payload: Dict[str, Any] = Body(...)):
    """
    Payload:
      {
        "region": "apac",
        "person_uid": "<person_uid>",
        "date": "YYYY-MM-DD",
        "start_ts": "<ISO or epoch ms>",
        "end_ts": "<ISO or epoch ms>",
        "reason": "user note",
        "user": "operator name (optional)"
      }
    Server computes seconds and stores override. Overrides are applied when /duration is called later.
    """
    try:
        region = (payload.get("region") or "").lower()
        person_uid = payload.get("person_uid")
        date_iso = payload.get("date")
        start_ts = payload.get("start_ts")
        end_ts = payload.get("end_ts")
        reason = payload.get("reason") or ""
        user = payload.get("user") or "unknown"

        if not region or not person_uid or not date_iso or not start_ts or not end_ts:
            raise HTTPException(status_code=400, detail="region, person_uid, date, start_ts and end_ts are required")

        def _parse_ts(x):
            # accept ISO-like or numeric epoch (ms or s)
            try:
                if isinstance(x, (int, float)):
                    # assume epoch seconds if small, ms if large
                    v = float(x)
                    if v > 1e12:
                        return datetime.fromtimestamp(v / 1000.0)
                    if v > 1e9:
                        return datetime.fromtimestamp(v)
                    return datetime.fromtimestamp(v)
                if isinstance(x, str):
                    x = x.strip()
                    # numeric string?
                    if re.match(r"^\d+$", x):
                        v = int(x)
                        if v > 1e12:
                            return datetime.fromtimestamp(v / 1000.0)
                        return datetime.fromtimestamp(v)
                    # ISO
                    try:
                        return datetime.fromisoformat(x.replace("Z", "+00:00"))
                    except Exception:
                        # try pandas
                        try:
                            return pd.to_datetime(x).to_pydatetime()
                        except Exception:
                            return None
                return None
            except Exception:
                return None

        sdt = _parse_ts(start_ts)
        edt = _parse_ts(end_ts)
        if sdt is None or edt is None:
            raise HTTPException(status_code=400, detail="Could not parse start_ts or end_ts")

        if edt < sdt:
            # swap or reject; we will swap for user-friendliness
            sdt, edt = edt, sdt

        seconds = max(0, int((edt - sdt).total_seconds()))
        key = _override_key(region, person_uid, date_iso)

        overrides = _load_overrides()
        overrides[key] = {
            "region": region,
            "person_uid": person_uid,
            "date": date_iso,
            "start_ts": sdt.isoformat(),
            "end_ts": edt.isoformat(),
            "seconds": seconds,
            "reason": reason,
            "user": user,
            "updated_at": datetime.utcnow().isoformat(),
        }
        _save_overrides(overrides)
        return JSONResponse({"status": "ok", "key": key, "seconds": seconds})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("duration_override failed")
        raise HTTPException(status_code=500, detail=str(e))

# ... (duration endpoints + duration_report module kept unchanged from previous code) ...
# For brevity the rest of the duration handling and inlined duration_report module 
# is unchanged from the earlier file — leave it as-is in your copy.
# (Everything above is the critical fix for SSE/cache unwrapping.)





















// C:\Users\W0024618\Desktop\global-page\frontend\src\pages\GlobalPage.jsx
import React, { useState, useEffect, useRef } from 'react';
import {
  Box, Typography, CircularProgress, IconButton, Button, Paper, Divider,
  LinearProgress, Snackbar, Alert, List, ListItem, ListItemText, Tooltip
} from '@mui/material';
import HomeIcon from '@mui/icons-material/Home';
import DescriptionIcon from '@mui/icons-material/Description';
import UploadFileIcon from '@mui/icons-material/UploadFile';
import MapChart from '../components/MapChart.jsx';
import api from '../api';
import { useAuth } from '../context/AuthContext';
import { useNavigate, Link } from 'react-router-dom';

import TimerIcon from "@mui/icons-material/AccessTime"; // duration icon
import { Link as RouterLink } from "react-router-dom";

/*
  Important:
  - Do NOT mix /api/headcount and /api/ccure/verify.
  - Region cards (APAC/EMEA/LACA/NAMER) come only from /api/headcount.
  - Live vs CCURE Summary now comes from /api/ccure/verify?raw=true.
  - Initial region totals are zero (keeps previous UI behaviour).
  - We implement polling for headcount and SSE for ccure/stream (realtime via SSE).
*/

export default function GlobalPage() {
  const navigate = useNavigate();
  const auth = useAuth();

  // -----------------------
  // Helpers: normalization + cache
  // -----------------------
  const CACHE_KEY = 'globalPage.averages';

  // normalize incoming backend payload into a canonical object we can read from reliably.
  // handles shapes like:
  //  - top-level keys (date, ccure_reported, averages, ...)
  //  - { raw: { ... } }
  //  - nested averages under .averages or .raw.averages
  const normalizePayload = (p) => {
    if (!p || typeof p !== 'object') return p;
    const raw = (p.raw && typeof p.raw === 'object') ? p.raw : null;
    // start with raw (if present) so raw values are available, then overlay top-level keys
    const merged = { ...(raw || {}), ...p };

    // build a merged "averages" object made from any averages we find
    const a1 = (raw && raw.averages && typeof raw.averages === 'object') ? raw.averages : {};
    const a2 = (p.averages && typeof p.averages === 'object') ? p.averages : {};
    merged.averages = { ...a1, ...a2 };

    return merged;
  };

  const saveAveragesToCache = (obj) => {
    try {
      if (!obj) {
        sessionStorage.removeItem(CACHE_KEY);
        return;
      }
      sessionStorage.setItem(CACHE_KEY, JSON.stringify(obj));
    } catch (e) {
      console.warn('Failed to save averages to sessionStorage', e);
    }
  };

  const loadAveragesFromCache = () => {
    try {
      const s = sessionStorage.getItem(CACHE_KEY);
      if (!s) return null;
      const parsed = JSON.parse(s);
      return parsed;
    } catch (e) {
      console.warn('Failed to load averages from sessionStorage', e);
      return null;
    }
  };

  // helper to attempt navigation or show access denied
  const attemptNav = (permCandidates = [], actionFn, label) => {
    const candidates = Array.isArray(permCandidates) ? permCandidates : [permCandidates];

    const allowed = candidates.some(p => {
      try {
        return !!auth?.hasPermission?.(p);
      } catch (e) {
        console.warn('hasPermission threw error for', p, e);
        return false;
      }
    });

    if (!allowed) {
      setSnack({ open: true, severity: 'warning', message: `Access denied to ${label || 'this resource'}` });
      return false;
    }

    try {
      actionFn && actionFn();
    } catch (e) {
      console.error('Navigation/action failed', e);
      setSnack({ open: true, severity: 'error', message: `Action failed: ${label || ''}` });
    }
    return true;
  };

  const [counts, setCounts] = useState({ apac: 0, emea: 0, laca: 0, namer: 0 });
  const [selected, setSelected] = useState('global');

  // Averages/ccure state (left panel)
  const [averages, setAverages] = useState(null);
  const [loadingAverages, setLoadingAverages] = useState(true);
  const [averagesError, setAveragesError] = useState(null);

  // upload state
  const [uploading, setUploading] = useState(false);
  const [uploadResult, setUploadResult] = useState(null);
  const [uploadError, setUploadError] = useState(null);

  const [uploadedEmployee, setUploadedEmployee] = useState(false);
  const [uploadedContractor, setUploadedContractor] = useState(false);

  const [exportReportPath, setExportReportPath] = useState(null);

  // top-row file inputs
  const fileInputEmpRef = useRef();
  const fileInputContrRef = useRef();
  const [snack, setSnack] = useState({ open: false, severity: 'info', message: '' });

  // date-range state for top-right controls
  const [startDate, setStartDate] = useState('');
  const [endDate, setEndDate] = useState('');

  // Polling refs for safe scheduling and backoff
  const headcountRef = useRef({ timerId: null, failureCount: 0, isFetching: false });
  const averagesRef = useRef({ timerId: null, failureCount: 0, isFetching: false });

  const [showCenterUploads, setShowCenterUploads] = useState(false);

  // small wrapper to always normalize + cache
  const setAveragesNormalized = (payload) => {
    try {
      const norm = normalizePayload(payload);
      setAverages(norm);
      saveAveragesToCache(norm);
    } catch (e) {
      console.warn('setAveragesNormalized failed', e);
      setAverages(payload);
    }
  };

  // -----------------------
  // HEADCOUNT POLLING ONLY (unchanged)
  // -----------------------
  useEffect(() => {
    let mounted = true;

    const fetchHeadcount = async () => {
      if (!mounted) return;
      if (headcountRef.current.isFetching) return;
      headcountRef.current.isFetching = true;

      try {
        const res = await api.get('/headcount');
        if (!mounted) return;
        const d = res.data;
        if (d && typeof d === 'object') {
          const newCounts = {
            apac: Number(d.apac || 0),
            emea: Number(d.emea || 0),
            laca: Number(d.laca || 0),
            namer: Number(d.namer || 0),
          };
          setCounts(prev => {
            if (
              prev.apac === newCounts.apac &&
              prev.emea === newCounts.emea &&
              prev.laca === newCounts.laca &&
              prev.namer === newCounts.namer
            ) {
              return prev;
            }
            return newCounts;
          });
        } else {
          console.warn('[headcount] unexpected response shape - ignoring', d);
        }
        headcountRef.current.failureCount = 0;
      } catch (err) {
        headcountRef.current.failureCount = (headcountRef.current.failureCount || 0) + 1;
        console.warn('[headcount] fetch failed:', err?.message || err);
      } finally {
        headcountRef.current.isFetching = false;
        const f = headcountRef.current.failureCount || 0;
        const backoffMs = 15000 * Math.pow(2, Math.min(Math.max(f - 1, 0), 4)); // 15s..240s
        headcountRef.current.timerId = setTimeout(fetchHeadcount, backoffMs);
      }
    };

    fetchHeadcount();

    return () => {
      mounted = false;
      if (headcountRef.current.timerId) clearTimeout(headcountRef.current.timerId);
      headcountRef.current.isFetching = false;
    };
  }, []); // run once

  // AVERAGES: use SSE (direct to Python backend) with fallback initial fetch
  useEffect(() => {
    let stopped = false;
    let es = null;
    let backoff = 1000;

    // Restore cached averages immediately so UI doesn't blank
    const cached = loadAveragesFromCache();
    if (cached) {
      setAverages(cached);
      // show that we're refreshing but DO NOT clear the displayed values
      setLoadingAverages(false);
      setAveragesError(null);
    } else {
      setLoadingAverages(true);
      setAveragesError(null);
    }

    // Allow override via VITE_PY_BACKEND; otherwise assume python at :8000
    const PY_BACKEND = (import.meta.env.VITE_PY_BACKEND || `${window.location.protocol}//${window.location.hostname}:8000`).replace(/\/$/, '');

    const connect = () => {
      if (stopped) return;
      try {
        // try proxy-relative path first (helps Vite dev server proxy /api)
        es = new EventSource(`/api/ccure/stream`);
      } catch (err) {
        console.warn('SSE creation (relative /api) failed', err);
        es = null;
      }

      // fallback to direct backend if proxy path didn't work
      if (!es) {
        try {
          es = new EventSource(`${PY_BACKEND}/ccure/stream`);
        } catch (err) {
          console.warn('SSE creation direct failed', err);
          es = null;
        }
      }

      if (!es) {
        // no SSE available — rely on initialFetch polling/fetch
        return;
      }

      es.onopen = () => {
        console.info('[SSE] connected to', es.url || `${PY_BACKEND}/ccure/stream`);
        backoff = 1000;
        setAveragesError(null);
      };

      es.onmessage = (evt) => {
        try {
          const payload = JSON.parse(evt.data);
          setAveragesNormalized(payload);
          setLoadingAverages(false);
          setAveragesError(null);
        } catch (e) {
          console.warn('Failed to parse SSE message', e);
        }
      };

      es.onerror = (err) => {
        console.warn('[SSE] error/closed, attempting reconnect', err);
        try { es.close(); } catch (e) { }
        es = null;
        if (stopped) return;
        setTimeout(() => {
          backoff = Math.min(backoff * 2, 30000);
          connect();
        }, backoff);
      };
    };

    const initialFetch = async () => {
      setAveragesError(null);
      setLoadingAverages(true);
      try {
        // primary: use api helper (likely proxied /api)
        const res = await api.get('/ccure/verify?raw=true');
        setAveragesNormalized(res.data);
        setLoadingAverages(false);
        setAveragesError(null);
      } catch (err) {
        // fallback: direct backend
        console.warn('initial /ccure/verify via api failed, falling back to direct backend', err);
        try {
          const resp = await fetch(`${PY_BACKEND}/ccure/verify?raw=true`);
          if (resp.ok) {
            const data = await resp.json();
            setAveragesNormalized(data);
            setLoadingAverages(false);
            setAveragesError(null);
            return;
          } else {
            console.warn('Direct backend verify returned non-ok', resp.status);
          }
        } catch (e) {
          console.warn('Direct backend verify attempt failed', e);
        }
        setLoadingAverages(false);
        setAveragesError(err);
      }
    };

    // start fetch + sse connection
    initialFetch();
    connect();

    return () => {
      stopped = true;
      if (es) {
        try { es.close(); } catch (e) { }
        es = null;
      }
    };
  }, []);

  // -----------------------
  // Upload helper (updated to use fetch, kept robust)
  // -----------------------
  const handleUpload = async (file, type) => {
    if (!file) return;
    const endpoint = type === 'employee' ? '/upload/active-employees' : '/upload/active-contractors';

    const PY_BACKEND = (import.meta.env.VITE_PY_BACKEND || `${window.location.protocol}//${window.location.hostname}:8000`).replace(/\/$/, '');
    const url = `${PY_BACKEND}${endpoint}`;

    const fd = new FormData();
    fd.append('file', file, file.name);

    setUploading(true);
    setUploadResult(null);
    setUploadError(null);

    try {
      console.info('Uploading to', url, file.name);
      const resp = await fetch(url, {
        method: 'POST',
        body: fd,
      });

      const rawText = await resp.text();
      let data = null;
      try { data = rawText ? JSON.parse(rawText) : null; } catch (e) { data = { raw: rawText }; }

      console.info('Upload response', resp.status, resp.statusText, data);

      if (!resp.ok) {
        throw new Error(`Upload failed HTTP ${resp.status} ${resp.statusText} - ${JSON.stringify(data)}`);
      }

      setUploadResult(data);
      setSnack({ open: true, severity: 'success', message: `Active Sheet Updated successfully: ${file.name}` });

      // mark canonical presence if backend returned detail.canonical_saved
      const saved = data && data.detail && (data.detail.canonical_saved || data.detail.canonical_saved === "");
      if (type === 'employee') setUploadedEmployee(!!saved);
      if (type === 'contractor') setUploadedContractor(!!saved);

      // best-effort refresh of averages/headcount
      try {
        const r1 = await fetch(`${PY_BACKEND}/ccure/verify?raw=true`);
        if (r1.ok) {
          const json = await r1.json();
          setAveragesNormalized(json);
        }
      } catch (e) { console.warn('refresh verify failed', e); }

      try {
        const r2 = await fetch(`${PY_BACKEND}/headcount`);
        if (r2.ok) {
          const d = await r2.json();
          if (d && typeof d === 'object') {
            setCounts({
              apac: Number(d.apac || 0),
              emea: Number(d.emea || 0),
              laca: Number(d.laca || 0),
              namer: Number(d.namer || 0)
            });
          }
        }
      } catch (e) { console.warn('refresh headcount failed', e); }

    } catch (err) {
      console.error('Upload failed', err);
      setUploadError(err);
      setSnack({ open: true, severity: 'error', message: `Upload failed: ${file.name} — ${err.message}` });
    } finally {
      setUploading(false);
    }
  };

  // --- Helpers: find CCURE arrays in 'averages' and generate CSV ---
  const _escapeCsv = (v) => {
    if (v === null || v === undefined) return '';
    if (typeof v === 'object') {
      try { v = JSON.stringify(v); } catch { v = String(v); }
    }
    const s = String(v).replace(/"/g, '""');
    return `"${s}"`;
  };

  // Find candidate arrays inside the averages object that look like CCURE comparison rows
  const findCcureArrays = (obj) => {
    const found = [];
    if (!obj || typeof obj !== 'object') return found;

    const inspect = (parentKey, val) => {
      if (!val) return;
      if (Array.isArray(val) && val.length > 0 && typeof val[0] === 'object') {
        const sample = val[0];
        // heuristic: many ccure rows have ccure_key or EmployeeID or EmpName fields
        if ('ccure_key' in sample || 'EmployeeID' in sample || 'EmpName' in sample) {
          found.push({ key: parentKey, arr: val });
        }
      } else if (typeof val === 'object') {
        for (const k of Object.keys(val)) {
          inspect(parentKey ? `${parentKey}.${k}` : k, val[k]);
        }
      }
    };

    inspect('', obj);
    return found;
  };

  // Build CSV rows with the requested columns:
  const generateCcureCompareCSV = (averagesPayload) => {
    if (!averagesPayload || typeof averagesPayload !== 'object') return null;

    const candidates = findCcureArrays(averagesPayload);
    if (!candidates || candidates.length === 0) return null;

    // choose the first plausible array
    const rows = candidates[0].arr;

    if (!Array.isArray(rows) || rows.length === 0) return null;

    const headers = ['ccure_key', 'EmployeeID', 'EmpName', 'VendorCompany', 'PersonnelType', 'Manager_Name', 'Profile_Disabled', 'Employee_Status'];
    const lines = [headers.map(h => _escapeCsv(h)).join(',')];

    const readField = (r, name) => {
      if (!r) return '';
      if (r[name] !== undefined && r[name] !== null) return r[name];
      if (r.raw && typeof r.raw === 'object' && r.raw[name] !== undefined && r.raw[name] !== null) return r.raw[name];
      if (typeof r.raw === 'string') {
        try {
          const parsed = JSON.parse(r.raw);
          if (parsed && parsed[name] !== undefined) return parsed[name];
        } catch { }
      }
      return '';
    };

    for (const r of rows) {
      const personnelType = readField(r, 'PersonnelType') || readField(r, 'Personnel_Type') || readField(r, 'PersonnelTypeName') || '';

      const ccure_key = readField(r, 'ccure_key') || readField(r, 'CcureKey') || readField(r, 'EmployeeID') || '';
      const employeeId = readField(r, 'EmployeeID') || readField(r, 'Employee_Id') || '';
      const empName = readField(r, 'EmpName') || readField(r, 'EmployeeName') || readField(r, 'Name') || '';
      const vendorCompany = readField(r, 'VendorCompany') || readField(r, 'Vendor_Company') || '';
      const managerName = readField(r, 'Manager_Name') || readField(r, 'ManagerName') || '';
      let profileDisabled = readField(r, 'Profile_Disabled');
      if (profileDisabled === true || profileDisabled === false) profileDisabled = String(profileDisabled);
      const employeeStatus = readField(r, 'Employee_Status') || readField(r, 'Status') || '';

      const rowValues = [
        ccure_key,
        employeeId,
        empName,
        vendorCompany,
        personnelType,
        managerName,
        profileDisabled,
        employeeStatus,
      ];

      lines.push(rowValues.map(v => _escapeCsv(v)).join(','));
    }

    const csvContent = lines.join('\n');
    return csvContent;
  };

  // Export: request server to generate compare report and download the xlsx (binary)
  const exportUploadedSheets = async () => {
    setUploading(true);
    try {
      const PY_BACKEND = (import.meta.env.VITE_PY_BACKEND || `${window.location.protocol}//${window.location.hostname}:8000`).replace(/\/$/, '');

      // attempt server-side generation as before
      let genJson = null;
      try {
        const genRes = await fetch(`${PY_BACKEND}/ccure/compare?export=true`, {
          method: 'GET',
        });

        try {
          genJson = await genRes.json();
        } catch (e) {
          const txt = await genRes.text().catch(() => '');
          throw new Error(`Export generation returned non-JSON response: ${txt}`);
        }

        if (!genRes.ok || !genJson || !genJson.report_path) {
          const msg = (genJson && (genJson.detail || genJson.error)) || JSON.stringify(genJson || {});
          console.warn('Server export generation failed or returned unexpected payload:', msg);
          setSnack({ open: true, severity: 'warning', message: 'Server export generation failed; attempting client CSV export' });
        } else {
          // server returned a report_path — attempt to download it
          const reportPath = genJson.report_path;
          setExportReportPath(reportPath);
          setSnack({ open: true, severity: 'success', message: 'Export created — downloading now' });

          try {
            const dlUrl = `${PY_BACKEND}/ccure/report/${encodeURIComponent(reportPath)}`;
            const fileRes = await fetch(dlUrl, { method: 'GET' });

            if (!fileRes.ok) {
              const txt = await fileRes.text().catch(() => '');
              console.warn('Failed to download server report:', txt);
            } else {
              const blob = await fileRes.blob();
              const blobUrl = window.URL.createObjectURL(blob);
              const a = document.createElement('a');
              a.href = blobUrl;
              a.download = reportPath || 'missing_vs_ccure.xlsx';
              document.body.appendChild(a);
              a.click();
              a.remove();
              window.URL.revokeObjectURL(blobUrl);
            }
          } catch (e) {
            console.warn('Server report download failed:', e);
          }
        }
      } catch (err) {
        // log and fall through to client-side CSV
        console.warn('exportUploadedSheets server call failed:', err);
      }

      // --- Client-side CSV fallback / supplemental export with requested columns ---
      try {
        const csv = generateCcureCompareCSV(averages || {});
        if (csv) {
          const blob = new Blob([csv], { type: 'text/csv;charset=utf-8;' });
          const url = window.URL.createObjectURL(blob);
          const a = document.createElement('a');
          a.href = url;
          // prefer a descriptive filename
          const fname = `missing_vs_ccure_custom_${(new Date()).toISOString().slice(0, 10)}.csv`;
          a.download = fname;
          document.body.appendChild(a);
          a.click();
          a.remove();
          window.URL.revokeObjectURL(url);
          setSnack({ open: true, severity: 'success', message: 'Custom CCURE CSV downloaded' });
        } else {
          setSnack(prev => ({ ...prev, open: true, severity: 'info', message: 'No CCURE rows found in current averages payload for CSV export' }));
        }
      } catch (e) {
        console.error('Client CSV generation failed', e);
        setSnack(prev => ({ ...prev, open: true, severity: 'error', message: `CSV generation failed: ${e?.message || e}` }));
      }

    } catch (err) {
      console.error('exportUploadedSheets error', err);
      setSnack({ open: true, severity: 'error', message: `Export failed: ${err.message || err}` });
    } finally {
      setUploading(false);
    }
  };

  const onChooseEmployeeFile = (e) => { const f = e.target.files && e.target.files[0]; if (f) handleUpload(f, 'employee'); e.target.value = null; };
  const onChooseContractorFile = (e) => { const f = e.target.files && e.target.files[0]; if (f) handleUpload(f, 'contractor'); e.target.value = null; };

  // apply date range to re-fetch /ccure/verify
  const applyDateRange = async (opts = { refresh: false }) => {
    if (!startDate || !endDate) {
      setSnack({ open: true, severity: 'warning', message: 'Please select start and end dates' });
      return;
    }
    setAveragesError(null);
    setLoadingAverages(true);
    try {
      const refreshFlag = opts.refresh ? '&refresh=true' : '';
      const res = await api.get(`/ccure/verify?raw=true&start_date=${startDate}&end_date=${endDate}${refreshFlag}`);
      setAveragesNormalized(res.data);
      setLoadingAverages(false);
      setSnack({ open: true, severity: 'success', message: 'Averages updated' });
    } catch (err) {
      console.warn('applyDateRange failed via api', err);
      // fallback to direct backend
      try {
        const PY_BACKEND = (import.meta.env.VITE_PY_BACKEND || `${window.location.protocol}//${window.location.hostname}:8000`).replace(/\/$/, '');
        const url = `${PY_BACKEND}/ccure/verify?raw=true&start_date=${startDate}&end_date=${endDate}${opts.refresh ? '&refresh=true' : ''}`;
        const resp = await fetch(url);
        if (resp.ok) {
          const d = await resp.json();
          setAveragesNormalized(d);
          setLoadingAverages(false);
          setSnack({ open: true, severity: 'success', message: 'Averages updated (direct backend)' });
          return;
        }
      } catch (e) {
        console.warn('applyDateRange fallback failed', e);
      }

      setLoadingAverages(false);
      setAveragesError(err);
      setSnack({ open: true, severity: 'error', message: 'Failed to update averages' });
    }
  };

  const getFromPayload = (path, fallback = null) => {
    if (!averages || typeof averages !== 'object') return fallback;

    // quick resolver that checks a few likely roots (top-level, averages.*)
    const tryPathOn = (obj, fullPath) => {
      try {
        return fullPath.split('.').reduce((o, key) => (o && Object.prototype.hasOwnProperty.call(o, key) ? o[key] : undefined), obj);
      } catch {
        return undefined;
      }
    };

    // try top-level and the merged averages object
    const candidates = [
      averages,
      averages.averages,        // e.g. some payloads nest under averages
    ];

    for (const root of candidates) {
      if (!root) continue;
      const val = tryPathOn(root, path);
      if (val !== undefined) return val;
    }

    return fallback;
  };

  // Derived values (using the robust getter)
  const ccureActiveEmployees =
    getFromPayload('ccure_reported.employees',
      getFromPayload('ccure_active.active_employees',
        getFromPayload('ccure_active.ccure_active_employees_reported', null)
      )
    );

  const ccureActiveContractors =
    getFromPayload('ccure_reported.contractors',
      getFromPayload('ccure_active.active_contractors',
        getFromPayload('ccure_active.ccure_active_contractors_reported', null)
      )
    );

  const headTotalVisited = getFromPayload('headcount_attendance_summary.total_visited_today',
    getFromPayload('headcount_details.total_visited_today', null)
  );
  const headEmployee = getFromPayload('headcount_attendance_summary.employee',
    getFromPayload('headcount_details.employee', null)
  );
  const headContractor = getFromPayload('headcount_attendance_summary.contractor',
    getFromPayload('headcount_details.contractor', null)
  );

  const liveCurrentTotal = getFromPayload('live_headcount_region_clients.currently_present_total',
    getFromPayload('live_headcount_details.currently_present_total',
      null
    )
  );
  const liveEmp = getFromPayload('live_headcount_region_clients.employee',
    getFromPayload('live_headcount_details.employee', null)
  );
  const liveContr = getFromPayload('live_headcount_region_clients.contractor',
    getFromPayload('live_headcount_details.contractor', null)
  );

  const empPct = getFromPayload('percentages_vs_ccure.head_employee_pct_vs_ccure_today',
    getFromPayload('averages.head_emp_pct_vs_ccure_today', null)
  );
  const conPct = getFromPayload('percentages_vs_ccure.head_contractor_pct_vs_ccure_today',
    getFromPayload('averages.head_contractor_pct_vs_ccure_today', null)
  );
  const overallPct = getFromPayload('percentages_vs_ccure.head_overall_pct_vs_ccure_today',
    getFromPayload('averages.headcount_overall_pct_vs_ccure_today', null)
  );

  const avg7 = getFromPayload('averages.history_avg_overall_last_7_days',
    getFromPayload('averages.avg_headcount_last_7_days',
      getFromPayload('averages.avg_headcount_last_7_days_db', null)
    )
  );

  const respDate = getFromPayload('date', null);

  const locationAvgsObj = getFromPayload('history_avg_by_location_last_7_days',
    getFromPayload('averages.history_avg_by_location_last_7_days',
      getFromPayload('raw.averages.history_avg_by_location_last_7_days', {})
    )
  );

  const locationAvgsList = React.useMemo(() => {
    if (!locationAvgsObj || typeof locationAvgsObj !== 'object') return [];
    const arr = Object.entries(locationAvgsObj).map(([loc, vals]) => {
      return {
        location: loc,
        avg_employee_last_7_days: vals.avg_employee_last_7_days ?? vals.history_avg_employee_last_7_days ?? vals.avg_employee ?? null,
        avg_contractor_last_7_days: vals.avg_contractor_last_7_days ?? vals.history_avg_contractor_last_7_days ?? vals.avg_contractor ?? null,
        avg_overall_last_7_days: vals.avg_overall_last_7_days ?? vals.history_avg_overall_last_7_days ?? vals.avg_overall ?? null,
        history_days_counted: vals.history_days_counted ?? null
      };
    });
    arr.sort((a, b) => (b.avg_overall_last_7_days ?? -Infinity) - (a.avg_overall_last_7_days ?? -Infinity));
    return arr;
  }, [locationAvgsObj]);

  const globalCount = Number((counts.apac || 0)) + Number((counts.emea || 0)) + Number((counts.laca || 0)) + Number((counts.namer || 0));

  const hideScrollbarSx = {
    overflowY: 'auto',
    '&::-webkit-scrollbar': { width: 0, height: 0 },
    scrollbarWidth: 'none',
    msOverflowStyle: 'none',
  };

  // ---- New: derive user name parts for right-side header display
  const userFullName = auth?.user ? (auth.user.EmployeeName || auth.user.username || '') : '';
  const nameParts = (userFullName || '').toString().split(/\s+/).filter(Boolean);
  const firstName = nameParts[0] || '';
  const restName = nameParts.slice(1).join(' ') || '';

  // Render
  return (
    <Box sx={{ display: 'flex', flexDirection: 'column', height: '100vh', overflow: 'hidden', bgcolor: 'background.default' }}>
      {/* Header */}
      <Box px={2} py={1} sx={{ backgroundColor: 'black', color: '#fff', borderBottom: '4px solid #FFD700', display: 'flex', alignItems: 'center', justifyContent: 'space-between' }}>
        <Box>
          <IconButton component={Link} to="/" sx={{ color: '#FFC72C' }}><HomeIcon fontSize="medium" /></IconButton>

          {/* Reports - always displayed; permission gate runs when clicked */}
          <IconButton
            component={Link}
            to="#"
            sx={{ color: '#FFC72C', ml: 1 }}
            onClick={(e) => {
              e.preventDefault();
              attemptNav(['gsoc_reports'], () => navigate('/reports'), 'Reports');
            }}
          >
            <DescriptionIcon fontSize="medium" />
          </IconButton>

          {/* Device camera link (protected by device_health) - if you want always visible, remove the guard here */}
          {auth?.hasPermission('device_health') && (
            <IconButton
              component="a"
              href="http://10.138.161.4:3000/dashboard/index.html"
              rel="noopener noreferrer"
              sx={{ color: '#FFC72C', ml: 1 }}
            >
              <i className="fa-solid fa-camera" style={{ fontSize: 20 }} />
            </IconButton>
          )}

          {/* Associate Verification Tool link */}
          {auth?.hasPermission('associate_verify') && (
            <IconButton
              component="a"
              href="http://10.199.22.57:3004/"
              rel="noopener noreferrer"
              sx={{ color: '#FFF', ml: 1 }}
            >
              <i className="bi bi-patch-check"></i>
            </IconButton>
          )}

{auth?.hasPermission('global_duration') && (
  <Tooltip title="Duration Reports">
    <IconButton
      component={RouterLink}
      to="/duration"
      size="large"
      aria-label="Duration Reports"
      sx={{ color: '#FFF', ml: 1 }}
    >
      <TimerIcon />
    </IconButton>
  </Tooltip>
)}

        </Box>

        <Box sx={{ flexGrow: 1, display: 'flex', alignItems: 'center', justifyContent: 'center' }}>
          <Box component="img" src="/wu-head-logo.png" alt="WU Logo" sx={{ height: { xs: 30, md: 55 }, mr: 2 }} />
          <Typography variant="h5" sx={{ fontWeight: 'bold', color: 'primary.main' }}>Global Headcount Dashboard</Typography>
        </Box>

        {/* --- RIGHT SIDE: moved Login/User / Logout / Access Manager into header (replaces previous separate block) --- */}
        <Box sx={{ display: 'flex', alignItems: 'center', gap: 1 }}>
          {auth?.user ? (
            <>
              {/* Display like "Sonu , Pandey" (first name, comma, rest-of-name) */}
              <Typography variant="body2" sx={{ color: '#fff', fontWeight: 700 }}>
                {firstName}{restName ? ',' : ''} {restName}
              </Typography>

              <Button
                size="small"
                variant="outlined"
                onClick={() => { auth.logout(); window.location.href = '/login'; }}
                sx={{ color: '#fff', borderColor: 'rgba(255,255,255,0.12)', textTransform: 'none' }}
              >
                Logout
              </Button>

              {auth.canGrant && (
                <Button size="small" variant="contained" onClick={() => navigate('/access-manager')} sx={{ ml: 1, textTransform: 'none', fontWeight: 700 }}>
                  Access Manager
                </Button>
              )}
            </>
          ) : (
            <Button size="small" variant="contained" onClick={() => navigate('/login')}>Login</Button>
          )}
        </Box>
      </Box>

      {/* Top row: Uploads | GLOBAL + Region Cards | Date selectors */}
      <Box sx={{ display: 'flex', alignItems: 'center', p: 1, px: 1, gap: 1 }}>
        {/* Left: fixed column with stacked upload/export buttons (aligned left) */}
        <Box sx={{ width: 260, display: 'flex', flexDirection: 'column', gap: 1 }}>
          <input type="file" accept=".xls,.xlsx,.csv" style={{ display: 'none' }} ref={fileInputEmpRef} onChange={onChooseEmployeeFile} />
          {/* Always visible now; action is permission-guarded */}
          <Button
            variant="contained"
            size="small"
            startIcon={<UploadFileIcon />}
            onClick={() => attemptNav(['upload_active_employees'], () => fileInputEmpRef.current && fileInputEmpRef.current.click(), 'Upload Active Employee Sheet')}
            sx={{ width: '100%', height: 25, textTransform: 'none', fontWeight: 700 }}
          >
            Upload Active Employee Sheet
          </Button>

          <input type="file" accept=".xls,.xlsx,.csv" style={{ display: 'none' }} ref={fileInputContrRef} onChange={onChooseContractorFile} />
          <Button
            variant="contained"
            size="small"
            startIcon={<UploadFileIcon />}
            onClick={() => attemptNav(['upload_active_contractors'], () => fileInputContrRef.current && fileInputContrRef.current.click(), 'Upload Active Contractor Sheet')}
            sx={{ width: '100%', height: 25, textTransform: 'none', fontWeight: 700 }}
          >
            Upload Active Contractor Sheet
          </Button>

          <Button
            variant="contained"
            size="small"
            startIcon={<DescriptionIcon />}
            onClick={() => attemptNav(['export_compare'], exportUploadedSheets, 'Export Comparison → Report')}
            sx={{ width: '100%', height: 25, textTransform: 'none', fontWeight: 700 }}
          >
            Export Comparison → Report
          </Button>
        </Box>

        {/* Center: flexible, keeps region cards exactly centered on screen */}
        <Box sx={{ flex: 1, display: 'flex', justifyContent: 'center' }}>
          <Box sx={{ display: 'flex', gap: 3, alignItems: 'center', justifyContent: 'center', flexWrap: 'wrap' }}>
            {[
              { key: 'global', label: 'GLOBAL', count: globalCount, url: null },
              { key: 'apac', label: 'APAC', count: counts.apac, url: 'http://10.199.22.57:3000/' },
              { key: 'emea', label: 'EMEA', count: counts.emea, url: 'http://10.199.22.57:3001/' },
              { key: 'laca', label: 'LACA', count: counts.laca, url: 'http://10.199.22.57:3003/' },
              { key: 'namer', label: 'NAMER', count: counts.namer, url: 'http://10.199.22.57:3002/' },
            ].map(region => (
              <Box
                key={region.key}
                onClick={() => {
                  if (region.key === 'global') {
                    const el = document.querySelector('[data-global-left-panel]');
                    if (el) el.scrollIntoView({ behavior: 'smooth', block: 'start' });
                    setSelected('global');
                    return;
                  }
                  // region-level permission candidates (any one is sufficient)
                  const permCandidates = [`headcount.${region.key}`, 'headcount', 'global_access'];
                  attemptNav(permCandidates, () => {
                    if (region.url) window.location.href = region.url;
                  }, `${region.label} headcount`);
                }}
                sx={{
                  cursor: 'pointer',
                  width: 200,
                  height: 88,
                  display: 'flex',
                  flexDirection: 'column',
                  justifyContent: 'center',
                  alignItems: 'center',
                  border: '4px solid rgba(255, 204, 0, 0.89)',
                  borderRadius: 2,
                  boxShadow: 3,
                  bgcolor: 'transparent',
                  '&:hover': { opacity: 0.95 },
                }}
              >
                <Typography variant="subtitle2" sx={{ fontWeight: 'bold', color: '#FFC72C', fontSize: { xs: '0.95rem', md: '1.2rem' } }}>
                  {region.label}
                </Typography>
                <Typography variant="h4" sx={{ fontWeight: 900, fontSize: { xs: '1.2rem', md: '1.6rem' }, color: '#FFFFFF' }}>
                  {region.count ?? 0}
                </Typography>
              </Box>
            ))}
          </Box>
        </Box>

        {/* Right: fixed column for date selectors (aligned right) */}
        <Box sx={{ width: 360, display: 'flex', flexDirection: 'column', gap: 1, alignItems: 'flex-end' }}>
          <Paper sx={{ p: 1, display: 'flex', gap: 1, alignItems: 'center', boxShadow: 1, width: '100%' }}>
            <Box sx={{ display: 'flex', flexDirection: 'column', width: 160 }}>
              <Typography variant="caption" color="text.secondary">Select Start date</Typography>
              <input
                type="date"
                value={startDate}
                onChange={(e) => setStartDate(e.target.value)}
                style={{ width: '100%', height: 34, borderRadius: 4, border: '1px solid rgba(255,255,255,0.06)', padding: 4, background: '#FFCC00', color: 'Black' }}
              />
            </Box>

            <Box sx={{ display: 'flex', flexDirection: 'column', width: 160 }}>
              <Typography variant="caption" color="text.secondary">Select End date</Typography>
              <input
                type="date"
                value={endDate}
                onChange={(e) => setEndDate(e.target.value)}
                style={{ width: '100%', height: 34, borderRadius: 4, border: '1px solid rgba(255,255,255,0.06)', padding: 4, background: '#FFCC00', color: 'Black' }}
              />
            </Box>
          </Paper>

          <Box sx={{ width: '100%', display: 'flex', justifyContent: 'flex-start', gap: 1 }}>
            <Button size="small" variant="contained" onClick={() => applyDateRange({ refresh: false })} sx={{ height: 36, textTransform: 'none', fontWeight: 700 }}>
              Apply
            </Button>
            <Button size="small" variant="outlined" color="error" onClick={() => applyDateRange({ refresh: true })} sx={{ height: 36, textTransform: 'none', fontWeight: 700 }}>
              Force Refresh
            </Button>
          </Box>
        </Box>
      </Box>


      {/* Main: left summary | center map | right averages */}
      <Box sx={{ display: 'flex', flex: 1, overflow: 'hidden' }}>
        {/* Left detail panel */}
        <Box
          data-global-left-panel
          sx={{
            width: { xs: 320, md: 360 },
            minWidth: { md: 320 },
            p: 2,
            bgcolor: 'background.paper',
            borderRight: '1px solid rgba(255,255,255,0.06)',
            display: 'flex',
            flexDirection: 'column',
            ...hideScrollbarSx,
            height: '100%',
          }}
        >
          <Typography variant="h6" sx={{ mb: 1, color: 'primary.main' }}>Live vs CCURE Summary</Typography>

          {/* Show a small loading bar if refreshing, but DO NOT hide current averages if present */}
          {loadingAverages && (
            <Box sx={{ py: 1 }}>
              <LinearProgress />
            </Box>
          )}

          {averages ? (
            <>
              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.02)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary">CCURE Active (reported)</Typography>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1, alignItems: 'center' }}>
                  <Box>
                    <Typography variant="h4" sx={{ fontWeight: 800 }}>{ccureActiveEmployees ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Active Employees</Typography>
                  </Box>
                  <Box sx={{ textAlign: 'right' }}>
                    <Typography variant="h5" sx={{ fontWeight: 800 }}>{ccureActiveContractors ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Active Contractors</Typography>
                  </Box>
                </Box>
              </Paper>

              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.02)' }} elevation={0}>
                <Box sx={{ display: 'flex', justifyContent: 'space-between' }}>
                  <Typography variant="subtitle2" color="text.secondary">Live Today</Typography>
                  <Typography variant="caption" color="text.secondary">{respDate ?? ''}</Typography>
                </Box>

                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Box>
                    <Typography variant="h5" sx={{ fontWeight: 800 }}>{headEmployee ?? liveEmp ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Employee</Typography>
                  </Box>
                  <Box>
                    <Typography variant="h5" sx={{ fontWeight: 800 }}>{headContractor ?? liveContr ?? '—'}</Typography>
                    <Typography variant="caption" color="text.secondary">Contractor</Typography>
                  </Box>
                </Box>

                <Divider sx={{ my: 1 }} />

                <Box>
                  <Typography variant="caption" color="text.secondary">Totals</Typography>
                  <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.75 }}>
                    <Typography variant="body2">Attendance total (today)</Typography>
                    <Typography variant="body2" sx={{ fontWeight: 700 }}>{headTotalVisited ?? '—'}</Typography>
                  </Box>

                  <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                    <Typography variant="body2">Live region total</Typography>
                    <Typography variant="body2" sx={{ fontWeight: 700 }}>{liveCurrentTotal ?? '—'}</Typography>
                  </Box>

                  {getFromPayload('headcount_details.total_visited_today', null) != null && (
                    <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                      <Typography variant="body2">Detail rows total</Typography>
                      <Typography variant="body2" sx={{ fontWeight: 700 }}>{getFromPayload('headcount_details.total_visited_today', '—')}</Typography>
                    </Box>
                  )}
                </Box>
              </Paper>

              <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.02)' }} elevation={0}>
                <Typography variant="subtitle2" color="text.secondary">Percentages vs CCURE</Typography>

                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Typography variant="body2">Employees</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{empPct != null ? `${empPct}%` : '—'}</Typography>
                </Box>
                <Box sx={{ display: 'flex', justifyContent: 'space-between' }}>
                  <Typography variant="body2">Contractors</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{conPct != null ? `${conPct}%` : '—'}</Typography>
                </Box>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 0.5 }}>
                  <Typography variant="body2">Overall</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{overallPct != null ? `${overallPct}%` : '—'}</Typography>
                </Box>

                <Divider sx={{ my: 1 }} />
                <Typography variant="caption" color="text.secondary">Averages</Typography>
                <Box sx={{ display: 'flex', justifyContent: 'space-between', mt: 1 }}>
                  <Typography variant="body2">7-day avg headcount</Typography>
                  <Typography variant="body2" sx={{ fontWeight: 700 }}>{avg7 ?? '—'}</Typography>
                </Box>
              </Paper>

              {averages && averages.notes && (
                <Paper sx={{ p: 2, mb: 2, bgcolor: 'rgba(255,255,255,0.01)' }}>
                  <Typography variant="body2" sx={{ mt: 1 }}>{averages.notes}</Typography>
                </Paper>
              )}
            </>
          ) : (
            loadingAverages ? (
              <Box sx={{ py: 4, display: 'flex', alignItems: 'center', justifyContent: 'center' }}>
                <CircularProgress />
              </Box>
            ) : averagesError ? (
              <Alert severity="error">Failed to load CCURE averages</Alert>
            ) : (
              <Typography variant="body2" color="text.secondary">No data</Typography>
            )
          )}
        </Box>

        {/* Center: map (flex) */}
        <Box sx={{ flex: 1, minWidth: 0, position: 'relative', display: 'flex', flexDirection: 'column' }}>
          <Box sx={{ flex: 1, minHeight: 0 }}>
            <MapChart
              selected={selected}
              initialZoom={1.8}
              onClickSite={(marker) => {
                // marker.region is the current selected region from MapChart call
                const regionKey = marker.region || selected || 'global';
                const permCandidates = [`headcount.${regionKey}`, 'headcount', 'global_access'];
                attemptNav(permCandidates, () => {
                  if (marker.url) window.location.href = marker.url;
                }, `${regionKey.toUpperCase()}`);
              }}
            />
          </Box>
        </Box>

        {/* Right side: Location averages panel */}
        <Box
          sx={{
            width: { xs: 320, md: 360 },
            minWidth: { md: 320 },
            borderLeft: '1px solid rgba(255,255,255,0.06)',
            bgcolor: 'background.paper',
            p: 2,
            display: 'flex',
            flexDirection: 'column',
            ...hideScrollbarSx,
            height: '100%',
          }}
        >
          <Typography variant="h6" sx={{ mb: 1, color: 'primary.main' }}>Location Averages</Typography>

          {/* Keep the spinner for the panel but don't clear data if averages exist */}
          {loadingAverages && !averages && (
            <Box sx={{ display: 'flex', alignItems: 'center', justifyContent: 'center', py: 4 }}>
              <CircularProgress />
            </Box>
          )}

          {averagesError ? (
            <Alert severity="error">Failed to load location averages</Alert>
          ) : locationAvgsList.length === 0 ? (
            averages ? (
              <Typography variant="body2" color="text.secondary">No location averages available</Typography>
            ) : (
              !loadingAverages && <Typography variant="body2" color="text.secondary">No location averages available</Typography>
            )
          ) : (
            <List dense disablePadding sx={{ flex: 1 }}>
              {locationAvgsList.map(item => (
                <ListItem key={item.location} sx={{ alignItems: 'flex-start', py: 1.25, borderBottom: '1px solid rgba(255,255,255,0.03)' }}>
                  <ListItemText
                    primary={
                      <Box sx={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>
                        <Typography sx={{ fontWeight: 800 }}>{item.location}</Typography>
                        <Typography variant="body2" sx={{ fontWeight: 800 }}>
                          {item.avg_overall_last_7_days != null ? Math.round(item.avg_overall_last_7_days) : '—'}
                        </Typography>
                      </Box>
                    }
                    secondary={
                      <Box sx={{ display: 'flex', gap: 2, mt: 0.5, flexWrap: 'wrap' }}>
                        <Typography variant="caption" color="text.secondary">Emp: <strong>{item.avg_employee_last_7_days != null ? Math.round(item.avg_employee_last_7_days) : '—'}</strong></Typography>
                        <Typography variant="caption" color="text.secondary">Contr: <strong>{item.avg_contractor_last_7_days != null ? Math.round(item.avg_contractor_last_7_days) : '—'}</strong></Typography>
                        {item.history_days_counted != null && <Typography variant="caption" color="text.secondary">Days: {item.history_days_counted}</Typography>}
                      </Box>
                    }
                    primaryTypographyProps={{ component: 'div' }}
                    secondaryTypographyProps={{ component: 'div' }}
                  />
                </ListItem>
              ))}
            </List>
          )}
        </Box>
      </Box>

      <Snackbar open={snack.open} autoHideDuration={3500} onClose={() => setSnack(prev => ({ ...prev, open: false }))}>
        <Alert severity={snack.severity} onClose={() => setSnack(prev => ({ ...prev, open: false }))}>{snack.message}</Alert>
      </Snackbar>

    </Box>
  );
}










