@app.route('/latest', methods=['GET'])
def latest_results():
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]

    start_date_iso = None
    end_date_iso = None
    try:
        m = re.search(r'(\d{8})', latest.name)
        if m:
            ymd = m.group(1)
            dt = datetime.strptime(ymd, "%Y%m%d").date()
            start_date_iso = dt.isoformat()
            end_date_iso = dt.isoformat()
    except Exception:
        start_date_iso = None
        end_date_iso = None

    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)

    df = _replace_placeholder_strings(df)

    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in df.columns), None)

    def _norm_val_for_latest(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                fv = float(s)
                if math.isfinite(fv) and fv.is_integer():
                    s = str(int(fv))
        except Exception:
            pass
        return s

    if id_col is None:
        unique_persons = int(len(df))
    else:
        ids_series = df[id_col].apply(_norm_val_for_latest) if id_col in df.columns else pd.Series([None]*len(df))
        if id_col != 'person_uid' and 'person_uid' in df.columns:
            ids_series = ids_series.fillna(df['person_uid'].astype(str).replace('nan','').replace('None',''))
        unique_persons = int(len(set([x for x in ids_series.unique() if x])))

    # build initial sample (list of dicts)
    sample = _clean_sample_df(df, max_rows=5)  # returns list

    # --- Enrich sample rows with EmployeeEmail and imageUrl (best-effort) ---
    try:
        if isinstance(sample, list) and sample:
            for s in sample:
                try:
                    # prefer EmployeeID / person_uid / EmployeeName as lookup token
                    lookup_token = s.get('EmployeeID') or s.get('person_uid') or s.get('EmployeeName') or s.get('EmployeeIdentity')
                    pi = {}
                    if lookup_token:
                        try:
                            pi = get_personnel_info(lookup_token) or {}
                        except Exception:
                            pi = {}
                    # set email if found from personnel info
                    if pi:
                        email = pi.get('EmailAddress') or pi.get('EmployeeEmail') or pi.get('Email') or None
                        if email:
                            s['EmployeeEmail'] = email
                        # prefer ObjectID then GUID for image parent
                        objid = pi.get('ObjectID') or pi.get('GUID') or None
                        if objid:
                            try:
                                base = (request.url_root or request.host_url).rstrip('/')
                            except Exception:
                                base = ''
                            s['imageUrl'] = f"{base}/employee/{objid}/image" if base else f"/employee/{objid}/image"
                            # best-effort flag whether image exists (may be False if DB missing)
                            try:
                                b = get_person_image_bytes(objid)
                                s['HasImage'] = True if b else False
                            except Exception:
                                s['HasImage'] = False

                    # fallback: if still no email, try common columns inside df (use first non-placeholder)
                    if not s.get('EmployeeEmail'):
                        try:
                            for col in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                                if col in df.columns:
                                    val = df.iloc[0].get(col)
                                    if val not in (None, '', 'nan'):
                                        s['EmployeeEmail'] = val
                                        break
                        except Exception:
                            pass

                    # fallback: if no imageUrl but EmployeeID present, build imageUrl using EmployeeID
                    if not s.get('imageUrl') and s.get('EmployeeID'):
                        try:
                            base = (request.url_root or request.host_url).rstrip('/')
                        except Exception:
                            base = ''
                        if base:
                            s['imageUrl'] = f"{base}/employee/{s.get('EmployeeID')}/image"
                        else:
                            s['imageUrl'] = f"/employee/{s.get('EmployeeID')}/image"
                except Exception:
                    continue
    except Exception:
        pass

    resp = {
        "file": latest.name,
        "rows_raw": int(len(df)),
        "rows": unique_persons,
        "sample": sample,
        "start_date": start_date_iso,
        "end_date": end_date_iso,
        "city": city_slug
    }
    return jsonify(resp)














I have upadte logic in app.py but still we dont get EMployee Image and Email so once check previous app.py logic carefully
below i share two file 1 is for refrance and 2nd is for Upadte in our Current folder 

so compare both file carefully and fix the issue 


1) Refrance app.py

# backend/app.py
from flask import Flask, jsonify, request, send_from_directory
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re
import io
import base64
import os
import difflib
from typing import Optional, List, Dict, Any
from duration_report import REGION_CONFIG
from datetime import date, timedelta, datetime
from flask import jsonify, request


from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR
from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE

# ---------- Ensure outputs directory exists early (so OVERRIDES_FILE can be defined safely) ----------
BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)

OVERRIDES_FILE = DEFAULT_OUTDIR / "overrides.csv"

def _load_overrides():
    if not OVERRIDES_FILE.exists():
        return {}
    try:
        df = pd.read_csv(OVERRIDES_FILE, dtype=str)
        out = {}
        for _, r in df.iterrows():
            emp = str(r.get('EmployeeID') or r.get('person_uid') or '').strip()
            if not emp:
                continue
            out[emp] = {
                'level': str(r.get('OverrideLevel') or '').strip(),
                'reason': str(r.get('Reason') or '').strip(),
                'ts': str(r.get('Timestamp') or '').strip()
            }
        return out
    except Exception:
        logging.exception("Failed reading overrides file")
        return {}

def _save_override(employee_key, level, reason):
    now = datetime.now().isoformat()
    row = {'EmployeeID': employee_key, 'OverrideLevel': level, 'Reason': reason or '', 'Timestamp': now}
    try:
        if OVERRIDES_FILE.exists():
            df = pd.read_csv(OVERRIDES_FILE, dtype=str)
            # pandas.DataFrame.append is deprecated -> use concat
            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        else:
            df = pd.DataFrame([row])
        df.to_csv(OVERRIDES_FILE, index=False)
        return True
    except Exception:
        logging.exception("Failed to save override")
        return False

# --- Use REGION_CONFIG servers to talk to ACVSCore (no separate ACVSCORE_DB_CONFIG) ---
# ODBC driver variable is already defined later: ODBC_DRIVER (safe to reference only at runtime)

_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20

def _get_acvscore_conn():
    """
    Try to connect to ACVSCore by reusing credentials from REGION_CONFIG.
    Loops through REGION_CONFIG entries and attempts:
      1) SQL auth (UID/PWD) to database "ACVSCore" using region server + credentials
      2) If SQL auth fails on that server, try Trusted_Connection (Windows auth) as a fallback
    If that fails, optionally attempt ACVSCORE_DB_CONFIG if defined (safe: checked via globals()).
    Returns first successful pyodbc connection or None.
    Implements a short backoff after recent failure to reduce log noise.
    """
    try:
        import pyodbc
    except Exception:
        logging.exception("pyodbc not installed; ACVSCore lookups unavailable.")
        return None

    # basic backoff: skip attempts if we just failed recently
    from datetime import datetime
    now = datetime.now().timestamp()
    last = _acvscore_backoff.get("ts")
    if last and _acvscore_backoff.get("failed") and (now - last) < _ACVSCORE_BACKOFF_SECONDS:
        logging.debug("Skipping ACVSCore connection attempt (backoff active).")
        return None

    # iterate region servers (use the same credentials defined in REGION_CONFIG)
    tried = []
    for region_key, rc in (REGION_CONFIG or {}).items():
        server = rc.get("server")
        user = rc.get("user")
        pwd = rc.get("password")
        if not server:
            continue

        # Try SQL auth first if credentials present
        if user and pwd:
            tried.append(f"{region_key}@{server}(sql)")
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={server};DATABASE=ACVSCore;UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;"
            )
            try:
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (sql auth).", server, region_key)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("SQL auth to %s failed: %s", server, e)

        # Try Trusted Connection fallback on same server
        tried.append(f"{region_key}@{server}(trusted)")
        conn_str_trusted = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={server};DATABASE=ACVSCore;Trusted_Connection=yes;"
            "TrustServerCertificate=Yes;"
        )
        try:
            conn = pyodbc.connect(conn_str_trusted, autocommit=True, timeout=5)
            logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (trusted connection).", server, region_key)
            _acvscore_backoff["ts"] = None
            _acvscore_backoff["failed"] = False
            return conn
        except Exception as e:
            logging.debug("Trusted connection to %s failed: %s", server, e)
            continue

    # Fallback: if a global ACVSCORE_DB_CONFIG exists, try it (safe check)
    try:
        if 'ACVSCORE_DB_CONFIG' in globals() and isinstance(globals().get('ACVSCORE_DB_CONFIG'), dict):
            cfg = globals().get('ACVSCORE_DB_CONFIG')
            server = cfg.get('server')
            user = cfg.get('user')
            pwd = cfg.get('password')
            database = cfg.get('database', 'ACVSCore')
            tried.append(f"ACVSCORE_DB_CONFIG@{server}")
            try:
                conn_str = (
                    f"DRIVER={{{ODBC_DRIVER}}};"
                    f"SERVER={server};DATABASE={database};UID={user};PWD={pwd};"
                    "TrustServerCertificate=Yes;"
                )
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore using ACVSCORE_DB_CONFIG (%s).", server)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("ACVSCORE_DB_CONFIG connection failed: %s", e)
    except Exception:
        # defensive: do not propagate any errors from fallback logic
        logging.debug("ACVSCORE_DB_CONFIG fallback not available or failed.")

    # record failure to backoff
    _acvscore_backoff["ts"] = now
    _acvscore_backoff["failed"] = True
    logging.error("Failed to connect to ACVSCore using REGION_CONFIG servers. Tried: %s", tried)
    return None


# ODBC driver (keep existing env-based driver)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data


def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    """
    Try to resolve personnel record using a flexible lookup.
    Returns dict with keys: ObjectID (may be None), GUID, Name, EmailAddress, ManagerEmail
    If resolution fails returns empty dict.
    """
    out: Dict[str, Any] = {}
    if candidate_identifier is None:
        return out
    conn = _get_acvscore_conn()
    if conn is None:
        return out

    try:
        cur = conn.cursor()
        # We'll attempt to match on multiple columns: ObjectID, GUID, Int1, Text12, Name
        # Use parameters (all strings) — SQL will handle mismatched types.
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        params = (cand, cand, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()
        if row:
            # columns: ObjectID, GUID, Name, EmailAddress, ManagerEmail
            try:
                out['ObjectID'] = row[0]
                out['GUID'] = row[1]
                out['Name'] = row[2]
                out['EmailAddress'] = row[3]
                out['ManagerEmail'] = row[4]
            except Exception:
                # defensive assignment by index
                out = {
                    'ObjectID': row[0] if len(row) > 0 else None,
                    'GUID': row[1] if len(row) > 1 else None,
                    'Name': row[2] if len(row) > 2 else None,
                    'EmailAddress': row[3] if len(row) > 3 else None,
                    'ManagerEmail': row[4] if len(row) > 4 else None
                }
    except Exception:
        logging.exception("Failed personnel lookup for candidate: %s", candidate_identifier)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out

def get_person_image_bytes(parent_id) -> Optional[bytes]:
    """
    Query ACVSCore.Access.Images for top image where ParentId = parent_id and return raw bytes.
    Returns None if not found or on error.
    """
    conn = _get_acvscore_conn()
    if conn is None:
        return None
    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 AI.Image
            FROM ACVSCore.Access.Images AI
            WHERE AI.ParentId = ?
              AND DATALENGTH(AI.Image) > 0
            ORDER BY AI.ObjectID DESC
        """
        cur.execute(sql, (str(parent_id),))
        row = cur.fetchone()
        if row and row[0] is not None:
            # pyodbc returns buffer/bytearray for varbinary; convert to bytes
            try:
                b = bytes(row[0])
                return b
            except Exception:
                # sometimes row[0] is already bytes-like
                return row[0]
    except Exception:
        logging.exception("Failed to fetch image for ParentId=%s", parent_id)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass
    return None

# ---------- New route to serve employee image ----------
# We'll import send_file later where used; define route after app created.

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

# send_file is needed for Excel responses
from flask import send_file
try:
    # optional import; used for styling
    from openpyxl import load_workbook
    from openpyxl.styles import Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        # convert floats NaN handled above
        return x
    try:
        # fallback to string
        return str(x)
    except Exception:
        return None


_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False


# Helper: format seconds to HH:MM:SS
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return None
        # guard against floats and NaN
        s = int(float(seconds))
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return None


# Placeholder tokens (keep consistent with trend_runner expectations)
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


def _resolve_field_from_record(record: dict, candidate_tokens: list):
    """
    Search a single row `record` (dict) for likely columns listed in candidate_tokens.
    Return first non-placeholder value found (converted to Python scalar), else None.
    """
    if record is None:
        return None

    # 1) exact key matches (case-sensitive & common casing)
    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)

    # 2) case-insensitive contains match
    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)

    # 3) xml / value parsing fallback for CardNumber
    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue

    # 4) final fallback: first non-placeholder value
    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)

    return None


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Clean a dataframe for JSON output (convert NaN -> None, pandas types -> native, format datetimes).
    """
    if df is None or df.empty:
        return []
    df = df.copy()

    # remove duplicate suffix columns
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    # Date normalization
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    # Datetime columns to ISO strings
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # Replace NaN/inf -> None
    df = df.where(pd.notnull(df), None)

    # Convert records to safe Python types
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        # Typical fields
        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        # ----- Schema-aware fallback resolution -----
        if not emp_id:
            emp_tokens = ['Int1', 'Text12', 'EmployeeID', 'empid', 'id']
            resolved_emp = _resolve_field_from_record(r, emp_tokens)
            if resolved_emp is not None:
                try:
                    s = str(resolved_emp).strip()
                    # remove trailing .0 for floats
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    if _looks_like_guid(s):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = s
                        emp_id = s
                except Exception:
                    if _looks_like_guid(resolved_emp):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = resolved_emp
                        emp_id = resolved_emp

        # Prefer Credential.CardNumber / CHUID / Card as CardNumber when missing — reject GUIDs/placeholders
        if out.get('CardNumber') in (None, '', 'nan'):
            card_tokens = ['CardNumber', 'CHUID', 'Card', 'card_no', 'cardnum']
            resolved_card = _resolve_field_from_record(r, card_tokens)
            if resolved_card is not None:
                try:
                    cs = str(resolved_card).strip()
                    if _looks_like_guid(cs) or _is_placeholder_str(cs):
                        out['CardNumber'] = None
                    else:
                        out['CardNumber'] = cs
                except Exception:
                    out['CardNumber'] = None

        # final safety: ensure EmployeeID/CardNumber are not GUID-like tokens
        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None
        if 'CardNumber' in out and isinstance(out['CardNumber'], str) and _looks_like_guid(out['CardNumber']):
            out['CardNumber'] = None

        # If EmployeeName empty or looks like a GUID, prefer EmployeeID (human id) over GUIDs
        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            if emp_id not in (None, '', 'nan') and not _looks_like_guid(emp_id):
                out['EmployeeName'] = str(emp_id)
            else:
                out['EmployeeName'] = None

        cleaned.append(out)
    return cleaned


@app.route('/')
def root():
    return "Trend Analysis API — Pune test"



@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.json or {}

    date_str = params.get('date')
    start_str = params.get('start')
    end_str = params.get('end')

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            # default: include yesterday and today so previous-day evidence gets generated by default
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    combined_rows = []
    files = []

    for d in dates:
        try:
            df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        if df is None or df.empty:
            continue

        # Ensure IsFlagged exists; Reasons only when flagged
        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        combined_rows.append(df)

    combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()

    # ---------- NEW: Deduplicate combined_df by best identifier and aggregate reasons/explanations ----------
    def _norm_id_val_local(v):
        # reuse same normalization logic used later in file
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                f = float(s)
                if math.isfinite(f) and f.is_integer():
                    s = str(int(f))
        except Exception:
            pass
        return s

    def _aggregate_group(g):
        """
        For a group of rows belonging to the same normalized id, pick a representative row
        (preference: highest AnomalyScore, then latest Date, else first). Also aggregate
        Reasons (unique) and Explanation (unique sentences) and IsFlagged (any).
        """
        # pick candidate by highest AnomalyScore if available
        if 'AnomalyScore' in g.columns and not g['AnomalyScore'].isnull().all():
            try:
                idx = g['AnomalyScore'].astype(float).idxmax()
                base = g.loc[idx].to_dict()
            except Exception:
                base = g.iloc[0].to_dict()
        else:
            # fallback to latest Date if present
            if 'Date' in g.columns:
                try:
                    # some Date values may be strings; try parsing
                    g_dates = pd.to_datetime(g['Date'], errors='coerce')
                    if not g_dates.isnull().all():
                        idx = g_dates.idxmax()
                        base = g.loc[idx].to_dict()
                    else:
                        base = g.iloc[0].to_dict()
                except Exception:
                    base = g.iloc[0].to_dict()
            else:
                base = g.iloc[0].to_dict()

        # aggregate Reasons (code-style) and Explanation (natural text)
        reasons_set = set()
        explanations_set = []
        for _, row in g.iterrows():
            # Reasons: may be semicolon separated codes
            r = row.get('Reasons')
            if r:
                # split on common separators
                parts = [x.strip() for x in re.split(r'[;|,]', str(r)) if x and x.strip()]
                for p in parts:
                    reasons_set.add(p)
            # Explanation: preserve order and uniqueness (sentence-level)
            ex = row.get('Explanation')
            if ex and isinstance(ex, str):
                # split by sentence terminators; keep original chunks for readability
                # Use a simple split by '.' and re-add periods
                for seg in [s.strip() for s in re.split(r'(?<=[\.\?\!])\s+', ex) if s and s.strip()]:
                    if seg not in explanations_set:
                        explanations_set.append(seg)

        reasons_combined = "; ".join(sorted(reasons_set)) if reasons_set else base.get('Reasons')
        explanation_combined = " ".join([s if s.endswith('.') else s + '.' for s in explanations_set]) if explanations_set else base.get('Explanation')

        # IsFlagged: if any row flagged -> True
        is_flagged_any = False
        if 'IsFlagged' in g.columns:
            try:
                is_flagged_any = bool(g['IsFlagged'].any())
            except Exception:
                is_flagged_any = any([bool(r.get('IsFlagged')) for _, r in g.iterrows()])

        base['Reasons'] = reasons_combined
        base['Explanation'] = explanation_combined
        base['IsFlagged'] = bool(is_flagged_any)
        return base

    deduped_df = pd.DataFrame()
    if not combined_df.empty:
        # Determine best identifier column in order of preference
        id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
        id_col = next((c for c in id_candidates if c in combined_df.columns), None)

        if id_col is None:
            # no decent id to dedupe on -> keep as-is
            deduped_df = combined_df.copy()
        else:
            # create normalized id column
            combined_df['_norm_id'] = combined_df[id_col].apply(_norm_id_val_local)
            # fallback: if norm_id null but person_uid exists use person_uid string as identifier
            if 'person_uid' in combined_df.columns:
                combined_df['_norm_id'] = combined_df['_norm_id'].fillna(combined_df['person_uid'].astype(str).replace('nan','').replace('None',''))
            # split groups: those with a non-empty _norm_id and those without
            combined_df['_norm_id'] = combined_df['_norm_id'].apply(lambda x: x if (x is not None and str(x).strip()!='') else None)
            grouped = []
         
            # group by normalized id where present
            non_null_mask = combined_df['_norm_id'].notna()
            if non_null_mask.any():
                for nid, g in combined_df[non_null_mask].groupby('_norm_id'):
                    try:
                        aggregated = _aggregate_group(g)
                        # make sure aggregated result carries the normalized id so later counting can use it
                        if isinstance(aggregated, dict):
                            aggregated['_norm_id'] = nid
                        else:
                            try:
                                aggregated = dict(aggregated)
                                aggregated['_norm_id'] = nid
                            except Exception:
                                pass
                        grouped.append(aggregated)
                    except Exception:
                        # ensure we still record the nid even if aggregation fails
                        row = g.iloc[0].to_dict()
                        row['_norm_id'] = nid
                        grouped.append(row)

                        
            # for rows without norm id, keep one row per original row (no dedupe)
            noid_rows = combined_df[~non_null_mask]
            for _, r in noid_rows.iterrows():
                grouped.append(r.to_dict())

            if grouped:
                try:
                    deduped_df = pd.DataFrame(grouped)
                except Exception:
                    # fallback: keep original combined_df
                    deduped_df = combined_df.copy()
            else:
                deduped_df = combined_df.copy()
    else:
        deduped_df = combined_df.copy()

    # Final safety: ensure IsFlagged/Reasons exist
    if 'IsFlagged' not in deduped_df.columns:
        deduped_df['IsFlagged'] = False
    if 'Reasons' not in deduped_df.columns:
        deduped_df['Reasons'] = None
    if 'Explanation' not in deduped_df.columns:
        deduped_df['Explanation'] = None

    # Build sample from deduped_df (prefer flagged)
    try:
        flagged = deduped_df[deduped_df['IsFlagged'] == True]
    except Exception:
        try:
            flagged = deduped_df[deduped_df.get('IsFlagged') == True]
        except Exception:
            flagged = pd.DataFrame()

    sample_df = flagged.head(10) if (not flagged.empty) else deduped_df.head(10)
    samples = _clean_sample_df(sample_df, max_rows=20)

    # Determine rows/flagged_rows counts based on deduped unique persons
    if deduped_df.empty:
        analysis_count = 0
        flagged_count = 0
    else:
        # Determine unique person count robustly using normalized id if present
        def _norm_val_for_count(v):
            try:
                if pd.isna(v):
                    return None
            except Exception:
                pass
            if v is None:
                return None
            s = str(v).strip()
            if s == '' or s.lower() == 'nan':
                return None
            try:
                if '.' in s:
                    fv = float(s)
                    if math.isfinite(fv) and fv.is_integer():
                        s = str(int(fv))
            except Exception:
                pass
            return s

        # Build set of unique identifiers
        unique_ids = set()

        # Preferred: use _norm_id if present (these are already normalized)
        if '_norm_id' in deduped_df.columns and deduped_df['_norm_id'].notna().any():
            for v in deduped_df['_norm_id'].astype(str).fillna('').map(lambda s: s.strip()):
                if v:
                    unique_ids.add(v)
        else:
            # fallback: gather normalized values from a set of candidate columns
            for col in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1', 'CardNumber'):
                if col in deduped_df.columns:
                    for v in deduped_df[col].dropna().astype(str):
                        nv = _norm_val_for_count(v)
                        if nv:
                            unique_ids.add(nv)

        # Last fallback: if no tokens found, treat each deduped row as unique
        if not unique_ids:
            analysis_count = int(len(deduped_df))
        else:
            analysis_count = int(len(unique_ids))

        # flagged_count: unique flagged persons (same approach applied to only flagged rows)
        if 'IsFlagged' in deduped_df.columns:
            flagged_unique_ids = set()
            flagged_rows = deduped_df[deduped_df['IsFlagged'] == True]
            if not flagged_rows.empty:
                if '_norm_id' in flagged_rows.columns and flagged_rows['_norm_id'].notna().any():
                    for v in flagged_rows['_norm_id'].astype(str).fillna('').map(lambda s: s.strip()):
                        if v:
                            flagged_unique_ids.add(v)
                else:
                    for col in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1', 'CardNumber'):
                        if col in flagged_rows.columns:
                            for v in flagged_rows[col].dropna().astype(str):
                                nv = _norm_val_for_count(v)
                                if nv:
                                    flagged_unique_ids.add(nv)
            flagged_count = int(len(flagged_unique_ids))
        else:
            flagged_count = 0



    # compute flagged rate safely
    flagged_rate_pct = 0.0
    try:
        if analysis_count > 0:
            flagged_rate_pct = round((flagged_count / analysis_count) * 100.0, 2)
    except Exception:
        flagged_rate_pct = 0.0

    # additional diagnostic: raw unique person_uids in the combined (pre-dedupe) for cross-checking
    try:
        if not combined_df.empty and 'person_uid' in combined_df.columns:
            raw_unique_person_uids = int(combined_df['person_uid'].dropna().astype(str).map(lambda s: s.strip()).nunique())
        else:
            raw_unique_person_uids = None
    except Exception:
        raw_unique_person_uids = None

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        # legacy raw total of concatenated rows (kept for diagnostics)
        "aggregated_rows_total_raw": int(len(combined_df)),
        # canonical unique-person counts (the values you actually want displayed)
        "aggregated_unique_persons": int(analysis_count),
        "rows": int(analysis_count),        # kept for backwards compatibility with frontend
        "flagged_rows": int(flagged_count),
        "flagged_rate_percent": float(flagged_rate_pct),
        "files": files,
        "sample": samples[:20],
        "_raw_unique_person_uids": raw_unique_person_uids
    }


    return jsonify(resp)



@app.route('/latest', methods=['GET'])
def latest_results():
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]
    # try to infer the date from the filename (trend_pune_YYYYMMDD.csv)
    start_date_iso = None
    end_date_iso = None
    try:
        m = re.search(r'(\d{8})', latest.name)
        if m:
            ymd = m.group(1)
            dt = datetime.strptime(ymd, "%Y%m%d").date()
            start_date_iso = dt.isoformat()
            end_date_iso = dt.isoformat()
    except Exception:
        start_date_iso = None
        end_date_iso = None

    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)

    # try to compute unique persons (use same id preference as run_trend)
    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in df.columns), None)

    def _norm_val_for_latest(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                fv = float(s)
                if math.isfinite(fv) and fv.is_integer():
                    s = str(int(fv))
        except Exception:
            pass
        return s

    if id_col is None:
        unique_persons = int(len(df))
    else:
        ids_series = df[id_col].apply(_norm_val_for_latest) if id_col in df.columns else pd.Series([None]*len(df))
        if id_col != 'person_uid' and 'person_uid' in df.columns:
            ids_series = ids_series.fillna(df['person_uid'].astype(str).replace('nan','').replace('None',''))
        unique_persons = int(len(set([x for x in ids_series.unique() if x])))

    sample = _clean_sample_df(df, max_rows=5)
    resp = {
        "file": latest.name,
        "rows_raw": int(len(df)),
        "rows": unique_persons,
        "sample": sample,
        # new: what date the latest file represents (helpful for frontend)
        "start_date": start_date_iso,
        "end_date": end_date_iso
    }
    return jsonify(resp)


@app.route('/record', methods=['GET'])
def get_record():
    """
    /record?employee_id=... or /record?person_uid=...
    Returns matching aggregated trend rows and filtered raw swipe rows (only for flagged persons).
    Updated: read ALL trend CSVs (not just the latest), so previous days are included.
    Also add Zone and SwipeGap (seconds) to raw_swipes returned as evidence.

    New behaviour:
      - By default, raw_swipes are returned only for aggregated rows where IsFlagged == True
      - Set include_unflagged=1 (or true/yes) to also fetch evidence for unflagged aggregated rows.
      - If an aggregated row has no Date/FirstSwipe/LastSwipe, we fall back to scanning all swipes files.
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    # Read all CSVs (concat) so /record will search previous days too
    df_list = []
    for fp in csvs:
        try:
            # removed deprecated infer_datetime_format argument
            tmp = pd.read_csv(fp, parse_dates=['FirstSwipe','LastSwipe'])
        except Exception:
            try:
                tmp = pd.read_csv(fp, dtype=str)
            except Exception:
                continue
        df_list.append(tmp)
    if not df_list:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200
    df = pd.concat(df_list, ignore_index=True)

    if q is None:
        cleaned = _clean_sample_df(df, max_rows=10)
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    q_str = str(q).strip()

    def normalize_series(s):
        if s is None:
            return pd.Series([''] * len(df))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if math.isfinite(fv) and fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    found_mask = pd.Series(False, index=df.index)

    if 'EmployeeID' in df.columns:
        emp_series = normalize_series(df['EmployeeID'])
        found_mask = found_mask | (emp_series == q_str)

    if 'person_uid' in df.columns:
        uid_series = normalize_series(df['person_uid'])
        found_mask = found_mask | (uid_series == q_str)

    # also check Int1 (Personnel.Int1) if present in CSV
    if 'Int1' in df.columns and not found_mask.any():
        int1_series = normalize_series(df['Int1'])
        found_mask = found_mask | (int1_series == q_str)

    if not found_mask.any():
        # try numeric equivalence
        try:
            q_numeric = float(q_str)
            if 'EmployeeID' in df.columns:
                emp_numeric = pd.to_numeric(df['EmployeeID'], errors='coerce')
                found_mask = found_mask | (emp_numeric == q_numeric)
            if 'Int1' in df.columns and not found_mask.any():
                int_numeric = pd.to_numeric(df['Int1'], errors='coerce')
                found_mask = found_mask | (int_numeric == q_numeric)
        except Exception:
            pass

    matched = df[found_mask]
    if matched.empty:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    cleaned_matched = _clean_sample_df(matched, max_rows=len(matched))


    # --- ENRICHMENT START ---
    # cleaned_matched is a list of dicts
    # Try to map each cleaned row to a personnel record and attach email/manager/image info
    try:
        # Build a quick index over matched DataFrame to find best lookup candidate for each cleaned row
        matched_indexed = matched.reset_index(drop=True)
        for idx_c, cleaned in enumerate(cleaned_matched):
            # Try to find a matching row in matched DataFrame by person_uid -> EmployeeID -> EmployeeName
            candidate_row = None
            try:
                if cleaned.get('person_uid'):
                    if 'person_uid' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('person_uid', '').astype(str) == str(cleaned['person_uid'])]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                if candidate_row is None and cleaned.get('EmployeeID'):
                    if 'EmployeeID' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('EmployeeID', '').astype(str) == str(cleaned['EmployeeID'])]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                if candidate_row is None and cleaned.get('EmployeeName'):
                    # names exact match (defensive)
                    if 'EmployeeName' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('EmployeeName', '').astype(str).str.strip().fillna('') == str(cleaned['EmployeeName']).strip()]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                # fallback: take first matched row if nothing else
                if candidate_row is None and len(matched_indexed) > 0:
                    candidate_row = matched_indexed.iloc[0].to_dict()
            except Exception:
                candidate_row = None

            # Determine the best candidate identifier to query Personnel
            lookup_candidates = []
            if candidate_row:
                for k in ('EmployeeObjID', 'EmployeeObjId', 'EmployeeIdentity', 'ObjectID', 'GUID', 'EmployeeID', 'Int1', 'Text12', 'EmployeeName'):
                    if k in candidate_row and candidate_row.get(k) not in (None, '', 'nan'):
                        lookup_candidates.append(candidate_row.get(k))
            # also include cleaned fields
            for k in ('EmployeeID', 'person_uid', 'EmployeeName'):
                if cleaned.get(k) not in (None, '', 'nan'):
                    lookup_candidates.append(cleaned.get(k))

            # try each lookup candidate until we find personnel info
            personnel_info = {}
            for cand in lookup_candidates:
                if cand is None:
                    continue
                try:
                    info = get_personnel_info(cand)
                    if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None or info.get('ManagerEmail') is not None):
                        personnel_info = info
                        break
                except Exception:
                    continue

            # attach if found
            if personnel_info:
                # safe assignments
                cleaned['EmployeeObjID'] = personnel_info.get('ObjectID')
                cleaned['EmployeeEmail'] = personnel_info.get('EmailAddress')
                cleaned['ManagerEmail'] = personnel_info.get('ManagerEmail')
                # HasImage and imageUrl
                if personnel_info.get('ObjectID') is not None:
                    cleaned['imageUrl'] = f"/employee/{personnel_info.get('ObjectID')}/image"
                    # quick check if image exists (non-blocking: try fetch bytes but ignore failure)
                    try:
                        b = get_person_image_bytes(personnel_info.get('ObjectID'))
                        cleaned['HasImage'] = True if b else False
                    except Exception:
                        cleaned['HasImage'] = False
                else:
                    cleaned['imageUrl'] = None
                    cleaned['HasImage'] = False
    except Exception:
        logging.exception("Failed to enrich aggregated rows with personnel/email/image info")
    # --- ENRICHMENT END ---


    # Resolve raw swipe file names by Date (collect all dates present in matched rows)
    raw_files = set()
    raw_swipes_out = []

    # Helper to add and dedupe swipe rows
    seen_swipe_keys = set()
    def _append_swipe(out_row, source_name):
        # create a dedupe key (date,time,door,direction,card)
        key = (
            out_row.get('Date') or '',
            out_row.get('Time') or '',
            (out_row.get('Door') or '').strip(),
            (out_row.get('Direction') or '').strip(),
            (out_row.get('CardNumber') or out_row.get('Card') or '').strip()
        )
        if key in seen_swipe_keys:
            return
        seen_swipe_keys.add(key)
        out_row['_source'] = source_name
        raw_swipes_out.append(out_row)

    # iterate matched aggregated rows and search raw swipe files
    for idx, agg_row in matched.iterrows():
        person_uid = agg_row.get('person_uid') if 'person_uid' in agg_row else None
        empid = agg_row.get('EmployeeID') if 'EmployeeID' in agg_row else None
        if (not empid) and 'Int1' in agg_row:
            empid = agg_row.get('Int1')
        card = agg_row.get('CardNumber') if 'CardNumber' in agg_row else None

        # build dates_for_row from Date / FirstSwipe / LastSwipe
        dates_for_row = set()
        if 'Date' in agg_row and pd.notna(agg_row['Date']):
            try:
                d = pd.to_datetime(agg_row['Date']).date()
                dates_for_row.add(d.isoformat())
            except Exception:
                pass
        for col in ('FirstSwipe','LastSwipe'):
            if col in agg_row and pd.notna(agg_row[col]):
                try:
                    d = pd.to_datetime(agg_row[col]).date()
                    dates_for_row.add(d.isoformat())
                except Exception:
                    pass

        # If aggregated row is not flagged and include_unflagged is False, skip fetching raw evidence
        is_flagged = bool(agg_row.get('IsFlagged', False))
        if (not is_flagged) and (not include_unflagged):
            continue

        # If no dates found, fallback to scanning all swipes files (so we don't miss evidence)
        dates_to_scan = dates_for_row or {None}

        for d in dates_to_scan:
            try:
                if d is None:
                    # scan all swipes files
                    candidates = list(Path(DEFAULT_OUTDIR).glob(f"swipes_*.csv"))
                else:
                    dd = d[:10]  # 'YYYY-MM-DD'
                    target = dd.replace('-', '')
                    candidates = list(Path(DEFAULT_OUTDIR).glob(f"swipes_*_{target}.csv"))

                if not candidates:
                    continue

                for fp in candidates:
                    raw_name = fp.name
                    raw_files.add(raw_name)
                    try:
                        # removed deprecated infer_datetime_format argument
                        raw_df = pd.read_csv(fp, parse_dates=['LocaleMessageTime'])
                    except Exception:
                        try:
                            raw_df = pd.read_csv(fp, dtype=str)
                        except Exception:
                            continue

                    cols_lower = {c.lower(): c for c in raw_df.columns}
                    tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
                    emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                    name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
                    card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
                    door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
                    dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
                    note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None

                    # build filter mask
                    mask = pd.Series(False, index=raw_df.index)
                    if person_uid is not None and 'person_uid' in raw_df.columns:
                        mask = mask | (raw_df['person_uid'].astype(str).str.strip() == str(person_uid).strip())
                    if emp_col:
                        if empid is not None:
                            try:
                                cmp_val = str(empid).strip()
                                if '.' in cmp_val:
                                    fv = float(cmp_val)
                                    if math.isfinite(fv) and fv.is_integer():
                                        cmp_val = str(int(fv))
                            except Exception:
                                cmp_val = str(empid).strip()
                            mask = mask | (raw_df[emp_col].astype(str).str.strip() == cmp_val)
                    if card_col and card is not None:
                        mask = mask | (raw_df[card_col].astype(str).str.strip() == str(card).strip())

                    if (not mask.any()) and name_col and 'EmployeeName' in agg_row and pd.notna(agg_row.get('EmployeeName')):
                        mask = mask | (raw_df[name_col].astype(str).str.strip() == str(agg_row.get('EmployeeName')).strip())

                    # filter by date if possible
                    if d is not None and tcol and tcol in raw_df.columns:
                        try:
                            raw_df[tcol] = pd.to_datetime(raw_df[tcol], errors='coerce')
                            mask = mask & (raw_df[tcol].dt.date == pd.to_datetime(d).date())
                        except Exception:
                            pass

                    filtered = raw_df[mask].copy()
                    if filtered.empty:
                        # xml value fallback for embedded card values
                        if card is not None:
                            for ccol in raw_df.columns:
                                cl = ccol.lower()
                                if 'xml' in cl or 'msg' in cl or 'value' == cl:
                                    try:
                                        vals = raw_df[ccol].dropna().astype(str)
                                        match_mask = vals.apply(lambda x: (_extract_card_from_xml_text(x) == str(card).strip()))
                                        if match_mask.any():
                                            idxs = match_mask.index[match_mask]
                                            filtered = raw_df.loc[idxs].copy()
                                            break
                                    except Exception:
                                        continue
                        if filtered.empty:
                            continue

                    # enrich filtered rows and append to output (deduped)
                    try:
                        if tcol and tcol in filtered.columns:
                            filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                        else:
                            if 'localemessagetime' in filtered.columns:
                                filtered['localemessagetime'] = pd.to_datetime(filtered['localemessagetime'], errors='coerce')
                                tcol = 'localemessagetime'
                    except Exception:
                        pass

                    if tcol and tcol in filtered.columns:
                        filtered = filtered.sort_values(by=tcol)
                        filtered['_prev_ts'] = filtered[tcol].shift(1)
                        try:
                            filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                        except Exception:
                            filtered['_swipe_gap_seconds'] = 0.0
                    else:
                        filtered['_swipe_gap_seconds'] = 0.0

                    try:
                        if door_col and door_col in filtered.columns:
                            if dir_col and dir_col in filtered.columns:
                                filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                            else:
                                filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
                        else:
                            if 'PartitionName2' in filtered.columns:
                                filtered['_zone'] = filtered['PartitionName2'].fillna('').astype(str).apply(lambda x: x if x else None)
                            else:
                                filtered['_zone'] = None
                    except Exception:
                        filtered['_zone'] = None

                    # normalize and append
                    for _, r in filtered.iterrows():
                        out = {}
                        out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in raw_df.columns else _to_python_scalar(agg_row.get('EmployeeName') or agg_row.get('person_uid'))

                        # EmployeeID
                        emp_val = None
                        if 'int1' in cols_lower and cols_lower.get('int1') in raw_df.columns:
                            emp_val = _to_python_scalar(r.get(cols_lower.get('int1')))
                        elif emp_col and emp_col in raw_df.columns:
                            emp_val = _to_python_scalar(r.get(emp_col))
                        else:
                            possible_emp = None
                            for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                                if cand.lower() in cols_lower:
                                    possible_emp = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                                    if possible_emp not in (None, '', 'nan'):
                                        break
                            emp_val = possible_emp if possible_emp not in (None, '', 'nan') else _to_python_scalar(agg_row.get('EmployeeID'))

                        if emp_val is not None:
                            try:
                                s = str(emp_val).strip()
                                if '.' in s:
                                    f = float(s)
                                    if math.isfinite(f) and f.is_integer():
                                        s = str(int(f))
                                if _looks_like_guid(s) or _is_placeholder_str(s):
                                    emp_val = None
                                else:
                                    emp_val = s
                            except Exception:
                                if _looks_like_guid(emp_val):
                                    emp_val = None
                        out['EmployeeID'] = emp_val

                        # CardNumber
                        card_val = None
                        if 'cardnumber' in cols_lower and cols_lower.get('cardnumber') in raw_df.columns:
                            card_val = _to_python_scalar(r.get(cols_lower.get('cardnumber')))
                        elif card_col and card_col in raw_df.columns:
                            card_val = _to_python_scalar(r.get(card_col))
                        else:
                            possible_card = None
                            for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                                if cand.lower() in cols_lower:
                                    possible_card = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                                    if possible_card not in (None, '', 'nan'):
                                        break
                            if possible_card in (None, '', 'nan'):
                                for ccc in raw_df.columns:
                                    cl = ccc.lower()
                                    if 'xml' in cl or 'msg' in cl or 'value' == cl:
                                        try:
                                            txt = r.get(ccc)
                                            extracted = _extract_card_from_xml_text(str(txt)) if txt is not None else None
                                            if extracted:
                                                possible_card = extracted
                                                break
                                        except Exception:
                                            continue
                            card_val = possible_card if possible_card not in (None, '', 'nan') else _to_python_scalar(agg_row.get('CardNumber'))

                        if card_val is not None:
                            try:
                                cs = str(card_val).strip()
                                if _looks_like_guid(cs) or _is_placeholder_str(cs):
                                    card_val = None
                                else:
                                    card_val = cs
                            except Exception:
                                card_val = None
                        out['CardNumber'] = card_val

                        # timestamp -> Date/Time
                        if tcol and tcol in raw_df.columns:
                            ts = r.get(tcol)
                            try:
                                ts_py = pd.to_datetime(ts)
                                out['Date'] = ts_py.date().isoformat()
                                out['Time'] = ts_py.time().isoformat()
                            except Exception:
                                txt = str(r.get(tcol))
                                out['Date'] = txt[:10]
                                out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                        else:
                            out['Date'] = d if d is not None else None
                            out['Time'] = None

                        out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
                        out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in r else None
                        out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None

                        try:
                            out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else map_door_to_zone(out['Door'], out['Direction'])
                        except Exception:
                            out['Zone'] = None
                        try:
                            gap = r.get('_swipe_gap_seconds') if '_swipe_gap_seconds' in r else None
                            out['SwipeGapSeconds'] = float(gap) if gap is not None else None
                            out['SwipeGap'] = format_seconds_to_hms(out['SwipeGapSeconds'])
                        except Exception:
                            out['SwipeGapSeconds'] = None
                            out['SwipeGap'] = None

                        _append_swipe(out, raw_name)
            except Exception as e:
                logging.exception("Error processing raw swipe file for date %s: %s", d, e)
                continue

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": sorted(list(raw_files)),
        "raw_swipes": raw_swipes_out
    }), 200


@app.route('/override', methods=['POST'])
def set_override():
    """
    POST JSON: { "employee_id": "<EmployeeID or person_uid>", "level": "Low|Medium|High|Clear", "reason": "justification text" }
    Saves override to outputs/overrides.csv. Run-time uses this file to adjust RiskLevel display.
    """
    try:
        payload = request.get_json(force=True)
        emp = payload.get('employee_id')
        level = payload.get('level')
        reason = payload.get('reason', '')
        if not emp or not level:
            return jsonify({'error':'employee_id and level required'}), 400
        ok = _save_override(str(emp).strip(), str(level).strip(), str(reason).strip())
        if not ok:
            return jsonify({'error':'failed to save override'}), 500
        return jsonify({'status':'ok'}), 200
    except Exception as e:
        logging.exception("Override save error")
        return jsonify({'error': str(e)}), 500



@app.route('/record/export', methods=['GET'])
def export_record_excel():
    """
    /record/export?employee_id=...&date=YYYY-MM-DD  OR /record/export?person_uid=...&date=YYYY-MM-DD
    Produces an Excel file (xlsx) filtered for the requested employee and date (if provided).
    Two sheets:
      - "Details — Evidence": EmployeeName, EmployeeID, Door, Direction, Zone, Date, LocaleMessageTime, SwipeGapSeconds, PartitionName2, _source_file
      - "Swipe timeline": Employee Name, Employee ID, Card, Date, Time, SwipeGapSeconds, Door, Direction, Zone, Note
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    date_str = request.args.get('date')  # optional 'YYYY-MM-DD' (single date)
    if not q:
        return jsonify({"error":"employee_id or person_uid is required"}), 400

    # Determine list of raw swipe files to scan. If date provided, restrict to that date only.
    files_to_scan = []
    p = Path(DEFAULT_OUTDIR)
    if date_str:
        try:
            dd = pd.to_datetime(date_str).date()
            target = dd.strftime("%Y%m%d")
            candidates = list(p.glob(f"swipes_*_{target}.csv"))
            files_to_scan = candidates
        except Exception:
            return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400
    else:
        # scan all swipes files (most recent first)
        files_to_scan = sorted(p.glob("swipes_*.csv"), reverse=True)

    if not files_to_scan:
        return jsonify({"error":"no raw swipe files found for requested date / outputs"}), 404

    all_rows = []
    for fp in files_to_scan:
        try:
            # removed deprecated infer_datetime_format argument
            raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
        except Exception:
            try:
                raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

        # normalize column names
        cols_lower = {c.lower(): c for c in raw_df.columns}
        # pick possible columns
        tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
        emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
        name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
        card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
        door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
        dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
        note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
        person_uid_col = cols_lower.get('person_uid')

        # build mask matching requested q: try person_uid, emp_col, card, name
        mask = pd.Series(False, index=raw_df.index)
        if person_uid_col and person_uid_col in raw_df.columns:
            mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
        if emp_col and emp_col in raw_df.columns:
            mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
        # try matching numeric equivalence
        if not mask.any() and emp_col and emp_col in raw_df.columns:
            try:
                q_numeric = float(q)
                emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                mask = mask | (emp_numeric == q_numeric)
            except Exception:
                pass
        # also try name match if nothing matched
        if not mask.any() and name_col and name_col in raw_df.columns:
            mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())

        if not mask.any():
            # nothing to include from this file
            continue

        filtered = raw_df[mask].copy()
        if filtered.empty:
            continue

        # ensure timestamp col exists and parsed
        if tcol and tcol in filtered.columns:
            try:
                filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
            except Exception:
                pass

        # compute swipe gaps (seconds)
        if tcol and tcol in filtered.columns:
            filtered = filtered.sort_values(by=tcol)
            filtered['_prev_ts'] = filtered[tcol].shift(1)
            try:
                filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
            except Exception:
                filtered['_swipe_gap_seconds'] = 0.0
        else:
            filtered['_swipe_gap_seconds'] = 0.0

        # compute zone per row (use door+direction if available)
        try:
            if door_col and door_col in filtered.columns:
                if dir_col and dir_col in filtered.columns:
                    filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                else:
                    filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
            else:
                filtered['_zone'] = filtered.get('PartitionName2', None)
        except Exception:
            filtered['_zone'] = None

        # normalize columns into common shape and append rows
        for _, r in filtered.iterrows():
            row = {}
            row['EmployeeName'] = _to_python_scalar(r.get(name_col)) if (name_col and name_col in filtered.columns) else None
            # EmployeeID attempts: Int1/Text12/EmployeeID
            emp_val = None
            if emp_col and emp_col in filtered.columns:
                emp_val = _to_python_scalar(r.get(emp_col))
            else:
                # fallbacks
                for cand in ('int1','text12','employeeid','employee_identity','employeeidentity'):
                    if cand in cols_lower and cols_lower[cand] in filtered.columns:
                        emp_val = _to_python_scalar(r.get(cols_lower[cand]))
                        if emp_val:
                            break
            row['EmployeeID'] = emp_val
            row['Card'] = _to_python_scalar(r.get(card_col)) if (card_col and card_col in filtered.columns) else None

            # Date and Time
            if tcol and tcol in filtered.columns:
                ts = r.get(tcol)
                try:
                    ts_py = pd.to_datetime(ts)
                    row['Date'] = ts_py.date().isoformat()
                    row['Time'] = ts_py.time().isoformat()
                    row['LocaleMessageTime'] = ts_py.isoformat()
                except Exception:
                    txt = str(r.get(tcol))
                    row['Date'] = txt[:10]
                    row['Time'] = txt[11:19] if len(txt) >= 19 else None
                    row['LocaleMessageTime'] = txt
            else:
                row['Date'] = None
                row['Time'] = None
                row['LocaleMessageTime'] = None

            row['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
            row['SwipeGap'] = format_seconds_to_hms(row['SwipeGapSeconds'])

            row['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
            row['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else None
            row['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None

            # Zone (computed above)
            try:
                zone_val = r.get('_zone') if '_zone' in r else None
                if zone_val is None:
                    # fallback from door/direction
                    zone_val = map_door_to_zone(row['Door'], row['Direction'])
                row['Zone'] = _to_python_scalar(zone_val)
            except Exception:
                row['Zone'] = None

            row['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
            row['_source_file'] = fp.name
            all_rows.append(row)

    if not all_rows:
        return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

    df_out = pd.DataFrame(all_rows)

    # Build two sheets as requested (with exact requested column order)
    details_cols = ['EmployeeName','EmployeeID','Door','Direction','Zone','Date','LocaleMessageTime','SwipeGapSeconds','PartitionName2','_source_file']
    timeline_cols = ['EmployeeName','EmployeeID','Card','Date','Time','SwipeGapSeconds','Door','Direction','Zone','Note','_source_file']

    details_df = df_out[[c for c in details_cols if c in df_out.columns]].copy()
    timeline_df = df_out[[c for c in timeline_cols if c in df_out.columns]].copy()

    # Create excel in-memory
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            details_df.to_excel(writer, sheet_name='Details — Evidence', index=False)
            timeline_df.to_excel(writer, sheet_name='Swipe timeline', index=False)
            writer.save()
            output.seek(0)
    except Exception as e:
        logging.exception("Failed to create Excel: %s", e)
        return jsonify({"error":"failed to create excel"}), 500

    # If openpyxl available, apply formatting (bold header, center align, borders)
    if OPENPYXL_AVAILABLE:
        try:
            wb = load_workbook(output)
            thin = Side(border_style="thin", color="000000")
            thick = Side(border_style="medium", color="000000")
            for ws in wb.worksheets:
                # header styling
                header = ws[1]
                for cell in header:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
                # data rows: center & thin border
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                        cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
                # autosize columns (best-effort)
                for col in ws.columns:
                    max_len = 0
                    col_letter = col[0].column_letter
                    for cell in col:
                        try:
                            v = str(cell.value) if cell.value is not None else ""
                        except Exception:
                            v = ""
                        if len(v) > max_len:
                            max_len = len(v)
                    # limit column width
                    width = min(max(10, max_len + 2), 50)
                    ws.column_dimensions[col_letter].width = width
            # write back to bytes
            out2 = io.BytesIO()
            wb.save(out2)
            out2.seek(0)
            return send_file(out2, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        except Exception:
            logging.exception("Excel styling failed, returning raw file")
            output.seek(0)
            return send_file(output, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    else:
        # fallback: return raw excel binary without styling
        output.seek(0)
        return send_file(output, as_attachment=True,
                         download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                         mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")


@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    """
    Serve raw swipe CSVs from outputs/ (filename should be the file name only).
    """
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    # send file
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)


@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500


# ------------------------------
# Improved chatbot endpoint & helpers (replacement)
# ------------------------------

# try to import helpers from trend_runner (they may or may not be present)
try:
    from trend_runner import _read_past_trend_csvs, _normalize_id_val, SCENARIO_EXPLANATIONS
except Exception:
    _read_past_trend_csvs = None
    _normalize_id_val = None
    SCENARIO_EXPLANATIONS = {}

# helper to load the most recent single trend CSV (fast for "today" queries)
def _load_latest_trend_df(outdir: Path):
    csvs = sorted(outdir.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return None, None
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        try:
            df = pd.read_csv(latest, dtype=str)
        except Exception:
            return None, None
    return df, latest.name

# helper: find person rows across recent days (useful for "last N days")
def _find_person_rows(identifier: str, days: int = 90, outdir: Path = DEFAULT_OUTDIR):
    # normalize token (strip ".0" floats etc)
    if _normalize_id_val:
        norm = _normalize_id_val(identifier)
    else:
        norm = str(identifier).strip()
        if '.' in norm:
            try:
                f = float(norm)
                if f.is_integer():
                    norm = str(int(f))
            except Exception:
                pass
    # read past trend CSVs using trend_runner helper when available
    today = datetime.now().date()
    try:
        if _read_past_trend_csvs:
            past = _read_past_trend_csvs(str(outdir), days, today)
        else:
            # fallback: read files manually
            files = sorted(Path(outdir).glob("trend_pune_*.csv"), reverse=True)
            dfs = []
            cutoff = today - timedelta(days=days)
            for fp in files:
                try:
                    tmp = pd.read_csv(fp, parse_dates=['Date'])
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                    tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                    dfs.append(tmp)
                except Exception:
                    try:
                        tmp = pd.read_csv(fp, dtype=str)
                        if 'Date' in tmp.columns:
                            tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                            tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                            dfs.append(tmp)
                    except Exception:
                        continue
            past = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
    except Exception:
        past = pd.DataFrame()

    if past is None or past.empty:
        return pd.DataFrame()

    # search by EmployeeID, person_uid, EmployeeIdentity, CardNumber, Int1/Text12 if present
    match_mask = pd.Series(False, index=past.index)
    for col in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
        if col in past.columns:
            try:
                match_mask = match_mask | (past[col].astype(str).fillna('').str.strip() == str(norm).strip())
            except Exception:
                continue

    # also try numeric equality
    if not match_mask.any():
        try:
            qnum = float(norm)
            for col in ('EmployeeID','Int1'):
                if col in past.columns:
                    try:
                        numcol = pd.to_numeric(past[col], errors='coerce')
                        match_mask = match_mask | (numcol == qnum)
                    except Exception:
                        continue
        except Exception:
            pass

    # fuzzy name match if nothing matched above
    if not match_mask.any() and 'EmployeeName' in past.columns:
        names = past['EmployeeName'].dropna().astype(str).unique().tolist()
        close = difflib.get_close_matches(str(identifier), names, n=5, cutoff=0.7)
        if close:
            match_mask = match_mask | past['EmployeeName'].astype(str).isin(close)

    return past[match_mask].copy()

# simple sentence builder for scenario codes -> human text
def _explain_scenario_code(code):
    if not code:
        return None
    code = str(code).strip()
    # prefer full mapping if available
    if code in SCENARIO_EXPLANATIONS:
        try:
            # SCENARIO_EXPLANATIONS lambdas expect a row; we can't run them here safely.
            # But we can return a friendly mapping key -> sentence fallback.
            # If mapping is a callable, call with empty dict to get neutral sentence.
            fn = SCENARIO_EXPLANATIONS.get(code)
            try:
                txt = fn({})
                return txt
            except Exception:
                return code.replace("_", " ").replace(">= ", "≥ ")
        except Exception:
            return code.replace("_", " ").replace(">= ", "≥ ")
    # fallback readable version
    return code.replace("_", " ").replace(">=", "≥")

# fallback mapping for numeric RiskScore -> (score, label)
def _map_score_to_label_fallback(score: float):
    try:
        s = float(score)
    except Exception:
        return (0.0, "Low")
    if s >= 0.75:
        return (s, "High")
    if s >= 0.4:
        return (s, "Medium")
    return (s, "Low")

@app.route('/chatbot/query', methods=['POST'])
def chatbot_query():
    """
    Improved chatbot query handler.
    Payload: { "q": "<free text query>", "top_k": 5, "lang": "en" }
    Recognised intents:
      - who is high risk today / who is low risk today
      - show me <EmployeeID|name> last <N> days
      - explain <scenario_code>
      - trend details for today / top reasons today
    Returns: { "answer": "<nl text>", "evidence": [ {source, snippet}, ... ] }
    """
    payload = request.get_json(force=True)
    q = (payload.get('q') or '').strip()
    if not q:
        return jsonify({"error":"query text 'q' required"}), 400

    lang = payload.get('lang')  # optional, frontend can provide (we will echo answer in same language string)
    q_l = q.lower().strip()

    # intent: who is high/low risk today
    if re.search(r"\bwho is (high|low) risk\b", q_l) or re.search(r"\b(high|low) risk (people|persons|people) (today)?\b", q_l):
        want = 'high' if 'high' in q_l else 'low' if 'low' in q_l else None
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer": "No trend data available.", "evidence": []})
        # ensure RiskLevel column exists
        if 'RiskLevel' not in df.columns:
            # try to map RiskScore to label if RiskScore present
            if 'RiskScore' in df.columns:
                def _map_rs(s):
                    try:
                        if pd.isna(s):
                            return 'Low'
                    except Exception:
                        pass
                    try:
                        # if external helper available, prefer it
                        if 'map_score_to_label' in globals() and callable(globals().get('map_score_to_label')):
                            try:
                                return globals().get('map_score_to_label')(float(s))[1]
                            except Exception:
                                pass
                        # fallback
                        return _map_score_to_label_fallback(float(s))[1]
                    except Exception:
                        return 'Low'
                df['RiskLevel'] = df['RiskScore'].apply(lambda s: _map_rs(s))
            else:
                df['RiskLevel'] = df.get('RiskLevel', 'Low')

        if want == 'high':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'high']
        elif want == 'low':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'low']
        else:
            sel = df

        names = sel['EmployeeName'].dropna().astype(str).unique().tolist()
        if not names:
            ans = f"No {want} risk persons found in the latest data." if want else "No persons found."
            return jsonify({"answer": ans, "evidence": []})
        else:
            # limit list length
            ans = f"{want.capitalize()} risk persons today: " + ", ".join(names[:40])
            sample = _clean_sample_df(sel.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})

    # intent: explain <scenario>
    m = re.match(r".*\bexplain\s+([A-Za-z0-9_\-]+)\b.*", q_l)
    if m:
        code = m.group(1).strip()
        explanation = _explain_scenario_code(code)
        ans = f"Explanation for '{code}': {explanation}"
        return jsonify({"answer": ans, "evidence": []})

    # intent: trend details for today — top reasons
    if 'trend details' in q_l or 'top reasons' in q_l or 'trend details for today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        # aggregate Reasons column
        if 'Reasons' in df.columns:
            reasons = {}
            for v in df['Reasons'].dropna().astype(str):
                for part in re.split(r'[;,\|]', v):
                    key = part.strip()
                    if key:
                        reasons[key] = reasons.get(key, 0) + 1
            # sort
            top = sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:10]
            if not top:
                return jsonify({"answer":"No reason counts available today.","evidence":[]})
            ans = "Top reasons today: " + ", ".join([f"{k} ({c})" for k,c in top])
            # include small evidence of rows where those reasons occurred
            sample = []
            try:
                top_reasons = [k for k,_ in top]
                mask = df['Reasons'].astype(str).apply(lambda s: any(tr in s for tr in top_reasons))
                sample_df = df[mask].head(10)
                sample = _clean_sample_df(sample_df, max_rows=10)
            except Exception:
                sample = []
            return jsonify({"answer": ans, "evidence": sample})

    # intent: show me <id|name> last N days
    m = re.match(r".*\bshow (?:me )?([A-Za-z0-9\-\:\s]+?) (?:for )?(?:last )?(\d+)\s*days\b", q_l)
    if not m:
        # also accept "show me 320172 last 90 days" or "show 320172 last 90 days" (looser)
        m = re.match(r".*\b(show|display)\s+(?:me\s+)?([A-Za-z0-9\-\:\s]+?)\s+last\s+(\d+)\s*days\b", q_l)
    if m:
        if len(m.groups()) == 2:
            identifier, days = m.group(1).strip(), int(m.group(2))
        else:
            # if earlier regex produced different groups
            identifier = m.group(1).strip()
            days = int(m.group(2))
        # find person rows in past `days`
        rows = _find_person_rows(identifier, days=days, outdir=DEFAULT_OUTDIR)
        if rows is None or rows.empty:
            return jsonify({"answer": f"No records found for '{identifier}' in last {days} days.", "evidence": []})
        # prepare textual summary
        flagged = rows[rows.get('IsFlagged', False) == True] if 'IsFlagged' in rows.columns else pd.DataFrame()
        flagged_count = int(len(flagged))
        total_days = int(len(rows))
        latest_row = rows.sort_values('Date', ascending=False).iloc[0].to_dict()
        name = latest_row.get('EmployeeName') or latest_row.get('person_uid') or latest_row.get('EmployeeID')
        ans = f"Found {total_days} day(s) for {name} in the last {days} days. Flagged days: {flagged_count}."
        sample = _clean_sample_df(rows.sort_values('Date', ascending=False).head(10), max_rows=10)
        return jsonify({"answer": ans, "evidence": sample})

    # fallback: try to detect single-employee "show me <id|name> last 90 days" shorter forms
    m2 = re.match(r".*\bshow (?:me )?([A-Za-z0-9\-\:\s]+)\b.*(90|30|7)\s*days?", q_l)
    if m2:
        identifier = m2.group(1).strip()
        days = int(m2.group(2))
        rows = _find_person_rows(identifier, days=days, outdir=DEFAULT_OUTDIR)
        if rows is None or rows.empty:
            return jsonify({"answer": f"No records found for '{identifier}' in last {days} days.", "evidence": []})
        flagged = rows[rows.get('IsFlagged', False) == True] if 'IsFlagged' in rows.columns else pd.DataFrame()
        ans = f"Found {len(rows)} day(s) for {identifier}. Flagged days: {len(flagged)}."
        sample = _clean_sample_df(rows.sort_values('Date', ascending=False).head(10), max_rows=10)
        return jsonify({"answer": ans, "evidence": sample})

    # last fallback: try simple heuristics like "who is present today" or "present today"
    if 'present today' in q_l or 'who is present today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'PresentToday' in df.columns:
            present = df[df['PresentToday'] == True]
            names = present['EmployeeName'].dropna().unique().tolist()
            ans = f"Present today: {', '.join(names[:40]) if names else 'None'}"
            sample = _clean_sample_df(present.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})
        else:
            return jsonify({"answer":"PresentToday field not available in latest trends.","evidence":[]})

    # final graceful fallback with helpful hint
    hint = "I can answer: 'Who is high risk today', 'Who is low risk today', 'Show me <EmployeeID|Name> last 90 days', 'Explain <scenario_code>', 'Trend details for today — top reasons'."
    return jsonify({"answer": f"I can help with trend & risk questions. I recognized: {q}. Try: {hint}", "evidence":[]})


# Serve employee image route (defined after app)
@app.route('/employee/<empid>/image', methods=['GET'])
def serve_employee_image(empid):
    """
    Serve the portrait for a personnel record where ParentId = empid (ACVSCore.Images).
    Uses the query you provided:
      SELECT AI.Image AS ImageBuffer FROM ACVSCore.Access.Images AI WHERE AI.ParentId = @id
    If found, returns binary with best-effort content-type detection.
    """
    if empid is None:
        return jsonify({"error": "employee id required"}), 400

    try:
        img_bytes = get_person_image_bytes(empid)
        if not img_bytes:
            return jsonify({"error": "no image found"}), 404

        # try to detect jpeg/png
        header = img_bytes[:8]
        content_type = 'application/octet-stream'
        if len(header) >= 2 and header[0] == 0xFF and header[1] == 0xD8:
            content_type = 'image/jpeg'
        elif header.startswith(b'\x89PNG\r\n\x1a\n'):
            content_type = 'image/png'
        # send as file-like
        bio = io.BytesIO(img_bytes)
        bio.seek(0)
        return send_file(bio, mimetype=content_type)
    except Exception:
        logging.exception("Error serving image for employee %s", empid)
        return jsonify({"error": "failed to serve image"}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)





2) refer above logic and upadte here ...only for image and Email 



# backend/app.py
from flask import Flask, jsonify, request, send_from_directory, send_file
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re
import io
import os
import difflib
from typing import Optional, Dict, Any

# ---------- CORS / App setup (moved early to avoid NameError) ----------
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

from flask import Flask  # ensure Flask is available here (already imported at top in your file)

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    import logging
    logging.warning("flask_cors not available; continuing without CORS.")


# project imports (may raise if not present; handled later)
from duration_report import REGION_CONFIG
from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE

# trend_runner helpers (some may not exist depending on your version)
try:
    from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR, read_90day_cache, compute_violation_days_map, _strip_uid_prefix as _strip_uid_prefix_tr
except Exception:
    # graceful fallback if some helpers are missing
    try:
        from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR
    except Exception:
        run_trend_for_date = None
        build_monthly_training = None
        OUTDIR = None
    # provide safe stubs for optional helpers used later
    compute_violation_days_map = None
    _strip_uid_prefix_tr = (lambda x: x)

# ---------- Ensure outputs directory exists early ----------
BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)

OVERRIDES_FILE = DEFAULT_OUTDIR / "overrides.csv"

# ---------- small helpers ----------
def _slug_city(city: str) -> str:
    if not city:
        return "pune"
    return str(city).strip().lower().replace(" ", "_")

# minimal id normalization helper
def _normalize_id_local(v):
    try:
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() in ('nan','none','null','-'):
            return None
        if '.' in s:
            try:
                f = float(s)
                if math.isfinite(f) and f.is_integer():
                    return str(int(f))
            except Exception:
                pass
        return s
    except Exception:
        return None


# ---------- overrides file helpers ----------
def _load_overrides():
    if not OVERRIDES_FILE.exists():
        return {}
    try:
        df = pd.read_csv(OVERRIDES_FILE, dtype=str)
        out = {}
        for _, r in df.iterrows():
            emp = str(r.get('EmployeeID') or r.get('person_uid') or '').strip()
            if not emp:
                continue
            out[emp] = {
                'level': str(r.get('OverrideLevel') or '').strip(),
                'reason': str(r.get('Reason') or '').strip(),
                'ts': str(r.get('Timestamp') or '').strip()
            }
        return out
    except Exception:
        logging.exception("Failed reading overrides file")
        return {}

def _save_override(employee_key, level, reason):
    now = datetime.now().isoformat()
    row = {'EmployeeID': employee_key, 'OverrideLevel': level, 'Reason': reason or '', 'Timestamp': now}
    try:
        if OVERRIDES_FILE.exists():
            df = pd.read_csv(OVERRIDES_FILE, dtype=str)
            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        else:
            df = pd.DataFrame([row])
        df.to_csv(OVERRIDES_FILE, index=False)
        return True
    except Exception:
        logging.exception("Failed to save override")
        return False

# ODBC driver (env or default)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")
if not has_cors:
    @app.after_request
    def _add_cors_headers(resp):
        # relaxed for dev — you can tighten this in prod
        resp.headers['Access-Control-Allow-Origin'] = '*'
        resp.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization'
        resp.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
        return resp

# ---------- ACVSCore connection + image helpers ----------
_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20

def _get_acvscore_conn():
    try:
        import pyodbc
    except Exception:
        logging.debug("pyodbc not installed; ACVSCore lookups unavailable.")
        return None

    now = datetime.now().timestamp()
    last = _acvscore_backoff.get("ts")
    if last and _acvscore_backoff.get("failed") and (now - last) < _ACVSCORE_BACKOFF_SECONDS:
        logging.debug("Skipping ACVSCore connection attempt (backoff active).")
        return None

    tried = []
    for region_key, rc in (REGION_CONFIG or {}).items():
        server = rc.get("server")
        user = rc.get("user")
        pwd = rc.get("password")
        if not server:
            continue

        # SQL auth
        if user and pwd:
            tried.append(f"{region_key}@{server}(sql)")
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};SERVER={server};DATABASE=ACVSCore;UID={user};PWD={pwd};TrustServerCertificate=Yes;"
            )
            try:
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (sql auth).", server, region_key)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("SQL auth to %s failed: %s", server, e)

        # Trusted connection fallback
        tried.append(f"{region_key}@{server}(trusted)")
        conn_str_trusted = (
            f"DRIVER={{{ODBC_DRIVER}}};SERVER={server};DATABASE=ACVSCore;Trusted_Connection=yes;TrustServerCertificate=Yes;"
        )
        try:
            conn = pyodbc.connect(conn_str_trusted, autocommit=True, timeout=5)
            logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (trusted connection).", server, region_key)
            _acvscore_backoff["ts"] = None
            _acvscore_backoff["failed"] = False
            return conn
        except Exception as e:
            logging.debug("Trusted connection to %s failed: %s", server, e)
            continue

    # fallback to global ACVSCORE_DB_CONFIG if present
    try:
        if 'ACVSCORE_DB_CONFIG' in globals() and isinstance(globals().get('ACVSCORE_DB_CONFIG'), dict):
            cfg = globals().get('ACVSCORE_DB_CONFIG')
            server = cfg.get('server')
            user = cfg.get('user')
            pwd = cfg.get('password')
            database = cfg.get('database', 'ACVSCore')
            tried.append(f"ACVSCORE_DB_CONFIG@{server}")
            try:
                conn_str = (
                    f"DRIVER={{{ODBC_DRIVER}}};SERVER={server};DATABASE={database};UID={user};PWD={pwd};TrustServerCertificate=Yes;"
                )
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore using ACVSCORE_DB_CONFIG (%s).", server)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("ACVSCORE_DB_CONFIG connection failed: %s", e)
    except Exception:
        logging.debug("ACVSCORE_DB_CONFIG fallback not available or failed.")

    _acvscore_backoff["ts"] = now
    _acvscore_backoff["failed"] = True
    logging.error("Failed to connect to ACVSCore. Tried: %s", tried)
    return None

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}
def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data



def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    """
    Try to resolve personnel record using a flexible lookup.
    Returns dict with keys: ObjectID (may be None), GUID, Name, EmailAddress, EmployeeEmail, ManagerEmail
    If resolution fails returns empty dict.
    """
    out: Dict[str, Any] = {}
    if candidate_identifier is None:
        return out
    conn = _get_acvscore_conn()
    if conn is None:
        return out

    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        params = (cand, cand, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()
        if row:
            # defensive: normalize and produce EmployeeEmail alias
            objid = row[0] if len(row) > 0 else None
            guid = row[1] if len(row) > 1 else None
            name = row[2] if len(row) > 2 else None
            email = row[3] if len(row) > 3 else None
            mgr =  row[4] if len(row) > 4 else None

            out['ObjectID'] = objid
            out['GUID'] = guid
            out['Name'] = name
            out['EmailAddress'] = email
            # add alias so callers that expect EmployeeEmail will find it
            out['EmployeeEmail'] = email
            out['ManagerEmail'] = mgr
    except Exception:
        logging.exception("Failed personnel lookup for candidate: %s", candidate_identifier)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out



def get_person_image_bytes(parent_id) -> Optional[bytes]:
    """
    Query ACVSCore.Access.Images for top image where ParentId ~ parent_id and return raw bytes.
    Handles common ParentId variants (numeric, 'emp:<id>' prefix, GUID-containing).
    Returns None if not found or on error.
    """
    conn = _get_acvscore_conn()
    if conn is None:
        return None
    try:
        cur = conn.cursor()

        # Try 1: exact match (most common)
        sql = """
            SELECT TOP 1 AI.Image
            FROM ACVSCore.Access.Images AI
            WHERE AI.ParentId = ?
              AND DATALENGTH(AI.Image) > 0
            ORDER BY AI.ObjectID DESC
        """
        try:
            cur.execute(sql, (str(parent_id),))
            row = cur.fetchone()
            if row and row[0] is not None:
                try:
                    return bytes(row[0])
                except Exception:
                    return row[0]
        except Exception:
            # swallow per-attempt errors and try fallbacks
            logging.debug("get_person_image_bytes: primary query failed for ParentId=%s", parent_id)

        # Try 2: prefixed parent id (common pattern: 'emp:<id>')
        try:
            pref = f"emp:{str(parent_id)}"
            sql2 = """
                SELECT TOP 1 AI.Image
                FROM ACVSCore.Access.Images AI
                WHERE (AI.ParentId = ?)
                  AND DATALENGTH(AI.Image) > 0
                ORDER BY AI.ObjectID DESC
            """
            cur.execute(sql2, (pref,))
            row = cur.fetchone()
            if row and row[0] is not None:
                try:
                    return bytes(row[0])
                except Exception:
                    return row[0]
        except Exception:
            logging.debug("get_person_image_bytes: prefixed query failed for ParentId=%s", parent_id)

        # Try 3: parent id might be embedded or GUID-like; use LIKE (slow but fallback)
        try:
            s = str(parent_id)
            # basic guard: only use LIKE for short tokens or GUID-like values
            if len(s) <= 64:
                sql3 = """
                    SELECT TOP 1 AI.Image
                    FROM ACVSCore.Access.Images AI
                    WHERE AI.ParentId LIKE '%' + ? + '%'
                      AND DATALENGTH(AI.Image) > 0
                    ORDER BY AI.ObjectID DESC
                """
                cur.execute(sql3, (s,))
                row = cur.fetchone()
                if row and row[0] is not None:
                    try:
                        return bytes(row[0])
                    except Exception:
                        return row[0]
        except Exception:
            logging.debug("get_person_image_bytes: LIKE fallback failed for ParentId=%s", parent_id)

    except Exception:
        logging.exception("Failed to fetch image for ParentId=%s", parent_id)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass
    return None

# ---------- CORS / App setup ----------
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

# optional excel styling
try:
    from openpyxl import load_workbook
    from openpyxl.styles import Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

# ---------- small utilities ----------
def _to_python_scalar(x):
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass
    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass
    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass
    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        return x
    try:
        return str(x)
    except Exception:
        return None

def format_seconds_to_hms(seconds) -> str:
    """
    Convert seconds (float/int/None) -> HH:MM:SS string.
    Accepts None or non-numeric and returns None.
    """
    try:
        if seconds is None:
            return None
        s = float(seconds)
        if not (s is not None and (s == s) and s >= 0):  # NaN check
            return None
        total = int(round(s))
        hh = total // 3600
        mm = (total % 3600) // 60
        ss = total % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return None


_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')
def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False

_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])
def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _replace_placeholder_strings(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    for col in df.columns:
        try:
            if df[col].dtype == object:
                df[col] = df[col].apply(lambda x: None if (isinstance(x, str) and x.strip().lower() in _PLACEHOLDER_STRS) else x)
        except Exception:
            continue
    return df

_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None

def _find_swipe_files(outdir: Path, date_obj: Optional[date] = None, city_slug: Optional[str] = None):
    p = Path(outdir)
    if not p.exists():
        return []
    candidates = []
    if date_obj:
        target = date_obj.strftime("%Y%m%d")
        patterns = [
            f"swipes_*_{target}.csv",
            f"swipes_{city_slug}_*_{target}.csv" if city_slug else None,
            f"swipe_*_{target}.csv",
            f"*_{target}.csv"
        ]
    else:
        patterns = [
            "swipes_*.csv",
            f"swipes_{city_slug}_*.csv" if city_slug else None,
            "swipe_*.csv",
            "swipes_*.csv"
        ]
    seen = set()
    for pat in patterns:
        if not pat:
            continue
        for fp in sorted(p.glob(pat), reverse=True):
            if fp.name not in seen:
                seen.add(fp.name)
                candidates.append(fp)
    if not candidates:
        for fp in sorted(p.glob("*.csv"), reverse=True):
            ln = fp.name.lower()
            if 'swipe' in ln or 'card' in ln or 'locale' in ln or 'localemessagetime' in ln:
                if fp.name not in seen:
                    seen.add(fp.name)
                    candidates.append(fp)
    return candidates

def _resolve_field_from_record(record: dict, candidate_tokens: list):
    if record is None:
        return None
    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)
    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)
    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue
    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)
    return None






def _clean_sample_df(df: pd.DataFrame,max_rows: int = 100):
    if df is None or df.empty:
        return []
    df = df.copy()
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass
    df = df.where(pd.notnull(df), None)
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []

    # helper regex to find GUIDs inside text
    GUID_IN_TEXT_RE = re.compile(r'[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}')

    for r in rows:
        out = {}
        # copy existing columns with scalar conversion
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        # Prefer feature/duration-derived name fields if present in the raw record
        # Candidate tokens in order:
        name_tokens = ['EmployeeName_feat', 'EmployeeName_dur', 'EmployeeName', 'ObjectName1', 'objectname1', 'employee_name', 'name']
        resolved_name = _resolve_field_from_record(r, name_tokens)
        if resolved_name:
            out['EmployeeName'] = resolved_name
        else:
            # if name is placeholder or looks like guid, try to use EmployeeID as fallback name only if no better name
            maybe_name = out.get('EmployeeName')
            emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity') or out.get('person_uid')
            if (maybe_name in (None, '', 'nan')) or (_looks_like_guid(maybe_name) if isinstance(maybe_name, str) else False):
                if emp_id and not _looks_like_guid(emp_id):
                    out['EmployeeName'] = str(emp_id)

        # --- NEW: Ensure EmployeeID surfaced from best candidates (feat/dur/int/text12) ---
        try:
            emp_candidates = ['EmployeeID_feat', 'EmployeeID_dur', 'EmployeeID', 'Int1', 'Text12']
            emp_val = _resolve_field_from_record(r, emp_candidates)
            if emp_val:
                # normalize floats like '320172.0' -> '320172'
                try:
                    s = str(emp_val).strip()
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    emp_val = s
                except Exception:
                    pass
                # avoid GUID-looking values for EmployeeID
                if not _looks_like_guid(str(emp_val)):
                    out['EmployeeID'] = emp_val
        except Exception:
            pass

        # Ensure EmployeeID normalized as string where possible (keep existing logic)
        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None

        # Duration: prefer explicit Duration strings, then _dur/_feat, then compute from seconds/minutes
        dur_val = _resolve_field_from_record(r, ['Duration', 'Duration_dur', 'Duration_feat'])
        if dur_val:
            out['Duration'] = str(dur_val)
        else:
            # try DurationSeconds
            ds = r.get('DurationSeconds') if 'DurationSeconds' in r else None
            dm = r.get('DurationMinutes') if 'DurationMinutes' in r else None
            try:
                if ds not in (None, '', 'nan') and not (isinstance(ds, float) and math.isnan(ds)):
                    out['Duration'] = format_seconds_to_hms(float(ds))
                elif dm not in (None, '', 'nan') and not (isinstance(dm, float) and math.isnan(dm)):
                    out['Duration'] = format_seconds_to_hms(float(dm) * 60.0)
            except Exception:
                out['Duration'] = out.get('Duration') or None

        # Also include DurationSeconds/Minutes explicitly when present for front-end use
        if 'DurationSeconds' in r and r.get('DurationSeconds') not in (None, '', 'nan'):
            try:
                out['DurationSeconds'] = float(r.get('DurationSeconds'))
            except Exception:
                out['DurationSeconds'] = _to_python_scalar(r.get('DurationSeconds'))
        if 'DurationMinutes' in r and r.get('DurationMinutes') not in (None, '', 'nan'):
            try:
                out['DurationMinutes'] = float(r.get('DurationMinutes'))
            except Exception:
                out['DurationMinutes'] = _to_python_scalar(r.get('DurationMinutes'))

        # --- NEW: coalesce CardNumber from feat/dur/raw columns ---
        try:
            card_val = _resolve_field_from_record(r, ['CardNumber_feat', 'CardNumber_dur', 'CardNumber', 'Card', 'CHUID', 'card', 'value', 'xmlmessage'])
            if card_val:
                cs = str(card_val).strip()
                if _looks_like_guid(cs) or _is_placeholder_str(cs):
                    card_val = None
                else:
                    card_val = cs
            out['CardNumber'] = card_val
        except Exception:
            out['CardNumber'] = out.get('CardNumber') or None

        # --- NEW: ensure EmployeeID is normalized also as string and present in out (fallbacks) ---
        try:
            if not out.get('EmployeeID'):
                emp_val2 = _resolve_field_from_record(r, ['EmployeeID_feat', 'EmployeeID_dur', 'EmployeeID', 'Int1', 'Text12'])
                if emp_val2:
                    s = str(emp_val2).strip()
                    try:
                        if '.' in s:
                            f = float(s)
                            if math.isfinite(f) and f.is_integer():
                                s = str(int(f))
                    except Exception:
                        pass
                    if not _looks_like_guid(s) and not _is_placeholder_str(s):
                        out['EmployeeID'] = s
        except Exception:
            pass

        # --- NEW: Replace GUIDs inside ViolationExplanation/Explanation using EmployeeName + EmployeeID if present ---
        try:
            expl = out.get('ViolationExplanation') or out.get('Explanation') or None
            if expl and isinstance(expl, str) and GUID_IN_TEXT_RE.search(expl):
                # compose best display identity: prefer name then id
                name_part = out.get('EmployeeName') or ''
                id_part = out.get('EmployeeID') or ''
                display = None
                if name_part and id_part:
                    display = f"{name_part} ({id_part})"
                elif name_part:
                    display = f"{name_part}"
                elif id_part:
                    display = f"{id_part}"
                if display:
                    out['ViolationExplanation'] = GUID_IN_TEXT_RE.sub(str(display), expl)
                    out['Explanation'] = out.get('ViolationExplanation')
        except Exception:
            pass

        cleaned.append(out)
    return cleaned


# -----------------------
# Routes
# -----------------------
@app.route('/')
def root():
    return "Trend Analysis API — Multi-city"

@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.get_json(force=True) or {}
        else:
            try:
                params = request.form.to_dict() or {}
            except Exception:
                params = {}

    date_str = (params.get('date') or params.get('Date') or '').strip() or None
    start_str = (params.get('start') or params.get('Start') or '').strip() or None
    end_str = (params.get('end') or params.get('End') or '').strip() or None

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    regions_param = params.get('regions') or params.get('region') or ''
    if regions_param:
        regions = [r.strip().lower() for r in re.split(r'[;,|]', str(regions_param)) if r.strip()]
    else:
        try:
            regions = [k.lower() for k in list(REGION_CONFIG.keys())]
        except Exception:
            regions = ['apac']

    valid_regions = []
    for r in regions:
        if r in (REGION_CONFIG or {}):
            valid_regions.append(r)
        else:
            logging.debug("Requested region '%s' not in REGION_CONFIG - skipping", r)
    if not valid_regions:
        valid_regions = [k.lower() for k in REGION_CONFIG.keys()] if REGION_CONFIG else ['apac']
    params['_regions_to_run'] = valid_regions

    city_param = params.get('city') or params.get('site') or params.get('site_name') or None
    city_slug = _slug_city(city_param) if city_param else None
    params['_city'] = city_slug

    combined_rows = []
    files = []

    for d in dates:
        try:
            if run_trend_for_date is None:
                raise RuntimeError("run_trend_for_date helper not available in trend_runner")
            try:
                df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug)
            except TypeError:
                try:
                    df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
                except Exception:
                    # Last-resort: try duration_report fallback if available
                    try:
                        from duration_report import run_for_date as _dr_run_for_date
                        region_results = _dr_run_for_date(d, valid_regions, str(DEFAULT_OUTDIR), city_param)
                        combined_list = []
                        for rkey, res in (region_results or {}).items():
                            try:
                                df_dur = res.get('durations')
                                if df_dur is not None and not df_dur.empty:
                                    combined_list.append(df_dur)
                            except Exception:
                                continue
                        df = pd.concat(combined_list, ignore_index=True) if combined_list else pd.DataFrame()
                    except Exception:
                        raise
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_{city_slug}_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        if df is None or (hasattr(df, 'empty') and df.empty):
            continue

        try:
            df = _replace_placeholder_strings(df)
        except Exception:
            pass

        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        combined_rows.append(df)

    combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()
    combined_df = _replace_placeholder_strings(combined_df)

    try:
        if not combined_df.empty:
            if 'person_uid' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['person_uid'].dropna().astype(str).nunique())
            elif 'EmployeeID' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['EmployeeID'].dropna().astype(str).nunique())
            else:
                raw_unique_person_uids = int(len(combined_df))
        else:
            raw_unique_person_uids = 0
    except Exception:
        raw_unique_person_uids = int(len(combined_df)) if combined_df is not None else 0

    try:
        if not combined_df.empty and 'IsFlagged' in combined_df.columns:
            flagged_df = combined_df[combined_df['IsFlagged'] == True].copy()
        else:
            flagged_df = pd.DataFrame()
    except Exception:
        flagged_df = pd.DataFrame()

    try:
        analysis_count = int(raw_unique_person_uids)
    except Exception:
        analysis_count = int(len(combined_df)) if combined_df is not None else 0

    try:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = float((flagged_count / analysis_count * 100.0) if analysis_count and analysis_count > 0 else 0.0)
    except Exception:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = 0.0

    try:
        sample_source = flagged_df if not flagged_df.empty else combined_df
        samples = _clean_sample_df(sample_source.head(10), max_rows=10) if sample_source is not None and not sample_source.empty else []
    except Exception:
        samples = []

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "aggregated_rows_total_raw": int(len(combined_df)),
        "aggregated_unique_persons": int(analysis_count),
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "flagged_rate_percent": float(flagged_rate_pct),
        "files": files,
        "sample": (samples[:10] if isinstance(samples, list) else samples),
        "reasons_count": {},
        "risk_counts": {},
        "flagged_persons": (samples if samples else []),
        "_raw_unique_person_uids": int(raw_unique_person_uids),
        "regions_run": params.get('_regions_to_run', []),
        "city_used": city_slug
    }

    return jsonify(resp)




@app.route('/latest', methods=['GET'])
def latest_results():
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]

    start_date_iso = None
    end_date_iso = None
    try:
        m = re.search(r'(\d{8})', latest.name)
        if m:
            ymd = m.group(1)
            dt = datetime.strptime(ymd, "%Y%m%d").date()
            start_date_iso = dt.isoformat()
            end_date_iso = dt.isoformat()
    except Exception:
        start_date_iso = None
        end_date_iso = None

    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)

    df = _replace_placeholder_strings(df)

    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in df.columns), None)

    def _norm_val_for_latest(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                fv = float(s)
                if math.isfinite(fv) and fv.is_integer():
                    s = str(int(fv))
        except Exception:
            pass
        return s

    if id_col is None:
        unique_persons = int(len(df))
    else:
        ids_series = df[id_col].apply(_norm_val_for_latest) if id_col in df.columns else pd.Series([None]*len(df))
        if id_col != 'person_uid' and 'person_uid' in df.columns:
            ids_series = ids_series.fillna(df['person_uid'].astype(str).replace('nan','').replace('None',''))
        unique_persons = int(len(set([x for x in ids_series.unique() if x])))

    sample = _clean_sample_df(df, max_rows=5)
    resp = {
        "file": latest.name,
        "rows_raw": int(len(df)),
        "rows": unique_persons,
        "sample": sample,
        "start_date": start_date_iso,
        "end_date": end_date_iso,
        "city": city_slug
    }
    return jsonify(resp)

    # --- Enrich sample rows with EmployeeEmail and imageUrl (best-effort) ---
    try:
        if isinstance(samples, list) and samples:
            for s in samples:
                try:
                    # prefer EmployeeID / person_uid / EmployeeName as lookup token
                    lookup_token = s.get('EmployeeID') or s.get('person_uid') or s.get('EmployeeName') or s.get('EmployeeIdentity')
                    pi = {}
                    if lookup_token:
                        try:
                            pi = get_personnel_info(lookup_token) or {}
                        except Exception:
                            pi = {}
                    # set email if found from personnel info
                    if pi:
                        email = pi.get('EmailAddress') or pi.get('EmployeeEmail') or pi.get('Email') or None
                        if email:
                            s['EmployeeEmail'] = email
                        # prefer ObjectID then GUID thenEmployeeID for image parent
                        objid = pi.get('ObjectID') or pi.get('GUID') or None
                        if objid:
                            try:
                                base = (request.url_root or request.host_url).rstrip('/')
                            except Exception:
                                base = ''
                            s['imageUrl'] = f"{base}/employee/{objid}/image" if base else f"/employee/{objid}/image"
                            # best-effort flag whether image exists (may be False if DB missing)
                            try:
                                b = get_person_image_bytes(objid)
                                s['HasImage'] = True if b else False
                            except Exception:
                                s['HasImage'] = False

                    # fallback: if still no email, try common columns inside combined_df (if available)
                    if not s.get('EmployeeEmail'):
                        try:
                            for col in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                                if col in combined_df.columns:
                                    val = combined_df.iloc[0].get(col)
                                    if val not in (None, '', 'nan'):
                                        s['EmployeeEmail'] = val
                                        break
                        except Exception:
                            pass

                    # fallback: if no imageUrl but EmployeeID present, build imageUrl using EmployeeID (frontend can fetch /employee/<id>/image)
                    if not s.get('imageUrl') and s.get('EmployeeID'):
                        try:
                            base = (request.url_root or request.host_url).rstrip('/')
                        except Exception:
                            base = ''
                        if base:
                            s['imageUrl'] = f"{base}/employee/{s.get('EmployeeID')}/image"
                        else:
                            s['imageUrl'] = f"/employee/{s.get('EmployeeID')}/image"
                except Exception:
                    # do not let one row enrichment break the whole response
                    continue
    except Exception:
        # non-fatal
        pass




@app.route('/record', methods=['GET'])
def get_record():
    q = request.args.get('employee_id') or request.args.get('person_uid')
    include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    df_list = []
    for fp in csvs:
        try:
            tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
        except Exception:
            try:
                tmp = pd.read_csv(fp, dtype=str)
                if 'Date' in tmp.columns:
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
            except Exception:
                continue
        df_list.append(tmp)
    if not df_list:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200
    df = pd.concat(df_list, ignore_index=True)

    df = _replace_placeholder_strings(df)

    if q is None:
        cleaned = _clean_sample_df(df, max_rows=10)
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    q_str = str(q).strip()
    def normalize_series(s):
        if s is None:
            return pd.Series([''] * len(df))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    found_mask = pd.Series(False, index=df.index)
    if 'EmployeeID' in df.columns:
        emp_series = normalize_series(df['EmployeeID'])
        found_mask = found_mask | (emp_series == q_str)
    if 'person_uid' in df.columns:
        uid_series = normalize_series(df['person_uid'])
        found_mask = found_mask | (uid_series == q_str)
    if 'Int1' in df.columns and not found_mask.any():
        int1_series = normalize_series(df['Int1'])
        found_mask = found_mask | (int1_series == q_str)

    if not found_mask.any():
        try:
            q_numeric = float(q_str)
            if 'EmployeeID' in df.columns:
                emp_numeric = pd.to_numeric(df['EmployeeID'], errors='coerce')
                found_mask = found_mask | (emp_numeric == q_numeric)
            if 'Int1' in df.columns and not found_mask.any():
                int_numeric = pd.to_numeric(df['Int1'], errors='coerce')
                found_mask = found_mask | (int_numeric == q_numeric)
        except Exception:
            pass

    matched = df[found_mask].copy()
    if matched.empty:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    cleaned_matched = _clean_sample_df(matched, max_rows=len(matched))



    # enrich aggregated rows (email, image, violation days, explanation)
    try:
        try:
            # lazy import from trend_runner (avoid import-time circular issues)
            from trend_runner import compute_violation_days_map, _strip_uid_prefix as _strip_uid_prefix_tr
            violation_map = compute_violation_days_map(str(DEFAULT_OUTDIR), 90, datetime.now().date())
        except Exception:
            violation_map = {}
            _strip_uid_prefix_tr = (lambda x: x)
    except Exception:
        violation_map = {}
        _strip_uid_prefix_tr = (lambda x: x)

    matched_indexed = matched.reset_index(drop=True)
  

    # more robust candidate resolution + explanation/email/image enrichment
    GUID_IN_TEXT_RE = re.compile(
        r'[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}'
    )

    for idx_c, cleaned in enumerate(cleaned_matched):
        candidate_row = None
        try:
            # 1) try exact matches by person_uid, EmployeeID, EmployeeName
            if cleaned.get('person_uid') and 'person_uid' in matched_indexed.columns:
                mr = matched_indexed[matched_indexed['person_uid'].astype(str).str.strip() == str(cleaned['person_uid']).strip()]
                if not mr.empty:
                    candidate_row = mr.iloc[0].to_dict()

            if candidate_row is None and cleaned.get('EmployeeID') and 'EmployeeID' in matched_indexed.columns:
                mr = matched_indexed[matched_indexed['EmployeeID'].astype(str).str.strip() == str(cleaned['EmployeeID']).strip()]
                if not mr.empty:
                    candidate_row = mr.iloc[0].to_dict()

            if candidate_row is None and cleaned.get('EmployeeName') and 'EmployeeName' in matched_indexed.columns:
                mr = matched_indexed[matched_indexed['EmployeeName'].astype(str).str.strip().str.lower() == str(cleaned['EmployeeName']).strip().lower()]
                if not mr.empty:
                    candidate_row = mr.iloc[0].to_dict()

            # 2) try other useful columns (CardNumber, EmployeeIdentity, Int1, Text12, ObjectIdentity1, ObjectID)
            if candidate_row is None and not matched_indexed.empty:
                for ccol in ('CardNumber', 'EmployeeIdentity', 'Int1', 'Text12', 'ObjectIdentity1', 'ObjectID', 'GUID'):
                    if ccol in matched_indexed.columns and cleaned.get(ccol) not in (None, '', 'nan'):
                        mr = matched_indexed[matched_indexed[ccol].astype(str).str.strip() == str(cleaned.get(ccol)).strip()]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                            break

            # 3) last resort - take the first matched row (if any)
            if candidate_row is None and not matched_indexed.empty:
                candidate_row = matched_indexed.iloc[0].to_dict()
        except Exception:
            candidate_row = None

        # Build Explanation: prefer explicit Explanation, else build from Reasons
        violation_expl = None
        try:
            if candidate_row:
                violation_expl = candidate_row.get('Explanation') or candidate_row.get('explanation') or None

            if not violation_expl:
                reasons = cleaned.get('Reasons') or (candidate_row.get('Reasons') if candidate_row else None)
                if reasons:
                    parts = [p.strip() for p in re.split(r'[;,\|]', str(reasons)) if p.strip()]
                    mapped = []
                    for p in parts:
                        try:
                            if 'SCENARIO_EXPLANATIONS' in globals() and p in SCENARIO_EXPLANATIONS:
                                mapped.append(SCENARIO_EXPLANATIONS[p](candidate_row or {}))
                            else:
                                mapped.append(p.replace("_", " ").replace(">=", "≥"))
                        except Exception:
                            mapped.append(p)
                    violation_expl = " ".join(mapped) if mapped else None

            # Replace GUIDS that appear inside longer text with a human name (if available)
            if violation_expl:
                try:
                    # prefer candidate_row name, then cleaned name
                    emp_name_for_expl = None
                    if candidate_row:
                        emp_name_for_expl = candidate_row.get('EmployeeName') or candidate_row.get('employee_name') or candidate_row.get('ObjectName1')
                    if not emp_name_for_expl:
                        emp_name_for_expl = cleaned.get('EmployeeName')
                    if emp_name_for_expl:
                        violation_expl = GUID_IN_TEXT_RE.sub(str(emp_name_for_expl), str(violation_expl))
                except Exception:
                    pass
        except Exception:
            violation_expl = None

        # Violation days lookup (unchanged logic)
        try:
            candidates = []
            for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
                if cleaned.get(k) not in (None, '', 'nan'):
                    candidates.append(cleaned.get(k))
            if candidate_row:
                for k in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
                    if candidate_row.get(k) not in (None,'','nan'):
                        candidates.append(candidate_row.get(k))
            vdays = 0
            if violation_map:
                for c in candidates:
                    if c is None:
                        continue
                    n = _normalize_id_local(c)
                    if n and n in violation_map:
                        vdays = int(violation_map.get(n, 0))
                        break
                    try:
                        stripped = _strip_uid_prefix_tr(str(n))
                        if stripped != n and stripped in violation_map:
                            vdays = int(violation_map.get(stripped, 0))
                            break
                    except Exception:
                        pass
        except Exception:
            vdays = 0

        # personnel lookup (try many tokens)
        personnel_info = {}
        try:
            lookup_candidates = []
            if candidate_row:
                for k in ('EmployeeObjID','EmployeeObjId','EmployeeIdentity','ObjectID','GUID','EmployeeID','Int1','Text12','EmployeeName'):
                    if candidate_row.get(k) not in (None,'','nan'):
                        lookup_candidates.append(candidate_row.get(k))
            for k in ('EmployeeID','person_uid','EmployeeName'):
                if cleaned.get(k) not in (None,'','nan'):
                    lookup_candidates.append(cleaned.get(k))

            # try each candidate till we get a useful personnel_info
            for cand in lookup_candidates:
                if cand is None:
                    continue
                try:
                    info = get_personnel_info(cand)
                    if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None or info.get('ManagerEmail') is not None):
                        personnel_info = info
                        break
                except Exception:
                    continue

            # Extra fallback: if still empty and cleaned has EmployeeID, attempt lookup explicitly by EmployeeID
            if not personnel_info:
                try:
                    cand = cleaned.get('EmployeeID') or cleaned.get('EmployeeIdentity')
                    if cand:
                        info = get_personnel_info(cand)
                        if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None):
                            personnel_info = info
                except Exception:
                    pass
        except Exception:
            personnel_info = {}

        # Attach computed fields
        try:
            cleaned['ViolationDaysLast90'] = int(vdays or 0)
            cleaned['ViolationExplanation'] = violation_expl
            cleaned['Explanation'] = violation_expl or cleaned.get('ViolationExplanation') or None

            # Ensure EmployeeName is present (prefer cleaned, then candidate_row, then personnel_info Name)
            try:
                if (not cleaned.get('EmployeeName')) or _looks_like_guid(cleaned.get('EmployeeName')):
                    if candidate_row and candidate_row.get('EmployeeName') and not _looks_like_guid(candidate_row.get('EmployeeName')):
                        cleaned['EmployeeName'] = candidate_row.get('EmployeeName')
                    elif personnel_info and personnel_info.get('Name'):
                        cleaned['EmployeeName'] = personnel_info.get('Name')
                # final pass: remove any GUIDs remaining in Explanation
                if cleaned.get('Explanation') and GUID_IN_TEXT_RE.search(str(cleaned.get('Explanation'))):
                    ename = cleaned.get('EmployeeName') or ''
                    cleaned['Explanation'] = GUID_IN_TEXT_RE.sub(str(ename), str(cleaned.get('Explanation')))
            except Exception:
                pass

            # Populate email from personnel_info OR candidate_row OR common columns in matched_indexed
            if personnel_info:
                cleaned['EmployeeObjID'] = personnel_info.get('ObjectID')
                cleaned['EmployeeEmail'] = personnel_info.get('EmailAddress')
                cleaned['ManagerEmail'] = personnel_info.get('ManagerEmail')
            else:
                # try to pick an email-like column from candidate_row
                if not cleaned.get('EmployeeEmail') and candidate_row:
                    for fk in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                        if candidate_row.get(fk) not in (None, '', 'nan'):
                            cleaned['EmployeeEmail'] = candidate_row.get(fk)
                            break
                # final attempt: look in matched_indexed columns (if candidate_row didn't have it)
                if not cleaned.get('EmployeeEmail'):
                    for col in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                        if col in matched_indexed.columns:
                            try:
                                val = matched_indexed.iloc[0].get(col)
                                if val not in (None, '', 'nan'):
                                    cleaned['EmployeeEmail'] = val
                                    break
                            except Exception:
                                continue

                # image resolution: prefer personnel_info.ObjectID, then candidate_row.ObjectID/EmployeeObjID, otherwise None
                try:
                    img_obj = None
                    if personnel_info and personnel_info.get('ObjectID') is not None:
                        img_obj = personnel_info.get('ObjectID')
                    elif candidate_row:
                        for k in ('EmployeeObjID', 'EmployeeObjId', 'ObjectID', 'ObjectIdentity1'):
                            if candidate_row.get(k) not in (None, '', 'nan'):
                                img_obj = candidate_row.get(k)
                                break

                    if img_obj:
                        # make the image URL absolute so frontend can fetch it regardless of page origin
                        try:
                            base = (request.url_root or request.host_url).rstrip('/')
                        except Exception:
                            base = ''

                        if base:
                            cleaned['imageUrl'] = f"{base}/employee/{img_obj}/image"
                        else:
                            # fallback to relative if request host not available
                            cleaned['imageUrl'] = f"/employee/{img_obj}/image"

                        try:
                            b = get_person_image_bytes(img_obj)
                            cleaned['HasImage'] = True if b else False
                        except Exception:
                            cleaned['HasImage'] = False
                    else:
                        cleaned['imageUrl'] = None
                        cleaned['HasImage'] = False
                except Exception:
                    # if anything went wrong in the image resolution logic, ensure safe defaults
                    cleaned['imageUrl'] = None
                    cleaned['HasImage'] = False
                   
            # ensure EmployeeID surfaced if present in candidate_row
            if not cleaned.get('EmployeeID') and candidate_row:
                for k in ('EmployeeID','Int1','Text12','EmployeeIdentity'):
                    if candidate_row.get(k) not in (None, '', 'nan'):
                        cleaned['EmployeeID'] = candidate_row.get(k)
                        break
        except Exception:
            pass



    # Build raw_swipes timeline by scanning swipe files
    raw_files = set()
    raw_swipes_out = []
    seen_swipe_keys = set()
    def _append_swipe(out_row, source_name):
        key = (
            out_row.get('Date') or '',
            out_row.get('Time') or '',
            (out_row.get('Door') or '').strip(),
            (out_row.get('Direction') or '').strip(),
            (out_row.get('CardNumber') or out_row.get('Card') or '').strip()
        )
        if key in seen_swipe_keys:
            return
        seen_swipe_keys.add(key)
        out_row['_source'] = source_name
        raw_swipes_out.append(out_row)

    # collate scan dates
    dates_to_scan = set()
    for _, agg_row in matched.iterrows():
        try:
            if 'Date' in agg_row and pd.notna(agg_row['Date']):
                try:
                    d = pd.to_datetime(agg_row['Date']).date()
                    dates_to_scan.add(d)
                except Exception:
                    pass
            for col in ('FirstSwipe','LastSwipe'):
                if col in agg_row and pd.notna(agg_row[col]):
                    try:
                        d = pd.to_datetime(agg_row[col]).date()
                        dates_to_scan.add(d)
                    except Exception:
                        pass
        except Exception:
            continue
    if not dates_to_scan:
        dates_to_scan = {None}

    for d in dates_to_scan:
        candidates = _find_swipe_files(DEFAULT_OUTDIR, date_obj=d, city_slug=city_slug)
        for fp in candidates:
            raw_files.add(fp.name)
            try:
                try:
                    raw_df = pd.read_csv(fp, parse_dates=['LocaleMessageTime'])
                except Exception:
                    raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

            raw_df = _replace_placeholder_strings(raw_df)
            cols_lower = {c.lower(): c for c in raw_df.columns}
            tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
            emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
            name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
            card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
            door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
            dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
            note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
            person_uid_col = cols_lower.get('person_uid')

            mask = pd.Series(False, index=raw_df.index)
            if person_uid_col and person_uid_col in raw_df.columns:
                mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
            if emp_col and emp_col in raw_df.columns:
                mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
            if not mask.any() and emp_col and emp_col in raw_df.columns:
                try:
                    q_numeric = float(q)
                    emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                    mask = mask | (emp_numeric == q_numeric)
                except Exception:
                    pass
            if not mask.any() and name_col and name_col in raw_df.columns:
                mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())

            if not mask.any():
                continue

            filtered = raw_df[mask].copy()
            if filtered.empty:
                continue

            if tcol and tcol in filtered.columns:
                try:
                    filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                except Exception:
                    pass

            if tcol and tcol in filtered.columns:
                filtered = filtered.sort_values(by=tcol)
                filtered['_prev_ts'] = filtered[tcol].shift(1)
                try:
                    filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                except Exception:
                    filtered['_swipe_gap_seconds'] = 0.0
                try:
                    cur_dates = filtered[tcol].dt.date
                    prev_dates = cur_dates.shift(1)
                    day_start_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
                    filtered.loc[day_start_mask, '_swipe_gap_seconds'] = 0.0
                except Exception:
                    pass
            else:
                filtered['_swipe_gap_seconds'] = 0.0

            try:
                if door_col and door_col in filtered.columns:
                    if dir_col and dir_col in filtered.columns:
                        # careful: apply needs column names from raw_df
                        filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                    else:
                        filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
                else:
                    filtered['_zone'] = filtered.get('PartitionName2', None)
            except Exception:
                filtered['_zone'] = None

            for _, r in filtered.iterrows():
                out = {}
                out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in filtered.columns else _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else q)
                emp_val = None
                if emp_col and emp_col in filtered.columns:
                    emp_val = _to_python_scalar(r.get(emp_col))
                else:
                    for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                        if cand.lower() in cols_lower:
                            emp_val = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                            if emp_val not in (None,'','nan'):
                                break
                    if emp_val in (None,'','nan'):
                        emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
                if emp_val is not None:
                    try:
                        s = str(emp_val).strip()
                        if '.' in s:
                            f = float(s)
                            if math.isfinite(f) and f.is_integer():
                                s = str(int(f))
                        if _looks_like_guid(s) or _is_placeholder_str(s):
                            emp_val = None
                        else:
                            emp_val = s
                    except Exception:
                        if _looks_like_guid(emp_val):
                            emp_val = None
                out['EmployeeID'] = emp_val

                card_val = None
                if card_col and card_col in filtered.columns:
                    card_val = _to_python_scalar(r.get(card_col))
                else:
                    for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                        if cand.lower() in cols_lower:
                            card_val = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                            if card_val not in (None,'','nan'):
                                break
                    if card_val in (None,'','nan'):
                        card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                if card_val is not None:
                    try:
                        cs = str(card_val).strip()
                        if _looks_like_guid(cs) or _is_placeholder_str(cs):
                            card_val = None
                        else:
                            card_val = cs
                    except Exception:
                        card_val = None
                out['CardNumber'] = card_val
                out['Card'] = card_val

                if tcol and tcol in filtered.columns:
                    ts = r.get(tcol)
                    try:
                        ts_py = pd.to_datetime(ts)
                        out['Date'] = ts_py.date().isoformat()
                        out['Time'] = ts_py.time().isoformat()
                        out['LocaleMessageTime'] = ts_py.isoformat()
                    except Exception:
                        txt = str(r.get(tcol))
                        out['Date'] = txt[:10]
                        out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                        out['LocaleMessageTime'] = txt
                else:
                    out['Date'] = None
                    out['Time'] = None
                    out['LocaleMessageTime'] = None

                out['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
                out['SwipeGap'] = format_seconds_to_hms(out['SwipeGapSeconds'])
                out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
                out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in r else None
                out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
                try:
                    out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else map_door_to_zone(out['Door'], out['Direction'])
                except Exception:
                    out['Zone'] = None
                out['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
                out['_source_file'] = fp.name
                _append_swipe(out, fp.name)

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": sorted(list(raw_files)),
        "raw_swipes": raw_swipes_out
    }), 200

@app.route('/record/export', methods=['GET'])
def export_record_excel():
    q = request.args.get('employee_id') or request.args.get('person_uid')
    date_str = request.args.get('date')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    if not q:
        return jsonify({"error":"employee_id or person_uid is required"}), 400

    p = Path(DEFAULT_OUTDIR)
    files_to_scan = []
    if date_str:
        try:
            dd = pd.to_datetime(date_str).date()
            files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=dd, city_slug=city_slug)
        except Exception:
            return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400
    else:
        files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=city_slug)

    if not files_to_scan:
        return jsonify({"error":"no raw swipe files found for requested date / outputs"}), 404

    all_rows = []
    for fp in files_to_scan:
        try:
            raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
        except Exception:
            try:
                raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

        raw_df = _replace_placeholder_strings(raw_df)
        cols_lower = {c.lower(): c for c in raw_df.columns}
        tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
        emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
        name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
        card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
        door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
        dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
        note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
        person_uid_col = cols_lower.get('person_uid')

        mask = pd.Series(False, index=raw_df.index)
        if person_uid_col and person_uid_col in raw_df.columns:
            mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
        if emp_col and emp_col in raw_df.columns:
            mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
        if not mask.any() and emp_col and emp_col in raw_df.columns:
            try:
                q_numeric = float(q)
                emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                mask = mask | (emp_numeric == q_numeric)
            except Exception:
                pass
        if not mask.any() and name_col and name_col in raw_df.columns:
            mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())

        if not mask.any():
            continue

        filtered = raw_df[mask].copy()
        if filtered.empty:
            continue

        if tcol and tcol in filtered.columns:
            try:
                filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
            except Exception:
                pass

        if tcol and tcol in filtered.columns:
            filtered = filtered.sort_values(by=tcol)
            filtered['_prev_ts'] = filtered[tcol].shift(1)
            try:
                filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
            except Exception:
                filtered['_swipe_gap_seconds'] = 0.0
        else:
            filtered['_swipe_gap_seconds'] = 0.0

        try:
            if door_col and door_col in filtered.columns:
                if dir_col and dir_col in filtered.columns:
                    filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                else:
                    filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
            else:
                filtered['_zone'] = filtered.get('PartitionName2', None)
        except Exception:
            filtered['_zone'] = None

        for _, r in filtered.iterrows():
            row = {}
            row['EmployeeName'] = _to_python_scalar(r.get(name_col)) if (name_col and name_col in filtered.columns) else None
            emp_val = None
            if emp_col and emp_col in filtered.columns:
                emp_val = _to_python_scalar(r.get(emp_col))
            else:
                for cand in ('int1','text12','employeeid','employee_identity','employeeidentity'):
                    if cand in cols_lower and cols_lower[cand] in filtered.columns:
                        emp_val = _to_python_scalar(r.get(cols_lower[cand]))
                        if emp_val:
                            break
            row['EmployeeID'] = emp_val
            row['Card'] = _to_python_scalar(r.get(card_col)) if (card_col and card_col in filtered.columns) else None

            if tcol and tcol in filtered.columns:
                ts = r.get(tcol)
                try:
                    ts_py = pd.to_datetime(ts)
                    row['Date'] = ts_py.date().isoformat()
                    row['Time'] = ts_py.time().isoformat()
                    row['LocaleMessageTime'] = ts_py.isoformat()
                except Exception:
                    txt = str(r.get(tcol))
                    row['Date'] = txt[:10]
                    row['Time'] = txt[11:19] if len(txt) >= 19 else None
                    row['LocaleMessageTime'] = txt
            else:
                row['Date'] = None
                row['Time'] = None
                row['LocaleMessageTime'] = None

            row['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
            row['SwipeGap'] = format_seconds_to_hms(row['SwipeGapSeconds'])
            row['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
            row['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else None
            row['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
            try:
                zone_val = r.get('_zone') if '_zone' in r else None
                if zone_val is None:
                    zone_val = map_door_to_zone(row['Door'], row['Direction'])
                row['Zone'] = _to_python_scalar(zone_val)
            except Exception:
                row['Zone'] = None
            row['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
            row['_source_file'] = fp.name
            all_rows.append(row)

    if not all_rows:
        return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

    df_out = pd.DataFrame(all_rows)
    details_cols = ['EmployeeName','EmployeeID','Door','Direction','Zone','Date','LocaleMessageTime','SwipeGapSeconds','PartitionName2','_source_file']
    timeline_cols = ['EmployeeName','EmployeeID','Card','Date','Time','SwipeGapSeconds','Door','Direction','Zone','Note','_source_file']

    details_df = df_out[[c for c in details_cols if c in df_out.columns]].copy()
    timeline_df = df_out[[c for c in timeline_cols if c in df_out.columns]].copy()

    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            details_df.to_excel(writer, sheet_name='Details — Evidence', index=False)
            timeline_df.to_excel(writer, sheet_name='Swipe timeline', index=False)
            writer.save()
            output.seek(0)
    except Exception as e:
        logging.exception("Failed to create Excel: %s", e)
        return jsonify({"error":"failed to create excel"}), 500

    if OPENPYXL_AVAILABLE:
        try:
            wb = load_workbook(output)
            thin = Side(border_style="thin", color="000000")
            thick = Side(border_style="medium", color="000000")
            for ws in wb.worksheets:
                header = ws[1]
                for cell in header:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                        cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
                for col in ws.columns:
                    max_len = 0
                    col_letter = col[0].column_letter
                    for cell in col:
                        try:
                            v = str(cell.value) if cell.value is not None else ""
                        except Exception:
                            v = ""
                        if len(v) > max_len:
                            max_len = len(v)
                    width = min(max(10, max_len + 2), 50)
                    ws.column_dimensions[col_letter].width = width
            out2 = io.BytesIO()
            wb.save(out2)
            out2.seek(0)
            return send_file(out2, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        except Exception:
            logging.exception("Excel styling failed, returning raw file")
            output.seek(0)
            return send_file(output, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    else:
        output.seek(0)
        return send_file(output, as_attachment=True,
                         download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                         mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)

@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        if build_monthly_training is None:
            raise RuntimeError("build_monthly_training not available")
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500

# chatbot helpers (kept mostly as-is)
try:
    from trend_runner import _read_past_trend_csvs, _normalize_id_val, SCENARIO_EXPLANATIONS
except Exception:
    _read_past_trend_csvs = None
    _normalize_id_val = None
    SCENARIO_EXPLANATIONS = {}

def _load_latest_trend_df(outdir: Path, city: str = "pune"):
    city_slug = _slug_city(city)
    csvs = sorted(outdir.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(outdir.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return None, None
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    df = _replace_placeholder_strings(df)
    return df, latest.name

def _find_person_rows(identifier: str, days: int = 90, outdir: Path = DEFAULT_OUTDIR):
    if _normalize_id_val:
        norm = _normalize_id_val(identifier)
    else:
        norm = str(identifier).strip()
        if '.' in norm:
            try:
                f = float(norm)
                if f.is_integer():
                    norm = str(int(f))
            except Exception:
                pass
    today = datetime.now().date()
    try:
        if _read_past_trend_csvs:
            past = _read_past_trend_csvs(str(outdir), days, today)
        else:
            files = sorted(Path(outdir).glob("trend_pune_*.csv"), reverse=True)
            dfs = []
            cutoff = today - timedelta(days=days)
            for fp in files:
                try:
                    tmp = pd.read_csv(fp, parse_dates=['Date'])
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                    tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                    dfs.append(tmp)
                except Exception:
                    try:
                        tmp = pd.read_csv(fp, dtype=str)
                        if 'Date' in tmp.columns:
                            tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                            tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                            dfs.append(tmp)
                    except Exception:
                        continue
            past = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
    except Exception:
        past = pd.DataFrame()

    if past is None or past.empty:
        return pd.DataFrame()

    past = _replace_placeholder_strings(past)
    match_mask = pd.Series(False, index=past.index)
    for col in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
        if col in past.columns:
            try:
                match_mask = match_mask | (past[col].astype(str).fillna('').str.strip() == str(norm).strip())
            except Exception:
                continue

    if not match_mask.any():
        try:
            qnum = float(norm)
            for col in ('EmployeeID','Int1'):
                if col in past.columns:
                    try:
                        numcol = pd.to_numeric(past[col], errors='coerce')
                        match_mask = match_mask | (numcol == qnum)
                    except Exception:
                        continue
        except Exception:
            pass

    if not match_mask.any() and 'EmployeeName' in past.columns:
        names = past['EmployeeName'].dropna().astype(str).unique().tolist()
        close = difflib.get_close_matches(str(identifier), names, n=5, cutoff=0.7)
        if close:
            match_mask = match_mask | past['EmployeeName'].astype(str).isin(close)

    return past[match_mask].copy()

def _explain_scenario_code(code):
    if not code:
        return None
    code = str(code).strip()
    if code in SCENARIO_EXPLANATIONS:
        try:
            fn = SCENARIO_EXPLANATIONS.get(code)
            try:
                txt = fn({})
                return txt
            except Exception:
                return code.replace("_", " ").replace(">= ", "≥ ")
        except Exception:
            return code.replace("_", " ").replace(">= ", "≥ ")
    return code.replace("_", " ").replace(">=", "≥")

def _map_score_to_label_fallback(score: float):
    try:
        s = float(score)
    except Exception:
        return (0.0, "Low")
    if s >= 0.75:
        return (s, "High")
    if s >= 0.4:
        return (s, "Medium")
    return (s, "Low")

@app.route('/chatbot/query', methods=['POST'])
def chatbot_query():
    payload = request.get_json(force=True)
    q = (payload.get('q') or '').strip()
    if not q:
        return jsonify({"error":"query text 'q' required"}), 400
    lang = payload.get('lang')
    q_l = q.lower().strip()

    if re.search(r"\bwho is (high|low) risk\b", q_l) or re.search(r"\b(high|low) risk (people|persons|people) (today)?\b", q_l):
        want = 'high' if 'high' in q_l else 'low' if 'low' in q_l else None
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer": "No trend data available.", "evidence": []})
        if 'RiskLevel' not in df.columns:
            if 'RiskScore' in df.columns:
                def _map_rs(s):
                    try:
                        if pd.isna(s):
                            return 'Low'
                    except Exception:
                        pass
                    try:
                        if 'map_score_to_label' in globals() and callable(globals().get('map_score_to_label')):
                            try:
                                return globals().get('map_score_to_label')(float(s))[1]
                            except Exception:
                                pass
                        return _map_score_to_label_fallback(float(s))[1]
                    except Exception:
                        return 'Low'
                df['RiskLevel'] = df['RiskScore'].apply(lambda s: _map_rs(s))
            else:
                df['RiskLevel'] = df.get('RiskLevel', 'Low')
        if want == 'high':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'high']
        elif want == 'low':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'low']
        else:
            sel = df
        names = sel['EmployeeName'].dropna().astype(str).unique().tolist()
        if not names:
            ans = f"No {want} risk persons found in the latest data." if want else "No persons found."
            return jsonify({"answer": ans, "evidence": []})
        else:
            ans = f"{want.capitalize()} risk persons today: " + ", ".join(names[:40])
            sample = _clean_sample_df(sel.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bexplain\s+([A-Za-z0-9_\-]+)\b.*", q_l)
    if m:
        code = m.group(1).strip()
        explanation = _explain_scenario_code(code)
        ans = f"Explanation for '{code}': {explanation}"
        return jsonify({"answer": ans, "evidence": []})

    if 'trend details' in q_l or 'top reasons' in q_l or 'trend details for today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'Reasons' in df.columns:
            reasons = {}
            for v in df['Reasons'].dropna().astype(str):
                for part in re.split(r'[;,\|]', v):
                    key = part.strip()
                    if key and not _is_placeholder_str(key):
                        reasons[key] = reasons.get(key, 0) + 1
            top = sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:10]
            if not top:
                return jsonify({"answer":"No reason counts available today.","evidence":[]})
            ans = "Top reasons today: " + ", ".join([f"{k} ({c})" for k,c in top])
            sample = []
            try:
                top_reasons = [k for k,_ in top]
                mask = df['Reasons'].astype(str).apply(lambda s: any(tr in s for tr in top_reasons))
                sample_df = df[mask].head(10)
                sample = _clean_sample_df(sample_df, max_rows=10)
            except Exception:
                sample = []
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bshow (?:me )?([A-Za-z0-9\-\:\s]+?) (?:for )?(?:last )?(\d+)\s*days\b", q_l)
    if not m:
        m = re.match(r".*\b(show|display)\s+(?:me\s+)?([A-Za-z0-9\-\:\s]+?)\s+last\s+(\d+)\s*days\b", q_l)
    if m:
        if len(m.groups()) == 2:
            identifier, days = m.group(1).strip(), int(m.group(2))
        else:
            identifier = m.group(1).strip()
            days = int(m.group(2))
        rows = _find_person_rows(identifier, days=days, outdir=DEFAULT_OUTDIR)
        if rows is None or rows.empty:
            return jsonify({"answer": f"No records found for '{identifier}' in last {days} days.", "evidence": []})
        flagged = rows[rows.get('IsFlagged', False) == True] if 'IsFlagged' in rows.columns else pd.DataFrame()
        flagged_count = int(len(flagged))
        total_days = int(len(rows))
        latest_row = rows.sort_values('Date', ascending=False).iloc[0].to_dict()
        name = latest_row.get('EmployeeName') or latest_row.get('person_uid') or latest_row.get('EmployeeID')
        ans = f"Found {total_days} day(s) for {name} in the last {days} days. Flagged days: {flagged_count}."
        sample = _clean_sample_df(rows.sort_values('Date', ascending=False).head(10), max_rows=10)
        return jsonify({"answer": ans, "evidence": sample})

    if 'present today' in q_l or 'who is present today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'PresentToday' in df.columns:
            present = df[df['PresentToday'] == True]
            names = present['EmployeeName'].dropna().unique().tolist()
            ans = f"Present today: {', '.join(names[:40]) if names else 'None'}"
            sample = _clean_sample_df(present.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})
        else:
            return jsonify({"answer":"PresentToday field not available in latest trends.","evidence":[]})

    hint = "I can answer: 'Who is high risk today', 'Who is low risk today', 'Show me <EmployeeID|Name> last 90 days', 'Explain <scenario_code>', 'Trend details for today — top reasons'."
    return jsonify({"answer": f"I can help with trend & risk questions. I recognized: {q}. Try: {hint}", "evidence":[]})

# @app.route('/employee/<empid>/image', methods=['GET'])
# def serve_employee_image(empid):
#     if empid is None:
#         return jsonify({"error": "employee id required"}), 400
#     try:
#         img_bytes = get_person_image_bytes(empid)
#         if not img_bytes:
#             return jsonify({"error": "no image found"}), 404
#         header = img_bytes[:8]
#         content_type = 'application/octet-stream'
#         if len(header) >= 2 and header[0] == 0xFF and header[1] == 0xD8:
#             content_type = 'image/jpeg'
#         elif header.startswith(b'\x89PNG\r\n\x1a\n'):
#             content_type = 'image/png'
#         bio = io.BytesIO(img_bytes)
#         bio.seek(0)
#         return send_file(bio, mimetype=content_type)
#     except Exception:
#         logging.exception("Error serving image for employee %s", empid)
#         return jsonify({"error": "failed to serve image"}), 500

@app.route('/employee/<empid>/image', methods=['GET'])
def serve_employee_image(empid):
    if empid is None:
        return jsonify({"error": "employee id required"}), 400
    try:
        img_bytes = get_person_image_bytes(empid)
        # If no image found, return a small inline SVG placeholder so <img> can still render
        if not img_bytes:
            svg = (
                '<svg xmlns="http://www.w3.org/2000/svg" width="160" height="160">'
                '<rect fill="#eef2f7" width="100%" height="100%"/>'
                '<text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle" fill="#64748b" font-size="18">'
                'No image</text></svg>'
            )
            bio = io.BytesIO(svg.encode('utf-8'))
            bio.seek(0)
            resp = send_file(bio, mimetype='image/svg+xml')
            # avoid aggressive caching of placeholder
            resp.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            return resp

        # detect content type heuristically (jpeg/png)
        header = img_bytes[:8] if isinstance(img_bytes, (bytes, bytearray)) else bytes(img_bytes)[:8]
        content_type = 'application/octet-stream'
        try:
            if len(header) >= 2 and header[0] == 0xFF and header[1] == 0xD8:
                content_type = 'image/jpeg'
            elif header.startswith(b'\x89PNG\r\n\x1a\n'):
                content_type = 'image/png'
            elif header.startswith(b'BM'):
                content_type = 'image/bmp'
            else:
                # fallback: try to detect common image signatures, otherwise serve as octet-stream
                content_type = 'application/octet-stream'
        except Exception:
            content_type = 'application/octet-stream'

        bio = io.BytesIO(img_bytes if isinstance(img_bytes, (bytes, bytearray)) else bytes(img_bytes))
        bio.seek(0)
        resp = send_file(bio, mimetype=content_type)
        # small caching is ok for real images
        resp.headers['Cache-Control'] = 'private, max-age=300'
        return resp
    except Exception:
        logging.exception("Error serving image for employee %s", empid)
        return jsonify({"error": "failed to serve image"}), 500



# run
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)



