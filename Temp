# Requires: Flask request/jsonify, pandas as pd, pathlib.Path, datetime, math, re, DEFAULT_OUTDIR (or OUTDIR)
# Place inside your app module where helpers like _safe_read_csv, _find_swipe_files, _clean_sample_df, _replace_placeholder_strings,
# get_personnel_info, get_person_image_bytes, map_door_to_zone exist. The function is defensive if some are missing.

@app.route('/record', methods=['GET'])
def record():
    from pathlib import Path
    import pandas as pd
    import math
    import re
    from datetime import datetime, date
    try:
        q = request.args.get('employee_id') or request.args.get('person_uid')
    except Exception:
        q = None
    include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'

    # pick outdir consistently
    try:
        base_out = Path(DEFAULT_OUTDIR)
    except Exception:
        try:
            base_out = Path(OUTDIR)
        except Exception:
            base_out = Path.cwd()

    # helper safe wrappers (use existing ones if present)
    def _safe_read(fp, **kwargs):
        try:
            if '_safe_read_csv' in globals():
                return _safe_read_csv(fp)
            return pd.read_csv(fp, **kwargs)
        except Exception:
            try:
                return pd.read_csv(fp, dtype=str, **{k: v for k, v in kwargs.items() if k != 'parse_dates'})
            except Exception:
                return pd.DataFrame()

    def _to_python_scalar(v):
        if pd.isna(v):
            return None
        try:
            return v.item() if hasattr(v, 'item') else v
        except Exception:
            return v

    # 1) find trend CSVs (city-specific first)
    def _slug(s):
        return re.sub(r'[^a-z0-9]+', '_', str(s or '').strip().lower()).strip('_')

    city_slug = _slug(city_param)
    trend_glob = list(base_out.glob(f"trend_{city_slug}_*.csv"))
    if not trend_glob:
        trend_glob = list(base_out.glob("trend_*.csv"))
    trend_glob = sorted(trend_glob, reverse=True)

    # if no trend files, still we should try to return raw swipes (evidence) by scanning swipes files
    df_list = []
    for fp in trend_glob:
        try:
            tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
        except Exception:
            try:
                tmp = pd.read_csv(fp, dtype=str)
                if 'Date' in tmp.columns:
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
            except Exception:
                continue
        df_list.append(tmp)
    if df_list:
        trends_df = pd.concat(df_list, ignore_index=True)
        try:
            trends_df = _replace_placeholder_strings(trends_df)
        except Exception:
            pass
    else:
        trends_df = pd.DataFrame()

    # if no query param, return a small sample of trend rows (if any)
    if q is None:
        try:
            if not trends_df.empty and '_clean_sample_df' in globals():
                cleaned = _clean_sample_df(trends_df, max_rows=10)
            elif not trends_df.empty:
                cleaned = trends_df.head(10).to_dict(orient='records')
            else:
                cleaned = []
        except Exception:
            cleaned = []
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    q_str = str(q).strip()

    # helper to normalise series values to comparable strings/numerics
    def normalize_series(s):
        if s is None:
            return pd.Series([''] * (len(trends_df) if not trends_df.empty else 0))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if math.isfinite(fv) and fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    # find matching rows in trends_df
    found_mask = pd.Series(False, index=trends_df.index) if not trends_df.empty else pd.Series(dtype=bool)
    candidates_cols = ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12', 'EmployeeName')
    for c in candidates_cols:
        if c in trends_df.columns:
            try:
                ser = normalize_series(trends_df[c])
                found_mask = found_mask | (ser == q_str)
            except Exception:
                pass

    # numeric fallback
    if (found_mask is None) or (not found_mask.any() if len(found_mask) else True):
        try:
            q_numeric = float(q_str)
            for c in ('EmployeeID', 'Int1'):
                if c in trends_df.columns:
                    try:
                        numser = pd.to_numeric(trends_df[c], errors='coerce')
                        found_mask = found_mask | (numser == q_numeric)
                    except Exception:
                        pass
        except Exception:
            pass

    matched = trends_df[found_mask].copy() if not trends_df.empty else pd.DataFrame()
    if matched.empty:
        # no aggregated trend rows found - still attempt to return raw swipe evidence by scanning swipes files
        cleaned_matched = []
    else:
        try:
            cleaned_matched = _clean_sample_df(matched, max_rows=len(matched)) if '_clean_sample_df' in globals() else matched.to_dict(orient='records')
        except Exception:
            cleaned_matched = matched.to_dict(orient='records')

    # enrich matched rows where possible
    try:
        if cleaned_matched and '_enrich_with_personnel_info' in globals():
            agg_df = pd.DataFrame(cleaned_matched)
            agg_df = _enrich_with_personnel_info(agg_df, image_endpoint_template="/employee/{}/image")
            cleaned_matched = agg_df.to_dict(orient='records')
    except Exception:
        pass

    # build list of dates to scan for swipe files (from matched rows)
    dates_to_scan = set()
    try:
        for _, r in (matched.iterrows() if not matched.empty else []):
            try:
                if 'Date' in r and pd.notna(r['Date']):
                    try:
                        d = pd.to_datetime(r['Date']).date()
                        dates_to_scan.add(d)
                    except Exception:
                        pass
                for col in ('FirstSwipe','LastSwipe'):
                    if col in r and pd.notna(r[col]):
                        try:
                            d = pd.to_datetime(r[col]).date()
                            dates_to_scan.add(d)
                        except Exception:
                            pass
            except Exception:
                continue
    except Exception:
        pass
    if not dates_to_scan:
        dates_to_scan = {None}  # indicates scan all swipes files

    # helper to find swipe files (use your _find_swipe_files if present)
    def _find_swipes_for_date(date_obj=None):
        files = []
        try:
            if '_find_swipe_files' in globals():
                cand = _find_swipe_files(str(base_out), date_obj=date_obj, city_slug=city_slug if city_slug else None)
                if cand:
                    return cand
            # fallback simple glob: swipes_*.csv (if date_obj provided try to match filename date)
            if date_obj is None:
                files = list(base_out.glob("swipes_*_*.csv")) + list(base_out.glob("swipes_*.csv"))
            else:
                # look for filenames containing YYYY-MM-DD or YYYYMMDD
                ymd = date_obj.strftime('%Y-%m-%d')
                ymd2 = date_obj.strftime('%Y%m%d')
                files = [p for p in base_out.glob("swipes_*_*.csv") if ymd in p.name or ymd2 in p.name] + \
                        [p for p in base_out.glob("swipes_*.csv") if ymd in p.name or ymd2 in p.name]
            return sorted(files, reverse=True)
        except Exception:
            return []

    raw_files_set = set()
    raw_swipes_out = []
    seen_keys = set()

    def _append_row(out_row, source_name):
        key = (out_row.get('Date') or '', out_row.get('Time') or '', (out_row.get('Door') or '').strip(), (out_row.get('Direction') or '').strip(), (out_row.get('CardNumber') or out_row.get('Card') or '').strip())
        if key in seen_keys:
            return
        seen_keys.add(key)
        out_row['_source'] = source_name
        raw_swipes_out.append(out_row)

    # scan swipe files for dates
    for d in dates_to_scan:
        swipe_candidates = _find_swipes_for_date(d)
        # if none and we asked for specific date, fall back to all swipes
        if not swipe_candidates and d is not None:
            swipe_candidates = _find_swipes_for_date(None)

        for fp in swipe_candidates:
            raw_files_set.add(fp.name)
            try:
                sdf = _safe_read(fp, parse_dates=['LocaleMessageTime'])
            except Exception:
                sdf = _safe_read(fp)
            if sdf.empty:
                continue

            # normalize column-name -> original mapping
            cols_lower = {c.lower(): c for c in sdf.columns}
            tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
            emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
            name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
            card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
            door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
            dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
            note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
            person_uid_col = cols_lower.get('person_uid')

            # build mask for this person q
            mask = pd.Series(False, index=sdf.index)
            try:
                if person_uid_col and person_uid_col in sdf.columns:
                    mask = mask | (sdf[person_uid_col].astype(str).str.strip() == q_str)
            except Exception:
                pass
            try:
                if emp_col and emp_col in sdf.columns:
                    mask = mask | (sdf[emp_col].astype(str).str.strip() == q_str)
            except Exception:
                pass
            # numeric fallback for emp_col
            if not mask.any() and emp_col and emp_col in sdf.columns:
                try:
                    q_numeric = float(q_str)
                    emp_numeric = pd.to_numeric(sdf[emp_col], errors='coerce')
                    mask = mask | (emp_numeric == q_numeric)
                except Exception:
                    pass
            # name fallback
            if not mask.any() and name_col and name_col in sdf.columns:
                try:
                    mask = mask | (sdf[name_col].astype(str).str.strip().str.lower() == q_str.lower())
                except Exception:
                    pass

            if not mask.any():
                continue

            filtered = sdf[mask].copy()
            if filtered.empty:
                continue

            # ensure time column is parsed
            if tcol and tcol in filtered.columns:
                try:
                    filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                except Exception:
                    pass
            else:
                # try to infer from common candidates
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp','Date'):
                    if cand in filtered.columns:
                        try:
                            filtered['LocaleMessageTime'] = pd.to_datetime(filtered[cand], errors='coerce')
                            tcol = 'LocaleMessageTime'
                            break
                        except Exception:
                            pass

            # sort by time if available and compute gaps
            if tcol and tcol in filtered.columns:
                try:
                    filtered = filtered.sort_values(by=tcol)
                    filtered['_prev_ts'] = filtered[tcol].shift(1)
                    try:
                        filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                    except Exception:
                        filtered['_swipe_gap_seconds'] = 0.0
                    # reset gap at day boundary
                    try:
                        cur_dates = filtered[tcol].dt.date
                        prev_dates = cur_dates.shift(1)
                        day_start_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
                        filtered.loc[day_start_mask, '_swipe_gap_seconds'] = 0.0
                    except Exception:
                        pass
                except Exception:
                    filtered['_swipe_gap_seconds'] = 0.0
            else:
                filtered['_swipe_gap_seconds'] = 0.0

            # zone mapping if possible
            try:
                if door_col and door_col in filtered.columns:
                    if dir_col and dir_col in filtered.columns and 'map_door_to_zone' in globals():
                        filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                    else:
                        filtered['_zone'] = filtered.get(door_col, None)
            except Exception:
                filtered['_zone'] = None

            # convert each row to output dict
            for _, r in filtered.iterrows():
                out = {}
                out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in filtered.columns else (matched.iloc[0].get('EmployeeName') if not matched.empty else q_str)
                # EmployeeID detection
                emp_val = None
                if emp_col and emp_col in filtered.columns:
                    emp_val = _to_python_scalar(r.get(emp_col))
                else:
                    for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                        cl = cols_lower.get(cand.lower())
                        if cl and cl in filtered.columns:
                            emp_val = _to_python_scalar(r.get(cl))
                            if emp_val not in (None, '', 'nan'):
                                break
                    if emp_val in (None, '', 'nan'):
                        emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
                # sanitize
                try:
                    if emp_val is not None:
                        s = str(emp_val).strip()
                        if '.' in s:
                            try:
                                f = float(s)
                                if math.isfinite(f) and f.is_integer():
                                    s = str(int(f))
                            except Exception:
                                pass
                        emp_val = s
                except Exception:
                    pass
                out['EmployeeID'] = emp_val

                # card
                card_val = None
                if card_col and card_col in filtered.columns:
                    card_val = _to_python_scalar(r.get(card_col))
                else:
                    for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                        cl = cols_lower.get(cand.lower())
                        if cl and cl in filtered.columns:
                            card_val = _to_python_scalar(r.get(cl))
                            if card_val not in (None, '', 'nan'):
                                break
                    if card_val in (None, '', 'nan'):
                        card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                try:
                    if card_val is not None:
                        card_val = str(card_val).strip()
                except Exception:
                    pass
                out['CardNumber'] = card_val
                out['Card'] = card_val

                # timestamps -> Date, Time, LocaleMessageTime
                if tcol and tcol in filtered.columns:
                    ts = r.get(tcol)
                    try:
                        ts_py = pd.to_datetime(ts)
                        out['Date'] = ts_py.date().isoformat()
                        out['Time'] = ts_py.time().isoformat()
                        out['LocaleMessageTime'] = ts_py.isoformat()
                    except Exception:
                        txt = str(r.get(tcol))
                        out['Date'] = txt[:10]
                        out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                        out['LocaleMessageTime'] = txt
                else:
                    out['Date'] = None
                    out['Time'] = None
                    out['LocaleMessageTime'] = None

                out['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
                # format seconds -> HH:MM:SS
                try:
                    s = int(out['SwipeGapSeconds'])
                    hh = s // 3600
                    mm = (s % 3600) // 60
                    ss = s % 60
                    out['SwipeGap'] = f"{hh:02d}:{mm:02d}:{ss:02d}"
                except Exception:
                    out['SwipeGap'] = None

                out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
                out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else (_to_python_scalar(r.get('Direction')) if 'Direction' in r else None)
                out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
                try:
                    out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else (map_door_to_zone(out['Door'], out['Direction']) if 'map_door_to_zone' in globals() else None)
                except Exception:
                    out['Zone'] = None

                out['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
                out['_source_file'] = fp.name
                _append_row(out, fp.name)

    # final raw_swipe_files list
    raw_swipe_files = sorted(list(raw_files_set))

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": raw_swipe_files,
        "raw_swipes": raw_swipes_out
    }), 200















Once compare both API carefully because when 

@app.route("/record")
def record_endpoint():

when comment this API We dont get Evidance file meas raw swipe details we need to fix this issue 

Once comapre both API and write Single APi Which need to work correctly



@app.route("/record")
def record_endpoint():

    employee_id = request.args.get('employee_id', '').strip()
    if not employee_id:
        return jsonify({'error': 'employee_id required'}), 400

    # 1) find the latest trend CSV that contains this employee
    trend_files = sorted(OUTDIR.glob("trend_*.csv"), reverse=True)
    aggregated_rows = []
    found_date_iso = None
    for tf in trend_files:
        try:
            df = pd.read_csv(tf, dtype=str)
            # normalise columns that might not exist
            cols = df.columns.str.lower()
            # try matching against common id columns
            mask = pd.Series(False, index=df.index)
            for col in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber'):
                if col in df.columns:
                    mask = mask | (df[col].astype(str).fillna('') == employee_id)
            if mask.any():
                # convert matched rows to proper dicts
                matches = df[mask].to_dict(orient='records')
                aggregated_rows = matches
                # pick date from the first match (if available)
                try:
                    d = matches[0].get('Date') or matches[0].get('DisplayDate') or None
                    found_date_iso = d
                except Exception:
                    found_date_iso = None
                break
        except Exception:
            continue

    # 2) list raw swipe files (any swipes_*.csv in outputs)
    raw_swipe_files = [p.name for p in sorted(OUTDIR.glob("swipes_*_*.csv"), reverse=True)]

    # 3) build raw_swipes filtered for this person & date if possible
    raw_swipes = []
    try:
        # read each swipes CSV and filter rows
        for swf in OUTDIR.glob("swipes_*_*.csv"):
            sdf = _safe_read_csv(swf)
            if sdf.empty:
                continue
            # ensure LocaleMessageTime exists as datetime
            if 'LocaleMessageTime' in sdf.columns:
                sdf['LocaleMessageTime'] = pd.to_datetime(sdf['LocaleMessageTime'], errors='coerce')
            else:
                # try fallback candidates
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp','Date'):
                    if cand in sdf.columns:
                        sdf['LocaleMessageTime'] = pd.to_datetime(sdf[cand], errors='coerce')
                        break
            # filter rows matching employee by either EmployeeID, person_uid, CardNumber or EmployeeIdentity
            sel_mask = pd.Series(False, index=sdf.index)
            for c in ('EmployeeID','person_uid','CardNumber','EmployeeIdentity'):
                if c in sdf.columns:
                    sel_mask = sel_mask | (sdf[c].astype(str).fillna('') == employee_id)
            filtered = sdf[sel_mask]
            if filtered.empty:
                continue
            # compute Date/Time and SwipeGapSeconds for the filtered rows (sorted)
            filtered = filtered.sort_values('LocaleMessageTime')
            filtered['Date'] = filtered['LocaleMessageTime'].dt.date.astype(str)
            filtered['Time'] = filtered['LocaleMessageTime'].dt.time.astype(str)
            filtered['SwipeGapSeconds'] = filtered['LocaleMessageTime'].diff().dt.total_seconds().fillna(0).astype(int)
            filtered['SwipeGap'] = filtered['SwipeGapSeconds'].apply(lambda s: f"{int(s//3600):02d}:{int((s%3600)//60):02d}:{int(s%60):02d}" if s is not None else "-")
            # ensure Zone (if not) by mapping Door/Direction using your map_door_to_zone in trend_runner (import it or reimplement)
            if 'Zone' not in filtered.columns and 'Door' in filtered.columns:
                try:
                    from trend_runner import map_door_to_zone
                    filtered['Zone'] = filtered.apply(lambda r: map_door_to_zone(r.get('Door'), r.get('Direction')), axis=1)
                except Exception:
                    filtered['Zone'] = filtered.get('Zone', None)
            # project the columns the frontend expects
            want = []
            for c in ('EmployeeName','EmployeeID','CardNumber','Date','Time','SwipeGap','SwipeGapSeconds','Door','Direction','Zone','Note','_source'):
                if c in filtered.columns:
                    want.append(c)
            if not want:
                # fallback: return basic fields
                want = list(filtered.columns[:12])
            rows = filtered[want].to_dict(orient='records')
            raw_swipes.extend(rows)
    except Exception:
        # non fatal - return what we have
        pass

    # 4) Enrich aggregated_rows by trying to add Email + imageUrl using trend_runner helper if available
    try:
        from trend_runner import _enrich_with_personnel_info
        if aggregated_rows:
            agg_df = pd.DataFrame(aggregated_rows)
            agg_df = _enrich_with_personnel_info(agg_df, image_endpoint_template="/employee/{}/image")
            aggregated_rows = agg_df.to_dict(orient='records')
    except Exception:
        pass

    return jsonify({
        'aggregated_rows': aggregated_rows,
        'raw_swipe_files': raw_swipe_files,
        'raw_swipes': raw_swipes
    })



#Above Working 


# @app.route('/record', methods=['GET'])
# def get_record():
#     """
#     Single unified /record handler:
#     - If no employee id passed, return sample of aggregated rows.
#     - If employee_id/person_uid passed, return enriched aggregated_rows + raw_swipes + raw_swipe_files.
#     Uses DEFAULT_OUTDIR consistently for reading trend and swipe CSVs.
#     """
#     q = request.args.get('employee_id') or request.args.get('person_uid')
#     include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
#     city_param = request.args.get('city') or request.args.get('site') or 'pune'
#     city_slug = _slug_city(city_param)

#     # Use DEFAULT_OUTDIR consistently (this module-level variable is created earlier)
#     p = Path(DEFAULT_OUTDIR)
#     csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
#     if not csvs:
#         csvs = sorted(p.glob("trend_*.csv"), reverse=True)
#     if not csvs:
#         # no trend files -> return empty result set gracefully
#         return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

#     # build a single DataFrame with all matching trend files
#     df_list = []
#     for fp in csvs:
#         try:
#             tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
#         except Exception:
#             try:
#                 tmp = pd.read_csv(fp, dtype=str)
#                 if 'Date' in tmp.columns:
#                     tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
#             except Exception:
#                 continue
#         df_list.append(tmp)
#     if not df_list:
#         return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200
#     df = pd.concat(df_list, ignore_index=True)
#     df = _replace_placeholder_strings(df)

#     # if no query param, return cleaned sample (head)
#     if q is None:
#         cleaned = _clean_sample_df(df, max_rows=10)
#         return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

#     # Normalize query
#     q_str = str(q).strip()
#     def normalize_series(s):
#         if s is None:
#             return pd.Series([''] * len(df))
#         s = s.fillna('').astype(str).str.strip()
#         def _norm_val(v):
#             if not v:
#                 return ''
#             try:
#                 if '.' in v:
#                     fv = float(v)
#                     if fv.is_integer():
#                         return str(int(fv))
#             except Exception:
#                 pass
#             return v
#         return s.map(_norm_val)

#     # find matching rows
#     found_mask = pd.Series(False, index=df.index)
#     if 'EmployeeID' in df.columns:
#         emp_series = normalize_series(df['EmployeeID'])
#         found_mask = found_mask | (emp_series == q_str)
#     if 'person_uid' in df.columns:
#         uid_series = normalize_series(df['person_uid'])
#         found_mask = found_mask | (uid_series == q_str)
#     if 'Int1' in df.columns and not found_mask.any():
#         int1_series = normalize_series(df['Int1'])
#         found_mask = found_mask | (int1_series == q_str)

#     if not found_mask.any():
#         # try numeric match
#         try:
#             q_numeric = float(q_str)
#             if 'EmployeeID' in df.columns:
#                 emp_numeric = pd.to_numeric(df['EmployeeID'], errors='coerce')
#                 found_mask = found_mask | (emp_numeric == q_numeric)
#             if 'Int1' in df.columns and not found_mask.any():
#                 int_numeric = pd.to_numeric(df['Int1'], errors='coerce')
#                 found_mask = found_mask | (int_numeric == q_numeric)
#         except Exception:
#             pass

#     matched = df[found_mask].copy()
#     if matched.empty:
#         return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

#     # Clean matched rows for JSON and enrichment
#     cleaned_matched = _clean_sample_df(matched, max_rows=len(matched))

#     # Enrich matched rows: email / image / explanations
#     # Try to find personnel info using multiple candidate tokens (but avoid DB hits for obvious missing tokens)
#     try:
#         # lazy import helpers from trend_runner if available (safe)
#         from trend_runner import compute_violation_days_map, _strip_uid_prefix as _strip_uid_prefix_tr
#         violation_map = compute_violation_days_map(str(DEFAULT_OUTDIR), 90, datetime.now().date())
#     except Exception:
#         violation_map = {}
#         _strip_uid_prefix_tr = (lambda x: x)

#     matched_indexed = matched.reset_index(drop=True)

#     # For each cleaned row build candidate and enrich
#     for idx_c, cleaned in enumerate(cleaned_matched):
#         candidate_row = None
#         try:
#             # 1) try to match by person_uid/EmployeeID/EmployeeName
#             if cleaned.get('person_uid') and 'person_uid' in matched_indexed.columns:
#                 mr = matched_indexed[matched_indexed['person_uid'].astype(str).str.strip() == str(cleaned['person_uid']).strip()]
#                 if not mr.empty:
#                     candidate_row = mr.iloc[0].to_dict()

#             if candidate_row is None and cleaned.get('EmployeeID') and 'EmployeeID' in matched_indexed.columns:
#                 mr = matched_indexed[matched_indexed['EmployeeID'].astype(str).str.strip() == str(cleaned['EmployeeID']).strip()]
#                 if not mr.empty:
#                     candidate_row = mr.iloc[0].to_dict()

#             if candidate_row is None and cleaned.get('EmployeeName') and 'EmployeeName' in matched_indexed.columns:
#                 mr = matched_indexed[matched_indexed['EmployeeName'].astype(str).str.strip().str.lower() == str(cleaned['EmployeeName']).strip().lower()]
#                 if not mr.empty:
#                     candidate_row = mr.iloc[0].to_dict()

#             # 2) fallback to other columns
#             if candidate_row is None and not matched_indexed.empty:
#                 for ccol in ('CardNumber', 'EmployeeIdentity', 'Int1', 'Text12', 'ObjectIdentity1', 'ObjectID', 'GUID'):
#                     if ccol in matched_indexed.columns and cleaned.get(ccol) not in (None, '', 'nan'):
#                         mr = matched_indexed[matched_indexed[ccol].astype(str).str.strip() == str(cleaned.get(ccol)).strip()]
#                         if not mr.empty:
#                             candidate_row = mr.iloc[0].to_dict()
#                             break

#             # 3) last resort — pick first
#             if candidate_row is None and not matched_indexed.empty:
#                 candidate_row = matched_indexed.iloc[0].to_dict()
#         except Exception:
#             candidate_row = None

#         # Build explanation text if present
#         violation_expl = None
#         try:
#             if candidate_row:
#                 violation_expl = candidate_row.get('Explanation') or candidate_row.get('explanation') or None

#             if not violation_expl:
#                 reasons = cleaned.get('Reasons') or (candidate_row.get('Reasons') if candidate_row else None)
#                 if reasons:
#                     parts = [p.strip() for p in re.split(r'[;,\|]', str(reasons)) if p.strip()]
#                     mapped = []
#                     for p in parts:
#                         try:
#                             if 'SCENARIO_EXPLANATIONS' in globals() and p in SCENARIO_EXPLANATIONS:
#                                 mapped.append(SCENARIO_EXPLANATIONS[p](candidate_row or {}))
#                             else:
#                                 mapped.append(p.replace("_", " ").replace(">=", "≥"))
#                         except Exception:
#                             mapped.append(p)
#                     violation_expl = " ".join(mapped) if mapped else None

#             # remove GUIDs inside explanation if possible
#             if violation_expl:
#                 try:
#                     emp_name_for_expl = None
#                     if candidate_row:
#                         emp_name_for_expl = candidate_row.get('EmployeeName') or candidate_row.get('employee_name') or candidate_row.get('ObjectName1')
#                     if not emp_name_for_expl:
#                         emp_name_for_expl = cleaned.get('EmployeeName')
#                     if emp_name_for_expl:
#                         violation_expl = GUID_IN_TEXT_RE.sub(str(emp_name_for_expl), str(violation_expl))
#                 except Exception:
#                     pass
#         except Exception:
#             violation_expl = None

#         # violation days lookup (if available)
#         try:
#             candidates = []
#             for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
#                 if cleaned.get(k) not in (None, '', 'nan'):
#                     candidates.append(cleaned.get(k))
#             if candidate_row:
#                 for k in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
#                     if candidate_row.get(k) not in (None,'','nan'):
#                         candidates.append(candidate_row.get(k))
#             vdays = 0
#             if violation_map:
#                 for c in candidates:
#                     if c is None:
#                         continue
#                     n = _normalize_id_local(c)
#                     if n and n in violation_map:
#                         vdays = int(violation_map.get(n, 0))
#                         break
#                     try:
#                         stripped = _strip_uid_prefix_tr(str(n))
#                         if stripped != n and stripped in violation_map:
#                             vdays = int(violation_map.get(stripped, 0))
#                             break
#                     except Exception:
#                         pass
#         except Exception:
#             vdays = 0

#         # Personnel lookup: attempt DB lookup only if we have useful tokens (avoid unnecessary DB hits)
#         personnel_info = {}
#         try:
#             lookup_candidates = []
#             if candidate_row:
#                 for k in ('EmployeeObjID','EmployeeObjId','EmployeeIdentity','ObjectID','GUID','EmployeeID','Int1','Text12','EmployeeName'):
#                     if candidate_row.get(k) not in (None,'','nan'):
#                         lookup_candidates.append(candidate_row.get(k))
#             for k in ('EmployeeID','person_uid','EmployeeName'):
#                 if cleaned.get(k) not in (None,'','nan'):
#                     lookup_candidates.append(cleaned.get(k))

#             for cand in lookup_candidates:
#                 if cand is None:
#                     continue
#                 try:
#                     info = get_personnel_info(cand)
#                     if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None or info.get('ManagerEmail') is not None):
#                         personnel_info = info
#                         break
#                 except Exception:
#                     continue

#             # Extra fallback: try by EmployeeID explicitly
#             if not personnel_info:
#                 try:
#                     cand = cleaned.get('EmployeeID') or cleaned.get('EmployeeIdentity')
#                     if cand:
#                         info = get_personnel_info(cand)
#                         if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None):
#                             personnel_info = info
#                 except Exception:
#                     pass
#         except Exception:
#             personnel_info = {}

#         # Attach computed fields (robust)
#         try:
#             cleaned['ViolationDaysLast90'] = int(vdays or 0)
#             cleaned['ViolationExplanation'] = violation_expl
#             cleaned['Explanation'] = violation_expl or cleaned.get('ViolationExplanation') or None

#             # Ensure EmployeeName
#             try:
#                 if (not cleaned.get('EmployeeName')) or _looks_like_guid(cleaned.get('EmployeeName')):
#                     if candidate_row and candidate_row.get('EmployeeName') and not _looks_like_guid(candidate_row.get('EmployeeName')):
#                         cleaned['EmployeeName'] = candidate_row.get('EmployeeName')
#                     elif personnel_info and personnel_info.get('Name'):
#                         cleaned['EmployeeName'] = personnel_info.get('Name')
#             except Exception:
#                 pass

#             # Populate EmployeeEmail robustly: prefer personnel_info, then candidate_row, then matched_indexed source row
#             email_val = None
#             if personnel_info:
#                 email_val = personnel_info.get('EmailAddress') or personnel_info.get('Email')
#             if not email_val and candidate_row:
#                 for fk in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
#                     if candidate_row.get(fk) not in (None, '', 'nan'):
#                         email_val = candidate_row.get(fk)
#                         break
#             if not email_val:
#                 # try the matched_indexed first row
#                 for col in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
#                     if col in matched_indexed.columns:
#                         try:
#                             val = matched_indexed.iloc[0].get(col)
#                             if val not in (None, '', 'nan'):
#                                 email_val = val
#                                 break
#                         except Exception:
#                             continue

#             # Final fallback: sanitize value
#             cleaned['EmployeeEmail'] = (email_val if email_val not in (None, '', 'nan') else None) or cleaned.get('EmployeeEmail') or None

#             # IMAGE resolution: prefer personnel_info.ObjectID then candidate_row fields, otherwise EmployeeID/person_uid
#             try:
#                 img_obj = None
#                 if personnel_info and personnel_info.get('ObjectID') is not None:
#                     img_obj = personnel_info.get('ObjectID')
#                 elif candidate_row:
#                     for k in ('EmployeeObjID', 'EmployeeObjId', 'ObjectID', 'ObjectIdentity1'):
#                         if candidate_row.get(k) not in (None, '', 'nan'):
#                             img_obj = candidate_row.get(k)
#                             break

#                 try:
#                     base = (request.url_root or request.host_url).rstrip('/')
#                 except Exception:
#                     base = ''

#                 if img_obj:
#                     if base:
#                         cleaned['imageUrl'] = f"{base}/employee/{img_obj}/image"
#                     else:
#                         cleaned['imageUrl'] = f"/employee/{img_obj}/image"
#                     try:
#                         b = get_person_image_bytes(img_obj)
#                         cleaned['HasImage'] = True if b else False
#                     except Exception:
#                         cleaned['HasImage'] = False
#                 else:
#                     emp_for_img = cleaned.get('EmployeeID') or cleaned.get('person_uid') or None
#                     if emp_for_img:
#                         if base:
#                             cleaned['imageUrl'] = f"{base}/employee/{emp_for_img}/image"
#                         else:
#                             cleaned['imageUrl'] = f"/employee/{emp_for_img}/image"
#                         try:
#                             b = get_person_image_bytes(emp_for_img)
#                             cleaned['HasImage'] = True if b else False
#                         except Exception:
#                             cleaned['HasImage'] = False
#                     else:
#                         cleaned['imageUrl'] = None
#                         cleaned['HasImage'] = False

#             except Exception:
#                 cleaned['imageUrl'] = cleaned.get('imageUrl') or None
#                 cleaned['HasImage'] = cleaned.get('HasImage') or False

#             # ensure EmployeeID surfaced if absent
#             if not cleaned.get('EmployeeID') and candidate_row:
#                 for k in ('EmployeeID','Int1','Text12','EmployeeIdentity'):
#                     if candidate_row.get(k) not in (None, '', 'nan'):
#                         cleaned['EmployeeID'] = candidate_row.get(k)
#                         break

#         except Exception:
#             # safe defaults if enrichment fails
#             cleaned.setdefault('EmployeeEmail', None)
#             cleaned.setdefault('imageUrl', None)
#             cleaned.setdefault('HasImage', False)

#     # Build raw_swipes timeline (scan swipe files using DEFAULT_OUTDIR)
#     raw_files = set()
#     raw_swipes_out = []
#     seen_swipe_keys = set()
#     def _append_swipe(out_row, source_name):
#         key = (
#             out_row.get('Date') or '',
#             out_row.get('Time') or '',
#             (out_row.get('Door') or '').strip(),
#             (out_row.get('Direction') or '').strip(),
#             (out_row.get('CardNumber') or out_row.get('Card') or '').strip()
#         )
#         if key in seen_swipe_keys:
#             return
#         seen_swipe_keys.add(key)
#         out_row['_source'] = source_name
#         raw_swipes_out.append(out_row)

#     # dates discovered from matched rows
#     dates_to_scan = set()
#     for _, agg_row in matched.iterrows():
#         try:
#             if 'Date' in agg_row and pd.notna(agg_row['Date']):
#                 try:
#                     d = pd.to_datetime(agg_row['Date']).date()
#                     dates_to_scan.add(d)
#                 except Exception:
#                     pass
#             for col in ('FirstSwipe','LastSwipe'):
#                 if col in agg_row and pd.notna(agg_row[col]):
#                     try:
#                         d = pd.to_datetime(agg_row[col]).date()
#                         dates_to_scan.add(d)
#                     except Exception:
#                         pass
#         except Exception:
#             continue
#     if not dates_to_scan:
#         dates_to_scan = {None}

#     for d in dates_to_scan:
#         candidates = _find_swipe_files(DEFAULT_OUTDIR, date_obj=d, city_slug=city_slug)
#         if not candidates:
#             candidates = _find_swipe_files(DEFAULT_OUTDIR, date_obj=d, city_slug=None)

#         for fp in candidates:
#             raw_files.add(fp.name)
#             try:
#                 try:
#                     raw_df = pd.read_csv(fp, parse_dates=['LocaleMessageTime'])
#                 except Exception:
#                     raw_df = pd.read_csv(fp, dtype=str)
#             except Exception:
#                 continue

#             raw_df = _replace_placeholder_strings(raw_df)
#             # lower->orig mapping
#             cols_lower = {c.lower(): c for c in raw_df.columns}
#             tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
#             emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
#             name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
#             card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
#             door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
#             dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
#             note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
#             person_uid_col = cols_lower.get('person_uid')

#             mask = pd.Series(False, index=raw_df.index)
#             if person_uid_col and person_uid_col in raw_df.columns:
#                 mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
#             if emp_col and emp_col in raw_df.columns:
#                 mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
#             if not mask.any() and emp_col and emp_col in raw_df.columns:
#                 try:
#                     q_numeric = float(q)
#                     emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
#                     mask = mask | (emp_numeric == q_numeric)
#                 except Exception:
#                     pass
#             if not mask.any() and name_col and name_col in raw_df.columns:
#                 try:
#                     mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())
#                 except Exception:
#                     pass

#             if not mask.any():
#                 continue

#             filtered = raw_df[mask].copy()
#             if filtered.empty:
#                 continue

#             if tcol and tcol in filtered.columns:
#                 try:
#                     filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
#                 except Exception:
#                     pass

#             if tcol and tcol in filtered.columns:
#                 filtered = filtered.sort_values(by=tcol)
#                 filtered['_prev_ts'] = filtered[tcol].shift(1)
#                 try:
#                     filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
#                 except Exception:
#                     filtered['_swipe_gap_seconds'] = 0.0
#                 try:
#                     cur_dates = filtered[tcol].dt.date
#                     prev_dates = cur_dates.shift(1)
#                     day_start_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
#                     filtered.loc[day_start_mask, '_swipe_gap_seconds'] = 0.0
#                 except Exception:
#                     pass
#             else:
#                 filtered['_swipe_gap_seconds'] = 0.0

#             try:
#                 if door_col and door_col in filtered.columns:
#                     if dir_col and dir_col in filtered.columns:
#                         filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
#                     else:
#                         filtered['_zone'] = filtered.get(door_col, None)
#             except Exception:
#                 filtered['_zone'] = None

#             for _, r in filtered.iterrows():
#                 out = {}
#                 out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in filtered.columns else _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else q)
#                 # employee id
#                 emp_val = None
#                 if emp_col and emp_col in filtered.columns:
#                     emp_val = _to_python_scalar(r.get(emp_col))
#                 else:
#                     for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
#                         if cand.lower() in cols_lower:
#                             emp_val = _to_python_scalar(r.get(cols_lower[cand.lower()]))
#                             if emp_val not in (None,'','nan'):
#                                 break
#                     if emp_val in (None,'','nan'):
#                         emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
#                 if emp_val is not None:
#                     try:
#                         s = str(emp_val).strip()
#                         if '.' in s:
#                             f = float(s)
#                             if math.isfinite(f) and f.is_integer():
#                                 s = str(int(f))
#                         if _looks_like_guid(s) or _is_placeholder_str(s):
#                             emp_val = None
#                         else:
#                             emp_val = s
#                     except Exception:
#                         if _looks_like_guid(emp_val):
#                             emp_val = None
#                 out['EmployeeID'] = emp_val

#                 # Card number
#                 card_val = None
#                 if card_col and card_col in filtered.columns:
#                     card_val = _to_python_scalar(r.get(card_col))
#                 else:
#                     for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
#                         if cand.lower() in cols_lower:
#                             card_val = _to_python_scalar(r.get(cols_lower[cand.lower()]))
#                             if card_val not in (None,'','nan'):
#                                 break
#                     if card_val in (None,'','nan'):
#                         card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
#                 if card_val is not None:
#                     try:
#                         cs = str(card_val).strip()
#                         if _looks_like_guid(cs) or _is_placeholder_str(cs):
#                             card_val = None
#                         else:
#                             card_val = cs
#                     except Exception:
#                         card_val = None
#                 out['CardNumber'] = card_val
#                 out['Card'] = card_val

#                 # timestamps
#                 if tcol and tcol in filtered.columns:
#                     ts = r.get(tcol)
#                     try:
#                         ts_py = pd.to_datetime(ts)
#                         out['Date'] = ts_py.date().isoformat()
#                         out['Time'] = ts_py.time().isoformat()
#                         out['LocaleMessageTime'] = ts_py.isoformat()
#                     except Exception:
#                         txt = str(r.get(tcol))
#                         out['Date'] = txt[:10]
#                         out['Time'] = txt[11:19] if len(txt) >= 19 else txt
#                         out['LocaleMessageTime'] = txt
#                 else:
#                     out['Date'] = None
#                     out['Time'] = None
#                     out['LocaleMessageTime'] = None

#                 out['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
#                 out['SwipeGap'] = format_seconds_to_hms(out['SwipeGapSeconds'])
#                 out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
#                 out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in r else None
#                 out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
#                 try:
#                     out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else map_door_to_zone(out['Door'], out['Direction'])
#                 except Exception:
#                     out['Zone'] = None
#                 out['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
#                 out['_source_file'] = fp.name
#                 _append_swipe(out, fp.name)

#     return jsonify({
#         "aggregated_rows": cleaned_matched,
#         "raw_swipe_files": sorted(list(raw_files)),
#         "raw_swipes": raw_swipes_out
#     }), 200
