Now i have update app.py file as per above suggestion now we got below error and i run trend for Pune in console its run for Pune , APAC , emea , laca , Namer 
this need to fix check below console details and fix this isue ...


 * Debug mode: on
INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8002
 * Running on http://10.199.47.235:8002
INFO:werkzeug:Press CTRL+C to quit
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 134-209-644
INFO:root:Fetching swipes for region apac on 2025-11-13
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-13
INFO:root:City filter 'pune' applied for region apac: rows before=17159 after=10315
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251113.csv (rows=490)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251113.csv (rows=10315)
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:PersonnelTypeName values example: ['Employee']
INFO:root:PersonnelTypeName filter applied: before=10315 after=10315
INFO:root:Saved raw swipes to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\swipes_pune_20251113.csv
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2141: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2216: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2216: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1671: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
INFO:root:Fetching swipes for region apac on 2025-11-13
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-13
INFO:root:City filter 'pune' applied for region apac: rows before=17159 after=10315
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251113.csv (rows=490)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251113.csv (rows=10315)
INFO:root:Fetching swipes for region emea on 2025-11-13
INFO:root:Databases present for server SRVWUFRA0986V: ['ACVSUJournal_00011029', 'ACVSUJournal_00011028', 'ACVSUJournal_00011027', 'ACVSUJournal_00011026', 'ACVSUJournal_00011025', 'ACVSUJournal_00011024', 'ACVSUJournal_00011023']
INFO:root:Built SQL for region emea, date 2025-11-13
INFO:root:City filter 'pune' applied for region emea: rows before=7120 after=6
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for emea to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\emea_duration_20251113.csv (rows=1)
INFO:root:Wrote swipes CSV for emea to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\emea_swipes_20251113.csv (rows=6)
INFO:root:Fetching swipes for region laca on 2025-11-13
INFO:root:Databases present for server SRVWUSJO0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region laca, date 2025-11-13
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 904, in run_trend
    df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 2262, in run_trend_for_date  
    trend_df = score_trends_from_durations(merged, swipes_df=sw_combined, outdir=str(outdir_path), target_date=target_date)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1717, in score_trends_from_durations
    df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)
               ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\frame.py", line 10401, in apply
    return op.apply().__finalize__(self, method="apply")
           ~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 916, in apply
    return self.apply_standard()
           ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 1063, in apply_standard
    results, res_index = self.apply_series_generator()
                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 1081, in apply_series_generator
    results[i] = self.func(v, *self.args, **self.kwargs)
                 ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1717, in <lambda>
    df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)
                                             ~^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 361, in scenario_zero_swipes 
    return int(row.get('CountSwipes', 0)) == 0
           ~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 1151, in emit
    msg = self.format(record)
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 712, in format
    record.message = record.getMessage()
                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 400, in getMessage
    msg = msg % self.args
          ~~~~^~~~~~~~~~~
TypeError: not enough arguments for format string
Call stack:
  File "C:\Program Files\Python313\Lib\threading.py", line 1014, in _bootstrap
    self._bootstrap_inner()
  File "C:\Program Files\Python313\Lib\threading.py", line 1043, in _bootstrap_inner
    self.run()
  File "C:\Program Files\Python313\Lib\threading.py", line 994, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 697, in process_request_thread
    self.finish_request(request, client_address)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 362, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 766, in __init__
    self.handle()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 398, in handle
    super().handle()
  File "C:\Program Files\Python313\Lib\http\server.py", line 436, in handle
    self.handle_one_request()
  File "C:\Program Files\Python313\Lib\http\server.py", line 424, in handle_one_request
    method()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 370, in run_wsgi
    execute(self.server.app)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 333, in execute
    for data in application_iter:
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\debug\__init__.py", line 343, in debug_application
    app_iter = self.app(environ, start_response)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1536, in __call__
    return self.wsgi_app(environ, start_response)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1511, in wsgi_app
    response = self.full_dispatch_request()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]  
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 907, in run_trend
    df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1959, in run_trend_for_date  
    results = run_for_date(target_date, regions, str(outdir_path), city)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 1946, in run_for_date     
    logging.info("City filter '%s' applied for region %s: rows before=%d after=%d (no matches)", city_raw, rkey, len(swipes))
Message: "City filter '%s' applied for region %s: rows before=%d after=%d (no matches)"
Arguments: ('pune', 'laca', 12352)
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for laca to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\laca_duration_20251113.csv (rows=908)
INFO:root:Wrote swipes CSV for laca to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\laca_swipes_20251113.csv (rows=12352)
INFO:root:Fetching swipes for region namer on 2025-11-13
INFO:root:Databases present for server SRVWUDEN0891V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region namer, date 2025-11-13
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 904, in run_trend
    df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 2262, in run_trend_for_date  
    trend_df = score_trends_from_durations(merged, swipes_df=sw_combined, outdir=str(outdir_path), target_date=target_date)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1717, in score_trends_from_durations
    df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)
               ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\frame.py", line 10401, in apply
    return op.apply().__finalize__(self, method="apply")
           ~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 916, in apply
    return self.apply_standard()
           ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 1063, in apply_standard
    results, res_index = self.apply_series_generator()
                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 1081, in apply_series_generator
    results[i] = self.func(v, *self.args, **self.kwargs)
                 ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1717, in <lambda>
    df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)
                                             ~^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 361, in scenario_zero_swipes 
    return int(row.get('CountSwipes', 0)) == 0
           ~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 1151, in emit
    msg = self.format(record)
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 712, in format
    record.message = record.getMessage()
                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 400, in getMessage
    msg = msg % self.args
          ~~~~^~~~~~~~~~~
TypeError: not enough arguments for format string
Call stack:
  File "C:\Program Files\Python313\Lib\threading.py", line 1014, in _bootstrap
    self._bootstrap_inner()
  File "C:\Program Files\Python313\Lib\threading.py", line 1043, in _bootstrap_inner
    self.run()
  File "C:\Program Files\Python313\Lib\threading.py", line 994, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 697, in process_request_thread
    self.finish_request(request, client_address)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 362, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 766, in __init__
    self.handle()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 398, in handle
    super().handle()
  File "C:\Program Files\Python313\Lib\http\server.py", line 436, in handle
    self.handle_one_request()
  File "C:\Program Files\Python313\Lib\http\server.py", line 424, in handle_one_request
    method()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 370, in run_wsgi
    execute(self.server.app)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 333, in execute
    for data in application_iter:
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\debug\__init__.py", line 343, in debug_application
    app_iter = self.app(environ, start_response)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1536, in __call__
    return self.wsgi_app(environ, start_response)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1511, in wsgi_app
    response = self.full_dispatch_request()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]  
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 907, in run_trend
    df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1959, in run_trend_for_date  
    results = run_for_date(target_date, regions, str(outdir_path), city)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 1946, in run_for_date     
    logging.info("City filter '%s' applied for region %s: rows before=%d after=%d (no matches)", city_raw, rkey, len(swipes))
Message: "City filter '%s' applied for region %s: rows before=%d after=%d (no matches)"
Arguments: ('pune', 'namer', 3681)
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for namer to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\namer_duration_20251113.csv (rows=568)
INFO:root:Wrote swipes CSV for namer to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\namer_swipes_20251113.csv (rows=3681)
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:PersonnelTypeName values example: ['Employee']
INFO:root:PersonnelTypeName filter applied: before=26354 after=26354
INFO:werkzeug: * Detected change in 'C:\\Users\\W0024618\\Desktop\\Trend Analysis\\backend\\app.py', reloading
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 134-209-644
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python app.py
>>
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 1479
    files = []
    ^^^^^
IndentationError: expected an indented block after function definition on line 1478
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python app.py
>>
 * Serving Flask app 'app'
 * Debug mode: on
INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8002
 * Running on http://10.199.47.235:8002
INFO:werkzeug:Press CTRL+C to quit
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 134-209-644
INFO:root:Fetching swipes for region apac on 2025-11-13
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-13
INFO:root:City filter 'pune' applied for region apac: rows before=17159 after=10315
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251113.csv (rows=490)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251113.csv (rows=10315)
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:PersonnelTypeName values example: ['Employee']
INFO:root:PersonnelTypeName filter applied: before=10315 after=10315
INFO:root:Saved raw swipes to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\swipes_pune_20251113.csv
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2141: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2216: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2216: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1671: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
INFO:root:Fetching swipes for region apac on 2025-11-13
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-13
INFO:root:City filter 'pune' applied for region apac: rows before=17159 after=10315
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251113.csv (rows=490)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251113.csv (rows=10315)
INFO:root:Fetching swipes for region emea on 2025-11-13
INFO:root:Databases present for server SRVWUFRA0986V: ['ACVSUJournal_00011029', 'ACVSUJournal_00011028', 'ACVSUJournal_00011027', 'ACVSUJournal_00011026', 'ACVSUJournal_00011025', 'ACVSUJournal_00011024', 'ACVSUJournal_00011023']
INFO:root:Built SQL for region emea, date 2025-11-13
INFO:root:City filter 'pune' applied for region emea: rows before=7120 after=6
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for emea to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\emea_duration_20251113.csv (rows=1)
INFO:root:Wrote swipes CSV for emea to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\emea_swipes_20251113.csv (rows=6)
INFO:root:Fetching swipes for region laca on 2025-11-13
INFO:root:Databases present for server SRVWUSJO0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region laca, date 2025-11-13
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 973, in run_trend
    df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 2262, in run_trend_for_date  
    trend_df = score_trends_from_durations(merged, swipes_df=sw_combined, outdir=str(outdir_path), target_date=target_date)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1717, in score_trends_from_durations
    df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)
               ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\frame.py", line 10401, in apply
    return op.apply().__finalize__(self, method="apply")
           ~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 916, in apply
    return self.apply_standard()
           ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 1063, in apply_standard
    results, res_index = self.apply_series_generator()
                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 1081, in apply_series_generator
    results[i] = self.func(v, *self.args, **self.kwargs)
                 ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1717, in <lambda>
    df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)
                                             ~^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 361, in scenario_zero_swipes 
    return int(row.get('CountSwipes', 0)) == 0
           ~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 1151, in emit
    msg = self.format(record)
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 712, in format
    record.message = record.getMessage()
                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 400, in getMessage
    msg = msg % self.args
          ~~~~^~~~~~~~~~~
TypeError: not enough arguments for format string
Call stack:
  File "C:\Program Files\Python313\Lib\threading.py", line 1014, in _bootstrap
    self._bootstrap_inner()
  File "C:\Program Files\Python313\Lib\threading.py", line 1043, in _bootstrap_inner
    self.run()
  File "C:\Program Files\Python313\Lib\threading.py", line 994, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 697, in process_request_thread
    self.finish_request(request, client_address)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 362, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 766, in __init__
    self.handle()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 398, in handle
    super().handle()
  File "C:\Program Files\Python313\Lib\http\server.py", line 436, in handle
    self.handle_one_request()
  File "C:\Program Files\Python313\Lib\http\server.py", line 424, in handle_one_request
    method()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 370, in run_wsgi
    execute(self.server.app)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 333, in execute
    for data in application_iter:
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\debug\__init__.py", line 343, in debug_application
    app_iter = self.app(environ, start_response)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1536, in __call__
    return self.wsgi_app(environ, start_response)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1511, in wsgi_app
    response = self.full_dispatch_request()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]  
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 976, in run_trend
    df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1959, in run_trend_for_date  
    results = run_for_date(target_date, regions, str(outdir_path), city)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 1946, in run_for_date     
    logging.info("City filter '%s' applied for region %s: rows before=%d after=%d (no matches)", city_raw, rkey, len(swipes))
Message: "City filter '%s' applied for region %s: rows before=%d after=%d (no matches)"
Arguments: ('pune', 'laca', 12352)
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for laca to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\laca_duration_20251113.csv (rows=908)
INFO:root:Wrote swipes CSV for laca to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\laca_swipes_20251113.csv (rows=12352)
INFO:root:Fetching swipes for region namer on 2025-11-13
INFO:root:Databases present for server SRVWUDEN0891V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region namer, date 2025-11-13
--- Logging error ---
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 973, in run_trend
    df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 2262, in run_trend_for_date  
    trend_df = score_trends_from_durations(merged, swipes_df=sw_combined, outdir=str(outdir_path), target_date=target_date)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1717, in score_trends_from_durations
    df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)
               ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\frame.py", line 10401, in apply
    return op.apply().__finalize__(self, method="apply")
           ~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 916, in apply
    return self.apply_standard()
           ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 1063, in apply_standard
    results, res_index = self.apply_series_generator()
                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\pandas\core\apply.py", line 1081, in apply_series_generator
    results[i] = self.func(v, *self.args, **self.kwargs)
                 ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1717, in <lambda>
    df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)
                                             ~^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 361, in scenario_zero_swipes 
    return int(row.get('CountSwipes', 0)) == 0
           ~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 1151, in emit
    msg = self.format(record)
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 999, in format
    return fmt.format(record)
           ~~~~~~~~~~^^^^^^^^
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 712, in format
    record.message = record.getMessage()
                     ~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\logging\__init__.py", line 400, in getMessage
    msg = msg % self.args
          ~~~~^~~~~~~~~~~
TypeError: not enough arguments for format string
Call stack:
  File "C:\Program Files\Python313\Lib\threading.py", line 1014, in _bootstrap
    self._bootstrap_inner()
  File "C:\Program Files\Python313\Lib\threading.py", line 1043, in _bootstrap_inner
    self.run()
  File "C:\Program Files\Python313\Lib\threading.py", line 994, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 697, in process_request_thread
    self.finish_request(request, client_address)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 362, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File "C:\Program Files\Python313\Lib\socketserver.py", line 766, in __init__
    self.handle()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 398, in handle
    super().handle()
  File "C:\Program Files\Python313\Lib\http\server.py", line 436, in handle
    self.handle_one_request()
  File "C:\Program Files\Python313\Lib\http\server.py", line 424, in handle_one_request
    method()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 370, in run_wsgi
    execute(self.server.app)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\serving.py", line 333, in execute
    for data in application_iter:
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\werkzeug\debug\__init__.py", line 343, in debug_application
    app_iter = self.app(environ, start_response)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1536, in __call__
    return self.wsgi_app(environ, start_response)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1511, in wsgi_app
    response = self.full_dispatch_request()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]  
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 976, in run_trend
    df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 1959, in run_trend_for_date  
    results = run_for_date(target_date, regions, str(outdir_path), city)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 1946, in run_for_date     
    logging.info("City filter '%s' applied for region %s: rows before=%d after=%d (no matches)", city_raw, rkey, len(swipes))
Message: "City filter '%s' applied for region %s: rows before=%d after=%d (no matches)"
Arguments: ('pune', 'namer', 3681)
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for namer to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\namer_duration_20251113.csv (rows=568)
INFO:root:Wrote swipes CSV for namer to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\namer_swipes_20251113.csv (rows=3681)
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:PersonnelTypeName values example: ['Employee']
INFO:root:PersonnelTypeName filter applied: before=26354 after=26354
INFO:root:Saved raw swipes to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\swipes_pune_20251113.csv
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2141: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2216: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2216: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1671: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
INFO:root:Fetching swipes for region apac on 2025-11-13
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-13
INFO:root:City filter 'Pune' applied for region apac: rows before=17159 after=10315
C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py:1770: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.
  df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251113.csv (rows=490)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251113.csv (rows=10315)
INFO:root:get_personnel_info: lookup called with candidate_identifier=324507
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=324507 -> ObjectID=2097204272 Email=Anteriksh.Pandey@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=319451
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=319451 -> ObjectID=2097197181 Email=Anant.Yadav@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=328504
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=328504 -> ObjectID=2097209184 Email=SaiPoojith.Pogula@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=319013
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=319013 -> ObjectID=2097196701 Email=Virendra.Phadtare@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=320643
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=320643 -> ObjectID=2097198821 Email=Smruti.Barik@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=321913
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=321913 -> ObjectID=2097200504 Email=Arjun.Chavan@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=321911
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=321911 -> ObjectID=2097200502 Email=Rishav.Sarkar@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=323220
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=323220 -> ObjectID=2097202445 Email=NIKHIL.GUPTA@wu.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=325230
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=325230 -> ObjectID=2097205204 Email=Mahesh.More@westernunion.com
INFO:root:get_personnel_info: lookup called with candidate_identifier=318147
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
INFO:root:get_personnel_info: found personnel row for candidate=318147 -> ObjectID=2097195670 Email=Amol.Wanave@wu.com
INFO:werkzeug:127.0.0.1 - - [17/Nov/2025 15:16:16] "GET /run?start=2025-11-13&end=2025-11-13&full=true&region=apac&city=Pune HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [17/Nov/2025 15:16:22] "GET /record?employee_id=320643 HTTP/1.1" 500 -
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1536, in __call__
    return self.wsgi_app(environ, start_response)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1514, in wsgi_app
    response = self.handle_exception(e)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ~^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1511, in wsgi_app
    response = self.full_dispatch_request()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 920, in full_dispatch_request
    return self.finalize_request(rv)
           ~~~~~~~~~~~~~~~~~~~~~^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 939, in finalize_request
    response = self.make_response(rv)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1212, in make_response
    raise TypeError(
    ^
TypeError: The view function for 'record' did not return a valid response. The function either returned None or ended without a return statement.
INFO:werkzeug:127.0.0.1 - - [17/Nov/2025 15:16:35] "GET /record?employee_id=321911 HTTP/1.1" 500 -
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1536, in __call__
    return self.wsgi_app(environ, start_response)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1514, in wsgi_app
    response = self.handle_exception(e)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask_cors\extension.py", line 176, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
                                                ~^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1511, in wsgi_app
    response = self.full_dispatch_request()
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 920, in full_dispatch_request
    return self.finalize_request(rv)
           ~~~~~~~~~~~~~~~~~~~~~^^^^
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 939, in finalize_request
    response = self.make_response(rv)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\.venv\Lib\site-packages\flask\app.py", line 1212, in make_response
    raise TypeError(
    ^
TypeError: The view function for 'record' did not return a valid response. The function either returned None or ended without a return statement.












Check error and below each file line by line and fix this issue ....



# backend/app.py
from flask import Flask, jsonify, request, send_from_directory, jsonify, send_file
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re
import io
import base64
import os
import difflib
from io import BytesIO
from flask import send_file  # add if not present
from pathlib import Path
from typing import Optional, List, Dict, Any
from duration_report import REGION_CONFIG
from datetime import date, timedelta, datetime
from flask import jsonify, request
import logging
logging.basicConfig(level=logging.INFO)
#from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR
from trend_runner import run_trend_for_date, build_monthly_training, _enrich_with_personnel_info
from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE


# app = Flask(__name__)



def _safe_read_csv(fp):
    try:
        return pd.read_csv(fp, parse_dates=['LocaleMessageTime'], low_memory=False)
    except Exception:
        try:
            return pd.read_csv(fp, low_memory=False)
        except Exception:
            return pd.DataFrame()


# ---------- Ensure outputs directory exists early (so OVERRIDES_FILE can be defined safely) ----------

BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)

OUTDIR = DEFAULT_OUTDIR

OVERRIDES_FILE = DEFAULT_OUTDIR / "overrides.csv"


def _load_overrides():
    if not OVERRIDES_FILE.exists():
        return {}
    try:
        df = pd.read_csv(OVERRIDES_FILE, dtype=str)
        out = {}
        for _, r in df.iterrows():
            emp = str(r.get('EmployeeID') or r.get('person_uid') or '').strip()
            if not emp:
                continue
            out[emp] = {
                'level': str(r.get('OverrideLevel') or '').strip(),
                'reason': str(r.get('Reason') or '').strip(),
                'ts': str(r.get('Timestamp') or '').strip()
            }
        return out
    except Exception:
        logging.exception("Failed reading overrides file")
        return {}

def _save_override(employee_key, level, reason):
    now = datetime.now().isoformat()
    row = {'EmployeeID': employee_key, 'OverrideLevel': level, 'Reason': reason or '', 'Timestamp': now}
    try:
        if OVERRIDES_FILE.exists():
            df = pd.read_csv(OVERRIDES_FILE, dtype=str)
            # pandas.DataFrame.append is deprecated -> use concat
            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        else:
            df = pd.DataFrame([row])
        df.to_csv(OVERRIDES_FILE, index=False)
        return True
    except Exception:
        logging.exception("Failed to save override")
        return False

def _slug_city(s):
    """
    Convert a city/site string into a safe slug: lowercase, alphanumeric+hyphen
    """
    if not s:
        return ''
    # Remove special chars, spaces to hyphens, lower
    slug = re.sub(r'[^\w\s-]', '', str(s)).strip().lower()
    slug = re.sub(r'[\s_]+', '-', slug)
    return slug


# --- Use REGION_CONFIG servers to talk to ACVSCore (no separate ACVSCORE_DB_CONFIG) ---
# ODBC driver variable is already defined later: ODBC_DRIVER (safe to reference only at runtime)

_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20

def _get_acvscore_conn():
    """
    Try to connect to ACVSCore by reusing credentials from REGION_CONFIG.
    Loops through REGION_CONFIG entries and attempts:
      1) SQL auth (UID/PWD) to database "ACVSCore" using region server + credentials
      2) If SQL auth fails on that server, try Trusted_Connection (Windows auth) as a fallback
    If that fails, optionally attempt ACVSCORE_DB_CONFIG if defined (safe: checked via globals()).
    Returns first successful pyodbc connection or None.
    Implements a short backoff after recent failure to reduce log noise.
    """
    try:
        import pyodbc
    except Exception:
        logging.exception("pyodbc not installed; ACVSCore lookups unavailable.")
        return None

    # basic backoff: skip attempts if we just failed recently
    from datetime import datetime
    now = datetime.now().timestamp()
    last = _acvscore_backoff.get("ts")
    if last and _acvscore_backoff.get("failed") and (now - last) < _ACVSCORE_BACKOFF_SECONDS:
        logging.debug("Skipping ACVSCore connection attempt (backoff active).")
        return None

    # iterate region servers (use the same credentials defined in REGION_CONFIG)
    tried = []
    for region_key, rc in (REGION_CONFIG or {}).items():
        server = rc.get("server")
        user = rc.get("user")
        pwd = rc.get("password")
        if not server:
            continue

        # Try SQL auth first if credentials present
        if user and pwd:
            tried.append(f"{region_key}@{server}(sql)")
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={server};DATABASE=ACVSCore;UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;"
            )
            try:
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (sql auth).", server, region_key)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("SQL auth to %s failed: %s", server, e)

        # Try Trusted Connection fallback on same server
        tried.append(f"{region_key}@{server}(trusted)")
        conn_str_trusted = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={server};DATABASE=ACVSCore;Trusted_Connection=yes;"
            "TrustServerCertificate=Yes;"
        )
        try:
            conn = pyodbc.connect(conn_str_trusted, autocommit=True, timeout=5)
            logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (trusted connection).", server, region_key)
            _acvscore_backoff["ts"] = None
            _acvscore_backoff["failed"] = False
            return conn
        except Exception as e:
            logging.debug("Trusted connection to %s failed: %s", server, e)
            continue

    # Fallback: if a global ACVSCORE_DB_CONFIG exists, try it (safe check)
    try:
        if 'ACVSCORE_DB_CONFIG' in globals() and isinstance(globals().get('ACVSCORE_DB_CONFIG'), dict):
            cfg = globals().get('ACVSCORE_DB_CONFIG')
            server = cfg.get('server')
            user = cfg.get('user')
            pwd = cfg.get('password')
            database = cfg.get('database', 'ACVSCore')
            tried.append(f"ACVSCORE_DB_CONFIG@{server}")
            try:
                conn_str = (
                    f"DRIVER={{{ODBC_DRIVER}}};"
                    f"SERVER={server};DATABASE={database};UID={user};PWD={pwd};"
                    "TrustServerCertificate=Yes;"
                )
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore using ACVSCORE_DB_CONFIG (%s).", server)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("ACVSCORE_DB_CONFIG connection failed: %s", e)
    except Exception:
        # defensive: do not propagate any errors from fallback logic
        logging.debug("ACVSCORE_DB_CONFIG fallback not available or failed.")

    # record failure to backoff
    _acvscore_backoff["ts"] = now
    _acvscore_backoff["failed"] = True
    logging.error("Failed to connect to ACVSCore using REGION_CONFIG servers. Tried: %s", tried)
    return None


# ODBC driver (keep existing env-based driver)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data



def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    out: Dict[str, Any] = {}
    logging.info("get_personnel_info: lookup called with candidate_identifier=%s", candidate_identifier)
    if candidate_identifier is None:
        logging.debug("get_personnel_info: no candidate provided")
        return out
    conn = _get_acvscore_conn()
    if conn is None:
        logging.info("get_personnel_info: ACVSCore connection unavailable (skipping DB lookup)")
        return out
    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        # avoid passing non-GUID strings to the GUID parameter (prevents SQL Server conversion errors)
        cand_guid = cand if _looks_like_guid(cand) else None
        params = (cand, cand_guid, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()
        if row:
            # columns: ObjectID, GUID, Name, EmailAddress, ManagerEmail
            try:
                out['ObjectID'] = row[0]
                out['GUID'] = row[1]
                out['Name'] = row[2]
                # canonical email fields (provide aliases for downstream code)
                email_val = row[3] if len(row) > 3 else None
                out['EmailAddress'] = email_val or None
                out['EmployeeEmail'] = email_val or None
                out['Email'] = email_val or None
                out['ManagerEmail'] = row[4] if len(row) > 4 else None
            except Exception:
                out = {
                    'ObjectID': row[0] if len(row) > 0 else None,
                    'GUID': row[1] if len(row) > 1 else None,
                    'Name': row[2] if len(row) > 2 else None,
                    'EmailAddress': row[3] if len(row) > 3 else None,
                    'EmployeeEmail': row[3] if len(row) > 3 else None,
                    'Email': row[3] if len(row) > 3 else None,
                    'ManagerEmail': row[4] if len(row) > 4 else None
                }
            logging.info("get_personnel_info: found personnel row for candidate=%s -> ObjectID=%s Email=%s",
                         candidate_identifier, out.get('ObjectID'), out.get('EmailAddress'))
        else:
            logging.debug("get_personnel_info: no personnel row found for candidate=%s", candidate_identifier)
    except Exception:
        logging.exception("Failed personnel lookup for candidate: %s", candidate_identifier)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out



def get_person_image_bytes(parent_id) -> Optional[bytes]:
    logging.info("get_person_image_bytes: lookup for ParentId=%s", parent_id)
    # 1) try DB (as before)  keep behaviour if available
    try:
        conn = _get_acvscore_conn()
        if conn is not None:
            try:
                cur = conn.cursor()
                sql = """
                    SELECT TOP 1 AI.Image
                    FROM ACVSCore.Access.Images AI
                    WHERE AI.ParentId = ?
                      AND DATALENGTH(AI.Image) > 0
                    ORDER BY AI.ObjectID DESC
                """
                cur.execute(sql, (str(parent_id),))
                row = cur.fetchone()
                if row and row[0] is not None:
                    logging.info("get_person_image_bytes: image found in DB for ParentId=%s (len=%d)", parent_id, len(row[0]) if row[0] else 0)
                    try:
                        b = bytes(row[0])
                        return b
                    except Exception:
                        return row[0]
            except Exception:
                logging.exception("Failed to fetch image for ParentId=%s via DB", parent_id)
            finally:
                try:
                    cur.close()
                except Exception:
                    pass
                try:
                    conn.close()
                except Exception:
                    pass
    except Exception:
        logging.debug("ACVSCore DB unavailable for image lookup; will try filesystem fallbacks for ParentId=%s", parent_id)

    # 2) Try filesystem fallbacks under DEFAULT_OUTDIR (use typical file extensions)
    try:
        # ensure DEFAULT_OUTDIR exists and convert parent_id to safe filename
        cand_ids = []
        if parent_id is None:
            return None
        pid_raw = str(parent_id).strip()
        # add raw and numeric-only variants
        cand_ids.append(pid_raw)
        try:
            # if numeric-like, add int form
            if '.' in pid_raw:
                f = float(pid_raw)
                if f.is_integer():
                    cand_ids.append(str(int(f)))
        except Exception:
            pass
        # also try stripped non-alphanumeric variants
        cand_ids = list(dict.fromkeys(cand_ids))

        for c in cand_ids:
            for folder in (Path(DEFAULT_OUTDIR) / "images", Path(DEFAULT_OUTDIR), Path(".")):
                if not folder.exists():
                    continue
                for ext in (".jpg", ".jpeg", ".png", ".bmp", ".gif", ".webp"):
                    fp = folder / (f"{c}{ext}")
                    logging.debug("get_person_image_bytes: checking path %s", fp)
                    if fp.exists() and fp.is_file():
                        logging.info("get_person_image_bytes: loaded image file %s", fp)
                        try:
                            return fp.read_bytes()
                        except Exception:
                            continue
                # also try files where parent_id might be part of filename
                for fp in folder.glob(f"*{c}*"):
                    logging.debug("get_person_image_bytes: checking glob match %s", fp)
                    if fp.is_file():
                        try:
                            b = fp.read_bytes()
                            if b:
                                logging.info("get_person_image_bytes: loaded image via glob %s", fp)
                                return b
                        except Exception:
                            continue
    except Exception:
        logging.exception("Filesystem image lookup failed for ParentId=%s", parent_id)

    # nothing found
    return None


# ---------- New route to serve employee image ----------
# We'll import send_file later where used; define route after app created.

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

# send_file is needed for Excel responses
from flask import send_file
try:
    # optional import; used for styling
    from openpyxl import load_workbook
    from openpyxl.styles import Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        # convert floats NaN handled above
        return x
    try:
        # fallback to string
        return str(x)
    except Exception:
        return None


_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False


# Helper: format seconds to HH:MM:SS
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return None
        # guard against floats and NaN
        s = int(float(seconds))
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return None


# Placeholder tokens (keep consistent with trend_runner expectations)
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '', '', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


def _resolve_field_from_record(record: dict, candidate_tokens: list):
    """
    Search a single row `record` (dict) for likely columns listed in candidate_tokens.
    Return first non-placeholder value found (converted to Python scalar), else None.
    """
    if record is None:
        return None

    # 1) exact key matches (case-sensitive & common casing)
    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)

    # 2) case-insensitive contains match
    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)

    # 3) xml / value parsing fallback for CardNumber
    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue

    # 4) final fallback: first non-placeholder value
    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)

    return None


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Clean a dataframe for JSON output (convert NaN -> None, pandas types -> native, format datetimes).
    """
    if df is None or df.empty:
        return []
    df = df.copy()

    # remove duplicate suffix columns
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    # Date normalization
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    # Datetime columns to ISO strings
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # Replace NaN/inf -> None
    df = df.where(pd.notnull(df), None)

    # Convert records to safe Python types
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        # Typical fields
        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        # ----- Schema-aware fallback resolution -----
        if not emp_id:
            emp_tokens = ['Int1', 'Text12', 'EmployeeID', 'empid', 'id']
            resolved_emp = _resolve_field_from_record(r, emp_tokens)
            if resolved_emp is not None:
                try:
                    s = str(resolved_emp).strip()
                    # remove trailing .0 for floats
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    if _looks_like_guid(s):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = s
                        emp_id = s
                except Exception:
                    if _looks_like_guid(resolved_emp):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = resolved_emp
                        emp_id = resolved_emp

        # Prefer Credential.CardNumber / CHUID / Card as CardNumber when missing  reject GUIDs/placeholders
        if out.get('CardNumber') in (None, '', 'nan'):
            card_tokens = ['CardNumber', 'CHUID', 'Card', 'card_no', 'cardnum']
            resolved_card = _resolve_field_from_record(r, card_tokens)
            if resolved_card is not None:
                try:
                    cs = str(resolved_card).strip()
                    if _looks_like_guid(cs) or _is_placeholder_str(cs):
                        out['CardNumber'] = None
                    else:
                        out['CardNumber'] = cs
                except Exception:
                    out['CardNumber'] = None

        # final safety: ensure EmployeeID/CardNumber are not GUID-like tokens
        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None
        if 'CardNumber' in out and isinstance(out['CardNumber'], str) and _looks_like_guid(out['CardNumber']):
            out['CardNumber'] = None

        # If EmployeeName empty or looks like a GUID, prefer EmployeeID (human id) over GUIDs
        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            if emp_id not in (None, '', 'nan') and not _looks_like_guid(emp_id):
                out['EmployeeName'] = str(emp_id)
            else:
                out['EmployeeName'] = None

        cleaned.append(out)
    return cleaned

# ----- Helpers added to match commented (Pune) file functionality but multi-city-aware -----

def _replace_placeholder_strings(obj):
    """
    If obj is a DataFrame, replace known placeholder strings with None (NaN).
    If obj is a scalar/string, return None for placeholder strings else return obj.
    """
    if obj is None:
        return obj
    try:
        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
            for col in df.columns:
                try:
                    # Replace placeholder strings (case-insensitive)
                    df[col] = df[col].apply(lambda x: None if _is_placeholder_str(x) else x)
                except Exception:
                    continue
            return df
        else:
            # scalar
            return None if _is_placeholder_str(obj) else obj
    except Exception:
        return obj

def _normalize_id_local(v):
    """
    Normalize an identifier for robust matching/counting:
    - treat NaN/None/empty as None
    - strip and convert float-like integers to integer strings
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == '' or s.lower() == 'nan':
        return None
    try:
        if '.' in s:
            fv = float(s)
            if math.isfinite(fv) and fv.is_integer():
                s = str(int(fv))
    except Exception:
        pass
    return s





# def _find_swipe_files(outdir: Path, date_obj: Optional[date] = None, city_slug: Optional[str] = None):
#     """
#     Robust swipe-file discovery.
#     - Supports filenames like:
#         - swipes_YYYYMMDD.csv
#         - swipes_<city>_YYYYMMDD.csv
#         - <region>_swipes_YYYYMMDD.csv
#         - <region>_swipes_*.csv
#         - any file containing '_swipes_' or starting with 'swipes'
#         - fallback: any file that ends with _YYYYMMDD.csv
#     - If date_obj is None, returns recent files that look like swipe files.
#     - Returns list of Path objects sorted by mtime (newest first).
#     """
#     p = Path(outdir)
#     files_set = set()
#     try:
#         # Normalize city slug for matching
#         city_slug_l = (city_slug or "").lower().strip()

#         def add_glob(pattern):
#             try:
#                 for fp in p.glob(pattern):
#                     if fp.is_file():
#                         files_set.add(fp)
#             except Exception:
#                 pass

#         if date_obj is None:
#             # recent swipe-like files
#             add_glob("*_swipes_*.csv")        # region_swipes_YYYY or region_swipes_any.csv
#             add_glob("swipes_*.csv")         # swipes_YYYY or swipes_city_YYYY
#             add_glob("*swipes*.csv")         # permissive
#             add_glob("*_swipes.csv")
#             # also include any file that includes 'swipe' (some exporters use 'swipe' singular)
#             add_glob("*swipe*.csv")
#             # city-specific guesses
#             if city_slug_l:
#                 add_glob(f"*{city_slug_l}*_swipes_*.csv")
#                 add_glob(f"*{city_slug_l}*swipes*.csv")
#                 add_glob(f"*{city_slug_l}*.csv")
#         else:
#             target = date_obj.strftime("%Y%m%d")
#             # common patterns observed in pipeline
#             patterns = [
#                 f"*_{target}.csv",                 # anything ending _YYYYMMDD.csv
#                 f"*_swipes_{target}.csv",         # region_swipes_YYYYMMDD.csv or swipes_YYYYMMDD
#                 f"swipes*_{target}.csv",
#                 f"swipes_{target}.csv",
#                 f"*swipes*_{target}.csv",
#                 f"*{city_slug_l}*_{target}.csv",
#                 f"*{city_slug_l}*swipes*_{target}.csv",
#                 f"*{city_slug_l}_{target}.csv"
#             ]
#             for pat in patterns:
#                 add_glob(pat)

#         # final fallback: any CSV in folder that contains '_swipe' (case-insensitive)
#         try:
#             for fp in p.iterdir():
#                 if not fp.is_file():
#                     continue
#                 name = fp.name.lower()
#                 if ('_swipe' in name) or ('swipe' in name and name.endswith('.csv')):
#                     files_set.add(fp)
#         except Exception:
#             pass

#     except Exception:
#         logging.exception("Error while searching for swipe files in %s", outdir)

#     # sort by modification time (most recent first)
#     files = sorted(list(files_set), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
#     return files



def _find_swipe_files(outdir: Path, date_obj: Optional[date] = None, city_slug: Optional[str] = None, include_shifted: bool = True):
    """
    Robust swipe-file discovery.
    - If include_shifted is False, files with 'shift' in the filename (e.g. _shifted) are excluded.
    - Supports various filename patterns; if date_obj is None returns recent swipe-like files.
    - Returns list of Path objects sorted by mtime (newest first).
    """
    p = Path(outdir)
    files_set = set()
    try:
        city_slug_l = (city_slug or "").lower().strip()

        def add_glob(pattern):
            try:
                for fp in p.glob(pattern):
                    if fp.is_file():
                        files_set.add(fp)
            except Exception:
                pass

        if date_obj is None:
            add_glob("*_swipes_*.csv")
            add_glob("swipes_*.csv")
            add_glob("*swipes*.csv")
            add_glob("*_swipes.csv")
            add_glob("*swipe*.csv")
            if city_slug_l:
                add_glob(f"*{city_slug_l}*_swipes_*.csv")
                add_glob(f"*{city_slug_l}*swipes*.csv")
                add_glob(f"*{city_slug_l}*.csv")
        else:
            target = date_obj.strftime("%Y%m%d")
            patterns = [
                f"*_{target}.csv",
                f"*_swipes_{target}.csv",
                f"swipes*_{target}.csv",
                f"swipes_{target}.csv",
                f"*swipes*_{target}.csv",
                f"*{city_slug_l}*_{target}.csv",
                f"*{city_slug_l}*swipes*_{target}.csv",
                f"*{city_slug_l}_{target}.csv"
            ]
            for pat in patterns:
                add_glob(pat)

        # fallback: any CSV containing 'swipe'/'swipes' in the name
        try:
            for fp in p.iterdir():
                if not fp.is_file():
                    continue
                name = fp.name.lower()
                if ('_swipe' in name) or ('swipe' in name and name.endswith('.csv')):
                    files_set.add(fp)
        except Exception:
            pass

    except Exception:
        logging.exception("Error while searching for swipe files in %s", outdir)

    # Filter out shifted files if requested
    files = sorted(list(files_set), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
    if not include_shifted:
        files = [f for f in files if 'shift' not in f.name.lower()]

    return files


# -----------------------
# Routes
# -----------------------




@app.route('/')
def root():
    return "Trend Analysis API  Multi-city"

@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.get_json(force=True) or {}
        else:
            try:
                params = request.form.to_dict() or {}
            except Exception:
                params = {}

    date_str = (params.get('date') or params.get('Date') or '').strip() or None
    start_str = (params.get('start') or params.get('Start') or '').strip() or None
    end_str = (params.get('end') or params.get('End') or '').strip() or None

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    regions_param = params.get('regions') or params.get('region') or ''
    if regions_param:
        regions = [r.strip().lower() for r in re.split(r'[;,|]', str(regions_param)) if r.strip()]
    else:
        try:
            regions = [k.lower() for k in list(REGION_CONFIG.keys())]
        except Exception:
            regions = ['apac']

    valid_regions = []
    for r in regions:
        if r in (REGION_CONFIG or {}):
            valid_regions.append(r)
        else:
            logging.debug("Requested region '%s' not in REGION_CONFIG - skipping", r)
    if not valid_regions:
        valid_regions = [k.lower() for k in REGION_CONFIG.keys()] if REGION_CONFIG else ['apac']
    params['_regions_to_run'] = valid_regions

    city_param = params.get('city') or params.get('site') or params.get('site_name') or None
    city_slug = _slug_city(city_param) if city_param else None
    params['_city'] = city_slug

    combined_rows = []
    files = []

    # ---------------------------
    # Run trend for each requested date
    # ---------------------------
    for d in dates:
        try:
            if run_trend_for_date is None:
                raise RuntimeError("run_trend_for_date helper not available in trend_runner")
            try:
                df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug)
            except TypeError:
                try:
                    df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
                except Exception:
                    # Last-resort: try duration_report fallback if available
                    try:
                        from duration_report import run_for_date as _dr_run_for_date
                        region_results = _dr_run_for_date(d, valid_regions, str(DEFAULT_OUTDIR), city_param)
                        combined_list = []
                        for rkey, res in (region_results or {}).items():
                            try:
                                df_dur = res.get('durations')
                                if df_dur is not None and not df_dur.empty:
                                    combined_list.append(df_dur)
                            except Exception:
                                continue
                        df = pd.concat(combined_list, ignore_index=True) if combined_list else pd.DataFrame()
                    except Exception:
                        raise
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_{city_slug}_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        if df is None or (hasattr(df, 'empty') and df.empty):
            continue

        try:
            df = _replace_placeholder_strings(df)
        except Exception:
            pass

        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        combined_rows.append(df)

    # *** Important: combine after loop to avoid UnboundLocalError and extra repeated concat inside loop ***
    try:
        combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()
    except Exception:
        combined_df = pd.DataFrame()

    try:
        if not combined_df.empty:
            if 'person_uid' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['person_uid'].dropna().astype(str).nunique())
            elif 'EmployeeID' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['EmployeeID'].dropna().astype(str).nunique())
            else:
                raw_unique_person_uids = int(len(combined_df))
        else:
            raw_unique_person_uids = 0
    except Exception:
        raw_unique_person_uids = int(len(combined_df)) if combined_df is not None else 0

    try:
        if not combined_df.empty and 'IsFlagged' in combined_df.columns:
            flagged_df = combined_df[combined_df['IsFlagged'] == True].copy()
        else:
            flagged_df = pd.DataFrame()
    except Exception:
        flagged_df = pd.DataFrame()

    try:
        analysis_count = int(raw_unique_person_uids)
    except Exception:
        analysis_count = int(len(combined_df)) if combined_df is not None else 0

    try:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = float((flagged_count / analysis_count * 100.0) if analysis_count and analysis_count > 0 else 0.0)
    except Exception:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = 0.0

    try:
        # If we have flagged rows, return ALL flagged rows (strict)
        if flagged_df is not None and not flagged_df.empty:
            sample_source = flagged_df
            # return exactly flagged_count rows (no hidden head(10) truncation)
            samples = _clean_sample_df(sample_source, max_rows=int(len(flagged_df)))
        else:
            # old behaviour: show a small sample of combined_df
            sample_source = combined_df
            samples = _clean_sample_df(sample_source.head(10), max_rows=10) if sample_source is not None and not sample_source.empty else []
    except Exception:
        samples = []

    # -----------------------------
    # NEW: Enrich the sample rows with EmployeeEmail and imageUrl using trend_runner helper
    # -----------------------------
    try:
        if isinstance(samples, list) and samples:
            try:
                base = (request.url_root or request.host_url).rstrip('/')
            except Exception:
                base = ''
            try:
                # build a small dataframe and call trend_runner enrichment helper
                tmp_df = pd.DataFrame(samples)
                # pass a fully-qualified image endpoint template so frontend img src works directly
                template = "/employee/{}/image"
                if base:
                    # result will be like: http://host:port/employee/<id>/image
                    template = f"{base}/employee/{{}}/image"
                enriched = _enrich_with_personnel_info(tmp_df, image_endpoint_template=template)
                # ensure Python native types and safe values via cleaning function
                enriched_clean = _clean_sample_df(enriched, max_rows=len(enriched))
                samples = enriched_clean
            except Exception:
                logging.exception("Failed to enrich /run sample with personnel info (non-fatal).")
    except Exception:
        logging.exception("Sample enrichment failed (non-fatal).")

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "aggregated_rows_total_raw": int(len(combined_df)),
        "aggregated_unique_persons": int(analysis_count),
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "flagged_rate_percent": float(flagged_rate_pct),
        "files": files,
         "sample": (samples if isinstance(samples, list) else samples),
       # "sample": (samples[:10] if isinstance(samples, list) else samples),
        "reasons_count": {},
        "risk_counts": {},
        #"flagged_persons": (samples if samples else []),
         "flagged_persons": (samples if samples else []),
        "_raw_unique_person_uids": int(raw_unique_person_uids),
        "regions_run": params.get('_regions_to_run', []),
        "city_used": city_slug
    }

    return jsonify(resp)



@app.route('/latest', methods=['GET'])
def latest_results():
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]

    start_date_iso = None
    end_date_iso = None
    try:
        m = re.search(r'(\d{8})', latest.name)
        if m:
            ymd = m.group(1)
            dt = datetime.strptime(ymd, "%Y%m%d").date()
            start_date_iso = dt.isoformat()
            end_date_iso = dt.isoformat()
    except Exception:
        start_date_iso = None
        end_date_iso = None

    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)

    df = _replace_placeholder_strings(df)

    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in df.columns), None)

    def _norm_val_for_latest(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                fv = float(s)
                if math.isfinite(fv) and fv.is_integer():
                    s = str(int(fv))
        except Exception:
            pass
        return s

    if id_col is None:
        unique_persons = int(len(df))
    else:
        ids_series = df[id_col].apply(_norm_val_for_latest) if id_col in df.columns else pd.Series([None]*len(df))
        if id_col != 'person_uid' and 'person_uid' in df.columns:
            ids_series = ids_series.fillna(df['person_uid'].astype(str).replace('nan','').replace('None',''))
        unique_persons = int(len(set([x for x in ids_series.unique() if x])))

    # build initial sample (list of dicts)
    sample = _clean_sample_df(df, max_rows=5)  # returns list




    # --- Enrich sample rows with EmployeeEmail and imageUrl (best-effort) ---
    try:
        if isinstance(sample, list) and sample:
            # compute base once
            try:
                base = (request.url_root or request.host_url).rstrip('/')
            except Exception:
                base = ''

            for s in sample:
                # ensure keys exist (consistent shape)
                s.setdefault('EmployeeEmail', None)
                s.setdefault('imageUrl', None)
                s.setdefault('HasImage', False)

                # pick a lookup token (EmployeeID / person_uid / EmployeeName / EmployeeIdentity)
                lookup_token = (
                    s.get('EmployeeID')
                    or s.get('person_uid')
                    or s.get('EmployeeName')
                    or s.get('EmployeeIdentity')
                )

                pi = {}
                if lookup_token:
                    try:
                        pi = get_personnel_info(lookup_token) or {}
                    except Exception:
                        pi = {}

                # Prefer personnel DB email if available
                if pi:
                    email = pi.get('EmailAddress') or pi.get('EmployeeEmail') or pi.get('Email') or None
                    if email:
                        s['EmployeeEmail'] = email

                    # prefer ObjectID / GUID for images
                    objid = pi.get('ObjectID') or pi.get('GUID') or None
                    if objid:
                        # provide relative image path (frontend resolves with API_BASE)
                        s['imageUrl'] = f"/employee/{objid}/image"
                        try:
                            b = get_person_image_bytes(objid)
                            s['HasImage'] = True if b else False
                        except Exception:
                            s['HasImage'] = False

                # fallback: look up email from the CSV rows read (df) if still missing
                if not s.get('EmployeeEmail') and isinstance(df, pd.DataFrame):
                    try:
                        match_mask = pd.Series(False, index=df.index)
                        if s.get('person_uid') and 'person_uid' in df.columns:
                            match_mask |= df['person_uid'].astype(str).str.strip() == str(s.get('person_uid')).strip()
                        if s.get('EmployeeID') and 'EmployeeID' in df.columns:
                            match_mask |= df['EmployeeID'].astype(str).str.strip() == str(s.get('EmployeeID')).strip()

                        if match_mask.any():
                            idx = df[match_mask].index[0]
                            for col in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                                if col in df.columns:
                                    val = df.at[idx, col]
                                    if val not in (None, '', 'nan'):
                                        s['EmployeeEmail'] = val
                                        break
                    except Exception:
                        pass

                # Ensure we at least provide a consistent image route (use EmployeeID/person_uid if no ObjectID)
                if not s.get('imageUrl'):
                    empid = s.get('EmployeeID') or s.get('person_uid')
                    if empid:
                        s['imageUrl'] = f"/employee/{empid}/image"
                        # don't try to check bytes here (avoid extra DB hit)  HasImage remains False if unknown

    except Exception:
        # if enrichment fails, continue (sample remains as-is)
        pass


    resp = {
        
        "file": latest.name,
        "rows_raw": int(len(df)),
        "rows": unique_persons,
        "sample": sample,
        "start_date": start_date_iso,
        "end_date": end_date_iso,
        "city": city_slug
    }
    return jsonify(resp)




#new endpoint


# Requires: Flask request/jsonify, pandas as pd, pathlib.Path, datetime, math, re, DEFAULT_OUTDIR (or OUTDIR)
# Place inside your app module where helpers like _safe_read_csv, _find_swipe_files, _clean_sample_df, _replace_placeholder_strings,
# get_personnel_info, get_person_image_bytes, map_door_to_zone exist. The function is defensive if some are missing.

@app.route('/record', methods=['GET'])
def record():
    from pathlib import Path
    import pandas as pd
    import math
    import re
    from datetime import datetime, date
    try:
        q = request.args.get('employee_id') or request.args.get('person_uid')
    except Exception:
        q = None
    include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'

    # pick outdir consistently
    try:
        base_out = Path(DEFAULT_OUTDIR)
    except Exception:
        try:
            base_out = Path(OUTDIR)
        except Exception:
            base_out = Path.cwd()

    # helper safe wrappers (use existing ones if present)
    def _safe_read(fp, **kwargs):
        try:
            if '_safe_read_csv' in globals():
                return _safe_read_csv(fp)
            return pd.read_csv(fp, **kwargs)
        except Exception:
            try:
                return pd.read_csv(fp, dtype=str, **{k: v for k, v in kwargs.items() if k != 'parse_dates'})
            except Exception:
                return pd.DataFrame()

    def _to_python_scalar(v):
        if pd.isna(v):
            return None
        try:
            return v.item() if hasattr(v, 'item') else v
        except Exception:
            return v

    # 1) find trend CSVs (city-specific first)
    def _slug(s):
        return re.sub(r'[^a-z0-9]+', '_', str(s or '').strip().lower()).strip('_')

    city_slug = _slug(city_param)
    trend_glob = list(base_out.glob(f"trend_{city_slug}_*.csv"))
    if not trend_glob:
        trend_glob = list(base_out.glob("trend_*.csv"))
    trend_glob = sorted(trend_glob, reverse=True)

    # if no trend files, still we should try to return raw swipes (evidence) by scanning swipes files
    df_list = []
    for fp in trend_glob:
        try:
            tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
        except Exception:
            try:
                tmp = pd.read_csv(fp, dtype=str)
                if 'Date' in tmp.columns:
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
            except Exception:
                continue
        df_list.append(tmp)
    if df_list:
        trends_df = pd.concat(df_list, ignore_index=True)
        try:
            trends_df = _replace_placeholder_strings(trends_df)
        except Exception:
            pass
    else:
        trends_df = pd.DataFrame()

    # if no query param, return a small sample of trend rows (if any)
    if q is None:
        try:
            if not trends_df.empty and '_clean_sample_df' in globals():
                cleaned = _clean_sample_df(trends_df, max_rows=10)
            elif not trends_df.empty:
                cleaned = trends_df.head(10).to_dict(orient='records')
            else:
                cleaned = []
        except Exception:
            cleaned = []
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    q_str = str(q).strip()

    # helper to normalise series values to comparable strings/numerics
    def normalize_series(s):
        if s is None:
            return pd.Series([''] * (len(trends_df) if not trends_df.empty else 0))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if math.isfinite(fv) and fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    # find matching rows in trends_df
    found_mask = pd.Series(False, index=trends_df.index) if not trends_df.empty else pd.Series(dtype=bool)
    candidates_cols = ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12', 'EmployeeName')
    for c in candidates_cols:
        if c in trends_df.columns:
            try:
                ser = normalize_series(trends_df[c])
                found_mask = found_mask | (ser == q_str)
            except Exception:
                pass

    # numeric fallback
    if (found_mask is None) or (not found_mask.any() if len(found_mask) else True):
        try:
            q_numeric = float(q_str)
            for c in ('EmployeeID', 'Int1'):
                if c in trends_df.columns:
                    try:
                        numser = pd.to_numeric(trends_df[c], errors='coerce')
                        found_mask = found_mask | (numser == q_numeric)
                    except Exception:
                        pass
        except Exception:
            pass

    matched = trends_df[found_mask].copy() if not trends_df.empty else pd.DataFrame()
    if matched.empty:
        # no aggregated trend rows found - still attempt to return raw swipe evidence by scanning swipes files
        cleaned_matched = []
    else:
        try:
            cleaned_matched = _clean_sample_df(matched, max_rows=len(matched)) if '_clean_sample_df' in globals() else matched.to_dict(orient='records')
        except Exception:
            cleaned_matched = matched.to_dict(orient='records')

    # enrich matched rows where possible
    try:
        if cleaned_matched and '_enrich_with_personnel_info' in globals():
            agg_df = pd.DataFrame(cleaned_matched)
            agg_df = _enrich_with_personnel_info(agg_df, image_endpoint_template="/employee/{}/image")
            cleaned_matched = agg_df.to_dict(orient='records')
    except Exception:
        pass

    # build list of dates to scan for swipe files (from matched rows)
    dates_to_scan = set()
    try:
        for _, r in (matched.iterrows() if not matched.empty else []):
            try:
                if 'Date' in r and pd.notna(r['Date']):
                    try:
                        d = pd.to_datetime(r['Date']).date()
                        dates_to_scan.add(d)
                    except Exception:
                        pass
                for col in ('FirstSwipe','LastSwipe'):
                    if col in r and pd.notna(r[col]):
                        try:
                            d = pd.to_datetime(r[col]).date()
                            dates_to_scan.add(d)
                        except Exception:
                            pass
            except Exception:
                continue
    except Exception:
        pass
    if not dates_to_scan:
        dates_to_scan = {None}  # indicates scan all swipes files

   
   
    # # helper to find swipe files (use your _find_swipe_files if present)
    # def _find_swipes_for_date(date_obj=None):
    #     files = []
    #     try:
    #         if '_find_swipe_files' in globals():
    #             cand = _find_swipe_files(str(base_out), date_obj=date_obj, city_slug=city_slug if city_slug else None)
    #             if cand:
    #                 return cand

def _find_swipes_for_date(date_obj=None):
    files = []
    try:
        # For Pune we want to strictly exclude shifted files by default
        include_shifted = True
        try:
            if city_slug and str(city_slug).strip().lower() == 'pune':
                include_shifted = False
        except Exception:
            include_shifted = True

        # Prefer using helper if it's available
        if '_find_swipe_files' in globals():
            try:
                cand = _find_swipe_files(str(base_out), date_obj=date_obj, city_slug=city_slug if city_slug else None, include_shifted=include_shifted)
                if cand:
                    return cand
            except Exception:
                logging.exception("_find_swipe_files helper failed; falling back to simple glob search.")

        # Fallback simple glob search
        if date_obj is None:
            # recent / any swipe-like files
            files = list(base_out.glob("swipes_*_*.csv")) + list(base_out.glob("swipes_*.csv")) + list(base_out.glob("*swipe*.csv"))
        else:
            ymd = date_obj.strftime('%Y-%m-%d')
            ymd2 = date_obj.strftime('%Y%m%d')
            files = [p for p in base_out.glob("swipes_*_*.csv") if ymd in p.name or ymd2 in p.name] + \
                    [p for p in base_out.glob("swipes_*.csv") if ymd in p.name or ymd2 in p.name]

        return sorted(files, reverse=True)
    except Exception:
        logging.exception("Error while searching for swipe files for date=%s city=%s", date_obj, city_slug)
        return []

    raw_files_set = set()
    raw_swipes_out = []
    seen_keys = set()

    def _append_row(out_row, source_name):
        key = (out_row.get('Date') or '', out_row.get('Time') or '', (out_row.get('Door') or '').strip(), (out_row.get('Direction') or '').strip(), (out_row.get('CardNumber') or out_row.get('Card') or '').strip())
        if key in seen_keys:
            return
        seen_keys.add(key)
        out_row['_source'] = source_name
        raw_swipes_out.append(out_row)

    # scan swipe files for dates
    for d in dates_to_scan:
        swipe_candidates = _find_swipes_for_date(d)
        # if none and we asked for specific date, fall back to all swipes
        if not swipe_candidates and d is not None:
            swipe_candidates = _find_swipes_for_date(None)

        for fp in swipe_candidates:
            raw_files_set.add(fp.name)
            try:
                sdf = _safe_read(fp, parse_dates=['LocaleMessageTime'])
            except Exception:
                sdf = _safe_read(fp)
            if sdf.empty:
                continue

            # normalize column-name -> original mapping
            cols_lower = {c.lower(): c for c in sdf.columns}
            tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
            emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
            name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
            card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
            door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
            dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
            note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
            person_uid_col = cols_lower.get('person_uid')

            # build mask for this person q
            mask = pd.Series(False, index=sdf.index)
            try:
                if person_uid_col and person_uid_col in sdf.columns:
                    mask = mask | (sdf[person_uid_col].astype(str).str.strip() == q_str)
            except Exception:
                pass
            try:
                if emp_col and emp_col in sdf.columns:
                    mask = mask | (sdf[emp_col].astype(str).str.strip() == q_str)
            except Exception:
                pass
            # numeric fallback for emp_col
            if not mask.any() and emp_col and emp_col in sdf.columns:
                try:
                    q_numeric = float(q_str)
                    emp_numeric = pd.to_numeric(sdf[emp_col], errors='coerce')
                    mask = mask | (emp_numeric == q_numeric)
                except Exception:
                    pass
            # name fallback
            if not mask.any() and name_col and name_col in sdf.columns:
                try:
                    mask = mask | (sdf[name_col].astype(str).str.strip().str.lower() == q_str.lower())
                except Exception:
                    pass

            if not mask.any():
                continue

            filtered = sdf[mask].copy()
            if filtered.empty:
                continue

            # ensure time column is parsed
            if tcol and tcol in filtered.columns:
                try:
                    filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                except Exception:
                    pass
            else:
                # try to infer from common candidates
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp','Date'):
                    if cand in filtered.columns:
                        try:
                            filtered['LocaleMessageTime'] = pd.to_datetime(filtered[cand], errors='coerce')
                            tcol = 'LocaleMessageTime'
                            break
                        except Exception:
                            pass

            # sort by time if available and compute gaps
            if tcol and tcol in filtered.columns:
                try:
                    filtered = filtered.sort_values(by=tcol)
                    filtered['_prev_ts'] = filtered[tcol].shift(1)
                    try:
                        filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                    except Exception:
                        filtered['_swipe_gap_seconds'] = 0.0
                    # reset gap at day boundary
                    try:
                        cur_dates = filtered[tcol].dt.date
                        prev_dates = cur_dates.shift(1)
                        day_start_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
                        filtered.loc[day_start_mask, '_swipe_gap_seconds'] = 0.0
                    except Exception:
                        pass
                except Exception:
                    filtered['_swipe_gap_seconds'] = 0.0
            else:
                filtered['_swipe_gap_seconds'] = 0.0

            # zone mapping if possible
            try:
                if door_col and door_col in filtered.columns:
                    if dir_col and dir_col in filtered.columns and 'map_door_to_zone' in globals():
                        filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                    else:
                        filtered['_zone'] = filtered.get(door_col, None)
            except Exception:
                filtered['_zone'] = None

            # convert each row to output dict
            for _, r in filtered.iterrows():
                out = {}
                out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in filtered.columns else (matched.iloc[0].get('EmployeeName') if not matched.empty else q_str)
                # EmployeeID detection
                emp_val = None
                if emp_col and emp_col in filtered.columns:
                    emp_val = _to_python_scalar(r.get(emp_col))
                else:
                    for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                        cl = cols_lower.get(cand.lower())
                        if cl and cl in filtered.columns:
                            emp_val = _to_python_scalar(r.get(cl))
                            if emp_val not in (None, '', 'nan'):
                                break
                    if emp_val in (None, '', 'nan'):
                        emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
                # sanitize
                try:
                    if emp_val is not None:
                        s = str(emp_val).strip()
                        if '.' in s:
                            try:
                                f = float(s)
                                if math.isfinite(f) and f.is_integer():
                                    s = str(int(f))
                            except Exception:
                                pass
                        emp_val = s
                except Exception:
                    pass
                out['EmployeeID'] = emp_val

                # card
                card_val = None
                if card_col and card_col in filtered.columns:
                    card_val = _to_python_scalar(r.get(card_col))
                else:
                    for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                        cl = cols_lower.get(cand.lower())
                        if cl and cl in filtered.columns:
                            card_val = _to_python_scalar(r.get(cl))
                            if card_val not in (None, '', 'nan'):
                                break
                    if card_val in (None, '', 'nan'):
                        card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                try:
                    if card_val is not None:
                        card_val = str(card_val).strip()
                except Exception:
                    pass
                out['CardNumber'] = card_val
                out['Card'] = card_val

                # timestamps -> Date, Time, LocaleMessageTime
                if tcol and tcol in filtered.columns:
                    ts = r.get(tcol)
                    try:
                        ts_py = pd.to_datetime(ts)
                        out['Date'] = ts_py.date().isoformat()
                        out['Time'] = ts_py.time().isoformat()
                        out['LocaleMessageTime'] = ts_py.isoformat()
                    except Exception:
                        txt = str(r.get(tcol))
                        out['Date'] = txt[:10]
                        out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                        out['LocaleMessageTime'] = txt
                else:
                    out['Date'] = None
                    out['Time'] = None
                    out['LocaleMessageTime'] = None

                out['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
                # format seconds -> HH:MM:SS
                try:
                    s = int(out['SwipeGapSeconds'])
                    hh = s // 3600
                    mm = (s % 3600) // 60
                    ss = s % 60
                    out['SwipeGap'] = f"{hh:02d}:{mm:02d}:{ss:02d}"
                except Exception:
                    out['SwipeGap'] = None

                out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
                out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else (_to_python_scalar(r.get('Direction')) if 'Direction' in r else None)
                out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
                try:
                    out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else (map_door_to_zone(out['Door'], out['Direction']) if 'map_door_to_zone' in globals() else None)
                except Exception:
                    out['Zone'] = None

                out['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
                out['_source_file'] = fp.name
                _append_row(out, fp.name)

    # final raw_swipe_files list
    raw_swipe_files = sorted(list(raw_files_set))

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": raw_swipe_files,
        "raw_swipes": raw_swipes_out
    }), 200






@app.route('/record/export', methods=['GET'])
def export_record_excel():
    q = request.args.get('employee_id') or request.args.get('person_uid')
    date_str = request.args.get('date')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    if not q:
        return jsonify({"error":"employee_id or person_uid is required"}), 400

    p = Path(DEFAULT_OUTDIR)
    files_to_scan = []
    if date_str:
        try:
            dd = pd.to_datetime(date_str).date()
            
            files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=dd, city_slug=city_slug, include_shifted=False if city_slug == 'pune' else True)
        except Exception:
            return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400
    else:
        files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=city_slug)
    if not files_to_scan:
        # show any available swipe-style files to help frontend debugging
        avail = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=None, include_shifted=False)

        avail_names = [f.name for f in avail] if avail else []
        logging.info(
            "export_record_excel: no files matched for date=%s city=%s; available swipe files=%s",
            date_str, city_slug, avail_names
        )
        return jsonify({
            "error": "no raw swipe files found for requested date / outputs",
            "available_swipe_files": avail_names
        }), 404

    all_rows = []
    for fp in files_to_scan:
        try:
            raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
        except Exception:
            try:
                raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

        raw_df = _replace_placeholder_strings(raw_df)
        cols_lower = {c.lower(): c for c in raw_df.columns}
        tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
        emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
        name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
        card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
        door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
        dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
        note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
        person_uid_col = cols_lower.get('person_uid')

        mask = pd.Series(False, index=raw_df.index)
        if person_uid_col and person_uid_col in raw_df.columns:
            mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
        if emp_col and emp_col in raw_df.columns:
            mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
        if not mask.any() and emp_col and emp_col in raw_df.columns:
            try:
                q_numeric = float(q)
                emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                mask = mask | (emp_numeric == q_numeric)
            except Exception:
                pass
        if not mask.any() and name_col and name_col in raw_df.columns:
            mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())

        if not mask.any():
            continue

        filtered = raw_df[mask].copy()
        if filtered.empty:
            continue

        if tcol and tcol in filtered.columns:
            try:
                filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
            except Exception:
                pass

        if tcol and tcol in filtered.columns:
            filtered = filtered.sort_values(by=tcol)
            filtered['_prev_ts'] = filtered[tcol].shift(1)
            try:
                filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
            except Exception:
                filtered['_swipe_gap_seconds'] = 0.0
        else:
            filtered['_swipe_gap_seconds'] = 0.0

        try:
            if door_col and door_col in filtered.columns:
                if dir_col and dir_col in filtered.columns:
                    filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                else:
                    filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
            else:
                filtered['_zone'] = filtered.get('PartitionName2', None)
        except Exception:
            filtered['_zone'] = None

        for _, r in filtered.iterrows():
            row = {}
            row['EmployeeName'] = _to_python_scalar(r.get(name_col)) if (name_col and name_col in filtered.columns) else None
            emp_val = None
            if emp_col and emp_col in filtered.columns:
                emp_val = _to_python_scalar(r.get(emp_col))
            else:
                for cand in ('int1','text12','employeeid','employee_identity','employeeidentity'):
                    if cand in cols_lower and cols_lower[cand] in filtered.columns:
                        emp_val = _to_python_scalar(r.get(cols_lower[cand]))
                        if emp_val:
                            break
            row['EmployeeID'] = emp_val
            row['Card'] = _to_python_scalar(r.get(card_col)) if (card_col and card_col in filtered.columns) else None

            if tcol and tcol in filtered.columns:
                ts = r.get(tcol)
                try:
                    ts_py = pd.to_datetime(ts)
                    row['Date'] = ts_py.date().isoformat()
                    row['Time'] = ts_py.time().isoformat()
                    row['LocaleMessageTime'] = ts_py.isoformat()
                except Exception:
                    txt = str(r.get(tcol))
                    row['Date'] = txt[:10]
                    row['Time'] = txt[11:19] if len(txt) >= 19 else None
                    row['LocaleMessageTime'] = txt
            else:
                row['Date'] = None
                row['Time'] = None
                row['LocaleMessageTime'] = None

            row['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
            row['SwipeGap'] = format_seconds_to_hms(row['SwipeGapSeconds'])
            row['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
            row['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else None
            row['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
            try:
                zone_val = r.get('_zone') if '_zone' in r else None
                if zone_val is None:
                    zone_val = map_door_to_zone(row['Door'], row['Direction'])
                row['Zone'] = _to_python_scalar(zone_val)
            except Exception:
                row['Zone'] = None
            row['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
            row['_source_file'] = fp.name
            all_rows.append(row)

    if not all_rows:
        return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

    df_out = pd.DataFrame(all_rows)
    details_cols = ['EmployeeName','EmployeeID','Door','Direction','Zone','Date','LocaleMessageTime','SwipeGapSeconds','PartitionName2','_source_file']
    timeline_cols = ['EmployeeName','EmployeeID','Card','Date','Time','SwipeGapSeconds','Door','Direction','Zone','Note','_source_file']

    details_df = df_out[[c for c in details_cols if c in df_out.columns]].copy()
    timeline_df = df_out[[c for c in timeline_cols if c in df_out.columns]].copy()

    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            details_df.to_excel(writer, sheet_name='Details  Evidence', index=False)
            timeline_df.to_excel(writer, sheet_name='Swipe timeline', index=False)
            writer.save()
            output.seek(0)
    except Exception as e:
        logging.exception("Failed to create Excel: %s", e)
        return jsonify({"error":"failed to create excel"}), 500

    if OPENPYXL_AVAILABLE:
        try:
            wb = load_workbook(output)
            thin = Side(border_style="thin", color="000000")
            thick = Side(border_style="medium", color="000000")
            for ws in wb.worksheets:
                header = ws[1]
                for cell in header:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                        cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
                for col in ws.columns:
                    max_len = 0
                    col_letter = col[0].column_letter
                    for cell in col:
                        try:
                            v = str(cell.value) if cell.value is not None else ""
                        except Exception:
                            v = ""
                        if len(v) > max_len:
                            max_len = len(v)
                    width = min(max(10, max_len + 2), 50)
                    ws.column_dimensions[col_letter].width = width
            out2 = io.BytesIO()
            wb.save(out2)
            out2.seek(0)
            return send_file(out2, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        except Exception:
            logging.exception("Excel styling failed, returning raw file")
            output.seek(0)
            return send_file(output, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    else:
        output.seek(0)
        return send_file(output, as_attachment=True,
                         download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                         mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)

@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        if build_monthly_training is None:
            raise RuntimeError("build_monthly_training not available")
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500


# @app.route("/employee/<pid>/image")
# def employee_image(pid):
#     """
#     Return image bytes for a person id used by frontend imageUrl template.
#     If you already have get_person_image_bytes(pid) helper, this will call it.
#     """
#     try:
#         # try to call app-level helper if present
#         import importlib
#         appmod = importlib.import_module('app')  # adjust module name if helper is in another module
#         gpib = getattr(appmod, 'get_person_image_bytes', None)
#         if gpib:
#             data = gpib(pid)  # should return bytes or None
#             if data:
#                 return send_file(BytesIO(data), mimetype='image/jpeg')
#     except Exception:
#         logging.exception("employee_image route failed")
#     # fallback: 404  frontend shows placeholder if not found
#     return ('', 404)



# chatbot helpers (kept mostly as-is)
try:
    from trend_runner import _read_past_trend_csvs, _normalize_id_val, SCENARIO_EXPLANATIONS
except Exception:
    _read_past_trend_csvs = None
    _normalize_id_val = None
    SCENARIO_EXPLANATIONS = {}

def _load_latest_trend_df(outdir: Path, city: str = "pune"):
    city_slug = _slug_city(city)
    csvs = sorted(outdir.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(outdir.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return None, None
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    df = _replace_placeholder_strings(df)
    return df, latest.name

def _find_person_rows(identifier: str, days: int = 90, outdir: Path = DEFAULT_OUTDIR):
    if _normalize_id_val:
        norm = _normalize_id_val(identifier)
    else:
        norm = str(identifier).strip()
        if '.' in norm:
            try:
                f = float(norm)
                if f.is_integer():
                    norm = str(int(f))
            except Exception:
                pass
    today = datetime.now().date()
    try:
        if _read_past_trend_csvs:
            past = _read_past_trend_csvs(str(outdir), days, today)
        else:
            files = sorted(Path(outdir).glob("trend_pune_*.csv"), reverse=True)
            dfs = []
            cutoff = today - timedelta(days=days)
            for fp in files:
                try:
                    tmp = pd.read_csv(fp, parse_dates=['Date'])
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                    tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                    dfs.append(tmp)
                except Exception:
                    try:
                        tmp = pd.read_csv(fp, dtype=str)
                        if 'Date' in tmp.columns:
                            tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                            tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                            dfs.append(tmp)
                    except Exception:
                        continue
            past = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
    except Exception:
        past = pd.DataFrame()

    if past is None or past.empty:
        return pd.DataFrame()

    past = _replace_placeholder_strings(past)
    match_mask = pd.Series(False, index=past.index)
    for col in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
        if col in past.columns:
            try:
                match_mask = match_mask | (past[col].astype(str).fillna('').str.strip() == str(norm).strip())
            except Exception:
                continue

    if not match_mask.any():
        try:
            qnum = float(norm)
            for col in ('EmployeeID','Int1'):
                if col in past.columns:
                    try:
                        numcol = pd.to_numeric(past[col], errors='coerce')
                        match_mask = match_mask | (numcol == qnum)
                    except Exception:
                        continue
        except Exception:
            pass

    if not match_mask.any() and 'EmployeeName' in past.columns:
        names = past['EmployeeName'].dropna().astype(str).unique().tolist()
        close = difflib.get_close_matches(str(identifier), names, n=5, cutoff=0.7)
        if close:
            match_mask = match_mask | past['EmployeeName'].astype(str).isin(close)

    return past[match_mask].copy()

def _explain_scenario_code(code):
    if not code:
        return None
    code = str(code).strip()
    if code in SCENARIO_EXPLANATIONS:
        try:
            fn = SCENARIO_EXPLANATIONS.get(code)
            try:
                txt = fn({})
                return txt
            except Exception:
                return code.replace("_", " ").replace(">= ", " ")
        except Exception:
            return code.replace("_", " ").replace(">= ", " ")
    return code.replace("_", " ").replace(">=", "")

def _map_score_to_label_fallback(score: float):
    try:
        s = float(score)
    except Exception:
        return (0.0, "Low")
    if s >= 0.75:
        return (s, "High")
    if s >= 0.4:
        return (s, "Medium")
    return (s, "Low")

@app.route('/chatbot/query', methods=['POST'])
def chatbot_query():
    payload = request.get_json(force=True)
    q = (payload.get('q') or '').strip()
    if not q:
        return jsonify({"error":"query text 'q' required"}), 400
    lang = payload.get('lang')
    q_l = q.lower().strip()

    if re.search(r"\bwho is (high|low) risk\b", q_l) or re.search(r"\b(high|low) risk (people|persons|people) (today)?\b", q_l):
        want = 'high' if 'high' in q_l else 'low' if 'low' in q_l else None
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer": "No trend data available.", "evidence": []})
        if 'RiskLevel' not in df.columns:
            if 'RiskScore' in df.columns:
                def _map_rs(s):
                    try:
                        if pd.isna(s):
                            return 'Low'
                    except Exception:
                        pass
                    try:
                        if 'map_score_to_label' in globals() and callable(globals().get('map_score_to_label')):
                            try:
                                return globals().get('map_score_to_label')(float(s))[1]
                            except Exception:
                                pass
                        return _map_score_to_label_fallback(float(s))[1]
                    except Exception:
                        return 'Low'
                df['RiskLevel'] = df['RiskScore'].apply(lambda s: _map_rs(s))
            else:
                df['RiskLevel'] = df.get('RiskLevel', 'Low')
        if want == 'high':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'high']
        elif want == 'low':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'low']
        else:
            sel = df
        names = sel['EmployeeName'].dropna().astype(str).unique().tolist()
        if not names:
            ans = f"No {want} risk persons found in the latest data." if want else "No persons found."
            return jsonify({"answer": ans, "evidence": []})
        else:
            ans = f"{want.capitalize()} risk persons today: " + ", ".join(names[:40])
            sample = _clean_sample_df(sel.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bexplain\s+([A-Za-z0-9_\-]+)\b.*", q_l)
    if m:
        code = m.group(1).strip()
        explanation = _explain_scenario_code(code)
        ans = f"Explanation for '{code}': {explanation}"
        return jsonify({"answer": ans, "evidence": []})

    if 'trend details' in q_l or 'top reasons' in q_l or 'trend details for today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'Reasons' in df.columns:
            reasons = {}
            for v in df['Reasons'].dropna().astype(str):
                for part in re.split(r'[;,\|]', v):
                    key = part.strip()
                    if key and not _is_placeholder_str(key):
                        reasons[key] = reasons.get(key, 0) + 1
            top = sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:10]
            if not top:
                return jsonify({"answer":"No reason counts available today.","evidence":[]})
            ans = "Top reasons today: " + ", ".join([f"{k} ({c})" for k,c in top])
            sample = []
            try:
                top_reasons = [k for k,_ in top]
                mask = df['Reasons'].astype(str).apply(lambda s: any(tr in s for tr in top_reasons))
                sample_df = df[mask].head(10)
                sample = _clean_sample_df(sample_df, max_rows=10)
            except Exception:
                sample = []
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bshow (?:me )?([A-Za-z0-9\-\:\s]+?) (?:for )?(?:last )?(\d+)\s*days\b", q_l)
    if not m:
        m = re.match(r".*\b(show|display)\s+(?:me\s+)?([A-Za-z0-9\-\:\s]+?)\s+last\s+(\d+)\s*days\b", q_l)
    if m:
        if len(m.groups()) == 2:
            identifier, days = m.group(1).strip(), int(m.group(2))
        else:
            identifier = m.group(1).strip()
            days = int(m.group(2))
        rows = _find_person_rows(identifier, days=days, outdir=DEFAULT_OUTDIR)
        if rows is None or rows.empty:
            return jsonify({"answer": f"No records found for '{identifier}' in last {days} days.", "evidence": []})
        flagged = rows[rows.get('IsFlagged', False) == True] if 'IsFlagged' in rows.columns else pd.DataFrame()
        flagged_count = int(len(flagged))
        total_days = int(len(rows))
        latest_row = rows.sort_values('Date', ascending=False).iloc[0].to_dict()
        name = latest_row.get('EmployeeName') or latest_row.get('person_uid') or latest_row.get('EmployeeID')
        ans = f"Found {total_days} day(s) for {name} in the last {days} days. Flagged days: {flagged_count}."
        sample = _clean_sample_df(rows.sort_values('Date', ascending=False).head(10), max_rows=10)
        return jsonify({"answer": ans, "evidence": sample})

    if 'present today' in q_l or 'who is present today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'PresentToday' in df.columns:
            present = df[df['PresentToday'] == True]
            names = present['EmployeeName'].dropna().unique().tolist()
            ans = f"Present today: {', '.join(names[:40]) if names else 'None'}"
            sample = _clean_sample_df(present.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})
        else:
            return jsonify({"answer":"PresentToday field not available in latest trends.","evidence":[]})

    hint = "I can answer: 'Who is high risk today', 'Who is low risk today', 'Show me <EmployeeID|Name> last 90 days', 'Explain <scenario_code>', 'Trend details for today  top reasons'."
    return jsonify({"answer": f"I can help with trend & risk questions. I recognized: {q}. Try: {hint}", "evidence":[]})

@app.route('/employee/<empid>/image', methods=['GET'])
def serve_employee_image(empid):
    if empid is None:
        return jsonify({"error": "employee id required"}), 400
    try:
        img_bytes = get_person_image_bytes(empid)
        # If no image found, return a small inline SVG placeholder so <img> can still render
        if not img_bytes:
            svg = (
                '<svg xmlns="http://www.w3.org/2000/svg" width="160" height="160">'
                '<rect fill="#eef2f7" width="100%" height="100%"/>'
                '<text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle" fill="#64748b" font-size="18">'
                'No image</text></svg>'
            )
            bio = io.BytesIO(svg.encode('utf-8'))
            bio.seek(0)
            resp = send_file(bio, mimetype='image/svg+xml')
            resp.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            return resp

        # detect content type heuristically (jpeg/png/bmp)
        header = img_bytes[:8] if isinstance(img_bytes, (bytes, bytearray)) else bytes(img_bytes)[:8]
        content_type = 'application/octet-stream'
        try:
            if len(header) >= 2 and header[0] == 0xFF and header[1] == 0xD8:
                content_type = 'image/jpeg'
            elif header.startswith(b'\x89PNG\r\n\x1a\n'):
                content_type = 'image/png'
            elif header.startswith(b'BM'):
                content_type = 'image/bmp'
            else:
                content_type = 'application/octet-stream'
        except Exception:
            content_type = 'application/octet-stream'

        bio = io.BytesIO(img_bytes if isinstance(img_bytes, (bytes, bytearray)) else bytes(img_bytes))
        bio.seek(0)
        resp = send_file(bio, mimetype=content_type)
        resp.headers['Cache-Control'] = 'private, max-age=300'
        return resp
    except Exception:
        logging.exception("Error serving image for employee %s", empid)
        return jsonify({"error": "failed to serve image"}), 500

# run
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)



