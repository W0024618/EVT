Still we got same error so fix this eerror carefully and share me fully updated file carefully.
check each file line by line and fix this issue carefully

INFO:     Will watch for changes in these directories: ['C:\\Users\\W0024618\\Desktop\\global-page\\backend\\attendance-analytics']
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [10676] using WatchFiles
INFO:     Started server process [26540]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     127.0.0.1:51346 - "GET /ccure/compare?mode=full&limit_list=200&export=false HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\uvicorn\protocols\http\httptools_impl.py", line 409, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        self.scope, self.receive, self.send
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\uvicorn\middleware\proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\fastapi\applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\applications.py", line 113, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\middleware\errors.py", line 186, in __call__
    raise exc
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\middleware\errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\middleware\exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\routing.py", line 78, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\routing.py", line 75, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\fastapi\routing.py", line 302, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
    )
    ^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\fastapi\routing.py", line 215, in run_endpoint_function
    return await run_in_threadpool(dependant.call, **values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\concurrency.py", line 38, in run_in_threadpool
    return await anyio.to_thread.run_sync(func)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\anyio\to_thread.py", line 56, in run_sync
    return await get_async_backend().run_sync_in_worker_thread(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\anyio\_backends\_asyncio.py", line 2476, in run_sync_in_worker_thread
    return await future
           ^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\anyio\_backends\_asyncio.py", line 967, in run
    result = context.run(func, *args)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py", line 35, in ccure_compare     
    return JSONResponse(res)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\responses.py", line 190, in __init__
    super().__init__(content, status_code, headers, media_type, background)
    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\responses.py", line 47, in __init__
    self.body = self.render(content)
                ~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\starlette\responses.py", line 193, in render
    return json.dumps(
           ~~~~~~~~~~^
        content,
        ^^^^^^^^
    ...<3 lines>...
        separators=(",", ":"),
        ^^^^^^^^^^^^^^^^^^^^^^
    ).encode("utf-8")
    ^
  File "C:\Program Files\Python313\Lib\json\__init__.py", line 238, in dumps
    **kw).encode(obj)
          ~~~~~~^^^^^
  File "C:\Program Files\Python313\Lib\json\encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Program Files\Python313\Lib\json\encoder.py", line 261, in iterencode
    return _iterencode(o, 0)
ValueError: Out of range float values are not JSON compliant: nan
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [26540]
INFO:     Stopping reloader process [10676]
(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> 




C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\compare_service.py

# compare_service.py
import pandas as pd
import numpy as np
from datetime import datetime, date, timezone
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary
import re
from dateutil import parser as dateutil_parser
import traceback
import logging

logger = logging.getLogger("compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# --- Helpers -----------------------------------------------------------------
def _to_native(value):
    if value is None:
        return None
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    if isinstance(value, (np.integer,)):
        return int(value)
    if isinstance(value, (np.floating,)):
        return float(value)
    if isinstance(value, (np.bool_, bool)):
        return bool(value)
    try:
        import datetime as _dt
        if isinstance(value, _dt.datetime):
            try:
                if value.tzinfo is not None:
                    utc = value.astimezone(timezone.utc)
                    return utc.replace(tzinfo=None).isoformat() + "Z"
                else:
                    return value.isoformat()
            except Exception:
                return str(value)
        if hasattr(value, 'isoformat'):
            try:
                return value.isoformat()
            except Exception:
                return str(value)
    except Exception:
        pass
    return value

def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

# timestamp parsing helpers (unchanged)
def _parse_timestamp_from_value(val):
    if val is None:
        return None
    import datetime as _dt
    if isinstance(val, _dt.datetime):
        dt = val
        try:
            if dt.tzinfo is not None:
                return dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            return dt
    try:
        import numpy as _np
        if isinstance(val, (int, float, _np.integer, _np.floating)):
            v = int(val)
            if v > 1e12:
                return _dt.fromtimestamp(v / 1000.0, tz=timezone.utc).replace(tzinfo=None)
            if v > 1e9:
                return _dt.fromtimestamp(v, tz=timezone.utc).replace(tzinfo=None)
    except Exception:
        pass
    if isinstance(val, str):
        s = val.strip()
        if s == "":
            return None
        try:
            dt = dateutil_parser.parse(s)
            if dt.tzinfo is not None:
                dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            fmts = ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M:%S.%f",
                    "%d/%m/%Y %H:%M:%S", "%d-%m-%Y %H:%M:%S",
                    "%Y-%m-%dT%H:%M:%S")
            for fmt in fmts:
                try:
                    return _dt.strptime(s, fmt)
                except Exception:
                    pass
    return None

def _extract_timestamp_from_detail(detail):
    fields = [
        "LocaleMessageDateTime", "LocalMessageDateTime", "LocaleMessageTime", "LocalMessageTime",
        "LocaleMessageDate", "Timestamp", "timestamp", "Time", "LocaleTime", "LocalTime",
        "time", "date", "LocaleMessageDateTimeUtc", "LocalMessageDateTimeUtc",
        "Swipe_Time", "SwipeTime", "SwipeTimeLocal", "SwipeTimestamp", "SwipeDateTime"
    ]
    if isinstance(detail, dict):
        for k in fields:
            if k in detail:
                dt = _parse_timestamp_from_value(detail.get(k))
                if dt is not None:
                    return dt
        for v in detail.values():
            dt = _parse_timestamp_from_value(v)
            if dt is not None:
                return dt
    else:
        return _parse_timestamp_from_value(detail)
    return None



# --- Main functions ----------------------------------------------------------

def ingest_live_details_list(details_list):
    """Persist details_list into LiveSwipe. returns counts."""
    from db import SessionLocal as _SessionLocal
    inserted = 0
    skipped = 0
    with _SessionLocal() as db:
        for d in details_list:
            try:
                ts_parsed = _extract_timestamp_from_detail(d)
            except Exception:
                ts_parsed = None
            if ts_parsed is None:
                # skip rows without parseable timestamp
                skipped += 1
                continue

            # robust extraction of employee id and card fields (many alias names)
            emp = None
            for k in ("EmployeeID", "employee_id", "employeeId", "Employee Id", "EmpID", "Emp Id"):
                if isinstance(d, dict) and k in d:
                    emp = d.get(k)
                    break
            emp = _normalize_employee_key(emp)

            card = None
            for k in ("CardNumber", "card_number", "Card", "Card No", "CardNo", "Badge", "BadgeNo", "badge_number", "IPassID", "iPass ID"):
                if isinstance(d, dict) and k in d:
                    card = d.get(k)
                    break
            card = _normalize_card_like(card)

            full_name = None
            for k in ("ObjectName1", "FullName", "full_name", "EmpName", "Name"):
                if isinstance(d, dict) and k in d:
                    full_name = d.get(k)
                    break

            partition = None
            for k in ("PartitionName2", "PartitionName1", "Partition", "PartitionName", "Region"):
                if isinstance(d, dict) and k in d:
                    partition = d.get(k)
                    break

            floor = d.get("Floor") if isinstance(d, dict) else None
            door = None
            for k in ("Door", "DoorName", "door"):
                if isinstance(d, dict) and k in d:
                    door = d.get(k)
                    break

            region = d.get("__region") if isinstance(d, dict) and "__region" in d else d.get("Region") if isinstance(d, dict) else None

            try:
                rec = LiveSwipe(
                    timestamp=ts_parsed,
                    employee_id=emp,
                    card_number=card,
                    full_name=full_name,
                    partition=partition,
                    floor=floor,
                    door=door,
                    region=region,
                    raw=d
                )
                db.add(rec)
                inserted += 1
            except Exception:
                # skip insertion errors but continue
                db.rollback()
                skipped += 1
                continue
        db.commit()
    print(f"[ingest_live_details_list] inserted={inserted} skipped={skipped}")
    return {"inserted": inserted, "skipped_invalid_timestamp": skipped}


def compute_daily_attendance(target_date: date):
    """Build AttendanceSummary rows for target_date (upserts)."""
    with SessionLocal() as db:
        start = datetime.combine(target_date, datetime.min.time())
        end = datetime.combine(target_date, datetime.max.time())
        swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
        if not swipes:
            print(f"[compute_daily_attendance] no swipes for {target_date}")
            return []

        rows = []
        for s in swipes:
            rows.append({
                "id": s.id,
                "timestamp": s.timestamp,
                "employee_id": _normalize_employee_key(s.employee_id),
                "card_number": _normalize_card_like(s.card_number),
                "full_name": s.full_name,
                "partition": s.partition,
                "floor": s.floor,
                "door": s.door
            })
        df = pd.DataFrame(rows)
        if df.empty:
            print(f"[compute_daily_attendance] dataframe empty after rows -> {target_date}")
            return []

        # create grouping key: prefer employee_id, otherwise card_number
        df['key'] = df['employee_id'].fillna(df['card_number'])
        df = df[df['key'].notna()]
        if df.empty:
            print("[compute_daily_attendance] no usable keys after filling employee_id/card")
            return []

        grouped = df.groupby('key', dropna=False).agg(
            presence_count=('id', 'count'),
            first_seen=('timestamp', 'min'),
            last_seen=('timestamp', 'max'),
            full_name=('full_name', 'first'),
            partition=('partition', 'first'),
            card_number=('card_number', 'first')
        ).reset_index().rename(columns={'key': 'employee_id'})

        # upsert AttendanceSummary rows (merge)
        for _, row in grouped.iterrows():
            try:
                derived_obj = {
                    "partition": (row.get('partition') or None),
                    "full_name": (row.get('full_name') or None),
                    "card_number": (row.get('card_number') or None)
                }
                rec = AttendanceSummary(
                    employee_id=str(row['employee_id']) if pd.notna(row['employee_id']) else None,
                    date=target_date,
                    presence_count=int(row['presence_count']),
                    first_seen=row['first_seen'],
                    last_seen=row['last_seen'],
                    derived=derived_obj
                )
                db.merge(rec)
            except Exception as e:
                print("[compute_daily_attendance] upsert error:", e)
                continue
        db.commit()
        print(f"[compute_daily_attendance] built {len(grouped)} attendance keys for {target_date}")
        return grouped.to_dict(orient='records')


def compare_with_active(target_date: date):
    """Compare AttendanceSummary for date with ActiveEmployee & ActiveContractor and return json-safe dict."""
    # NOTE: we intentionally do NOT import get_global_stats_or_none from ccure_client;
    # a local helper wrapper below will call ccure_client.get_global_stats() safely.
    with SessionLocal() as db:
        att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == target_date).all()
        if not att_rows:
            att_df = pd.DataFrame(columns=["employee_id", "presence_count", "first_seen", "last_seen", "card_number", "partition", "full_name"])
        else:
            att_df = pd.DataFrame([{
                "employee_id": _normalize_employee_key(a.employee_id),
                "presence_count": a.presence_count,
                "first_seen": a.first_seen,
                "last_seen": a.last_seen,
                "card_number": _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None),
                "partition": (a.derived.get('partition') if (a.derived and isinstance(a.derived, dict)) else None),
                "full_name": (a.derived.get('full_name') if (a.derived and isinstance(a.derived, dict)) else None)
            } for a in att_rows])

        act_rows = db.query(ActiveEmployee).all()
        contractor_rows = db.query(ActiveContractor).all()

        # Build maps & active list
        act_list = []
        card_to_emp = {}
        name_to_emp = {}

        # Employees
        for e in act_rows:
            emp_id_norm = _normalize_employee_key(e.employee_id)
            # extract card-like from raw if present
            card_from_raw = None
            try:
                rr = e.raw_row or {}
                if isinstance(rr, dict):
                    ck_list = [
                        "CardNumber","card_number","Card","Card No","CardNo","IPassID","IpassID","iPass ID","IPASSID",
                        "Badge Number","BadgeNo","Badge"
                    ]
                    for ck in ck_list:
                        v = rr.get(ck)
                        if v:
                            ckey = _normalize_card_like(v)
                            if ckey:
                                card_from_raw = ckey
                                break
                    # fallback: scan all values for numeric candidate
                    if not card_from_raw:
                        for v in rr.values():
                            try:
                                tmp = _normalize_card_like(v)
                                if tmp and 3 <= len(tmp) <= 12:
                                    card_from_raw = tmp
                                    break
                            except Exception:
                                pass
            except Exception:
                card_from_raw = None

            act_list.append({
                "employee_id": emp_id_norm,
                "full_name": e.full_name,
                "location_city": e.location_city,
                "status": e.current_status,
                "card_number": card_from_raw
            })
            if emp_id_norm:
                card_to_emp[emp_id_norm] = emp_id_norm
            if card_from_raw:
                card_to_emp[card_from_raw] = emp_id_norm
            n = _normalize_name(e.full_name)
            if n:
                name_to_emp[n] = emp_id_norm

        # Contractors
        for c in contractor_rows:
            worker_id = _normalize_employee_key(c.worker_system_id)
            ipass = _normalize_employee_key(c.ipass_id)
            w_ipass = ("W" + ipass) if ipass and not str(ipass).startswith("W") else ipass
            primary_id = worker_id or ipass or None
            act_list.append({
                "employee_id": primary_id,
                "full_name": c.full_name,
                "location_city": c.location,
                "status": c.status,
                "card_number": None
            })
            if primary_id:
                card_to_emp[primary_id] = primary_id
            if ipass:
                card_to_emp[ipass] = primary_id
            if w_ipass:
                card_to_emp[w_ipass] = primary_id
            try:
                rr = c.raw_row or {}
                if isinstance(rr, dict):
                    for ck in ("Worker System Id","Worker System ID","iPass ID","IPASSID","CardNumber","card_number"):
                        if ck in rr and rr.get(ck):
                            key = _normalize_card_like(rr.get(ck))
                            if key:
                                card_to_emp[key] = primary_id
            except Exception:
                pass
            n = _normalize_name(c.full_name)
            if n:
                name_to_emp[n] = primary_id

        act_df = pd.DataFrame(act_list)

        # If no active rows, return attendance-only view
        if act_df.empty:
            if att_df.empty:
                return {"by_location": [], "merged": [], "ccure": get_global_stats_or_none()}
            att_df['partition'] = att_df.get('partition').fillna('Unknown')
            att_df['presence_count'] = att_df['presence_count'].fillna(0)
            att_df['present_today'] = att_df['presence_count'].apply(lambda x: bool(x and x != 0))
            loc_group = att_df.groupby('partition', dropna=False).agg(
                total_n=('employee_id', 'count'),
                present_n=('present_today', 'sum')
            ).reset_index().rename(columns={'partition':'location_city'})
            loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
            by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]
            merged_list = []
            for r in att_df.to_dict(orient='records'):
                merged_list.append({
                    "employee_id": _to_native(r.get('employee_id')),
                    "presence_count": _to_native(r.get('presence_count')),
                    "first_seen": _to_native(r.get('first_seen')),
                    "last_seen": _to_native(r.get('last_seen')),
                    "full_name": _to_native(r.get('full_name')),
                    "location_city": _to_native(r.get('partition')),
                    "present_today": _to_native(r.get('present_today'))
                })
            return {"by_location": by_location, "merged": merged_list, "ccure": get_global_stats_or_none()}

        # normalize columns
        act_df['employee_id'] = act_df['employee_id'].astype(object).apply(_normalize_employee_key)
        att_df['employee_id'] = att_df['employee_id'].astype(object).apply(_normalize_employee_key)
        act_df['card_number'] = act_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in act_df.columns else pd.Series([pd.NA]*len(act_df))
        att_df['card_number'] = att_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in att_df.columns else pd.Series([pd.NA]*len(att_df))

        # ensure card_to_emp includes act_df card_numbers
        for r in act_df.to_dict(orient='records'):
            c = r.get('card_number')
            eid = r.get('employee_id')
            if c and eid:
                card_to_emp[c] = eid
            if eid:
                # also map numeric-only forms of eid
                n = re.sub(r'\D','', str(eid))
                if n:
                    card_to_emp[n.lstrip('0') or n] = eid

        # mapping function tries multiple strategies
        emp_set = set([x for x in act_df['employee_id'].dropna().astype(str)])

        def numeric_variants(s):
            s = str(s)
            clean = re.sub(r'\D','', s)
            variants = set()
            if clean:
                variants.add(clean)
                variants.add(clean.lstrip('0') or clean)
                if not s.startswith('W'):
                    variants.add('W' + clean)
            return list(variants)

        def remap_att_key(row):
            primary = row.get('employee_id') or None
            card = row.get('card_number') or None

            primary_norm = _normalize_employee_key(primary)
            card_norm = _normalize_card_like(card)

            # 1) exact employee id exists in active list
            if primary_norm and primary_norm in emp_set:
                return primary_norm

            # 2) numeric-variants of primary may map to card_to_emp
            if primary_norm:
                for v in numeric_variants(primary_norm):
                    if v in card_to_emp:
                        return card_to_emp[v]
                if primary_norm in card_to_emp:
                    return card_to_emp[primary_norm]

            # 3) direct card mapping
            if card_norm:
                if card_norm in card_to_emp:
                    return card_to_emp[card_norm]
                if (card_norm.lstrip('0') or card_norm) in card_to_emp:
                    return card_to_emp[card_norm.lstrip('0') or card_norm]
                if ('W' + card_norm) in card_to_emp:
                    return card_to_emp['W' + card_norm]

            # 4) name matching fallback
            fname = _normalize_name(row.get('full_name') or row.get('full_name_att') or None)
            if fname and fname in name_to_emp:
                return name_to_emp[fname]

            # 5) last resort - return primary_norm (maybe non-mapped) so it still shows up
            return primary_norm or card_norm or None

        att_df['mapped_employee_id'] = att_df.apply(remap_att_key, axis=1)

        # drop original employee_id column to avoid duplicate label conflict
        att_merge_df = att_df.drop(columns=['employee_id'], errors='ignore').copy()

        # merge left: act_df left_on employee_id, right_on mapped_employee_id
        merged = pd.merge(
            act_df,
            att_merge_df,
            left_on='employee_id',
            right_on='mapped_employee_id',
            how='left',
            suffixes=('', '_att')
        )

        # fill and finalize
        merged['presence_count'] = merged.get('presence_count', pd.Series([0]*len(merged))).fillna(0)
        # ensure ints when possible
        def safe_int(v):
            try:
                if pd.isna(v):
                    return 0
                iv = int(float(v))
                return iv
            except Exception:
                return v
        merged['presence_count'] = merged['presence_count'].apply(safe_int)
        merged['present_today'] = merged['presence_count'].apply(lambda x: bool(x and x != 0))
        merged['location_city'] = merged.get('location_city').fillna('Unknown')

        # by_location
        loc_group = merged.groupby('location_city', dropna=False).agg(
            total_n=('employee_id', 'count'),
            present_n=('present_today', 'sum')
        ).reset_index()
        loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
        by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]

        merged_list = []
        for r in merged.to_dict(orient='records'):
            clean = {k:_to_native(v) for k,v in r.items()}
            # unify keys for clarity in API response
            clean['mapped_employee_id'] = clean.get('mapped_employee_id')
            clean['card_number_att'] = clean.get('card_number') or clean.get('card_number_att') or None
            # include status if present
            if 'status' not in clean:
                clean['status'] = None
            # ensure employee_id key exists
            if 'employee_id' not in clean:
                clean['employee_id'] = None
            merged_list.append(clean)

        # CCURE stats fetch (best-effort)
        ccure_stats = get_global_stats_or_none()

        # compare counts summary between CCure and Active sheets
        try:
            ccure_summary = ccure_stats or {}
            cc_total_profiles = ccure_summary.get('TotalProfiles')
            cc_active_profiles = ccure_summary.get('ActiveProfiles')
            cc_active_emps = ccure_summary.get('ActiveEmployees')
            cc_active_contractors = ccure_summary.get('ActiveContractors')
        except Exception:
            cc_total_profiles = cc_active_profiles = cc_active_emps = cc_active_contractors = None

        # local sheet counts
        active_emp_count = len(act_rows)
        active_contract_count = len(contractor_rows)

        diff = {
            "active_sheet_employee_count": active_emp_count,
            "active_sheet_contractor_count": active_contract_count,
            "ccure_active_employees": cc_active_emps,
            "ccure_active_contractors": cc_active_contractors,
            "delta_employees": (cc_active_emps - active_emp_count) if (isinstance(cc_active_emps, int) and isinstance(active_emp_count, int)) else None,
            "delta_contractors": (cc_active_contractors - active_contract_count) if (isinstance(cc_active_contractors, int) and isinstance(active_contract_count, int)) else None
        }

        result = {
            "by_location": by_location,
            "merged": merged_list,
            "ccure": ccure_stats,
            "count_comparison": diff
        }
        return result

# Helper wrapper
def get_global_stats_or_none():
    try:
        from ccure_client import get_global_stats
        return get_global_stats()
    except Exception:
        logger.debug("ccure_client.get_global_stats not available", exc_info=True)
        return None









# app.py
from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Query
from fastapi.responses import JSONResponse, FileResponse
import shutil, uuid, json
from settings import UPLOAD_DIR, OUTPUT_DIR
from pathlib import Path
import logging

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from ccure_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("ccure_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")
    # run compare (the function itself is defensive)
    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    # Ensure result is a dict for JSONResponse
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)



@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = Path(OUTPUT_DIR) / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(
            str(full),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            filename=safe_name
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")


@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = Path(UPLOAD_DIR) / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_employee_excel
    except Exception as e:
        logger.exception("ingest_excel import failed")
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_employee_excel(dest)
    return {"status":"ok", "path": str(dest)}

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = Path(UPLOAD_DIR) / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_contractor_excel
    except Exception as e:
        logger.exception("ingest_excel import failed")
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_contractor_excel(dest)
    return {"status":"ok", "path": str(dest)}

# Keep other endpoints unchanged (ingest/fetch-all, reports/daily)...
# (If you want, I can paste the rest verbatim â€” I left them as before to minimize changes.)


@app.post("/ingest/live-details")
async def ingest_live(request: Request):
    """
    Accepts:
      - Raw JSON array in body (preferred)
      - JSON object with {"details": [...]} in body
      - multipart/form-data where a form field 'details' contains a JSON string
    """
    details = None
    # Try direct JSON
    try:
        body = await request.json()
        if isinstance(body, dict) and 'details' in body:
            details = body['details']
        else:
            details = body
    except Exception:
        # not JSON, try form
        try:
            form = await request.form()
            if 'details' in form:
                raw = form['details']
                if isinstance(raw, str):
                    details = json.loads(raw)
                else:
                    try:
                        details = json.loads((await raw.read()).decode('utf-8'))
                    except Exception:
                        details = list(form.getlist('details'))
            else:
                # attempt first field
                first = None
                for v in form.values():
                    first = v
                    break
                if isinstance(first, str):
                    details = json.loads(first)
                else:
                    raise HTTPException(status_code=400, detail="No JSON payload found")
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Could not parse request body as JSON or form: {e}")

    if not isinstance(details, (list, tuple)):
        raise HTTPException(status_code=400, detail="Expected top-level array (JSON list) of detail objects")

    try:
        from compare_service import ingest_live_details_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    try:
        res = ingest_live_details_list(details)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to ingest details: {e}")

    if isinstance(res, dict):
        return {"status": "ok", **res}
    return {"status": "ok", "inserted": len(details)}


@app.get("/ingest/fetch-all")
def fetch_all_and_ingest():
    try:
        import region_clients
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"region_clients unavailable: {e}")

    details = region_clients.fetch_all_details()
    if not isinstance(details, list):
        raise HTTPException(status_code=500, detail="Unexpected data from region_clients.fetch_all_details")

    try:
        from compare_service import ingest_live_details_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    res = ingest_live_details_list(details)
    if isinstance(res, dict):
        return {"status":"ok", **res}
    return {"status":"ok", "inserted": len(details)}


@app.get("/reports/daily/{yyyymmdd}")
def daily_report(yyyymmdd: str):
    import datetime
    try:
        dt = datetime.datetime.strptime(yyyymmdd, "%Y%m%d").date()
    except Exception:
        raise HTTPException(status_code=400, detail="Date must be in YYYYMMDD format")

    try:
        from compare_service import compute_daily_attendance, compare_with_active
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    # compute attendance summary (will build attendance_summary rows)
    compute_daily_attendance(dt)

    # compare vs active and CCURE
    summary = compare_with_active(dt)
    return JSONResponse(summary)


# additional endpoints to fetch CCure stats directly (optional)
@app.get("/ccure/stats")
def ccure_stats():
    try:
        import ccure_client
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ccure_client import failed: {e}")
    try:
        stats = ccure_client.get_global_stats()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ccure_client.get_global_stats failed: {e}")
    return stats



# ccure_client.py
"""
Lightweight CCURE client wrappers used by compare service.
This file is defensive: missing 'requests' or network failures return None instead of raising.
"""

import math
import logging
from requests.exceptions import RequestException

logger = logging.getLogger("ccure_client")
logger.setLevel(logging.INFO)
if not logger.handlers:
    import sys
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Base URL for CCURE API - adjust if necessary
BASE = "http://10.199.22.57:5001"
DEFAULT_TIMEOUT = 10

HEADERS = {
    "Accept": "application/json"
}

# Defensive import of requests
try:
    import requests
except Exception:
    requests = None
    logger.warning("requests module not available; ccure_client will return None for HTTP calls")

def _safe_get(path, params=None, timeout=DEFAULT_TIMEOUT):
    """
    Safe GET wrapper. Returns parsed JSON on success or None on failure.
    path may include leading slash or not; we join safely.
    """
    if requests is None:
        logger.debug("_safe_get: requests not available")
        return None
    # ensure path begins with '/'
    if not path.startswith("/"):
        path = "/" + path
    url = BASE.rstrip("/") + path
    try:
        r = requests.get(url, params=params, headers=HEADERS, timeout=timeout)
        r.raise_for_status()
        return r.json()
    except RequestException as e:
        logger.warning(f"[ccure_client] request failed {url} params={params} -> {e}")
        return None
    except ValueError:
        logger.warning(f"[ccure_client] response JSON decode error for {url}")
        return None

def fetch_all_employees_full():
    return _safe_get("/api/employees")

def fetch_stats_page(detail, page=1, limit=500):
    params = {"details": detail, "page": page, "limit": limit}
    return _safe_get("/api/stats", params=params)

def fetch_all_stats(detail, limit=1000):
    first = fetch_stats_page(detail, page=1, limit=limit)
    if not first:
        return None
    data = first.get("data") or []
    total = int(first.get("total") or len(data) or 0)
    if total <= len(data):
        return data
    pages = int(math.ceil(total / float(limit)))
    for p in range(2, pages + 1):
        page_res = fetch_stats_page(detail, page=p, limit=limit)
        if not page_res:
            break
        data.extend(page_res.get("data") or [])
    return data

def get_global_stats():
    """
    Best-effort summary using /api/employees or /api/stats.
    Returns dict or None.
    """
    full = fetch_all_employees_full()
    if isinstance(full, list):
        try:
            total = len(full)
            active_profiles = sum(1 for r in full if (r.get("Employee_Status") or "").lower() == "active")
            active_emps = sum(1 for r in full if (r.get("PersonnelType") or "").lower().startswith("employee") and (r.get("Employee_Status") or "").lower() == "active")
            active_contractors = sum(1 for r in full if (r.get("PersonnelType") or "").lower().startswith("contractor") and (r.get("Employee_Status") or "").lower() == "active")
            terminated = sum(1 for r in full if (r.get("Employee_Status") or "").lower() in ("deactive", "deactivated", "inactive", "terminated"))
            return {
                "TotalProfiles": total,
                "ActiveProfiles": active_profiles,
                "ActiveEmployees": active_emps,
                "ActiveContractors": active_contractors,
                "TerminatedProfiles": terminated
            }
        except Exception:
            logger.exception("Error calculating global stats from full dump")
            return None

    details = ["ActiveProfiles", "ActiveEmployees", "ActiveContractors",
               "TerminatedProfiles", "TerminatedEmployees", "TerminatedContractors"]
    out = {}
    for d in details:
        resp = fetch_stats_page(d, page=1, limit=1)
        if isinstance(resp, dict) and 'total' in resp:
            out[d] = resp['total']
    if out:
        return {
            "TotalProfiles": out.get("TotalProfiles"),
            "ActiveProfiles": out.get("ActiveProfiles"),
            "ActiveEmployees": out.get("ActiveEmployees"),
            "ActiveContractors": out.get("ActiveContractors"),
            "TerminatedProfiles": out.get("TerminatedProfiles"),
            "TerminatedEmployees": out.get("TerminatedEmployees"),
            "TerminatedContractors": out.get("TerminatedContractors"),
        }
    return None





# compare_service.py
import pandas as pd
import numpy as np
from datetime import datetime, date, timezone
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary
import re
from dateutil import parser as dateutil_parser
import traceback
import logging

logger = logging.getLogger("compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# --- Helpers -----------------------------------------------------------------
def _to_native(value):
    if value is None:
        return None
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    if isinstance(value, (np.integer,)):
        return int(value)
    if isinstance(value, (np.floating,)):
        return float(value)
    if isinstance(value, (np.bool_, bool)):
        return bool(value)
    try:
        import datetime as _dt
        if isinstance(value, _dt.datetime):
            try:
                if value.tzinfo is not None:
                    utc = value.astimezone(timezone.utc)
                    return utc.replace(tzinfo=None).isoformat() + "Z"
                else:
                    return value.isoformat()
            except Exception:
                return str(value)
        if hasattr(value, 'isoformat'):
            try:
                return value.isoformat()
            except Exception:
                return str(value)
    except Exception:
        pass
    return value

def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

# timestamp parsing helpers (unchanged)
def _parse_timestamp_from_value(val):
    if val is None:
        return None
    import datetime as _dt
    if isinstance(val, _dt.datetime):
        dt = val
        try:
            if dt.tzinfo is not None:
                return dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            return dt
    try:
        import numpy as _np
        if isinstance(val, (int, float, _np.integer, _np.floating)):
            v = int(val)
            if v > 1e12:
                return _dt.fromtimestamp(v / 1000.0, tz=timezone.utc).replace(tzinfo=None)
            if v > 1e9:
                return _dt.fromtimestamp(v, tz=timezone.utc).replace(tzinfo=None)
    except Exception:
        pass
    if isinstance(val, str):
        s = val.strip()
        if s == "":
            return None
        try:
            dt = dateutil_parser.parse(s)
            if dt.tzinfo is not None:
                dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            fmts = ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M:%S.%f",
                    "%d/%m/%Y %H:%M:%S", "%d-%m-%Y %H:%M:%S",
                    "%Y-%m-%dT%H:%M:%S")
            for fmt in fmts:
                try:
                    return _dt.strptime(s, fmt)
                except Exception:
                    pass
    return None

def _extract_timestamp_from_detail(detail):
    fields = [
        "LocaleMessageDateTime", "LocalMessageDateTime", "LocaleMessageTime", "LocalMessageTime",
        "LocaleMessageDate", "Timestamp", "timestamp", "Time", "LocaleTime", "LocalTime",
        "time", "date", "LocaleMessageDateTimeUtc", "LocalMessageDateTimeUtc",
        "Swipe_Time", "SwipeTime", "SwipeTimeLocal", "SwipeTimestamp", "SwipeDateTime"
    ]
    if isinstance(detail, dict):
        for k in fields:
            if k in detail:
                dt = _parse_timestamp_from_value(detail.get(k))
                if dt is not None:
                    return dt
        for v in detail.values():
            dt = _parse_timestamp_from_value(v)
            if dt is not None:
                return dt
    else:
        return _parse_timestamp_from_value(detail)
    return None



# --- Main functions ----------------------------------------------------------

def ingest_live_details_list(details_list):
    """Persist details_list into LiveSwipe. returns counts."""
    from db import SessionLocal as _SessionLocal
    inserted = 0
    skipped = 0
    with _SessionLocal() as db:
        for d in details_list:
            try:
                ts_parsed = _extract_timestamp_from_detail(d)
            except Exception:
                ts_parsed = None
            if ts_parsed is None:
                # skip rows without parseable timestamp
                skipped += 1
                continue

            # robust extraction of employee id and card fields (many alias names)
            emp = None
            for k in ("EmployeeID", "employee_id", "employeeId", "Employee Id", "EmpID", "Emp Id"):
                if isinstance(d, dict) and k in d:
                    emp = d.get(k)
                    break
            emp = _normalize_employee_key(emp)

            card = None
            for k in ("CardNumber", "card_number", "Card", "Card No", "CardNo", "Badge", "BadgeNo", "badge_number", "IPassID", "iPass ID"):
                if isinstance(d, dict) and k in d:
                    card = d.get(k)
                    break
            card = _normalize_card_like(card)

            full_name = None
            for k in ("ObjectName1", "FullName", "full_name", "EmpName", "Name"):
                if isinstance(d, dict) and k in d:
                    full_name = d.get(k)
                    break

            partition = None
            for k in ("PartitionName2", "PartitionName1", "Partition", "PartitionName", "Region"):
                if isinstance(d, dict) and k in d:
                    partition = d.get(k)
                    break

            floor = d.get("Floor") if isinstance(d, dict) else None
            door = None
            for k in ("Door", "DoorName", "door"):
                if isinstance(d, dict) and k in d:
                    door = d.get(k)
                    break

            region = d.get("__region") if isinstance(d, dict) and "__region" in d else d.get("Region") if isinstance(d, dict) else None

            try:
                rec = LiveSwipe(
                    timestamp=ts_parsed,
                    employee_id=emp,
                    card_number=card,
                    full_name=full_name,
                    partition=partition,
                    floor=floor,
                    door=door,
                    region=region,
                    raw=d
                )
                db.add(rec)
                inserted += 1
            except Exception:
                # skip insertion errors but continue
                db.rollback()
                skipped += 1
                continue
        db.commit()
    print(f"[ingest_live_details_list] inserted={inserted} skipped={skipped}")
    return {"inserted": inserted, "skipped_invalid_timestamp": skipped}


def compute_daily_attendance(target_date: date):
    """Build AttendanceSummary rows for target_date (upserts)."""
    with SessionLocal() as db:
        start = datetime.combine(target_date, datetime.min.time())
        end = datetime.combine(target_date, datetime.max.time())
        swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
        if not swipes:
            print(f"[compute_daily_attendance] no swipes for {target_date}")
            return []

        rows = []
        for s in swipes:
            rows.append({
                "id": s.id,
                "timestamp": s.timestamp,
                "employee_id": _normalize_employee_key(s.employee_id),
                "card_number": _normalize_card_like(s.card_number),
                "full_name": s.full_name,
                "partition": s.partition,
                "floor": s.floor,
                "door": s.door
            })
        df = pd.DataFrame(rows)
        if df.empty:
            print(f"[compute_daily_attendance] dataframe empty after rows -> {target_date}")
            return []

        # create grouping key: prefer employee_id, otherwise card_number
        df['key'] = df['employee_id'].fillna(df['card_number'])
        df = df[df['key'].notna()]
        if df.empty:
            print("[compute_daily_attendance] no usable keys after filling employee_id/card")
            return []

        grouped = df.groupby('key', dropna=False).agg(
            presence_count=('id', 'count'),
            first_seen=('timestamp', 'min'),
            last_seen=('timestamp', 'max'),
            full_name=('full_name', 'first'),
            partition=('partition', 'first'),
            card_number=('card_number', 'first')
        ).reset_index().rename(columns={'key': 'employee_id'})

        # upsert AttendanceSummary rows (merge)
        for _, row in grouped.iterrows():
            try:
                derived_obj = {
                    "partition": (row.get('partition') or None),
                    "full_name": (row.get('full_name') or None),
                    "card_number": (row.get('card_number') or None)
                }
                rec = AttendanceSummary(
                    employee_id=str(row['employee_id']) if pd.notna(row['employee_id']) else None,
                    date=target_date,
                    presence_count=int(row['presence_count']),
                    first_seen=row['first_seen'],
                    last_seen=row['last_seen'],
                    derived=derived_obj
                )
                db.merge(rec)
            except Exception as e:
                print("[compute_daily_attendance] upsert error:", e)
                continue
        db.commit()
        print(f"[compute_daily_attendance] built {len(grouped)} attendance keys for {target_date}")
        return grouped.to_dict(orient='records')


def compare_with_active(target_date: date):
    """Compare AttendanceSummary for date with ActiveEmployee & ActiveContractor and return json-safe dict."""
    # NOTE: we intentionally do NOT import get_global_stats_or_none from ccure_client;
    # a local helper wrapper below will call ccure_client.get_global_stats() safely.
    with SessionLocal() as db:
        att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == target_date).all()
        if not att_rows:
            att_df = pd.DataFrame(columns=["employee_id", "presence_count", "first_seen", "last_seen", "card_number", "partition", "full_name"])
        else:
            att_df = pd.DataFrame([{
                "employee_id": _normalize_employee_key(a.employee_id),
                "presence_count": a.presence_count,
                "first_seen": a.first_seen,
                "last_seen": a.last_seen,
                "card_number": _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None),
                "partition": (a.derived.get('partition') if (a.derived and isinstance(a.derived, dict)) else None),
                "full_name": (a.derived.get('full_name') if (a.derived and isinstance(a.derived, dict)) else None)
            } for a in att_rows])

        act_rows = db.query(ActiveEmployee).all()
        contractor_rows = db.query(ActiveContractor).all()

        # Build maps & active list
        act_list = []
        card_to_emp = {}
        name_to_emp = {}

        # Employees
        for e in act_rows:
            emp_id_norm = _normalize_employee_key(e.employee_id)
            # extract card-like from raw if present
            card_from_raw = None
            try:
                rr = e.raw_row or {}
                if isinstance(rr, dict):
                    ck_list = [
                        "CardNumber","card_number","Card","Card No","CardNo","IPassID","IpassID","iPass ID","IPASSID",
                        "Badge Number","BadgeNo","Badge"
                    ]
                    for ck in ck_list:
                        v = rr.get(ck)
                        if v:
                            ckey = _normalize_card_like(v)
                            if ckey:
                                card_from_raw = ckey
                                break
                    # fallback: scan all values for numeric candidate
                    if not card_from_raw:
                        for v in rr.values():
                            try:
                                tmp = _normalize_card_like(v)
                                if tmp and 3 <= len(tmp) <= 12:
                                    card_from_raw = tmp
                                    break
                            except Exception:
                                pass
            except Exception:
                card_from_raw = None

            act_list.append({
                "employee_id": emp_id_norm,
                "full_name": e.full_name,
                "location_city": e.location_city,
                "status": e.current_status,
                "card_number": card_from_raw
            })
            if emp_id_norm:
                card_to_emp[emp_id_norm] = emp_id_norm
            if card_from_raw:
                card_to_emp[card_from_raw] = emp_id_norm
            n = _normalize_name(e.full_name)
            if n:
                name_to_emp[n] = emp_id_norm

        # Contractors
        for c in contractor_rows:
            worker_id = _normalize_employee_key(c.worker_system_id)
            ipass = _normalize_employee_key(c.ipass_id)
            w_ipass = ("W" + ipass) if ipass and not str(ipass).startswith("W") else ipass
            primary_id = worker_id or ipass or None
            act_list.append({
                "employee_id": primary_id,
                "full_name": c.full_name,
                "location_city": c.location,
                "status": c.status,
                "card_number": None
            })
            if primary_id:
                card_to_emp[primary_id] = primary_id
            if ipass:
                card_to_emp[ipass] = primary_id
            if w_ipass:
                card_to_emp[w_ipass] = primary_id
            try:
                rr = c.raw_row or {}
                if isinstance(rr, dict):
                    for ck in ("Worker System Id","Worker System ID","iPass ID","IPASSID","CardNumber","card_number"):
                        if ck in rr and rr.get(ck):
                            key = _normalize_card_like(rr.get(ck))
                            if key:
                                card_to_emp[key] = primary_id
            except Exception:
                pass
            n = _normalize_name(c.full_name)
            if n:
                name_to_emp[n] = primary_id

        act_df = pd.DataFrame(act_list)

        # If no active rows, return attendance-only view
        if act_df.empty:
            if att_df.empty:
                return {"by_location": [], "merged": [], "ccure": get_global_stats_or_none()}
            att_df['partition'] = att_df.get('partition').fillna('Unknown')
            att_df['presence_count'] = att_df['presence_count'].fillna(0)
            att_df['present_today'] = att_df['presence_count'].apply(lambda x: bool(x and x != 0))
            loc_group = att_df.groupby('partition', dropna=False).agg(
                total_n=('employee_id', 'count'),
                present_n=('present_today', 'sum')
            ).reset_index().rename(columns={'partition':'location_city'})
            loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
            by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]
            merged_list = []
            for r in att_df.to_dict(orient='records'):
                merged_list.append({
                    "employee_id": _to_native(r.get('employee_id')),
                    "presence_count": _to_native(r.get('presence_count')),
                    "first_seen": _to_native(r.get('first_seen')),
                    "last_seen": _to_native(r.get('last_seen')),
                    "full_name": _to_native(r.get('full_name')),
                    "location_city": _to_native(r.get('partition')),
                    "present_today": _to_native(r.get('present_today'))
                })
            return {"by_location": by_location, "merged": merged_list, "ccure": get_global_stats_or_none()}

        # normalize columns
        act_df['employee_id'] = act_df['employee_id'].astype(object).apply(_normalize_employee_key)
        att_df['employee_id'] = att_df['employee_id'].astype(object).apply(_normalize_employee_key)
        act_df['card_number'] = act_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in act_df.columns else pd.Series([pd.NA]*len(act_df))
        att_df['card_number'] = att_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in att_df.columns else pd.Series([pd.NA]*len(att_df))

        # ensure card_to_emp includes act_df card_numbers
        for r in act_df.to_dict(orient='records'):
            c = r.get('card_number')
            eid = r.get('employee_id')
            if c and eid:
                card_to_emp[c] = eid
            if eid:
                # also map numeric-only forms of eid
                n = re.sub(r'\D','', str(eid))
                if n:
                    card_to_emp[n.lstrip('0') or n] = eid

        # mapping function tries multiple strategies
        emp_set = set([x for x in act_df['employee_id'].dropna().astype(str)])

        def numeric_variants(s):
            s = str(s)
            clean = re.sub(r'\D','', s)
            variants = set()
            if clean:
                variants.add(clean)
                variants.add(clean.lstrip('0') or clean)
                if not s.startswith('W'):
                    variants.add('W' + clean)
            return list(variants)

        def remap_att_key(row):
            primary = row.get('employee_id') or None
            card = row.get('card_number') or None

            primary_norm = _normalize_employee_key(primary)
            card_norm = _normalize_card_like(card)

            # 1) exact employee id exists in active list
            if primary_norm and primary_norm in emp_set:
                return primary_norm

            # 2) numeric-variants of primary may map to card_to_emp
            if primary_norm:
                for v in numeric_variants(primary_norm):
                    if v in card_to_emp:
                        return card_to_emp[v]
                if primary_norm in card_to_emp:
                    return card_to_emp[primary_norm]

            # 3) direct card mapping
            if card_norm:
                if card_norm in card_to_emp:
                    return card_to_emp[card_norm]
                if (card_norm.lstrip('0') or card_norm) in card_to_emp:
                    return card_to_emp[card_norm.lstrip('0') or card_norm]
                if ('W' + card_norm) in card_to_emp:
                    return card_to_emp['W' + card_norm]

            # 4) name matching fallback
            fname = _normalize_name(row.get('full_name') or row.get('full_name_att') or None)
            if fname and fname in name_to_emp:
                return name_to_emp[fname]

            # 5) last resort - return primary_norm (maybe non-mapped) so it still shows up
            return primary_norm or card_norm or None

        att_df['mapped_employee_id'] = att_df.apply(remap_att_key, axis=1)

        # drop original employee_id column to avoid duplicate label conflict
        att_merge_df = att_df.drop(columns=['employee_id'], errors='ignore').copy()

        # merge left: act_df left_on employee_id, right_on mapped_employee_id
        merged = pd.merge(
            act_df,
            att_merge_df,
            left_on='employee_id',
            right_on='mapped_employee_id',
            how='left',
            suffixes=('', '_att')
        )

        # fill and finalize
        merged['presence_count'] = merged.get('presence_count', pd.Series([0]*len(merged))).fillna(0)
        # ensure ints when possible
        def safe_int(v):
            try:
                if pd.isna(v):
                    return 0
                iv = int(float(v))
                return iv
            except Exception:
                return v
        merged['presence_count'] = merged['presence_count'].apply(safe_int)
        merged['present_today'] = merged['presence_count'].apply(lambda x: bool(x and x != 0))
        merged['location_city'] = merged.get('location_city').fillna('Unknown')

        # by_location
        loc_group = merged.groupby('location_city', dropna=False).agg(
            total_n=('employee_id', 'count'),
            present_n=('present_today', 'sum')
        ).reset_index()
        loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
        by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]

        merged_list = []
        for r in merged.to_dict(orient='records'):
            clean = {k:_to_native(v) for k,v in r.items()}
            # unify keys for clarity in API response
            clean['mapped_employee_id'] = clean.get('mapped_employee_id')
            clean['card_number_att'] = clean.get('card_number') or clean.get('card_number_att') or None
            # include status if present
            if 'status' not in clean:
                clean['status'] = None
            # ensure employee_id key exists
            if 'employee_id' not in clean:
                clean['employee_id'] = None
            merged_list.append(clean)

        # CCURE stats fetch (best-effort)
        ccure_stats = get_global_stats_or_none()

        # compare counts summary between CCure and Active sheets
        try:
            ccure_summary = ccure_stats or {}
            cc_total_profiles = ccure_summary.get('TotalProfiles')
            cc_active_profiles = ccure_summary.get('ActiveProfiles')
            cc_active_emps = ccure_summary.get('ActiveEmployees')
            cc_active_contractors = ccure_summary.get('ActiveContractors')
        except Exception:
            cc_total_profiles = cc_active_profiles = cc_active_emps = cc_active_contractors = None

        # local sheet counts
        active_emp_count = len(act_rows)
        active_contract_count = len(contractor_rows)

        diff = {
            "active_sheet_employee_count": active_emp_count,
            "active_sheet_contractor_count": active_contract_count,
            "ccure_active_employees": cc_active_emps,
            "ccure_active_contractors": cc_active_contractors,
            "delta_employees": (cc_active_emps - active_emp_count) if (isinstance(cc_active_emps, int) and isinstance(active_emp_count, int)) else None,
            "delta_contractors": (cc_active_contractors - active_contract_count) if (isinstance(cc_active_contractors, int) and isinstance(active_contract_count, int)) else None
        }

        result = {
            "by_location": by_location,
            "merged": merged_list,
            "ccure": ccure_stats,
            "count_comparison": diff
        }
        return result

# Helper wrapper
def get_global_stats_or_none():
    try:
        from ccure_client import get_global_stats
        return get_global_stats()
    except Exception:
        logger.debug("ccure_client.get_global_stats not available", exc_info=True)
        return None

