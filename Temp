"""
duration_report.py

Updated: includes Door, EmployeeName, CardNumber extraction (XML fallback),
CompanyName, PrimaryLocation, Direction, and per-swipe output alongside per-person durations.

Place into your backend directory and restart the server.
"""
import argparse
import logging
import os
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd

# Optional: import pyodbc only when connecting (allows importing this module even without driver)
try:
    import pyodbc
except Exception:
    pyodbc = None

# --------------------- Configuration ---------------------
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "database": "ACVSUJournal_00010029",
        "partitions": [
            "APAC.Default", "CN.Beijing", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur"
        ]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "database": "ACVSUJournal_00011028",
        "partitions": [
            "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
            "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
        ]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "database": "ACVSUJournal_00010029",
        "partitions": [
            "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
            "PA.Panama City", "PE.Lima"
        ]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "database": "ACVSUJournal_00010029",
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# --------------------- SQL Builder ---------------------
# This SQL attempts to extract Card from XML and falls back to t2.Text12.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,                       -- Person name from ACVS
    t1.[ObjectName2] AS Door,                               -- Door / reader name
    CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    -- try extract card from xml CHUID/Card or CHUID element, then fallback to shred 'sc' and finally t2.Text12
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
INNER JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
INNER JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
;
"""


def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    return GENERIC_SQL_TEMPLATE.format(db=rc["database"], date=date_str, region_filter=region_filter)


# --------------------- DB Utilities ---------------------
def get_connection(region_key: str):
    """Create and return a pyodbc connection for the region configuration."""
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)


def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    """
    Fetch CardAdmitted swipes for the given region and target_date.
    Returns a pandas DataFrame with normalized column names:
      ['EmployeeName','Door','EmployeeID','CardNumber','PersonnelTypeName','EmployeeIdentity',
       'PartitionName2','LocaleMessageTime','MessageType','Direction','CompanyName','PrimaryLocation']
    If pyodbc is not available, returns an empty skeleton DataFrame with those columns.
    """
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "CardNumber", "PersonnelTypeName", "EmployeeIdentity",
        "PartitionName2", "LocaleMessageTime", "MessageType", "Direction", "CompanyName", "PrimaryLocation"
    ]
    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = get_connection(region_key)
    try:
        df = pd.read_sql(sql, conn)
    finally:
        conn.close()

    # Ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Parse datetime
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    return df[cols]


# --------------------- Duration Calculation ---------------------
def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute first and last swipe per person per day and duration.
    - uses EmployeeIdentity (GUID) as primary dedupe key
    - falls back to EmployeeID|CardNumber|EmployeeName when GUID missing
    - returns DataFrame with columns:
      person_uid, EmployeeIdentity, EmployeeID, EmployeeName, CardNumber,
      Date, FirstSwipe, LastSwipe, FirstDoor, LastDoor, CountSwipes,
      DurationSeconds, Duration, PersonnelTypeName, PartitionName2,
      CompanyName, PrimaryLocation, FirstDirection, LastDirection
    """
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()

    # Ensure expected columns present (avoid KeyError)
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # Parse datetime if required
    if df["LocaleMessageTime"].dtype == object:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    # Drop exact duplicate swipe records (same GUID, same timestamp, same card, same door)
    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    df = df.drop_duplicates(subset=dedupe_cols, keep="first")

    # Create Date column (uses local date from LocaleMessageTime)
    df["Date"] = df["LocaleMessageTime"].dt.date

    # Build person_uid: prefer EmployeeIdentity (GUID); otherwise concat EmployeeID|CardNumber|EmployeeName
    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    df["person_uid"] = df.apply(make_person_uid, axis=1)
    df = df[df["person_uid"].notna()].copy()

    # Group by person_uid + Date and compute aggregates
    def agg_for_group(g):
        g_sorted = g.sort_values("LocaleMessageTime")
        first = g_sorted.iloc[0]
        last = g_sorted.iloc[-1]

        # First/last directions might be missing or identical; preserve both
        first_dir = first.get("Direction")
        last_dir = last.get("Direction")

        return pd.Series({
            "person_uid": first["person_uid"],
            "EmployeeIdentity": first.get("EmployeeIdentity"),
            "EmployeeID": first.get("EmployeeID"),
            "EmployeeName": first.get("EmployeeName"),
            "CardNumber": first.get("CardNumber"),
            "Date": first["Date"],
            "FirstSwipe": first["LocaleMessageTime"],
            "LastSwipe": last["LocaleMessageTime"],
            "FirstDoor": first.get("Door"),
            "LastDoor": last.get("Door"),
            "CountSwipes": int(len(g_sorted)),
            "PersonnelTypeName": first.get("PersonnelTypeName"),
            "PartitionName2": first.get("PartitionName2"),
            "CompanyName": first.get("CompanyName"),
            "PrimaryLocation": first.get("PrimaryLocation"),
            "FirstDirection": first_dir,
            "LastDirection": last_dir
        })

    grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    # compute duration fields
    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    # Reorder columns and return
    return grouped[out_cols]


# --------------------- Main Runner ---------------------
def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    """
    Run duration reports for the requested regions on target_date.
    Returns a dict: region -> {"swipes": DataFrame, "durations": DataFrame}
    """
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        try:
            swipes = fetch_swipes_for_region(r, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", r)
            swipes = pd.DataFrame()

        # optional city filter: match PartitionName2 OR PrimaryLocation OR Door OR EmployeeName (case-insensitive)
        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                swipes = swipes[combined_mask].copy()
            else:
                # no matching columns -> keep original
                logging.debug("City filter provided but no matching columns to filter on for region %s", r)

        # compute durations (de-duplicated, per-person)
        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", r)
            durations = pd.DataFrame()

        # save CSVs
        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
        try:
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", r)
        try:
            # write swipes with a stable column order if possible
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", r)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
        results[r] = {"swipes": swipes, "durations": durations}

    return results


def parse_args():
    p = argparse.ArgumentParser(description="Generate daily duration report from ACVSUJournal swipe logs.")
    p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
    p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
                   default="apac,emea,laca,namer")
    p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
    p.add_argument("--city", help="Optional city/location filter (e.g. Pune)", default=None)
    return p.parse_args()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    args = parse_args()

    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    else:
        tz = ZoneInfo("Asia/Kolkata")
        target_date = datetime.now(tz).date()

    regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
    outdir = args.outdir

    logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
    results = run_for_date(target_date, regions, outdir, city=args.city)

    for r, obj in results.items():
        dur = obj.get("durations")
        logging.info("Region %s: %d persons with computed durations", r, len(dur) if dur is not None else 0)
    logging.info("Completed. CSVs are in %s", Path(outdir).absolute())

















In Below API i have review some issue so refer below  query and nad Update all logic carefully..
in below API responce we got Result like ..

{
  "date": "2025-09-05",
  "regions": {
    "apac": {
      "rows": 557,
      "swipe_rows": 10586,
      "durations_sample": [
        {
          "person_uid": "96650F42-16F7-458D-A11D-DB6B15C47572",
          "EmployeeIdentity": "96650F42-16F7-458D-A11D-DB6B15C47572",
          "EmployeeID": "324265",
          "EmployeeName": "Mia",
          "CardNumber": "",
          "Date": "2025-09-05",
          "FirstSwipe": "2025-09-05T00:30:01",
          "LastSwipe": "2025-09-05T05:06:34",
          "FirstDoor": null,
          "LastDoor": null,
          "CountSwipes": 20,
          "DurationSeconds": 16593,
          "Duration": "4:36:33",
          "PersonnelTypeName": "Employee",
          "PartitionName2": "PH.Manila"
        },
        {
          "person_uid": "4C3D5E93-32BA-450E-9FD4-20E0FFEF68C0",
          "EmployeeIdentity": "4C3D5E93-32BA-450E-9FD4-20E0FFEF68C0",
          "EmployeeID": "244777",
          "EmployeeName": "Michael John",
          "CardNumber": "",
          "Date": "2025-09-05",
          "FirstSwipe": "2025-09-05T00:01:46",
          "LastSwipe": "2025-09-05T05:42:30",
          "FirstDoor": null,
          "LastDoor": null,
          "CountSwipes": 22,
          "DurationSeconds": 20444,
          "Duration": "5:40:44",
          "PersonnelTypeName": "Employee",
          "PartitionName2": "PH.Manila"
        },




Refer below query and add door. name , 
refer below query and add


  {
      "LocaleMessageTime": "2025-09-05T13:19:15.000Z",
      "Dateonly": "2025-09-05",
      "Swipe_Time": "13:19:15",
      "EmployeeID": "321004",
      "PersonGUID": "4A79BF18-162B-4BBD-AA89-BE9F9DC3CC8C",
      "ObjectName1": "Cardoso, Miguel Lopes", -----------this is name 
      "Door": "EMEA_LT_VNO_GAMA_8th Flr_Main Entrance",  ----- this is door name 
      "PersonnelType": "Employee",  --------------PersonnelType
      "CardNumber": "619289",--------------------This is CardNumber
      "Text5": "Vilnius - Gama Business Center",  -----
      "PartitionName2": "LT.Vilnius",--------Location
      "AdmitCode": "Admit",----------------message type
      "Direction": "InDirection",---------------Direction
      "CompanyName": "WU Processing Lithuania, UAB",--------------Company name 
      "PrimaryLocation": "Vilnius - Gama Business Center",---------Primary location
   
    },


Regfer below Query carefully and upadte all null APi responce json format..


 * Live occupancy (today) for APAC
 */
exports.fetchLiveOccupancy = async () => {
  const pool = await poolPromise;
  const parts = quoteList(partitionList);

  const query = `

    WITH CombinedEmployeeData AS (
      SELECT
        t1.ObjectName1,
        t1.ObjectName2             AS Door,               -- include Door
        CASE WHEN t2.Int1 = 0 THEN t2.Text12 ELSE CAST(t2.Int1 AS NVARCHAR) END AS EmployeeID,
        t3.Name                    AS PersonnelType,
        t1.ObjectIdentity1         AS PersonGUID,
        -- extract CardNumber from XML or shred table
        COALESCE(
          TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
          TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
          sc.value
        )                          AS CardNumber,
        CASE
          WHEN t1.ObjectName2 LIKE 'APAC_PI%' THEN 'Taguig City'
          WHEN t1.ObjectName2 LIKE 'APAC_PH%' THEN 'Quezon City'
          WHEN t1.ObjectName2 LIKE '%PUN%'   THEN 'Pune'
          WHEN t1.ObjectName2 LIKE 'APAC_JPN%' THEN 'JP.Tokyo'
          WHEN t1.ObjectName2 LIKE 'APAC_MY%'  THEN 'MY.Kuala Lumpur'
          ELSE t1.PartitionName2
        END                        AS PartitionName2,
        DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS LocaleMessageTime,
        t5d.value                  AS Direction,
        t2.Text4                   AS CompanyName,        -- ✅ added
        t2.Text5                   AS PrimaryLocation     -- ✅ added
      FROM ACVSUJournal_00010029.dbo.ACVSUJournalLog t1
      JOIN ACVSCore.Access.Personnel       t2 ON t1.ObjectIdentity1 = t2.GUID
      JOIN ACVSCore.Access.PersonnelType   t3 ON t2.PersonnelTypeID = t3.ObjectID

      LEFT JOIN ACVSUJournal_00010029.dbo.ACVSUJournalLogxmlShred t5d
      ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')

      LEFT JOIN ACVSUJournal_00010029.dbo.ACVSUJournalLogxml t_xml
        ON t1.XmlGUID = t_xml.GUID
      LEFT JOIN (
        SELECT GUID, value
        FROM ACVSUJournal_00010029.dbo.ACVSUJournalLogxmlShred
        WHERE Name IN ('Card','CHUID')
      ) AS sc
        ON t1.XmlGUID = sc.GUID
      WHERE
        t1.MessageType = 'CardAdmitted'
        AND t1.PartitionName2 IN (${parts})
        AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC))
            = CONVERT(DATE, GETDATE())
    ), Ranked AS (
      SELECT *,
        ROW_NUMBER() OVER (PARTITION BY PersonGUID ORDER BY LocaleMessageTime DESC) AS rn
      FROM CombinedEmployeeData
      
    )
    SELECT
      ObjectName1,
      Door,                            -- door
      PersonnelType,
      EmployeeID,
      CardNumber,                      -- now returned
      PartitionName2,
      LocaleMessageTime,
      Direction,
      PersonGUID,
      CompanyName,                      -- ✅ added
      PrimaryLocation                   -- ✅ added
    FROM Ranked
    WHERE rn = 1;
  `;

  const result = await pool.request().query(query);
  return result.recordset;
};




exports.fetchHistoricalOccupancy = async (location) =>
  exports.fetchHistoricalData({ location: location || null });

exports.fetchHistoricalData = async ({ location = null }) => {
  const pool = await poolPromise;

  // 1. Get all ACVSUJournal_* database names dynamically
  const dbResult = await pool.request().query(`
    SELECT name 
    FROM sys.databases
    WHERE name LIKE 'ACVSUJournal[_]%'
    ORDER BY CAST(REPLACE(name, 'ACVSUJournal_', '') AS INT)
  `);

  // Map DBs and pick last 2 only
  const databases = dbResult.recordset.map(r => r.name);
  const selectedDbs = databases.slice(-2); // newest and previous

  if (selectedDbs.length === 0) {
    throw new Error("No ACVSUJournal_* databases found.");
  }

  // 2. Outer filter
  const outerFilter = location
    ? `WHERE PartitionNameFriendly = @location`
    : `WHERE PartitionNameFriendly IN (${quoteList([
        'Pune','Quezon City','JP.Tokyo','MY.Kuala Lumpur','Taguig City'
      ])})`;

  // 3. Build UNION ALL query across selected DBs only
  const unionQueries = selectedDbs.map(db => `
    SELECT
      DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS LocaleMessageTime,
      t1.ObjectName1,
      t1.ObjectName2               AS Door,
      CASE WHEN t2.Int1 = 0 THEN t2.Text12 ELSE CAST(t2.Int1 AS NVARCHAR) END AS EmployeeID,
      t3.Name                      AS PersonnelType,
      t1.ObjectIdentity1           AS PersonGUID,
     t2.Text4                   AS CompanyName,   -- ✅ company
     t2.Text5                   AS PrimaryLocation, -- ✅ location
      COALESCE(
        CASE
          WHEN t1.ObjectName2 LIKE 'APAC_PI%'   THEN 'Taguig City'
          WHEN t1.ObjectName2 LIKE 'APAC_PH%'   THEN 'Quezon City'
          WHEN t1.ObjectName2 LIKE '%PUN%'      THEN 'Pune'
          WHEN t1.ObjectName2 LIKE 'APAC_JPN%'  THEN 'JP.Tokyo'
          WHEN t1.ObjectName2 LIKE 'APAC_MY%'   THEN 'MY.Kuala Lumpur'
          ELSE t1.PartitionName2
        END,
        'APAC.Default'
      ) AS PartitionNameFriendly,


      
      COALESCE(
        TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
        TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
        sc.value
      ) AS CardNumber,
      t5d.value AS Direction
    FROM ${db}.dbo.ACVSUJournalLog t1
    JOIN ACVSCore.Access.Personnel       t2 ON t1.ObjectIdentity1 = t2.GUID
    JOIN ACVSCore.Access.PersonnelType   t3 ON t2.PersonnelTypeID = t3.ObjectID

    LEFT JOIN ${db}.dbo.ACVSUJournalLogxmlShred t5d
      ON t1.XmlGUID = t5d.GUID 
      AND t5d.Value IN ('InDirection','OutDirection')

    LEFT JOIN ${db}.dbo.ACVSUJournalLogxml t_xml
      ON t1.XmlGUID = t_xml.GUID

    LEFT JOIN (
      SELECT GUID, value
      FROM ${db}.dbo.ACVSUJournalLogxmlShred
      WHERE Name IN ('Card','CHUID')
    ) AS sc
      ON t1.XmlGUID = sc.GUID
    WHERE t1.MessageType = 'CardAdmitted'
  `).join('\nUNION ALL\n');

  // 4. Final query
  const query = `
    WITH Hist AS (
      ${unionQueries}
    )
    SELECT *
    FROM Hist
    ${outerFilter}
    ORDER BY LocaleMessageTime ASC;
  `;

  const req = pool.request();
  if (location) {
    req.input('location', sql.NVarChar, location);
  }
  const result = await req.query(query);
  return result.recordset;
};

// keep this for occupancy
exports.fetchHistoricalOccupancy = async (location) =>
  exports.fetchHistoricalData({ location: location || null });
















check below duration.py file and share me fully updated file carefully so i can easily swap file each other...



"""
duration_report.py

Purpose:
  - Connects to ACVSUJournal MSSQL databases for APAC, EMEA, LACA, NAMER regions (credentials provided by user)
  - Extracts CardAdmitted swipe events for a specified date and computes daily duration per employee
    using first swipe (min) and last swipe (max) timestamps for that date.
  - Writes per-region CSV duration reports and returns pandas DataFrames for further use.
  - Contains clear TODO hooks to later improve shift-aware logic.

Notes & Usage:
  - Place this file in:
      C:/Users/W0024618/Desktop/global-page/backend/attendance-analytics
    (or any other folder).

  - Requires Python packages: pyodbc, pandas, python-dateutil
    Install (example):
      pip install pyodbc pandas python-dateutil

  - ODBC driver: adjust DRIVER in the connection string depending on the host environment.
    Example uses 'ODBC Driver 17 for SQL Server' by default.
    If you use a different driver (e.g. 18), change the DRIVER value.

  - Example CLI usage:
      python duration_report.py --date 2025-07-21 --regions apac,namer --outdir ./out_reports

  - The script by default picks "today" in Asia/Kolkata timezone if --date is omitted.

Security:
  - This script contains credentials as provided by the user. In production, consider moving credentials
    to environment variables or a secure vault.

TODO (future improvements):
  - Shift-aware logic: handle cases where employees have predefined shift windows spanning midnight,
    and decide which swipes belong to which shift.
  - Handle badge-in-only or badge-out-only cases (e.g., tele-getting or push-button exits)
  - More advanced deduplication of multiple swipes in short windows.

"""

import argparse
import logging
import os
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from pathlib import Path
from typing import Optional
import pandas as pd

# Optional: import pyodbc only when connecting (allows importing this module even without driver)
try:
    import pyodbc
except Exception:
    pyodbc = None

# --------------------- Configuration ---------------------
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "database": "ACVSUJournal_00010029",
        # partitions to include (from provided query)
        "partitions": [
            "APAC.Default", "CN.Beijing", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur"
        ]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "database": "ACVSUJournal_00011028",
        "partitions": [
            "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
            "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
        ]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "database": "ACVSUJournal_00010029",
        "partitions": [
            "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
            "PA.Panama City", "PE.Lima"
        ]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "database": "ACVSUJournal_00010029",
        # For NAMER we'll filter by ObjectName2 patterns (HQ, Austin, Miami, NYC)
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}


# --------------------- SQL Builder ---------------------
GENERIC_SQL_TEMPLATE = r"""

SELECT
    t1.[ObjectName1] AS DoorName,
    t1.[ObjectName2],
    CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Text12] AS CardNumber,
    t2.[Text1] AS EmployeeName,
    t2.[PersonnelTypeID],
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType
FROM [{db}].[dbo].[ACVSUJournalLog] AS t1
INNER JOIN [ACVSCore].[Access].[Personnel] AS t2 ON t1.ObjectIdentity1 = t2.GUID
INNER JOIN [ACVSCore].[Access].[PersonnelType] AS t3 ON t2.PersonnelTypeID = t3.ObjectID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
;
"""


def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    return GENERIC_SQL_TEMPLATE.format(db=rc["database"], date=date_str, region_filter=region_filter)


# --------------------- DB Utilities ---------------------
def get_connection(region_key: str):
    """Create and return a pyodbc connection for the region configuration."""
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)


def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    """Fetch CardAdmitted swipes for the given region and target_date."""
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        cols = ["ObjectName1", "ObjectName2", "EmployeeID", "PersonnelTypeID", "PersonnelTypeName",
                "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "MessageType"]
        return pd.DataFrame(columns=cols)

    conn = get_connection(region_key)
    try:
        df = pd.read_sql(sql, conn)
    finally:
        conn.close()
    if "LocaleMessageTime" in df.columns:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"])
    return df


# --------------------- Duration Calculation ---------------------


def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    """Compute first and last swipe per person per day and duration.
    - preserves and uses EmployeeIdentity (GUID) as primary dedupe key
    - falls back to EmployeeID|CardNumber|EmployeeName when GUID missing
    - returns DataFrame with columns:
      person_uid, EmployeeIdentity, EmployeeID, EmployeeName, CardNumber,
      Date, FirstSwipe, LastSwipe, FirstDoor, LastDoor, CountSwipes,
      DurationSeconds, Duration, PersonnelTypeName, PartitionName2
    """
    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=[
            "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
            "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
            "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2"
        ])

    df = swipes_df.copy()

    # Ensure expected columns available
    for col in ("EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "ObjectName1"):
        if col not in df.columns:
            df[col] = None

    # parse datetime
    if df["LocaleMessageTime"].dtype == object:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    # drop exact duplicate swipe records (same person, same timestamp, same door/card)
    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "ObjectName1"]
    df = df.drop_duplicates(subset=dedupe_cols, keep="first")

    # date column (local date already applied in SQL)
    df["Date"] = df["LocaleMessageTime"].dt.date

    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        # fallback combine
        pieces = [str(row.get("EmployeeID") or "").strip(), str(row.get("CardNumber") or "").strip(), str(row.get("EmployeeName") or "").strip()]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    df["person_uid"] = df.apply(make_person_uid, axis=1)
    # drop rows where person uid could not be built
    df = df[df["person_uid"].notna()]

    # group by person_uid + date and compute aggregates (use first/last swipe to get doors and names)
    def agg_for_group(g):
        g_sorted = g.sort_values("LocaleMessageTime")
        first = g_sorted.iloc[0]
        last = g_sorted.iloc[-1]
        return pd.Series({
            "person_uid": first["person_uid"],
            "EmployeeIdentity": first.get("EmployeeIdentity"),
            "EmployeeID": first.get("EmployeeID"),
            "EmployeeName": first.get("EmployeeName"),
            "CardNumber": first.get("CardNumber"),
            "Date": first["Date"],
            "FirstSwipe": first["LocaleMessageTime"],
            "LastSwipe": last["LocaleMessageTime"],
            "FirstDoor": first.get("ObjectName1"),
            "LastDoor": last.get("ObjectName1"),
            "CountSwipes": int(len(g_sorted)),
            "PersonnelTypeName": first.get("PersonnelTypeName"),
            "PartitionName2": first.get("PartitionName2")
        })

    grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    # compute duration fields
    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    # Reorder columns for clarity
    cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2"
    ]
    return grouped[cols]


# --------------------- Main Runner ---------------------


def run_for_date(target_date: date, regions: list, outdir: str, city: Optional[str] = None) -> dict:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        swipes = fetch_swipes_for_region(r, target_date)

        # optional city filter: match PartitionName2 or ObjectName2 (case-insensitive)
        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask = (
                swipes.get("PartitionName2", swipes.columns[:0]).fillna("").astype(str).str.lower().str.contains(city_l, na=False)
                | swipes.get("ObjectName2", swipes.columns[:0]).fillna("").astype(str).str.lower().str.contains(city_l, na=False)
            )
            swipes = swipes[mask].copy()

        # compute durations (de-duplicated, per-person)
        durations = compute_daily_durations(swipes)

        # save CSVs
        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
        try:
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", r)
        try:
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", r)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
        results[r] = {"swipes": swipes, "durations": durations}
    return results




def parse_args():
    p = argparse.ArgumentParser(description="Generate daily duration report from ACVSUJournal swipe logs.")
    p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
    p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
                   default="apac,emea,laca,namer")
    p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
    return p.parse_args()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    args = parse_args()

    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    else:
        tz = ZoneInfo("Asia/Kolkata")
        target_date = datetime.now(tz).date()

    regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
    outdir = args.outdir

    logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
    results = run_for_date(target_date, regions, outdir)

    for r, df in results.items():
        logging.info("Region %s: %d employees with computed durations", r, len(df))
    logging.info("Completed. CSVs are in %s", Path(outdir).absolute())









