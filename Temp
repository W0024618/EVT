from fastapi import Query

@app.get("/ccure/compare_v2")
def ccure_compare_v2(
    mode: str = Query("full"),
    stats_detail: str = Query("ActiveProfiles"),
    limit_list: int = Query(200, ge=1, le=5000),
    export: bool = Query(False),
    region_filter: Optional[str] = Query(None, description="Region code e.g. APAC"),
    location_city: Optional[str] = Query(None, description="Location city e.g. Pune"),
    location_state: Optional[str] = Query(None, description="Location state/province"),
    location_description: Optional[str] = Query(None, description="Location Description"),
    week_ref_date: Optional[str] = Query(None, description="YYYY-MM-DD for week (Mon-Fri)")
):
    try:
        from data_compare_service_v2 import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service_v2 import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(
        mode=mode,
        stats_detail=stats_detail,
        limit_list=limit_list,
        export=export,
        region_filter=region_filter,
        location_city=location_city,
        location_state=location_state,
        location_description=location_description,
        week_ref_date=week_ref_date
    )
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)







http://localhost:8000/ccure/compare_v2



http://localhost:8000/ccure/compare_v2?region_filter=APAC&location_city=Pune


http://localhost:8000/ccure/compare_v2?region_filter=APAC&location_city=Pune&location_state=Maharashtra&location_description=Pune%20Business%20Bay



http://localhost:8000/ccure/compare_v2?region_filter=APAC&location_city=Pune&week_ref_date=2025-08-25





http://localhost:8000/ccure/compare_v2?region_filter=APAC&location_city=Pune&export=true












Now Check Below File carefully and and Explain me how to check APi Responce give me APi endpoint to check data on browser

C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service_v2.py
"""
New comparison service (v2)

Features:
- Loads canonical Active Employee sheet from DATA_DIR/active_employee.*
- Filters by Region Code, Location City, Location Description, Location State/Province
- Cross-checks against AttendanceSummary DB (today and Mon-Fri of current week)
- Calculates:
    - total active in selection (from sheet)
    - visited today count and percentage vs sheet total
    - current status (on leave) counts
    - employee type counts (Regular vs WFH, etc.)
    - For Regular employees: compute days present in Mon-Fri week, counts for present>=3, present==5
    - Defaulter lists (present only 1 time, present <3 times) with leave flag
- Optional export to Excel (writes to OUTPUT_DIR, returns filename)
- Public function: compare_ccure_vs_sheets(...) - backwards-compatible signature with optional extra filters
"""
import pandas as pd
import os
from pathlib import Path
from datetime import date, datetime, timedelta
from typing import Optional, Dict, Any, List
import logging
import uuid

# DB imports (same as your project)
from db import SessionLocal
from models import AttendanceSummary, ActiveEmployee

# Use settings if present to find DATA_DIR / OUTPUT_DIR
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"

DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger("data_compare_service_v2")
logger.setLevel(logging.INFO)
if not logger.handlers:
    import sys
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)


# ----------------------------
# Helpers
# ----------------------------
def _find_active_employee_file():
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_employee{ext}"
        if p.exists():
            return p
    # try case-insensitive search
    for p in DATA_DIR.iterdir():
        if p.is_file() and "active_employee" in p.name.lower():
            return p
    return None

def _normalize_key(k):
    if k is None:
        return None
    return str(k).strip()

def _safe_int(v, default=0):
    try:
        if v is None or v == "":
            return default
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return default

def _week_monday_and_friday(ref_date: Optional[date] = None):
    if ref_date is None:
        ref_date = date.today()
    # ISO weekday: Monday=1
    iso = ref_date.isoweekday()
    monday = ref_date - timedelta(days=(iso - 1))
    friday = monday + timedelta(days=4)
    return monday, friday

def _maybe_mark_on_leave(status_str: Optional[str]) -> bool:
    if not status_str:
        return False
    s = str(status_str).strip().lower()
    for tok in ("leave", "vacation", "on leave", "holiday", "sabbatical", "furlough", "loa"):
        if tok in s:
            return True
    return False

# ----------------------------
# Core loader: active employees (sheet)
# ----------------------------
def load_active_employees_dataframe() -> pd.DataFrame:
    """
    Loads canonical active_employee spreadsheet into a normalized DataFrame.
    Returns DataFrame with (at least) these normalized columns:
      employee_id, full_name, location_city, location_desc, location_state, region_code,
      current_status, employee_type, raw_row (JSON)
    """
    src = _find_active_employee_file()
    if not src:
        raise FileNotFoundError(f"Active employee canonical file not found in {DATA_DIR}")
    ext = src.suffix.lower()
    if ext in (".xls", ".xlsx"):
        df = pd.read_excel(src, sheet_name=0, dtype=str)
    else:
        df = pd.read_csv(src, dtype=str)

    # normalize column names (strip)
    df.columns = [c.strip() for c in df.columns]

    def _first_present(row, candidates):
        for c in candidates:
            if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
                return row[c]
        return None

    # build normalized rows
    rows = []
    for _, row in df.iterrows():
        emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id'])
        full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or (
            f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
        )
        location_city = _first_present(row, ['Location City','Location City ' ,'Location','City','LocationCity'])
        location_desc = _first_present(row, ['Location Description','Location Description ','LocationDescription'])
        location_state = _first_present(row, ['Location State / Province','Location State / Province ','Location State','State','Province'])
        region_code = _first_present(row, ['Region Code','Region','RegionCode'])
        current_status = _first_present(row, ['Current Status','Status','Employee Status','Employee_Status'])
        employee_type = _first_present(row, ['Employee Type','Type','Time Type'])
        rows.append({
            "employee_id": _normalize_key(emp_id),
            "full_name": _normalize_key(full_name),
            "location_city": _normalize_key(location_city),
            "location_desc": _normalize_key(location_desc),
            "location_state": _normalize_key(location_state),
            "region_code": (str(region_code).strip() if region_code is not None else None),
            "current_status": _normalize_key(current_status),
            "employee_type": _normalize_key(employee_type),
            "raw_row": row.to_dict()
        })
    ndf = pd.DataFrame(rows)
    # ensure columns exist
    for col in ("employee_id","full_name","location_city","location_desc","location_state","region_code","current_status","employee_type","raw_row"):
        if col not in ndf:
            ndf[col] = None
    # drop rows without employee_id (these cannot be matched)
    ndf = ndf[ndf["employee_id"].notna() & (ndf["employee_id"].str.strip() != "")]
    ndf.reset_index(drop=True, inplace=True)
    return ndf

# ----------------------------
# Attendance summary queries
# ----------------------------
def _fetch_presence_for_employees(employee_ids: List[str], start_date: date, end_date: date) -> Dict[str, Dict[date,int]]:
    """
    Return mapping:
      { employee_id: { date_obj: presence_count (int), ... }, ... }
    presence_count will be 0 if not present / no row found.
    """
    if not employee_ids:
        return {}
    result = {}
    with SessionLocal() as db:
        try:
            q = db.query(AttendanceSummary).filter(
                AttendanceSummary.date >= start_date,
                AttendanceSummary.date <= end_date,
                AttendanceSummary.employee_id.in_(employee_ids)
            )
            rows = q.all()
        except Exception:
            logger.exception("DB fetch error in _fetch_presence_for_employees")
            rows = []

    # default zeros
    for eid in employee_ids:
        result[eid] = {}

    for r in rows:
        try:
            eid = _normalize_key(r.employee_id)
            if not eid:
                continue
            d = r.date
            present = 0
            try:
                present = int(r.presence_count or 0)
            except Exception:
                present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
            # set as 1 if >0 (we only need boolean presence per day)
            result.setdefault(eid, {})
            result[eid][d] = 1 if present > 0 else 0
        except Exception:
            continue

    # fill missing dates with 0
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            result.setdefault(eid, {})
            if cur not in result[eid]:
                result[eid][cur] = 0
        cur = cur + timedelta(days=1)

    return result

# ----------------------------
# Main comparison function
# ----------------------------
def compare_ccure_vs_sheets(
    mode: str = "full",
    stats_detail: str = "ActiveProfiles",
    limit_list: int = 200,
    export: bool = False,
    # extra optional filters (if present, will be applied)
    region_filter: Optional[str] = None,        # e.g. "APAC"
    location_city: Optional[str] = None,       # e.g. "Pune"
    location_state: Optional[str] = None,
    location_description: Optional[str] = None,
    week_ref_date: Optional[str] = None        # "YYYY-MM-DD" - week to evaluate (Mon-Fri)
) -> Dict[str, Any]:
    """
    Rich comparison API:
      - mode/stats_detail kept for compatibility
      - additional optional filters: region_filter, location_city, location_state, location_description
      - week_ref_date optional: if provided, week considered is that date's Mon-Fri; else current week
      - export: if True, writes an excel summary report to OUTPUT_DIR and returns filename in result

    Returns:
      {
        "summary": { ...counts & percentages... },
        "regular_attendance": { meeters_5_day_count, meeters_3_day_count, defaulter_count, lists... },
        "details": { "present_5_days": [...], "present_3_days": [...], "defaulters": [...] },
        "report_path": "<filename>"   # if export True
      }
    """
    try:
        # load active employees
        df = load_active_employees_dataframe()
    except Exception as e:
        logger.exception("Failed to load active employees")
        return {"error": f"active sheet load failed: {e}"}

    # normalize filters
    rf = region_filter.strip().lower() if region_filter else None
    lc = location_city.strip().lower() if location_city else None
    ls = location_state.strip().lower() if location_state else None
    ld = location_description.strip().lower() if location_description else None

    # apply filters
    sel = df.copy()
    if rf:
        sel = sel[sel["region_code"].fillna("").str.strip().str.lower() == rf]
    if lc:
        sel = sel[sel["location_city"].fillna("").str.strip().str.lower() == lc]
    if ls:
        sel = sel[sel["location_state"].fillna("").str.strip().str.lower() == ls]
    if ld:
        sel = sel[sel["location_desc"].fillna("").str.strip().str.lower() == ld]

    total_active = len(sel)
    # gather lists
    employee_ids = sel["employee_id"].astype(str).str.strip().tolist()

    # get "today" headcount by checking AttendanceSummary presence_count > 0 for date==today
    today = date.today()
    monday, friday = _week_monday_and_friday(date.fromisoformat(week_ref_date)) if week_ref_date else _week_monday_and_friday(today)

    # fetch presence for week (Mon-Fri)
    presence_map = _fetch_presence_for_employees(employee_ids, monday, friday)

    # compute today count (today might be within Mon-Fri window)
    today_count = 0
    for eid in employee_ids:
        pm = presence_map.get(eid, {})
        # presence_map stores per-day 0/1 for the Mon-Fri window; if today in map use that, otherwise fallback DB query for single date
        if today in pm:
            if pm[today] > 0:
                today_count += 1
        else:
            # fallback single-day check
            with SessionLocal() as db:
                try:
                    row = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == eid, AttendanceSummary.date == today).first()
                    if row and getattr(row, "presence_count", 0) > 0:
                        today_count += 1
                except Exception:
                    continue

    today_pct = round((today_count / float(total_active)) * 100.0, 2) if total_active > 0 else None

    # compute current status -> leave checks
    sel["on_leave"] = sel["current_status"].apply(lambda x: _maybe_mark_on_leave(x))
    leave_count = int(sel["on_leave"].sum())

    # employee type breakdown
    sel["employee_type_norm"] = sel["employee_type"].fillna("").str.strip().str.lower()
    type_counts = sel["employee_type_norm"].value_counts().to_dict()

    # Focus on Regular employees for presence checks
    regular_df = sel[sel["employee_type_norm"].str.contains("regular", na=False)]
    regular_ids = regular_df["employee_id"].astype(str).str.strip().tolist()

    # compute days present Monday-Friday per regular employee
    regular_presence = {}
    for eid in regular_ids:
        week_map = presence_map.get(eid, {})
        # consider only Mon-Fri keys from week_map (already so)
        days_present = sum(1 for d,v in week_map.items() if v and (monday <= d <= friday))
        # ensure days_present is int
        days_present = int(days_present)
        regular_presence[eid] = {
            "days_present": days_present,
            "on_leave": bool(sel[sel["employee_id"] == eid]["on_leave"].any()),
            "full_name": sel[sel["employee_id"] == eid]["full_name"].iloc[0] if not sel[sel["employee_id"] == eid].empty else None
        }

    # counts
    present_5_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 5)
    present_3_or_more_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 3)
    present_less_than_3_count = sum(1 for v in regular_presence.values() if v["days_present"] < 3)
    present_only_1_count = sum(1 for v in regular_presence.values() if v["days_present"] == 1)

    # lists (limit each to limit_list)
    present_5_list = []
    present_3_list = []
    defaulters_list = []   # those present 1 or <3 depending

    for eid, info in regular_presence.items():
        entry = {
            "employee_id": eid,
            "full_name": info["full_name"],
            "days_present": info["days_present"],
            "on_leave": info["on_leave"]
        }
        if info["days_present"] >= 5:
            present_5_list.append(entry)
        if info["days_present"] >= 3:
            present_3_list.append(entry)
        if info["days_present"] < 3:
            defaulters_list.append(entry)

    # Limit returned lists
    present_5_list = sorted(present_5_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    present_3_list = sorted(present_3_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    defaulters_list = sorted(defaulters_list, key=lambda x: (x["days_present"], x["on_leave"], x["full_name"] or ""))[:limit_list]

    # Build response
    summary = {
        "filters": {
            "region": region_filter,
            "location_city": location_city,
            "location_state": location_state,
            "location_description": location_description,
            "week_monday": monday.isoformat(),
            "week_friday": friday.isoformat()
        },
        "counts": {
            "total_active_in_sheet": total_active,
            "today_headcount_from_summary": today_count,
            "today_headcount_pct_vs_sheet": today_pct,
            "on_leave_count_in_sheet": leave_count,
            "employee_type_counts": type_counts
        },
        "regular_attendance_summary": {
            "regular_total": len(regular_ids),
            "present_5_day_count": present_5_count,
            "present_3_or_more_count": present_3_or_more_count,
            "present_less_than_3_count": present_less_than_3_count,
            "present_only_1_day_count": present_only_1_count
        }
    }

    details = {
        "present_5_days": present_5_list,
        "present_3_or_more_days": present_3_list,
        "defaulters_less_than_3_days": defaulters_list
    }

    report_path = None
    if export:
        try:
            report_name = f"attendance_compare_{(region_filter or 'all')}_{(location_city or 'all')}_{uuid.uuid4().hex[:8]}.xlsx"
            report_file = OUTPUT_DIR / report_name
            # write Excel with multiple sheets
            with pd.ExcelWriter(report_file, engine="openpyxl") as writer:
                # original selection snapshot
                sel_df_for_export = sel.copy()
                # flatten raw_row for convenience: keep as string
                sel_df_for_export["raw_row_str"] = sel_df_for_export["raw_row"].apply(lambda r: str(r) if not pd.isna(r) else "")
                sel_df_for_export.to_excel(writer, sheet_name="active_sheet_selection", index=False)
                # summary / counts as small DataFrame
                pd.DataFrame([summary["counts"]]).to_excel(writer, sheet_name="summary_counts", index=False)
                # regular attendance summary
                pd.DataFrame([summary["regular_attendance_summary"]]).to_excel(writer, sheet_name="regular_summary", index=False)
                # lists
                pd.DataFrame(details["present_5_days"]).to_excel(writer, sheet_name="present_5_days", index=False)
                pd.DataFrame(details["present_3_or_more_days"]).to_excel(writer, sheet_name="present_3_plus", index=False)
                pd.DataFrame(details["defaulters_less_than_3_days"]).to_excel(writer, sheet_name="defaulters_lt3", index=False)
            report_path = str(report_file.name)
        except Exception:
            logger.exception("Failed to write export report")
            report_path = None

    out = {
        "mode": mode,
        "stats_detail": stats_detail,
        "summary": summary,
        "details": details
    }
    if report_path:
        out["report_path"] = report_path
    return out


# small convenience test function (not run at import)
if __name__ == "__main__":
    # quick smoke test: compute for APAC Pune
    res = compare_ccure_vs_sheets(region_filter="APAC", location_city="Pune", export=False, limit_list=50)
    import json
    print(json.dumps(res, indent=2, default=str))







 # app.py (keep only /ccure/verify, removed /ccure/averages)
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import shutil
import uuid
import json
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
import re
import asyncio
from typing import Optional, Dict, Any

# --- DB / models imports (kept for endpoints that still use DB like headcount/attendance fallback) ---
from db import SessionLocal
from models import LiveSwipe, AttendanceSummary

# --- settings (optional override) ---
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"

RAW_UPLOADS_DIR = DATA_DIR / "raw_uploads"
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# ----------------- GLOBAL TIMEOUTS (UNIFY) -----------------
REGION_TIMEOUT_SECONDS = 20
COMPUTE_WAIT_TIMEOUT_SECONDS = 30
COMPUTE_SYNC_TIMEOUT_SECONDS = 60
# ----------------------------------------------------------

_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

_broadcaster_clients = set()

def broadcast_ccure_update(payload: dict):
    if not _broadcaster_clients:
        return
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = None
    for q in list(_broadcaster_clients):
        try:
            if loop and loop.is_running():
                loop.call_soon_threadsafe(q.put_nowait, payload)
            else:
                q.put_nowait(payload)
        except Exception:
            logger.exception("Failed to push payload to SSE client (will remove client)")
            try:
                _broadcaster_clients.discard(q)
            except Exception:
                pass

async def _sse_event_generator(client_queue: asyncio.Queue):
    try:
        while True:
            payload = await client_queue.get()
            try:
                data = json.dumps(payload, default=str)
            except Exception:
                data = json.dumps({"error": "serialization error", "payload": str(payload)})
            yield f"data: {data}\n\n"
    finally:
        try:
            _broadcaster_clients.discard(client_queue)
        except Exception:
            pass
        return

@app.get("/ccure/stream")
async def ccure_stream():
    q = asyncio.Queue()
    _broadcaster_clients.add(q)
    generator = _sse_event_generator(q)
    headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    return StreamingResponse(generator, media_type="text/event-stream", headers=headers)

def _guess_region_from_text(txt: str) -> str:
    if not txt:
        return "unknown"
    s = str(txt).strip().lower()
    s = re.sub(r"[,\-/()]", " ", s)
    if any(k in s for k in ("pune","quezon city","taguig city","bengaluru","hyderabad","chennai","manila","singapore","hong kong","beijing","shanghai","jakarta","kuala","osaka","tokyo","seoul","bangkok")):
        return "apac"
    if any(k in s for k in ("london","dublin","paris","frankfurt","amsterdam","stockholm","cape town","johannesburg","berlin","brussels","madrid","rome","milan")):
        return "emea"
    if any(k in s for k in ("mexico","bogota","buenos","santiago","sao","salvador","lima","caracas")):
        return "laca"
    if any(k in s for k in ("denver","new york","ny","chicago","toronto","vancouver","los angeles","san francisco","boston","houston","atlanta","miami")):
        return "namer"
    return "unknown"

@app.get("/headcount")
def api_headcount():
    try:
        totals = {"apac": 0, "emea": 0, "laca": 0, "namer": 0, "unknown": 0}
        with SessionLocal() as db:
            try:
                today = date.today()
                rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                if rows:
                    for r in rows:
                        try:
                            partition = None
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                            loc = partition or "unknown"
                            region = _guess_region_from_text(loc)
                            totals[region] = totals.get(region, 0) + 1
                        except Exception:
                            totals["unknown"] += 1
                else:
                    start = datetime.combine(today, datetime.min.time())
                    end = datetime.combine(today, datetime.max.time())
                    swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
                    for s in swipes:
                        loc = s.partition or "unknown"
                        region = _guess_region_from_text(loc)
                        totals[region] = totals.get(region, 0) + 1
            except Exception:
                logger.exception("Failed to compute headcount regions")
        out = {
            "apac": int(totals.get("apac", 0)),
            "emea": int(totals.get("emea", 0)),
            "laca": int(totals.get("laca", 0)),
            "namer": int(totals.get("namer", 0))
        }
        return JSONResponse(out)
    except Exception as exc:
        logger.exception("api_headcount failed")
        raise HTTPException(status_code=500, detail=f"headcount error: {exc}")

# ---------- Helpers retained (normalize / safe conversions) -------------
def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- build_ccure_averages (fallback) ------------------------------
def build_ccure_averages(start_date: Optional[str] = None, end_date: Optional[str] = None):
    """
    Fallback averages computation using AttendanceSummary only.
    Returns a compact shape (live_today, ccure_active, averages, notes).
    """
    try:
        def _parse_date_param(s):
            if not s:
                return None
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                try:
                    return date.fromisoformat(s)
                except Exception:
                    return None

        today = date.today()
        start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
        end_obj = _parse_date_param(end_date) if end_date else today
        if start_obj is None or end_obj is None or start_obj > end_obj:
            start_obj = today - timedelta(days=6)
            end_obj = today

        with SessionLocal() as db:
            try:
                att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
            except Exception:
                logger.exception("Failed to query AttendanceSummary")
                att_rows = []

            live_emp = 0
            live_contr = 0
            unknown_count = 0
            seen_keys = set()

            def classify_from_derived(derived):
                try:
                    if not derived or not isinstance(derived, dict):
                        return "contractor"
                    for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                        v = derived.get(k)
                        if v and "employee" in str(v).strip().lower():
                            return "employee"
                    for k in ("Employee_Status","Employee Status","Status"):
                        v = derived.get(k)
                        if v and "terminated" in str(v).strip().lower():
                            return "employee"
                    return "contractor"
                except Exception:
                    return "contractor"

            if att_rows:
                for a in att_rows:
                    key = None
                    try:
                        key = _normalize_employee_key(a.employee_id)
                    except Exception:
                        key = None
                    if not key:
                        try:
                            key = _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None)
                        except Exception:
                            key = None
                    if not key:
                        unknown_count += 1
                        continue
                    if key in seen_keys:
                        continue
                    seen_keys.add(key)
                    cls = classify_from_derived(a.derived)
                    if cls == "employee":
                        live_emp += 1
                    elif cls == "contractor":
                        live_contr += 1
                    else:
                        unknown_count += 1

                live_total_reported = live_emp + live_contr + unknown_count
                live_total_details = len(att_rows)
            else:
                live_total_reported = 0
                live_total_details = 0
                try:
                    import region_clients
                    details = []
                    try:
                        details = region_clients.fetch_all_details(timeout=REGION_TIMEOUT_SECONDS) or []
                    except Exception:
                        logger.exception("region_clients.fetch_all_details failed in build_ccure_averages()")
                        details = []
                    if details:
                        for d in details:
                            try:
                                cls = "contractor"
                                for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                                    v = d.get(k)
                                    if v and "employee" in str(v).strip().lower():
                                        cls = "employee"
                                        break
                                if cls == "employee":
                                    live_emp += 1
                                else:
                                    live_contr += 1
                                live_total_details += 1
                            except Exception:
                                continue
                        live_total_reported = live_emp + live_contr
                    else:
                        try:
                            regions = region_clients.fetch_all_regions(timeout=REGION_TIMEOUT_SECONDS) or []
                            for r in regions:
                                try:
                                    c = r.get("count")
                                    if isinstance(c, (int, float)):
                                        live_total_reported += int(c)
                                except Exception:
                                    continue
                        except Exception:
                            logger.exception("region_clients.fetch_all_regions failed in build_ccure_averages()")
                except Exception:
                    logger.exception("region_clients not importable in build_ccure_averages()")

            # compute avg range using AttendanceSummary if possible
            avg_range = None
            try:
                q = db.query(AttendanceSummary.date, AttendanceSummary.employee_id, AttendanceSummary.presence_count)\
                      .filter(AttendanceSummary.date >= start_obj, AttendanceSummary.date <= end_obj).all()
                by_date = {}
                for row in q:
                    d = row[0]
                    key = (row[1] or "").strip() if row[1] else None
                    if not key:
                        continue
                    if d not in by_date:
                        by_date[d] = set()
                    try:
                        presence_val = getattr(row, 'presence_count', row[2])
                        if int(presence_val) > 0:
                            by_date[d].add(key)
                    except Exception:
                        by_date[d].add(key)
                days_count = (end_obj - start_obj).days + 1
                daily_counts = [len(by_date.get(start_obj + timedelta(days=i), set())) for i in range(days_count)]
                if days_count and any(daily_counts):
                    avg_range = int(round(sum(daily_counts) / float(days_count)))
                else:
                    avg_range = None
            except Exception:
                logger.exception("Failed computing range average from AttendanceSummary")
                avg_range = None

        # fallback: use region history to compute avg_range if still None
        if avg_range is None:
            try:
                import region_clients
                entries = region_clients.fetch_all_history(timeout=REGION_TIMEOUT_SECONDS) or []
                agg = {}
                for e in entries:
                    try:
                        dstr = e.get("date")
                        if not dstr:
                            continue
                        region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                        emp = None
                        con = None
                        tot = None
                        if region_obj:
                            emp = _safe_int(region_obj.get("Employee"))
                            con = _safe_int(region_obj.get("Contractor"))
                            tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                        else:
                            emp = _safe_int(e.get("Employee"))
                            con = _safe_int(e.get("Contractor"))
                            tot = _safe_int(e.get("total"))
                        if emp is None and con is None:
                            continue
                        if tot is None:
                            tot = (emp or 0) + (con or 0)
                        if dstr not in agg:
                            agg[dstr] = {"total": 0, "count": 0}
                        agg[dstr]["total"] += tot or 0
                        agg[dstr]["count"] += 1
                    except Exception:
                        continue
                per_date_totals = []
                days_count = (end_obj - start_obj).days + 1
                for i in range(days_count):
                    dcheck = (start_obj + timedelta(days=i)).isoformat()
                    if dcheck in agg and agg[dcheck]["count"] > 0:
                        per_day_avg = float(agg[dcheck]["total"]) / float(agg[dcheck]["count"])
                        per_date_totals.append(per_day_avg)
                if per_date_totals:
                    avg_range = int(round(sum(per_date_totals) / float(len(per_date_totals))))
            except Exception:
                logger.exception("Failed computing avg_range from region history in fallback")

        # get ccure stats if available
        ccure_stats = {}
        try:
            import ccure_client
            if hasattr(ccure_client, "get_global_stats"):
                ccure_stats = ccure_client.get_global_stats() or {}
        except Exception:
            logger.debug("ccure_client.get_global_stats not available", exc_info=True)

        cc_active_emps = None
        cc_active_contractors = None
        try:
            if isinstance(ccure_stats, dict):
                a = ccure_stats.get("ActiveEmployees") or ccure_stats.get("active_employees") or None
                b = ccure_stats.get("ActiveContractors") or ccure_stats.get("active_contractors") or None
                if a is not None and str(a).strip() != "":
                    cc_active_emps = int(a)
                if b is not None and str(b).strip() != "":
                    cc_active_contractors = int(b)
        except Exception:
            cc_active_emps = cc_active_contractors = None

        emp_pct = None
        contr_pct = None
        overall_pct = None
        try:
            if isinstance(cc_active_emps, int) and cc_active_emps > 0:
                emp_pct = round((live_emp / float(cc_active_emps)) * 100.0, 2)
            if isinstance(cc_active_contractors, int) and cc_active_contractors > 0:
                contr_pct = round((live_contr / float(cc_active_contractors)) * 100.0, 2)
            if isinstance(cc_active_emps, int) and isinstance(cc_active_contractors, int) and (cc_active_emps + cc_active_contractors) > 0:
                overall_pct = round(((live_emp + live_contr) / float(cc_active_emps + cc_active_contractors)) * 100.0, 2)
        except Exception:
            emp_pct = contr_pct = overall_pct = None

        resp = {
            "date": today.isoformat(),
            "notes": f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}" if (start_date or end_date) else None,
            "live_today": {
                "employee": live_emp,
                "contractor": live_contr,
                "total_reported": live_total_reported,
                "total_from_details": live_total_details
            },
            "ccure_active": {
                "active_employees": cc_active_emps,
                "active_contractors": cc_active_contractors,
                "ccure_active_employees_reported": cc_active_emps,
                "ccure_active_contractors_reported": cc_active_contractors
            },
            "averages": {
                "employee_pct": emp_pct,
                "contractor_pct": contr_pct,
                "overall_pct": overall_pct,
                "avg_headcount_last_7_days": avg_range,
                "head_emp_pct_vs_ccure_today": emp_pct,
                "head_contractor_pct_vs_ccure_today": contr_pct,
                "headcount_overall_pct_vs_ccure_today": overall_pct,
                "history_avg_overall_last_7_days": avg_range
            }
        }

        return resp
    except Exception:
        logger.exception("build_ccure_averages failed")
        raise

# -------------------------
# Upload endpoints (store uploads under data/ and data/raw_uploads/ and rotate)
# -------------------------
ALLOWED_EXT = (".xls", ".xlsx", ".csv")

def _remove_old_files_for_kind(kind: str):
    """
    Remove previous canonical files and previous raw uploads that include kind in filename.
    """
    try:
        # canonical in DATA_DIR
        for p in DATA_DIR.iterdir():
            if p.is_file() and f"active_{kind}" in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old canonical file %s", p)
        # remove raw uploads for same kind to keep only latest raw (user requested)
        for p in RAW_UPLOADS_DIR.iterdir():
            if p.is_file() and kind in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old raw file %s", p)
    except Exception:
        logger.exception("error while removing old files for kind=%s", kind)

def _save_upload_and_rotate(upload_file: UploadFile, kind: str) -> dict:
    """
    Save the uploaded file as:
      - data/active_<kind>.<ext>     (canonical)
      - data/raw_uploads/<timestamp>_<kind>_<origname>  (raw trace)
    Remove previous files for same kind (both canonical & raw).
    Returns metadata dict.
    """
    fname = Path(upload_file.filename).name
    ext = Path(fname).suffix.lower()
    if ext not in ALLOWED_EXT:
        raise HTTPException(status_code=400, detail="Only .xls .xlsx .csv allowed")

    # rotate old files
    _remove_old_files_for_kind(kind)

    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    raw_name = f"{ts}_{kind}_{fname}"
    raw_path = RAW_UPLOADS_DIR / raw_name

    # Save raw
    try:
        with raw_path.open("wb") as out_f:
            shutil.copyfileobj(upload_file.file, out_f)
    finally:
        try:
            upload_file.file.close()
        except Exception:
            pass

    # Write canonical: keep same extension as original (simpler)
    canonical_path = DATA_DIR / f"active_{kind}{ext}"
    try:
        shutil.copy(raw_path, canonical_path)
    except Exception:
        logger.exception("Failed to write canonical copy for %s", canonical_path)

    info = {
        "raw_saved": str(raw_path),
        "canonical_saved": str(canonical_path),
        "original_filename": fname,
        "size_bytes": raw_path.stat().st_size if raw_path.exists() else None
    }
    logger.info("Uploaded %s: %s", kind, info)
    return info





@app.post("/api/upload/active-employees")
async def upload_active_employees_api(file: UploadFile = File(...)):
    return await upload_active_employees(file)

@app.post("/api/upload/active-contractors")
async def upload_active_contractors_api(file: UploadFile = File(...)):
    return await upload_active_contractors(file)



@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    """
    Upload Active Employee sheet:
      - stores raw to data/raw_uploads and canonical to data/active_employee.*
      - removes previous uploaded employee sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="employee")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("employee upload failed")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    """
    Upload Active Contractor sheet:
      - stores raw to data/raw_uploads and canonical to data/active_contractor.*
      - removes previous uploaded contractor sheets (raw + canonical)
      - does NOT write rows into DB
    """
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="contractor")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("contractor upload failed")
        raise HTTPException(status_code=500, detail=str(e))

# ---------- map detailed -> compact (used when compute returns detailed) ----
def _map_detailed_to_resp(detailed: Dict[str, Any]) -> Dict[str, Any]:
    # unchanged mapping from earlier implementation (kept identical to previous)
    live_h = detailed.get("live_headcount", {}) or {}
    head_h = detailed.get("headcount", {}) or {}
    ccure_active_obj = detailed.get("ccure_active", {}) or {}
    averages_obj = detailed.get("averages", {}) or {}

    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    live_employee = int(live_h.get("employee") or head_h.get("employee") or 0)
    live_contractor = int(live_h.get("contractor") or head_h.get("contractor") or 0)
    total_reported = int(
        live_h.get("currently_present_total")
        or head_h.get("total_visited_today")
        or (live_employee + live_contractor)
        or 0
    )
    total_from_details = int(head_h.get("total_visited_today") or 0)

    mapped_headcount = {
        "total_visited_today": int(head_h.get("total_visited_today") or 0),
        "employee": int(head_h.get("employee") or 0),
        "contractor": int(head_h.get("contractor") or 0),
        "by_location": head_h.get("by_location") or {}
    }

    resp = {
        "date": detailed.get("date"),
        "notes": detailed.get("notes"),
        "live_today": {
            "employee": live_employee,
            "contractor": live_contractor,
            "total_reported": total_reported,
            "total_from_details": total_from_details
        },
        "headcount": mapped_headcount,
        "live_headcount": live_h,
        "ccure_active": {
            "active_employees": ccure_active_obj.get("ccure_active_employees_reported")
                             or ccure_active_obj.get("active_employees"),
            "active_contractors": ccure_active_obj.get("ccure_active_contractors_reported")
                               or ccure_active_obj.get("active_contractors"),
            "ccure_active_employees_reported": ccure_active_obj.get("ccure_active_employees_reported"),
            "ccure_active_contractors_reported": ccure_active_obj.get("ccure_active_contractors_reported")
        },
        "averages": averages_obj
    }
    return resp

# ---------- build a verify-compatible summary from mapped payload -----------
def _build_verify_like_summary_from_mapped(mapped: Dict[str, Any], include_raw: bool = False) -> Dict[str, Any]:
    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    cc = mapped.get("ccure_active", {}) or {}
    head = mapped.get("headcount", {}) or {}
    live_head = mapped.get("live_headcount", {}) or {}
    averages = mapped.get("averages", {}) or {}

    cc_emp = to_int(cc.get("ccure_active_employees_reported") or cc.get("active_employees"))
    cc_con = to_int(cc.get("ccure_active_contractors_reported") or cc.get("active_contractors"))

    head_total = to_int(head.get("total_visited_today") or mapped.get("live_today", {}).get("total_from_details"))
    head_emp = to_int(head.get("employee") or mapped.get("live_today", {}).get("employee"))
    head_con = to_int(head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    live_total = to_int(live_head.get("currently_present_total") or mapped.get("live_today", {}).get("total_reported"))
    live_emp = to_int(live_head.get("employee") or mapped.get("live_today", {}).get("employee"))
    live_con = to_int(live_head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    history_emp_avg = averages.get("history_avg_employee_last_7_days")
    history_con_avg = averages.get("history_avg_contractor_last_7_days")
    history_overall_avg = averages.get("history_avg_overall_last_7_days")

    def pct(n, d):
        try:
            if n is None or d is None:
                return None
            if float(d) == 0:
                return None
            return round((float(n) / float(d)) * 100.0, 2)
        except Exception:
            return None

    summary = {
        "date": mapped.get("date"),
        "ccure_reported": {
            "employees": cc_emp,
            "contractors": cc_con,
            "total_reported": (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None
        },
        "headcount_attendance_summary": {
            "total_visited_today": head_total,
            "employee": head_emp,
            "contractor": head_con,
        },
        "live_headcount_region_clients": {
            "currently_present_total": live_total,
            "employee": live_emp,
            "contractor": live_con,
        },
        "percentages_vs_ccure": {
            "head_employee_pct_vs_ccure_today": pct(head_emp, cc_emp),
            "head_contractor_pct_vs_ccure_today": pct(head_con, cc_con),
            "head_overall_pct_vs_ccure_today": pct(head_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "live_employee_pct_vs_ccure_today": pct(live_emp, cc_emp),
            "live_contractor_pct_vs_ccure_today": pct(live_con, cc_con),
            "live_overall_pct_vs_ccure_today": pct(live_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "history_employee_pct_vs_ccure": pct(history_emp_avg, cc_emp),
            "history_contractor_pct_vs_ccure": pct(history_con_avg, cc_con),
            "history_overall_pct_vs_ccure": pct(history_overall_avg, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None)
        },
        "averages": {
            "history_avg_employee_last_7_days": history_emp_avg,
            "history_avg_contractor_last_7_days": history_con_avg,
            "history_avg_overall_last_7_days": history_overall_avg,
            "avg_headcount_last_7_days_db": averages.get("avg_headcount_last_7_days") or averages.get("avg_headcount_last_7_days_db"),
            "avg_headcount_per_site_last_7_days": averages.get("avg_headcount_per_site_last_7_days"),
            "employee_pct": averages.get("employee_pct"),
            "contractor_pct": averages.get("contractor_pct"),
            "overall_pct": averages.get("overall_pct"),
            **({k: v for k, v in averages.items() if k not in (
                "history_avg_employee_last_7_days",
                "history_avg_contractor_last_7_days",
                "history_avg_overall_last_7_days",
                "avg_headcount_last_7_days",
                "avg_headcount_last_7_days_db",
                "avg_headcount_per_site_last_7_days",
                "employee_pct","contractor_pct","overall_pct"
            )})
        },
        "notes": mapped.get("notes")
    }

    summary["headcount_details"] = {
        "total_visited_today": head_total,
        "employee": head_emp,
        "contractor": head_con,
        "by_location": head.get("by_location") if isinstance(head.get("by_location"), dict) else {}
    }
    summary["live_headcount_details"] = {
        "currently_present_total": live_total,
        "employee": live_emp,
        "contractor": live_con,
        "by_location": live_head.get("by_location") if isinstance(live_head.get("by_location"), dict) else {}
    }

    summary["ccure_active"] = {
        "active_employees": cc.get("active_employees") or cc.get("ccure_active_employees_reported"),
        "active_contractors": cc.get("active_contractors") or cc.get("ccure_active_contractors_reported"),
        "ccure_active_employees_reported": cc.get("ccure_active_employees_reported"),
        "ccure_active_contractors_reported": cc.get("ccure_active_contractors_reported")
    }

    return summary









# ---------- /ccure/verify (already present in your original) -----
@app.get("/ccure/verify")
def ccure_verify(
    raw: bool = Query(False, description="if true, include the raw compute payload for debugging"),
    start_date: Optional[str] = Query(None, description="YYYY-MM-DD start date (inclusive)"),
    end_date: Optional[str] = Query(None, description="YYYY-MM-DD end date (inclusive)")
):
    """
    Synchronous verification endpoint. Prefer compute_visit_averages() (synchronous call).
    If compute raises or fails, fall back to build_ccure_averages() so output shape remains consistent.
    """
    try:
        detailed = None
        try:
            from ccure_compare_service import compute_visit_averages
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
        except Exception:
            logger.exception("compute_visit_averages() failed inside /ccure/verify; falling back")
            detailed = None

        if isinstance(detailed, dict):
            mapped = _map_detailed_to_resp(detailed)
            summary = _build_verify_like_summary_from_mapped(mapped, include_raw=raw)
            if raw and isinstance(detailed, dict):
                summary["raw"] = detailed
            return JSONResponse(summary)
        else:
            fallback = build_ccure_averages(start_date, end_date)
            mapped_fallback = {
                "date": fallback.get("date"),
                "notes": fallback.get("notes"),
                "live_today": fallback.get("live_today", {}),
                "headcount": {
                    "total_visited_today": fallback.get("live_today", {}).get("total_from_details") or fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "live_headcount": {
                    "currently_present_total": fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "ccure_active": fallback.get("ccure_active", {}),
                "averages": fallback.get("averages", {})
            }

            summary = _build_verify_like_summary_from_mapped(mapped_fallback, include_raw=raw)
            if raw:
                summary["raw"] = mapped_fallback
       
            return JSONResponse(summary)
    except Exception as e:
        logger.exception("ccure_verify failed")
        raise HTTPException(status_code=500, detail=f"ccure verify error: {e}")

# ---------- /ccure/compare -> uses data_compare_service.compare_ccure_vs_sheets ----------
@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from data_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)




@app.get("/ccure/export_uploaded")
def ccure_export_uploaded():
    """
    Export the currently uploaded canonical sheets into one workbook.
    Returns JSON: { status: "ok", report_path: "<filename>" }
    Downloadable at /ccure/report/{filename}
    """
    try:
        from data_compare_service import export_uploaded_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed for export_uploaded")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    fname = export_uploaded_sheets()
    if not fname:
        raise HTTPException(status_code=500, detail="failed to export uploaded sheets")
    return JSONResponse({"status": "ok", "report_path": fname})



@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = OUTPUT_DIR / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(str(full),
                            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            filename=safe_name)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")
    




    

# End of app.py





