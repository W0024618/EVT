# backend/duration_report.py
from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name (override with environment variable if needed)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
try:
    from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    # fallback — keep behaviour if config file unavailable
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

    def map_door_to_zone(door: object, direction: object = None) -> str:
        """
        Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
        (This is only used if config.door_zone can't be imported.)
        """
        try:
            if door is None:
                return None
            s = str(door).strip()
            if not s:
                return None
            s_l = s.lower()
            # fallback: direction-based inference
            if direction and isinstance(direction, str):
                d = direction.strip().lower()
                if "out" in d:
                    return OUT_OF_OFFICE_ZONE
                if "in" in d:
                    # assume reception/working
                    return "Reception Area"
            # heuristic fallback
            if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
                return OUT_OF_OFFICE_ZONE
            # else treat as working area
            return "Working Area"
        except Exception:
            return None

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "SG.Singapore", "PH.Manila","IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023"
        ],
        "partitions": ["LT.Vilnius","IT.Rome","UK.London","IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition","PA.Panama City","PE.Lima", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        # NOTE: these should match the PartitionName2 values used in DB query WHERE clause.
        # keep as DB-side partition names (example provided earlier used things like 'US.CO.OBS', 'USA/Canada Default', etc).
        "partitions": ["US.CO.OBS", "USA/Canada Default", "US.FL.Miami", "US.NYC"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]  # kept for reference but not used for filtering now
    }
}


# --- Note: Added AdjustedMessageTime into the generic template, date condition templated as {date_condition}
# Also added a hard filter: only PersonnelType = 'Employee' (push filtering to DB).
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND t3.[Name] = 'Employee'
  {date_condition}
  {region_filter}
"""

# Helpers (must exist before build_region_query)
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        # updated: use explicit PartitionName2 list for NAMER (matches the SQL example you provided)
        partitions = rc.get("partitions", [])
        if partitions:
            parts_sql = ", ".join(f"'{p}'" for p in partitions)
            region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
        else:
            # fallback to the old logical_like behaviour (kept for safety)
            likes = rc.get("logical_like", [])
            if likes:
                like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
                region_filter = f"AND ({like_sql})"
            else:
                region_filter = ""
    else:
        region_filter = ""

    # Build date_condition:
    # - For APAC we must fetch LocaleMessageTime rows for target_date AND target_date + 1 day
    #   because AdjustedMessageTime = LocaleMessageTime - 2 hours; some swipes near midnight on next day
    #   belong to the shifted date group.
    if region_key == "apac":
        next_date_str = (target_date + timedelta(days=1)).strftime("%Y-%m-%d")
        date_condition = (
            "AND (CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d1}' "
            "OR CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d2}')"
            .format(d1=date_str, d2=next_date_str)
        )
    else:
        date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql
    
# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    # use first database in list if present
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
    "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
    "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
    "Direction", "CompanyName", "PrimaryLocation"
]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    # ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Dates parsing
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    try:
        if "AdjustedMessageTime" in df.columns:
            df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
        else:
            df["AdjustedMessageTime"] = pd.NaT
    except Exception:
        df["AdjustedMessageTime"] = pd.to_datetime(df.get("AdjustedMessageTime").astype(str), errors="coerce") if "AdjustedMessageTime" in df.columns else pd.NaT

    # defensive: make text fields strings (avoid object type surprises)
    for tcol in ("Door", "PartitionName2", "PersonnelTypeName", "EmployeeName", "CompanyName", "PrimaryLocation"):
        if tcol in df.columns:
            df[tcol] = df[tcol].fillna("").astype(str)

    # Filter: only Employees (defensive; the SQL template already requests t3.Name = 'Employee')
    try:
        if "PersonnelTypeName" in df.columns:
            df = df[df["PersonnelTypeName"].str.strip().str.lower() == "employee"].copy()
    except Exception:
        # If PersonnelTypeName missing or unexpected, keep df as-is but log
        logging.debug("Could not apply PersonnelTypeName filter for region %s", region_key)

    # maintain person_uid same as compute logic
    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined if joined else None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)

    # ----- REGION SPECIFIC NORMALIZATIONS -----
    # APAC: map PartitionName2/Door patterns to friendly names:
    if region_key == "apac" and not df.empty:
        def normalize_apac_partition(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part or ""
            # Door-based mappings
            if "APAC_PI" in d or d.startswith("APAC_PI"):
                return "Taguig City"
            if "APAC_PH" in d or d.startswith("APAC_PH"):
                return "Quezon City"
            # Pune detection (APAC.Default or PUN in door/partition)
            if "PUN" in d or "PUNE" in d or ("APAC.DEFAULT" in p.upper() or p.upper().strip() == "APAC.DEFAULT"):
                return "Pune"
            if "APAC_MY" in d or "MY.KUALA" in p.upper() or "KUALA" in d:
                return "MY.Kuala Lumpur"
            if "IN.HYD" in p.upper() or "HYD" in d:
                return "IN.HYD"
            if "SG.SINGAPORE" in p or "SINGAPORE" in d:
                return "SG.Singapore"
            # fallback to existing PartitionName2
            return part
        df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)

    # NAMER: normalize PartitionName2 and add LogicalLocation per the SQL example you provided
    if region_key == "namer" and not df.empty:
        def namer_partition_and_logical(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()
            normalized = part
            logical = "Other"

            if ("US.CO.HQ" in d) or ("HQ" in d and "HQ" in d[:20]) or ("DENVER" in d) or (p == "US.CO.OBS"):
                normalized = "US.CO.OBS"
                logical = "Denver-HQ"
            elif "AUSTIN" in d or "AUSTIN TX" in d or p == "USA/CANADA DEFAULT":
                normalized = "USA/Canada Default"
                logical = "Austin Texas"
            elif "MIAMI" in d or p == "US.FL.MIAMI":
                normalized = "US.FL.Miami"
                logical = "Miami"
            elif "NYC" in d or "NEW YORK" in d or p == "US.NYC":
                normalized = "US.NYC"
                logical = "New York"
            else:
                # fallback mappings for common PartitionName2 values
                if p == "US.CO.OBS":
                    normalized = "US.CO.OBS"; logical = "Denver-HQ"
                elif p == "USA/CANADA DEFAULT":
                    normalized = "USA/Canada Default"; logical = "Austin Texas"
                elif p == "US.FL.MIAMI":
                    normalized = "US.FL.Miami"; logical = "Miami"
                elif p == "US.NYC":
                    normalized = "US.NYC"; logical = "New York"
                else:
                    normalized = part
                    logical = "Other"
            return pd.Series({"PartitionName2": normalized, "LogicalLocation": logical})

        mapped = df.apply(namer_partition_and_logical, axis=1)
        df["PartitionName2"] = mapped["PartitionName2"].astype(str)
        df["LogicalLocation"] = mapped["LogicalLocation"].astype(str)

    # ensure PartitionName2 column exists as string
    if "PartitionName2" not in df.columns:
        df["PartitionName2"] = ""

    # ensure LogicalLocation exists (maybe empty for non-NAMER rows)
    if "LogicalLocation" not in df.columns:
        df["LogicalLocation"] = ""

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]


# ---------------------------------------------------------------------
# compute_daily_durations
# (restored implementation — used by run_for_date and trend_runner)
# ---------------------------------------------------------------------
# def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "AdjustedMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes if present
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    if "AdjustedMessageTime" in df.columns and df["AdjustedMessageTime"].dtype == object:
        df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")

    # drop exact duplicates (defensive)
    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    try:
        df = df.drop_duplicates(subset=dedupe_cols, keep="first")
    except Exception:
        # if columns not present as expected, fallback to no-dedupe
        pass

    # Date assignment:
    # - For Pune (PartitionName2 == 'APAC.Default') and if AdjustedMessageTime exists, use adjusted date (shifted date)
    # - Otherwise use LocaleMessageTime.date()
    try:
        df["Date"] = df["LocaleMessageTime"].dt.date
        mask = (df.get("PartitionName2") == "APAC.Default") & (pd.notna(df.get("AdjustedMessageTime")))
        if mask.any():
            df.loc[mask, "Date"] = df.loc[mask, "AdjustedMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure a person_uid column (compat with other code)
    df["person_uid"] = df.apply(
        lambda row: row.get("person_uid")
        if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
        else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
        axis=1
    )
    df = df[df["person_uid"].notna()].copy()

    # Group and aggregate
    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", "first"),
            EmployeeName=("EmployeeName", "first"),
            CardNumber=("CardNumber", "first"),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        # fallback groupby implementation
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": first.get("EmployeeID"),
                "EmployeeName": first.get("EmployeeName"),
                "CardNumber": first.get("CardNumber"),
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(
        lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    )

    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    return grouped[out_cols]

def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "AdjustedMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes if present
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    if "AdjustedMessageTime" in df.columns and df["AdjustedMessageTime"].dtype == object:
        df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")

    # drop exact duplicates (defensive)
    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    try:
        df = df.drop_duplicates(subset=dedupe_cols, keep="first")
    except Exception:
        pass

    # Date assignment:
    # - For Pune rows we want to apply the AdjustedMessageTime (2:00 AM shifted boundary).
    # - Detect Pune robustly (PartitionName2 or PrimaryLocation or Door containing 'pun'/'pune').
    try:
        # default: use LocaleMessageTime date
        df["Date"] = df["LocaleMessageTime"].dt.date

        # detect Pune rows
        def _is_pune_row(r):
            try:
                part = str(r.get("PartitionName2") or "").strip().lower()
                ploc = str(r.get("PrimaryLocation") or "").strip().lower()
                door = str(r.get("Door") or "").strip().lower()
                if "pun" in part or "pune" in part:
                    return True
                if "pun" in ploc or "pune" in ploc:
                    return True
                if "pun" in door or "pune" in door:
                    return True
                return False
            except Exception:
                return False

        if "AdjustedMessageTime" in df.columns:
            mask_pune = df.apply(_is_pune_row, axis=1) & df["AdjustedMessageTime"].notna()
            if mask_pune.any():
                df.loc[mask_pune, "Date"] = df.loc[mask_pune, "AdjustedMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure a person_uid column (compat with other code)
    df["person_uid"] = df.apply(
        lambda row: row.get("person_uid")
        if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
        else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
        axis=1
    )
    df = df[df["person_uid"].notna()].copy()

    # Group and aggregate
    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", "first"),
            EmployeeName=("EmployeeName", "first"),
            CardNumber=("CardNumber", "first"),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": first.get("EmployeeID"),
                "EmployeeName": first.get("EmployeeName"),
                "CardNumber": first.get("CardNumber"),
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(
        lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    )

    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    return grouped[out_cols]

# ---------------------------------------------------------------------
# run_for_date (already present in your newer file; kept and wired to compute_daily_durations)
# ---------------------------------------------------------------------
def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    """
    Fetch swipes for each region, compute durations and write CSV files.
    Returns a dict keyed by lower-case region name:
      { 'apac': {'swipes': DataFrame, 'durations': DataFrame}, ... }
    """
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        if not r:
            continue
        rkey = r.lower()
        if rkey not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", rkey, target_date)
        try:
            swipes = fetch_swipes_for_region(rkey, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", r)
            swipes = pd.DataFrame()

      
      
      
        # optional city filter (defensive): accept common variants (spaces / hyphens / dots / underscores / title-case)
        if city and not swipes.empty:
            city_raw = str(city).strip()
            city_l = city_raw.lower()
            # build a small set of tokens/variants to try matching against text columns
            alt_tokens = set()
            alt_tokens.add(city_l)
            alt_tokens.add(city_l.replace('-', ' '))
            alt_tokens.add(city_l.replace('_', ' '))
            alt_tokens.add(city_l.replace('.', ' '))
            alt_tokens.add(city_l.replace('-', ' ').replace('.', ' '))
            alt_tokens.add(city_l.replace(' ', '-'))
            # add a stripped/simple token (no punctuation) to catch compact forms
            import re
            simple = re.sub(r'[^a-z0-9]', '', city_l)
            if simple:
                alt_tokens.add(simple)
            # also add Title case versions (useful for PartitionName2 exact-like values)
            alt_tokens.update({t.title() for t in list(alt_tokens)})

            # New: normalize-match PartitionName2 exactly (ignoring punctuation/case) to avoid partial collisions.
            def _norm_for_cmp(s):
                try:
                    if s is None:
                        return ''
                    return re.sub(r'[^a-z0-9]', '', str(s).strip().lower())
                except Exception:
                    return ''

            mask = pd.Series(False, index=swipes.index)
            for t in alt_tokens:
                try:
                    t_norm = _norm_for_cmp(t)
                    if not t_norm:
                        continue

                    # PartitionName2: exact normalized equality (strong match)
                    if "PartitionName2" in swipes.columns:
                        try:
                            part_norm = swipes["PartitionName2"].fillna("").astype(str).str.lower().apply(_norm_for_cmp)
                            mask = mask | (part_norm == t_norm)
                        except Exception:
                            pass

                    # PrimaryLocation: normalized contains (so 'Vilnius Office' matches 'Vilnius')
                    if "PrimaryLocation" in swipes.columns:
                        try:
                            pl_norm = swipes["PrimaryLocation"].fillna("").astype(str).str.lower().apply(_norm_for_cmp)
                            # contains on normalized forms (safe)
                            mask = mask | pl_norm.str.contains(t_norm, na=False)
                        except Exception:
                            pass

                    # Door and EmployeeName: keep permissive contains (user-friendly free text)
                    for col in ("Door", "EmployeeName"):
                        if col in swipes.columns:
                            try:
                                mask = mask | swipes[col].fillna("").astype(str).str.lower().str.contains(str(t).lower(), na=False)
                            except Exception:
                                continue
                except Exception:
                    continue

            if mask.any():
                before = len(swipes)
                swipes = swipes[mask].copy()
                logging.info("City filter '%s' applied for region %s: rows before=%d after=%d (tokens=%s)", city_raw, rkey, before, len(swipes), list(alt_tokens)[:10])
            else:
                logging.info("City filter '%s' applied for region %s: rows before=%d after=%d (no matches) (tokens=%s)", city_raw, rkey, len(swipes), 0, list(alt_tokens)[:10])







        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", rkey)
            durations = pd.DataFrame()

        # write outputs for visibility/compatibility with existing pipeline
        try:
            csv_path = outdir_path / f"{rkey}_duration_{target_date.strftime('%Y%m%d')}.csv"
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", rkey)
        try:
            swipes_csv_path = outdir_path / f"{rkey}_swipes_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", rkey)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", rkey, csv_path if 'csv_path' in locals() else '<unknown>', len(durations) if durations is not None else 0)
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", rkey, swipes_csv_path if 'swipes_csv_path' in locals() else '<unknown>', len(swipes) if swipes is not None else 0)

        results[rkey] = {"swipes": swipes, "durations": durations}

    return results


# end of file





