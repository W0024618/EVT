# backend/app.py
from flask import Flask, jsonify, request, send_from_directory, send_file
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re
import io
import base64
import os
import difflib
from typing import Optional, List, Dict, Any

from duration_report import REGION_CONFIG
from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR, read_90day_cache, compute_violation_days_map
from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE

# ---------- Ensure outputs directory exists early (so OVERRIDES_FILE can be defined safely) ----------
BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)

OVERRIDES_FILE = DEFAULT_OUTDIR / "overrides.csv"

# near top of file (add helper)
def _slug_city(city: str) -> str:
    if not city:
        return "pune"
    return str(city).strip().lower().replace(" ", "_")


def _load_overrides():
    if not OVERRIDES_FILE.exists():
        return {}
    try:
        df = pd.read_csv(OVERRIDES_FILE, dtype=str)
        out = {}
        for _, r in df.iterrows():
            emp = str(r.get('EmployeeID') or r.get('person_uid') or '').strip()
            if not emp:
                continue
            out[emp] = {
                'level': str(r.get('OverrideLevel') or '').strip(),
                'reason': str(r.get('Reason') or '').strip(),
                'ts': str(r.get('Timestamp') or '').strip()
            }
        return out
    except Exception:
        logging.exception("Failed reading overrides file")
        return {}

def _save_override(employee_key, level, reason):
    now = datetime.now().isoformat()
    row = {'EmployeeID': employee_key, 'OverrideLevel': level, 'Reason': reason or '', 'Timestamp': now}
    try:
        if OVERRIDES_FILE.exists():
            df = pd.read_csv(OVERRIDES_FILE, dtype=str)
            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        else:
            df = pd.DataFrame([row])
        df.to_csv(OVERRIDES_FILE, index=False)
        return True
    except Exception:
        logging.exception("Failed to save override")
        return False

# ODBC driver (keep existing env-based driver)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Backoff / connector helper for ACVSCore (restored so image lookups work)
_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20

def _get_acvscore_conn():
    """
    Try to connect to ACVSCore using REGION_CONFIG or ACVSCORE_DB_CONFIG fallback.
    Returns first successful pyodbc connection or None.
    """
    try:
        import pyodbc
    except Exception:
        logging.exception("pyodbc not installed; ACVSCore lookups unavailable.")
        return None

    # basic backoff: skip attempts if we just failed recently
    from datetime import datetime
    now = datetime.now().timestamp()
    last = _acvscore_backoff.get("ts")
    if last and _acvscore_backoff.get("failed") and (now - last) < _ACVSCORE_BACKOFF_SECONDS:
        logging.debug("Skipping ACVSCore connection attempt (backoff active).")
        return None

    tried = []
    for region_key, rc in (REGION_CONFIG or {}).items():
        server = rc.get("server")
        user = rc.get("user")
        pwd = rc.get("password")
        if not server:
            continue

        # Try SQL auth first if credentials present
        if user and pwd:
            tried.append(f"{region_key}@{server}(sql)")
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={server};DATABASE=ACVSCore;UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;"
            )
            try:
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (sql auth).", server, region_key)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("SQL auth to %s failed: %s", server, e)

        # Try Trusted Connection fallback on same server
        tried.append(f"{region_key}@{server}(trusted)")
        conn_str_trusted = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={server};DATABASE=ACVSCore;Trusted_Connection=yes;"
            "TrustServerCertificate=Yes;"
        )
        try:
            conn = pyodbc.connect(conn_str_trusted, autocommit=True, timeout=5)
            logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (trusted connection).", server, region_key)
            _acvscore_backoff["ts"] = None
            _acvscore_backoff["failed"] = False
            return conn
        except Exception as e:
            logging.debug("Trusted connection to %s failed: %s", server, e)
            continue

    # Fallback: if a global ACVSCORE_DB_CONFIG exists, try it (safe check)
    try:
        if 'ACVSCORE_DB_CONFIG' in globals() and isinstance(globals().get('ACVSCORE_DB_CONFIG'), dict):
            cfg = globals().get('ACVSCORE_DB_CONFIG')
            server = cfg.get('server')
            user = cfg.get('user')
            pwd = cfg.get('password')
            database = cfg.get('database', 'ACVSCore')
            tried.append(f"ACVSCORE_DB_CONFIG@{server}")
            try:
                conn_str = (
                    f"DRIVER={{{ODBC_DRIVER}}};"
                    f"SERVER={server};DATABASE={database};UID={user};PWD={pwd};"
                    "TrustServerCertificate=Yes;"
                )
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore using ACVSCORE_DB_CONFIG (%s).", server)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("ACVSCORE_DB_CONFIG connection failed: %s", e)
    except Exception:
        logging.debug("ACVSCORE_DB_CONFIG fallback not available or failed.")

    # record failure to backoff
    _acvscore_backoff["ts"] = now
    _acvscore_backoff["failed"] = True
    logging.error("Failed to connect to ACVSCore using REGION_CONFIG servers. Tried: %s", tried)
    return None

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data

def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    """
    Try to resolve personnel record using a flexible lookup.
    Returns dict with keys: ObjectID (may be None), GUID, Name, EmailAddress, ManagerEmail
    If resolution fails returns empty dict.
    """
    out: Dict[str, Any] = {}
    if candidate_identifier is None:
        return out
    try:
        conn = _get_acvscore_conn()
    except Exception:
        conn = None
    if conn is None:
        return out

    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        params = (cand, cand, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()
        if row:
            try:
                out['ObjectID'] = row[0]
                out['GUID'] = row[1]
                out['Name'] = row[2]
                out['EmailAddress'] = row[3]
                out['ManagerEmail'] = row[4]
            except Exception:
                out = {
                    'ObjectID': row[0] if len(row) > 0 else None,
                    'GUID': row[1] if len(row) > 1 else None,
                    'Name': row[2] if len(row) > 2 else None,
                    'EmailAddress': row[3] if len(row) > 3 else None,
                    'ManagerEmail': row[4] if len(row) > 4 else None
                }
    except Exception:
        logging.exception("Failed personnel lookup for candidate: %s", candidate_identifier)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out

def get_person_image_bytes(parent_id) -> Optional[bytes]:
    """
    Query ACVSCore.Access.Images for top image where ParentId = parent_id and return raw bytes.
    Returns None if not found or on error.
    """
    try:
        conn = _get_acvscore_conn()
    except Exception:
        conn = None
    if conn is None:
        return None
    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 AI.Image
            FROM ACVSCore.Access.Images AI
            WHERE AI.ParentId = ?
              AND DATALENGTH(AI.Image) > 0
            ORDER BY AI.ObjectID DESC
        """
        cur.execute(sql, (str(parent_id),))
        row = cur.fetchone()
        if row and row[0] is not None:
            try:
                b = bytes(row[0])
                return b
            except Exception:
                return row[0]
    except Exception:
        logging.exception("Failed to fetch image for ParentId=%s", parent_id)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass
    return None


# ---------- New route to serve employee image ----------
# We'll import send_file earlier where used; define route after app created.

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

# optional import; used for styling
try:
    from openpyxl import load_workbook
    from openpyxl.styles import Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        return x
    try:
        return str(x)
    except Exception:
        return None

_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False

# Helper: format seconds to HH:MM:SS
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return None
        s = int(float(seconds))
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return None

_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _replace_placeholder_strings(df: pd.DataFrame) -> pd.DataFrame:
    """
    Replace common placeholder literal strings (e.g. 'nan', 'None', 'null', '-', 'n/a') in object/string columns with None.
    This prevents 'nan' strings showing up in JSON and ensures downstream logic treats them as missing values.
    Operates in-place on object columns and returns the dataframe (for chaining).
    """
    if df is None or df.empty:
        return df
    # operate only on object (string) columns to avoid corrupting numeric columns
    for col in df.columns:
        try:
            if df[col].dtype == object:
                df[col] = df[col].apply(lambda x: None if (isinstance(x, str) and x.strip().lower() in _PLACEHOLDER_STRS) else x)
        except Exception:
            # defensive: skip columns that error
            continue
    return df

_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None

def _resolve_field_from_record(record: dict, candidate_tokens: list):
    if record is None:
        return None

    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)

    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)

    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue

    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)

    return None

def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    if df is None or df.empty:
        return []
    df = df.copy()

    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # convert NaN -> None
    df = df.where(pd.notnull(df), None)

    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        if not emp_id:
            emp_tokens = ['Int1', 'Text12', 'EmployeeID', 'empid', 'id']
            resolved_emp = _resolve_field_from_record(r, emp_tokens)
            if resolved_emp is not None:
                try:
                    s = str(resolved_emp).strip()
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    if _looks_like_guid(s):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = s
                        emp_id = s
                except Exception:
                    if _looks_like_guid(resolved_emp):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = resolved_emp
                        emp_id = resolved_emp

        if out.get('CardNumber') in (None, '', 'nan'):
            card_tokens = ['CardNumber', 'CHUID', 'Card', 'card_no', 'cardnum']
            resolved_card = _resolve_field_from_record(r, card_tokens)
            if resolved_card is not None:
                try:
                    cs = str(resolved_card).strip()
                    if _looks_like_guid(cs) or _is_placeholder_str(cs):
                        out['CardNumber'] = None
                    else:
                        out['CardNumber'] = cs
                except Exception:
                    out['CardNumber'] = None

        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None
        if 'CardNumber' in out and isinstance(out['CardNumber'], str) and _looks_like_guid(out['CardNumber']):
            out['CardNumber'] = None

        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            if emp_id not in (None, '', 'nan') and not _looks_like_guid(emp_id):
                out['EmployeeName'] = str(emp_id)
            else:
                out['EmployeeName'] = None

        cleaned.append(out)
    return cleaned

@app.route('/')
def root():
    return "Trend Analysis API — Pune test"

# ---------------- Single run endpoint (merged & deduped) ----------------
@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    """
    Run trend for one or more dates.
    Accepts:
      - GET: query params
      - POST JSON: body params
    Params:
      - date=YYYY-MM-DD  (single day)
      - start=YYYY-MM-DD & end=YYYY-MM-DD (range inclusive)
      - regions=apac,emea  (comma/pipe/semicolon separated)
      - city / site / site_name  (optional)
    Returns a compact JSON summary with sample rows.
    """
    # --- parse params robustly (prevent NameError) ---
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        # POST
        if request.is_json:
            try:
                params = request.get_json(force=True) or {}
            except Exception:
                params = {}
        else:
            # form-encoded fallback
            try:
                params = request.form.to_dict() or {}
            except Exception:
                params = {}

    # parse dates
    date_str = (params.get('date') or params.get('Date') or '').strip() or None
    start_str = (params.get('start') or params.get('Start') or '').strip() or None
    end_str = (params.get('end') or params.get('End') or '').strip() or None

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            # default to yesterday and today
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    # parse regions param (comma/pipe/semicolon separated)
    regions_param = params.get('regions') or params.get('region') or ''
    if regions_param:
        regions = [r.strip().lower() for r in re.split(r'[;,|]', str(regions_param)) if r.strip()]
    else:
        # default to keys from REGION_CONFIG if available
        try:
            regions = [k.lower() for k in list(REGION_CONFIG.keys())]
        except Exception:
            regions = ['apac']

    # validate against REGION_CONFIG (silently skip unknown)
    valid_regions = []
    for r in regions:
        if r in (REGION_CONFIG or {}):
            valid_regions.append(r)
        else:
            logging.debug("Requested region '%s' not in REGION_CONFIG - skipping", r)
    if not valid_regions:
        valid_regions = [k.lower() for k in REGION_CONFIG.keys()] if REGION_CONFIG else ['apac']

    params['_regions_to_run'] = valid_regions

    # city / site param
    city_param = params.get('city') or params.get('site') or params.get('site_name') or 'pune'
    city_slug = _slug_city(city_param)
    params['_city'] = city_slug

    combined_rows = []
    files = []

    # Run per-date pipeline
    for d in dates:
        try:
            # prefer new signature with regions + city
            try:
                df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug)
            except TypeError:
                # fallback: older signature may not accept regions/city
                try:
                    df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
                except TypeError:
                    # final fallback to duration_report.run_for_date then combine durations
                    logging.info("run_trend_for_date signature mismatch; falling back to duration_report.run_for_date")
                    from duration_report import run_for_date as _dr_run_for_date
                    region_results = _dr_run_for_date(d, valid_regions, str(DEFAULT_OUTDIR), city_param)
                    combined_list = []
                    for rkey, res in (region_results or {}).items():
                        try:
                            df_dur = res.get('durations')
                            if df_dur is not None and not df_dur.empty:
                                combined_list.append(df_dur)
                        except Exception:
                            continue
                    df = pd.concat(combined_list, ignore_index=True) if combined_list else pd.DataFrame()
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        # collect possible output CSV name(s) for UI visibility
        csv_path = DEFAULT_OUTDIR / f"trend_{city_slug}_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        # accept DataFrame-like results
        if df is None or (hasattr(df, 'empty') and df.empty):
            continue

        # normalize placeholder literal strings
        try:
            df = _replace_placeholder_strings(df)
        except Exception:
            pass

        # ensure minimal expected columns for downstream
        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        combined_rows.append(df)

    combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()
    # normalize placeholders again defensively
    combined_df = _replace_placeholder_strings(combined_df)

    # compute some basic metrics
    try:
        if not combined_df.empty:
            if 'person_uid' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['person_uid'].dropna().astype(str).nunique())
            elif 'EmployeeID' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['EmployeeID'].dropna().astype(str).nunique())
            else:
                raw_unique_person_uids = int(len(combined_df))
        else:
            raw_unique_person_uids = 0
    except Exception:
        raw_unique_person_uids = int(len(combined_df)) if combined_df is not None else 0

    try:
        if not combined_df.empty and 'IsFlagged' in combined_df.columns:
            flagged_df = combined_df[combined_df['IsFlagged'] == True].copy()
        else:
            flagged_df = pd.DataFrame()
    except Exception:
        flagged_df = pd.DataFrame()

    try:
        analysis_count = int(raw_unique_person_uids)
    except Exception:
        analysis_count = int(len(combined_df)) if combined_df is not None else 0

    try:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = float((flagged_count / analysis_count * 100.0) if analysis_count and analysis_count > 0 else 0.0)
    except Exception:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = 0.0

    # sample for UI
    try:
        sample_source = flagged_df if not flagged_df.empty else combined_df
        samples = _clean_sample_df(sample_source.head(10), max_rows=10) if sample_source is not None and not sample_source.empty else []
    except Exception:
        samples = []

    # compute reasons_count and risk_counts (previously left empty)
    reasons_count = {}
    try:
        if not flagged_df.empty and 'Reasons' in flagged_df.columns:
            for v in flagged_df['Reasons'].dropna().astype(str):
                for part in re.split(r'[;,\|]', v):
                    key = part.strip()
                    if key and not _is_placeholder_str(key):
                        reasons_count[key] = reasons_count.get(key, 0) + 1
    except Exception:
        logging.exception("Failed computing reasons_count for response")
        reasons_count = {}

    risk_counts = { "Low":0, "Low Medium":0, "Medium":0, "Medium High":0, "High":0 }
    try:
        if not flagged_df.empty and 'RiskLevel' in flagged_df.columns:
            for v in flagged_df['RiskLevel'].fillna('').astype(str):
                if v:
                    risk_counts[v] = risk_counts.get(v, 0) + 1
    except Exception:
        logging.exception("Failed computing risk_counts for response")
        risk_counts = { "Low":0, "Low Medium":0, "Medium":0, "Medium High":0, "High":0 }

    # flagged persons cleaned (for UI)
    try:
        flagged_persons_cleaned = _clean_sample_df(flagged_df, max_rows=10) if (flagged_df is not None and not flagged_df.empty) else (samples if samples else [])
    except Exception:
        flagged_persons_cleaned = samples if samples else []

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "aggregated_rows_total_raw": int(len(combined_df)),
        "aggregated_unique_persons": int(analysis_count),
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "flagged_rate_percent": float(flagged_rate_pct),
        "files": files,
        "sample": (samples[:10] if isinstance(samples, list) else samples),
        "reasons_count": reasons_count,
        "risk_counts": risk_counts,
        "flagged_persons": (flagged_persons_cleaned[:10] if isinstance(flagged_persons_cleaned, list) else flagged_persons_cleaned),
        "_raw_unique_person_uids": int(raw_unique_person_uids),
        "regions_run": params.get('_regions_to_run', []),
        "city_used": city_slug
    }

    return jsonify(resp)


@app.route('/latest', methods=['GET'])
def latest_results():
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    # fallback: if no city-specific files, try the generic trend_*.csv (helps backwards-compat)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]

    # try to infer the date from the filename (trend_pune_YYYYMMDD.csv)
    start_date_iso = None
    end_date_iso = None
    try:
        m = re.search(r'(\d{8})', latest.name)
        if m:
            ymd = m.group(1)
            dt = datetime.strptime(ymd, "%Y%m%d").date()
            start_date_iso = dt.isoformat()
            end_date_iso = dt.isoformat()
    except Exception:
        start_date_iso = None
        end_date_iso = None

    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)

    # normalize placeholder strings
    df = _replace_placeholder_strings(df)

    # try to compute unique persons (use same id preference as run_trend)
    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in df.columns), None)

    def _norm_val_for_latest(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                fv = float(s)
                if math.isfinite(fv) and fv.is_integer():
                    s = str(int(fv))
        except Exception:
            pass
        return s

    if id_col is None:
        unique_persons = int(len(df))
    else:
        ids_series = df[id_col].apply(_norm_val_for_latest) if id_col in df.columns else pd.Series([None]*len(df))
        if id_col != 'person_uid' and 'person_uid' in df.columns:
            ids_series = ids_series.fillna(df['person_uid'].astype(str).replace('nan','').replace('None',''))
        unique_persons = int(len(set([x for x in ids_series.unique() if x])))

    sample = _clean_sample_df(df, max_rows=5)
    resp = {
        "file": latest.name,
        "rows_raw": int(len(df)),
        "rows": unique_persons,
        "sample": sample,
        # new: what date the latest file represents (helpful for frontend)
        "start_date": start_date_iso,
        "end_date": end_date_iso,
        "city": city_slug
    }
    return jsonify(resp)


# ------------------------------
# RESTORE /record endpoint (full implementation)
# ------------------------------
# This was previously replaced by '...' in your file which removed all evidence/timeline functionality.
# The implementation below:
#  - reads all trend CSVs (for the site)
#  - finds aggregated rows that match employee_id/person_uid
#  - builds cleaned aggregated_rows (with personnel email/image info)
#  - searches raw swipe CSVs (swipes_*.csv) intelligently and returns a deduped timeline
#  - attaches ViolationDays90 from past trend CSVs (uses compute_violation_days_map)
@app.route('/record', methods=['GET'])
def get_record():
    q = request.args.get('employee_id') or request.args.get('person_uid')
    include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        # fallback to any trend_* in outputs (backwards compat)
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    # Read all CSVs (concat) so /record will search previous days too
    df_list = []
    for fp in csvs:
        try:
            tmp = pd.read_csv(fp, parse_dates=['FirstSwipe','LastSwipe'])
        except Exception:
            try:
                tmp = pd.read_csv(fp, dtype=str)
            except Exception:
                continue
        df_list.append(tmp)
    if not df_list:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200
    df = pd.concat(df_list, ignore_index=True)

    # normalize placeholder literal strings
    df = _replace_placeholder_strings(df)

    if q is None:
        cleaned = _clean_sample_df(df, max_rows=10)
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    q_str = str(q).strip()

    def normalize_series(s):
        if s is None:
            return pd.Series([''] * len(df))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if math.isfinite(fv) and fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    found_mask = pd.Series(False, index=df.index)

    if 'EmployeeID' in df.columns:
        emp_series = normalize_series(df['EmployeeID'])
        found_mask = found_mask | (emp_series == q_str)

    if 'person_uid' in df.columns:
        uid_series = normalize_series(df['person_uid'])
        found_mask = found_mask | (uid_series == q_str)

    # also check Int1 (Personnel.Int1) if present in CSV
    if 'Int1' in df.columns and not found_mask.any():
        int1_series = normalize_series(df['Int1'])
        found_mask = found_mask | (int1_series == q_str)

    if not found_mask.any():
        # try numeric equivalence
        try:
            q_numeric = float(q_str)
            if 'EmployeeID' in df.columns:
                emp_numeric = pd.to_numeric(df['EmployeeID'], errors='coerce')
                found_mask = found_mask | (emp_numeric == q_numeric)
            if 'Int1' in df.columns and not found_mask.any():
                int_numeric = pd.to_numeric(df['Int1'], errors='coerce')
                found_mask = found_mask | (int_numeric == q_numeric)
        except Exception:
            pass

    matched = df[found_mask]
    if matched.empty:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    cleaned_matched = _clean_sample_df(matched, max_rows=len(matched))

    # --- ENRICHMENT START ---
    # cleaned_matched is a list of dicts
    try:
        # Build a quick index over matched DataFrame to find best lookup candidate for each cleaned row
        matched_indexed = matched.reset_index(drop=True)

        # prepare violation-days map (last 90 days) for this city/outdir
        try:
            violation_map = compute_violation_days_map(str(DEFAULT_OUTDIR), 90, datetime.now().date())
        except Exception:
            violation_map = {}

        for idx_c, cleaned in enumerate(cleaned_matched):
            # Try to find a matching row in matched DataFrame by person_uid -> EmployeeID -> EmployeeName
            candidate_row = None
            try:
                if cleaned.get('person_uid'):
                    if 'person_uid' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('person_uid', '').astype(str) == str(cleaned['person_uid'])]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                if candidate_row is None and cleaned.get('EmployeeID'):
                    if 'EmployeeID' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('EmployeeID', '').astype(str) == str(cleaned['EmployeeID'])]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                if candidate_row is None and cleaned.get('EmployeeName'):
                    # names exact match (defensive)
                    if 'EmployeeName' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('EmployeeName', '').astype(str).str.strip().fillna('') == str(cleaned['EmployeeName']).strip()]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                # fallback: take first matched row if nothing else
                if candidate_row is None and len(matched_indexed) > 0:
                    candidate_row = matched_indexed.iloc[0].to_dict()
            except Exception:
                candidate_row = None

            # Determine the best candidate identifier to query Personnel
            lookup_candidates = []
            if candidate_row:
                for k in ('EmployeeObjID', 'EmployeeObjId', 'EmployeeIdentity', 'ObjectID', 'GUID', 'EmployeeID', 'Int1', 'Text12', 'EmployeeName'):
                    if k in candidate_row and candidate_row.get(k) not in (None, '', 'nan'):
                        lookup_candidates.append(candidate_row.get(k))
            # also include cleaned fields
            for k in ('EmployeeID', 'person_uid', 'EmployeeName'):
                if cleaned.get(k) not in (None, '', 'nan'):
                    lookup_candidates.append(cleaned.get(k))

            # try each lookup candidate until we find personnel info
            personnel_info = {}
            for cand in lookup_candidates:
                if cand is None:
                    continue
                try:
                    info = get_personnel_info(cand)
                    if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None or info.get('ManagerEmail') is not None):
                        personnel_info = info
                        break
                except Exception:
                    continue

            # attach if found
            if personnel_info:
                # safe assignments
                cleaned['EmployeeObjID'] = personnel_info.get('ObjectID')
                cleaned['EmployeeEmail'] = personnel_info.get('EmailAddress')
                cleaned['ManagerEmail'] = personnel_info.get('ManagerEmail')
                # HasImage and imageUrl
                if personnel_info.get('ObjectID') is not None:
                    cleaned['imageUrl'] = f"/employee/{personnel_info.get('ObjectID')}/image"
                    # quick check if image exists: DB or filesystem fallback
                    has_img = False
                    try:
                        b = get_person_image_bytes(personnel_info.get('ObjectID'))
                        has_img = True if b else False
                    except Exception:
                        has_img = False
                    # filesystem fallback: outputs/images/<ObjectID>.(jpg|png)
                    if not has_img:
                        images_dir = DEFAULT_OUTDIR / "images"
                        for ext in ('jpg','jpeg','png'):
                            fp = images_dir / f"{str(personnel_info.get('ObjectID'))}.{ext}"
                            if fp.exists():
                                has_img = True
                                cleaned['imageUrl'] = f"/swipes/{fp.name}" if False else f"/employee/{personnel_info.get('ObjectID')}/image"
                                break
                    cleaned['HasImage'] = True if has_img else False
                else:
                    cleaned['imageUrl'] = None
                    cleaned['HasImage'] = False

            # Attach ViolationDays90 if available (matching several identifier forms)
            try:
                vd = 0
                # check employee id, person_uid and card (if present)
                for idcand in ('EmployeeID','person_uid','CardNumber','EmployeeIdentity','Int1','Text12'):
                    if cleaned.get(idcand):
                        key = str(cleaned.get(idcand)).strip()
                        if key in violation_map:
                            vd = max(vd, int(violation_map.get(key, 0)))
                        # also try stripped prefixes (emp:, uid:, name:)
                        if ':' in key:
                            kstr = key.split(':',1)[1]
                            if kstr in violation_map:
                                vd = max(vd, int(violation_map.get(kstr, 0)))
                cleaned['ViolationDays90'] = int(vd)
            except Exception:
                cleaned['ViolationDays90'] = None

    except Exception:
        logging.exception("Failed to enrich aggregated rows with personnel/email/image info")
    # --- ENRICHMENT END ---

    # Resolve raw swipe file names by Date (collect all dates present in matched rows)
    raw_files = set()
    raw_swipes_out = []

    # Helper to add and dedupe swipe rows
    seen_swipe_keys = set()
    def _append_swipe(out_row, source_name):
        # create a dedupe key (date,time,door,direction,card)
        key = (
            out_row.get('Date') or '',
            out_row.get('Time') or '',
            (out_row.get('Door') or '').strip(),
            (out_row.get('Direction') or '').strip(),
            (out_row.get('Card') or out_row.get('CardNumber') or '').strip()
        )
        if key in seen_swipe_keys:
            return
        seen_swipe_keys.add(key)
        out_row['_source'] = source_name
        raw_swipes_out.append(out_row)

    # iterate matched aggregated rows and search raw swipe files
    for idx, agg_row in matched.iterrows():
        person_uid = agg_row.get('person_uid') if 'person_uid' in agg_row else None
        empid = agg_row.get('EmployeeID') if 'EmployeeID' in agg_row else None
        if (not empid) and 'Int1' in agg_row:
            empid = agg_row.get('Int1')
        card = agg_row.get('CardNumber') if 'CardNumber' in agg_row else None

        # build dates_for_row from Date / FirstSwipe / LastSwipe (include AdjustedMessageTime / ShiftedDate / OriginalLocaleMessageTime if present)
        dates_for_row = set()
        if 'Date' in agg_row and pd.notna(agg_row['Date']):
            try:
                d = pd.to_datetime(agg_row['Date']).date()
                dates_for_row.add(d.isoformat())
            except Exception:
                pass
        for col in ('FirstSwipe', 'LastSwipe'):
            if col in agg_row and pd.notna(agg_row[col]):
                try:
                    d = pd.to_datetime(agg_row[col]).date()
                    dates_for_row.add(d.isoformat())
                except Exception:
                    pass

        # include AdjustedMessageTime / ShiftedDate / OriginalLocaleMessageTime if present
        for cand_col in ('AdjustedMessageTime', 'ShiftedDate', 'OriginalLocaleMessageTime'):
            if cand_col in agg_row and pd.notna(agg_row.get(cand_col)):
                try:
                    d = pd.to_datetime(agg_row.get(cand_col)).date()
                    dates_for_row.add(d.isoformat())
                except Exception:
                    # if it's already a date string fallback
                    try:
                        if isinstance(agg_row.get(cand_col), str) and len(agg_row.get(cand_col)) >= 8:
                            dates_for_row.add(str(agg_row.get(cand_col))[:10])
                    except Exception:
                        pass

        # If the aggregated row is not flagged and include_unflagged is False, skip fetching raw evidence
        is_flagged = bool(agg_row.get('IsFlagged', False))
        if (not is_flagged) and (not include_unflagged):
            continue

        # If no dates or if date-specific candidates are missing, scan all swipes files as a fallback
        dates_to_scan = dates_for_row or {None}

        for d in dates_to_scan:
            try:
                candidates = []
                if d is None:
                    candidates = list(Path(DEFAULT_OUTDIR).glob("swipes_*.csv"))
                else:
                    dd = str(d)[:10]  # 'YYYY-MM-DD'
                    target = dd.replace('-', '')
                    # be more permissive: match any file that contains the date string (e.g. swipes_YYYYMMDD.csv or swipes_site_YYYYMMDD.csv)
                    candidates = list(Path(DEFAULT_OUTDIR).glob(f"*{target}*.csv"))

                # fallback: if exact per-date candidates not found, search all swipes files
                if not candidates:
                    candidates = list(Path(DEFAULT_OUTDIR).glob("swipes_*.csv"))
                    if not candidates:
                        continue

                for fp in candidates:
                    raw_files.add(fp.name)

                for fp in candidates:
                    raw_name = fp.name
                    try:
                        # attempt to parse timestamps; if this fails, fallback to dtype=str
                        try:
                            raw_df = pd.read_csv(fp, parse_dates=['LocaleMessageTime'])
                        except Exception:
                            raw_df = pd.read_csv(fp, dtype=str)
                    except Exception:
                        continue

                    # normalize placeholder strings in raw_df as well (object columns)
                    raw_df = _replace_placeholder_strings(raw_df)

                    cols_lower = {c.lower(): c for c in raw_df.columns}
                    tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
                    emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                    name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
                    card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
                    door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
                    dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
                    note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None

                    # build filter mask
                    mask = pd.Series(False, index=raw_df.index)
                    if person_uid is not None and 'person_uid' in raw_df.columns:
                        mask = mask | (raw_df['person_uid'].astype(str).str.strip() == str(person_uid).strip())
                    if emp_col:
                        if empid is not None:
                            try:
                                cmp_val = str(empid).strip()
                                if '.' in cmp_val:
                                    fv = float(cmp_val)
                                    if math.isfinite(fv) and fv.is_integer():
                                        cmp_val = str(int(fv))
                            except Exception:
                                cmp_val = str(empid).strip()
                            mask = mask | (raw_df[emp_col].astype(str).str.strip() == cmp_val)
                    if card_col and card is not None:
                        mask = mask | (raw_df[card_col].astype(str).str.strip() == str(card).strip())

                    if (not mask.any()) and name_col and 'EmployeeName' in agg_row and pd.notna(agg_row.get('EmployeeName')):
                        mask = mask | (raw_df[name_col].astype(str).str.strip() == str(agg_row.get('EmployeeName')).strip())

                    # filter by date if possible
                    if d is not None and tcol and tcol in raw_df.columns:
                        try:
                            raw_df[tcol] = pd.to_datetime(raw_df[tcol], errors='coerce')
                            mask = mask & (raw_df[tcol].dt.date == pd.to_datetime(d).date())
                        except Exception:
                            pass

                    filtered = raw_df[mask].copy()
                    if filtered.empty:
                        # xml value fallback for embedded card values
                        if card is not None:
                            for ccol in raw_df.columns:
                                cl = ccol.lower()
                                if 'xml' in cl or 'msg' in cl or 'value' == cl:
                                    try:
                                        vals = raw_df[ccol].dropna().astype(str)
                                        match_mask = vals.apply(lambda x: (_extract_card_from_xml_text(x) == str(card).strip()))
                                        if match_mask.any():
                                            idxs = match_mask.index[match_mask]
                                            filtered = raw_df.loc[idxs].copy()
                                            break
                                    except Exception:
                                        continue
                        if filtered.empty:
                            continue

                    # enrich filtered rows and append to output (deduped)
                    try:
                        if tcol and tcol in filtered.columns:
                            filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                        else:
                            if 'localemessagetime' in filtered.columns:
                                filtered['localemessagetime'] = pd.to_datetime(filtered['localemessagetime'], errors='coerce')
                                tcol = 'localemessagetime'
                    except Exception:
                        pass
                    if tcol and tcol in filtered.columns:
                        filtered = filtered.sort_values(by=tcol)
                        filtered['_prev_ts'] = filtered[tcol].shift(1)
                        try:
                            filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                        except Exception:
                            filtered['_swipe_gap_seconds'] = 0.0

                        # ZERO gaps when the previous swipe belongs to a different calendar day
                        try:
                            cur_dates = filtered[tcol].dt.date
                            prev_dates = cur_dates.shift(1)
                            day_start_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
                            filtered.loc[day_start_mask, '_swipe_gap_seconds'] = 0.0
                        except Exception:
                            pass

                    else:
                        filtered['_swipe_gap_seconds'] = 0.0

                    try:
                        if door_col and door_col in filtered.columns:
                            if dir_col and dir_col in filtered.columns:
                                filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                            else:
                                filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
                        else:
                            if 'PartitionName2' in filtered.columns:
                                filtered['_zone'] = filtered['PartitionName2'].fillna('').astype(str).apply(lambda x: x if x else None)
                            else:
                                filtered['_zone'] = None
                    except Exception:
                        filtered['_zone'] = None

                    # normalize and append
                    for _, r in filtered.iterrows():
                        out = {}
                        out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in raw_df.columns else _to_python_scalar(agg_row.get('EmployeeName') or agg_row.get('person_uid'))

                        # EmployeeID
                        emp_val = None
                        if 'int1' in cols_lower and cols_lower.get('int1') in raw_df.columns:
                            emp_val = _to_python_scalar(r.get(cols_lower.get('int1')))
                        elif emp_col and emp_col in raw_df.columns:
                            emp_val = _to_python_scalar(r.get(emp_col))
                        else:
                            possible_emp = None
                            for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                                if cand.lower() in cols_lower:
                                    possible_emp = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                                    if possible_emp not in (None, '', 'nan'):
                                        break
                            emp_val = possible_emp if possible_emp not in (None, '', 'nan') else _to_python_scalar(agg_row.get('EmployeeID'))

                        if emp_val is not None:
                            try:
                                s = str(emp_val).strip()
                                if '.' in s:
                                    f = float(s)
                                    if math.isfinite(f) and f.is_integer():
                                        s = str(int(f))
                                if _looks_like_guid(s) or _is_placeholder_str(s):
                                    emp_val = None
                                else:
                                    emp_val = s
                            except Exception:
                                if _looks_like_guid(emp_val):
                                    emp_val = None
                        out['EmployeeID'] = emp_val

                        # CardNumber
                        card_val = None
                        if 'cardnumber' in cols_lower and cols_lower.get('cardnumber') in raw_df.columns:
                            card_val = _to_python_scalar(r.get(cols_lower.get('cardnumber')))
                        elif card_col and card_col in raw_df.columns:
                            card_val = _to_python_scalar(r.get(card_col))
                        else:
                            possible_card = None
                            for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                                if cand.lower() in cols_lower:
                                    possible_card = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                                    if possible_card not in (None, '', 'nan'):
                                        break
                            if possible_card in (None, '', 'nan'):
                                for ccc in raw_df.columns:
                                    cl = ccc.lower()
                                    if 'xml' in cl or 'msg' in cl or 'value' == cl:
                                        try:
                                            txt = r.get(ccc)
                                            extracted = _extract_card_from_xml_text(str(txt)) if txt is not None else None
                                            if extracted:
                                                possible_card = extracted
                                                break
                                        except Exception:
                                            continue
                            card_val = possible_card if possible_card not in (None, '', 'nan') else _to_python_scalar(agg_row.get('CardNumber'))

                        if card_val is not None:
                            try:
                                cs = str(card_val).strip()
                                if _looks_like_guid(cs) or _is_placeholder_str(cs):
                                    card_val = None
                                else:
                                    card_val = cs
                            except Exception:
                                card_val = None
                        out['CardNumber'] = card_val

                        # timestamp -> Date/Time
                        if tcol and tcol in filtered.columns:
                            ts = r.get(tcol)
                            try:
                                ts_py = pd.to_datetime(ts)
                                out['Date'] = ts_py.date().isoformat()
                                out['Time'] = ts_py.time().isoformat()
                                out['LocaleMessageTime'] = ts_py.isoformat()
                            except Exception:
                                txt = str(r.get(tcol))
                                out['Date'] = txt[:10]
                                out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                                out['LocaleMessageTime'] = txt
                        else:
                            out['Date'] = d if d is not None else None
                            out['Time'] = None
                            out['LocaleMessageTime'] = None

                        out['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
                        out['SwipeGap'] = format_seconds_to_hms(out['SwipeGapSeconds'])

                        out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
                        out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in r else None
                        out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None

                        try:
                            out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else map_door_to_zone(out['Door'], out['Direction'])
                        except Exception:
                            out['Zone'] = None

                        out['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
                        out['_source_file'] = fp.name

                        _append_swipe(out, raw_name)
                    except Exception as e:
                        logging.exception("Error processing raw swipe row: %s", e)
                        continue

            except Exception as e:
                logging.exception("Error processing raw swipe file for date %s: %s", d, e)
                continue

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": sorted(list(raw_files)),
        "raw_swipes": raw_swipes_out
    }), 200


@app.route('/record/export', methods=['GET'])
def export_record_excel():
    """
    /record/export?employee_id=...&date=YYYY-MM-DD  OR /record/export?person_uid=...&date=YYYY-MM-DD
    Produces an Excel file (xlsx) filtered for the requested employee and date (if provided).
    Two sheets:
      - "Details — Evidence": EmployeeName, EmployeeID, Door, Direction, Zone, Date, LocaleMessageTime, SwipeGapSeconds, PartitionName2, _source_file
      - "Swipe timeline": Employee Name, Employee ID, Card, Date, Time, SwipeGapSeconds, Door, Direction, Zone, Note
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    date_str = request.args.get('date')  # optional 'YYYY-MM-DD' (single date)
    if not q:
        return jsonify({"error":"employee_id or person_uid is required"}), 400

    # Determine list of raw swipe files to scan. If date provided, restrict to that date only.
    files_to_scan = []
    p = Path(DEFAULT_OUTDIR)
    if date_str:
        try:
            dd = pd.to_datetime(date_str).date()
            target = dd.strftime("%Y%m%d")
            # permissive matching: any filename containing target
            candidates = [fp for fp in p.glob("*.csv") if target in fp.name and 'swipes' in fp.name]
            files_to_scan = candidates
        except Exception:
            return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400
    else:
        # scan all swipes files (most recent first)
        files_to_scan = sorted(p.glob("swipes_*.csv"), reverse=True)

    if not files_to_scan:
        # fallback: try any CSVs containing 'swipes' (some environments use different naming)
        files_to_scan = [fp for fp in p.glob("*.csv") if 'swipes' in fp.name]
        if not files_to_scan:
            return jsonify({"error":"no raw swipe files found for requested date / outputs"}), 404

    all_rows = []
    for fp in files_to_scan:
        try:
            # parse dates where possible
            try:
                raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
            except Exception:
                raw_df = pd.read_csv(fp, dtype=str)
        except Exception:
            continue

        # normalize column names
        raw_df = _replace_placeholder_strings(raw_df)

        cols_lower = {c.lower(): c for c in raw_df.columns}
        # pick possible columns
        tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
        emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
        name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
        card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
        door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
        dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
        note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
        person_uid_col = cols_lower.get('person_uid')

        # build mask matching requested q: try person_uid, emp_col, card, name
        mask = pd.Series(False, index=raw_df.index)
        if person_uid_col and person_uid_col in raw_df.columns:
            mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
        if emp_col and emp_col in raw_df.columns:
            mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
        # try matching numeric equivalence
        if not mask.any() and emp_col and emp_col in raw_df.columns:
            try:
                q_numeric = float(q)
                emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                mask = mask | (emp_numeric == q_numeric)
            except Exception:
                pass
        # also try name match if nothing matched
        if not mask.any() and name_col and name_col in raw_df.columns:
            mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())

        if not mask.any():
            # nothing to include from this file
            continue

        filtered = raw_df[mask].copy()
        if filtered.empty:
            continue

        # ensure timestamp col exists and parsed
        if tcol and tcol in filtered.columns:
            try:
                filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
            except Exception:
                pass

        # compute swipe gaps (seconds)
        if tcol and tcol in filtered.columns:
            filtered = filtered.sort_values(by=tcol)
            filtered['_prev_ts'] = filtered[tcol].shift(1)
            try:
                filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
            except Exception:
                filtered['_swipe_gap_seconds'] = 0.0
        else:
            filtered['_swipe_gap_seconds'] = 0.0

        # compute zone per row (use door+direction if available)
        try:
            if door_col and door_col in filtered.columns:
                if dir_col and dir_col in filtered.columns:
                    filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                else:
                    filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
            else:
                filtered['_zone'] = filtered.get('PartitionName2', None)
        except Exception:
            filtered['_zone'] = None

        # normalize columns into common shape and append rows
        for _, r in filtered.iterrows():
            row = {}
            row['EmployeeName'] = _to_python_scalar(r.get(name_col)) if (name_col and name_col in filtered.columns) else None
            # EmployeeID attempts: Int1/Text12/EmployeeID
            emp_val = None
            if emp_col and emp_col in filtered.columns:
                emp_val = _to_python_scalar(r.get(emp_col))
            else:
                # fallbacks
                for cand in ('int1','text12','employeeid','employee_identity','employeeidentity'):
                    if cand in cols_lower and cols_lower[cand] in filtered.columns:
                        emp_val = _to_python_scalar(r.get(cols_lower[cand]))
                        if emp_val:
                            break
            row['EmployeeID'] = emp_val
            row['Card'] = _to_python_scalar(r.get(card_col)) if (card_col and card_col in filtered.columns) else None

            # Date and Time
            if tcol and tcol in filtered.columns:
                ts = r.get(tcol)
                try:
                    ts_py = pd.to_datetime(ts)
                    row['Date'] = ts_py.date().isoformat()
                    row['Time'] = ts_py.time().isoformat()
                    row['LocaleMessageTime'] = ts_py.isoformat()
                except Exception:
                    txt = str(r.get(tcol))
                    row['Date'] = txt[:10]
                    row['Time'] = txt[11:19] if len(txt) >= 19 else None
                    row['LocaleMessageTime'] = txt
            else:
                row['Date'] = None
                row['Time'] = None
                row['LocaleMessageTime'] = None

            row['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
            row['SwipeGap'] = format_seconds_to_hms(row['SwipeGapSeconds'])

            row['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
            row['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else None
            row['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None

            # Zone (computed above)
            try:
                zone_val = r.get('_zone') if '_zone' in r else None
                if zone_val is None:
                    zone_val = map_door_to_zone(row['Door'], row['Direction'])
                row['Zone'] = _to_python_scalar(zone_val)
            except Exception:
                row['Zone'] = None

            row['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
            row['_source_file'] = fp.name
            all_rows.append(row)

    if not all_rows:
        return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

    df_out = pd.DataFrame(all_rows)

    # Build two sheets as requested (with exact requested column order)
    details_cols = ['EmployeeName','EmployeeID','Door','Direction','Zone','Date','LocaleMessageTime','SwipeGapSeconds','PartitionName2','_source_file']
    timeline_cols = ['EmployeeName','EmployeeID','Card','Date','Time','SwipeGapSeconds','Door','Direction','Zone','Note','_source_file']

    details_df = df_out[[c for c in details_cols if c in df_out.columns]].copy()
    timeline_df = df_out[[c for c in timeline_cols if c in df_out.columns]].copy()

    # Create excel in-memory
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            details_df.to_excel(writer, sheet_name='Details — Evidence', index=False)
            timeline_df.to_excel(writer, sheet_name='Swipe timeline', index=False)
            writer.save()
            output.seek(0)
    except Exception as e:
        logging.exception("Failed to create Excel: %s", e)
        return jsonify({"error":"failed to create excel"}), 500

    # If openpyxl available, apply formatting (bold header, center align, borders)
    if OPENPYXL_AVAILABLE:
        try:
            wb = load_workbook(output)
            thin = Side(border_style="thin", color="000000")
            thick = Side(border_style="medium", color="000000")
            for ws in wb.worksheets:
                # header styling
                header = ws[1]
                for cell in header:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
                # data rows: center & thin border
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                        cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
                # autosize columns (best-effort)
                for col in ws.columns:
                    max_len = 0
                    col_letter = col[0].column_letter
                    for cell in col:
                        try:
                            v = str(cell.value) if cell.value is not None else ""
                        except Exception:
                            v = ""
                        if len(v) > max_len:
                            max_len = len(v)
                    # limit column width
                    width = min(max(10, max_len + 2), 50)
                    ws.column_dimensions[col_letter].width = width
            # write back to bytes
            out2 = io.BytesIO()
            wb.save(out2)
            out2.seek(0)
            return send_file(out2, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        except Exception:
            logging.exception("Excel styling failed, returning raw file")
            output.seek(0)
            return send_file(output, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    else:
        # fallback: return raw excel binary without styling
        output.seek(0)
        return send_file(output, as_attachment=True,
                         download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                         mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")


@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    """
    Serve raw swipe CSVs from outputs/ (filename should be the file name only).
    """
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    # send file
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)


@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500


# ------------------------------
# Chatbot endpoint (trimmed safe stub)
# Replaced original/omitted chatbot code with a minimal, safe stub to keep the file syntactically valid.
@app.route('/chat', methods=['POST'])
def chat():
    data = request.get_json(silent=True) or {}
    msg = data.get('message') or data.get('q') or ''
    # This is a simple placeholder. Replace with your full chatbot implementation if needed.
    return jsonify({"reply": "chat endpoint is disabled in this trimmed build", "input": msg}), 200


# Serve employee image route (defined after app)
@app.route('/employee/<empid>/image', methods=['GET'])
def serve_employee_image(empid):
    """
    Serve the portrait for a personnel record where ParentId = empid (ACVSCore.Images).
    Uses database lookup first; if that fails, tries filesystem fallback:
      outputs/images/<empid>.(jpg|png)
    """
    if empid is None:
        return jsonify({"error": "employee id required"}), 400

    try:
        # filesystem fallback first (fast)
        images_dir = DEFAULT_OUTDIR / "images"
        for ext in ('jpg','jpeg','png'):
            fp = images_dir / f"{str(empid)}.{ext}"
            if fp.exists():
                try:
                    return send_file(str(fp), mimetype=f"image/{ext if ext!='jpg' else 'jpeg'}")
                except Exception:
                    break

        img_bytes = get_person_image_bytes(empid)
        if not img_bytes:
            return jsonify({"error": "no image found"}), 404

        # try to detect jpeg/png
        header = img_bytes[:8]
        content_type = 'application/octet-stream'
        if len(header) >= 2 and header[0] == 0xFF and header[1] == 0xD8:
            content_type = 'image/jpeg'
        elif header.startswith(b'\x89PNG\r\n\x1a\n'):
            content_type = 'image/png'
        # send as file-like
        bio = io.BytesIO(img_bytes)
        bio.seek(0)
        return send_file(bio, mimetype=content_type)
    except Exception:
        logging.exception("Error serving image for employee %s", empid)
        return jsonify({"error": "failed to serve image"}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)








