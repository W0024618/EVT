# ccure_fetch.py
import requests
import math
import pandas as pd
from requests.exceptions import RequestException

BASE = "http://10.199.22.57:5001"
DEFAULT_TIMEOUT = 10

# If CCure needs auth, set HEADERS accordingly:
HEADERS = {
    # 'Authorization': 'Bearer <token>',
    'Accept': 'application/json'
}

def fetch_all_employees_full():
    """Fetch full list from /api/employees (may return a large list). Returns list or None."""
    try:
        r = requests.get(f"{BASE}/api/employees", headers=HEADERS, timeout=DEFAULT_TIMEOUT)
        r.raise_for_status()
        return r.json()  # expects a list
    except RequestException as e:
        print("ccure: full fetch failed:", e)
        return None

def fetch_stats_page(detail, page=1, limit=500):
    """
    Fetch one page from /api/stats?details=detail&page=page&limit=limit
    Returns dict or None.
    """
    try:
        params = {"details": detail, "page": page, "limit": limit}
        r = requests.get(f"{BASE}/api/stats", params=params, headers=HEADERS, timeout=DEFAULT_TIMEOUT)
        r.raise_for_status()
        return r.json()
    except RequestException as e:
        print(f"ccure: stats page error detail={detail} page={page}:", e)
        return None

def fetch_all_stats(detail, limit=1000):
    """
    Iterate pages and return full list for a given 'details' filter.
    """
    first = fetch_stats_page(detail, page=1, limit=limit)
    if not first:
        return None
    total = first.get("total", 0)
    data = first.get("data", []) or []
    if total <= len(data):
        return data
    pages = math.ceil(total / limit)
    for p in range(2, pages + 1):
        page_res = fetch_stats_page(detail, page=p, limit=limit)
        if not page_res:
            break
        data.extend(page_res.get("data", []) or [])
    return data

def ccure_to_dataframe(list_of_dicts):
    """Return DataFrame normalised with key columns."""
    if not list_of_dicts:
        return pd.DataFrame()
    df = pd.DataFrame(list_of_dicts)
    # standardize column names if needed
    df.columns = [c.strip() for c in df.columns]
    # ensure EmployeeID exists and is string
    if 'EmployeeID' in df.columns:
        df['EmployeeID'] = df['EmployeeID'].astype(str).str.strip().replace({'nan': None})
    else:
        df['EmployeeID'] = None
    return df








# compare_ccure_with_sheets.py
import pandas as pd
from ccure_fetch import fetch_all_employees_full, fetch_all_stats, ccure_to_dataframe
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor
from pathlib import Path

OUT_DIR = Path("data/outputs")
OUT_DIR.mkdir(parents=True, exist_ok=True)

def normalize_empid(e):
    if e is None:
        return None
    s = str(e).strip()
    if s == "" or s.lower() in ("nan","none","null"):
        return None
    return s

def get_local_active_sets():
    with SessionLocal() as db:
        emps = db.query(ActiveEmployee).all()
        conts = db.query(ActiveContractor).all()

        emp_df = pd.DataFrame([{
            "employee_id": normalize_empid(e.employee_id),
            "full_name": e.full_name,
            "location_city": e.location_city,
            "status": e.current_status
        } for e in emps])

        cont_df = pd.DataFrame([{
            "worker_system_id": normalize_empid(c.worker_system_id),
            "ipass_id": normalize_empid(c.ipass_id),
            "full_name": c.full_name,
            "location": c.location,
            "status": c.status
        } for c in conts])
    return emp_df, cont_df

def compare():
    # 1) fetch ccure master
    ccure_list = fetch_all_employees_full()
    if ccure_list is None:
        # fallback: try fetching ActiveProfiles
        ccure_list = fetch_all_stats("ActiveProfiles", limit=1000)
    ccure_df = ccure_to_dataframe(ccure_list)
    # normalize employee id column name
    ccure_df['EmployeeID_norm'] = ccure_df['EmployeeID'].apply(normalize_empid)

    # 2) local active sets
    emp_df, cont_df = get_local_active_sets()

    # 3) create sets for comparison
    ccure_emp_set = set(ccure_df[ccure_df['EmployeeID_norm'].notna()]['EmployeeID_norm'].unique())
    local_emp_set = set(emp_df[emp_df['employee_id'].notna()]['employee_id'].unique())
    # contractor IDs from CCure sometimes prefixed with W...
    local_cont_set = set(cont_df['worker_system_id'].dropna().unique())
    ccure_contractors = ccure_df[ccure_df['PersonnelType'].str.lower().str.contains('contractor', na=False)]
    ccure_cont_set = set(ccure_contractors['EmployeeID_norm'].dropna().unique())

    # 4) differences
    in_ccure_not_local = sorted(list(ccure_emp_set - local_emp_set))
    in_local_not_ccure = sorted(list(local_emp_set - ccure_emp_set))

    in_ccure_cont_not_local = sorted(list(ccure_cont_set - local_cont_set))
    in_local_cont_not_ccure = sorted(list(local_cont_set - ccure_cont_set))

    # 5) detailed rows for export
    rows_ccure_only = ccure_df[ccure_df['EmployeeID_norm'].isin(in_ccure_not_local)]
    rows_local_only = emp_df[emp_df['employee_id'].isin(in_local_not_ccure)]

    # 6) summary counts
    summary = {
        "ccure_total_profiles": len(ccure_df),
        "ccure_active_profiles": int(ccure_df[ccure_df['Employee_Status'].str.lower().eq('active')].shape[0]) if 'Employee_Status' in ccure_df.columns else None,
        "local_active_employees": emp_df.shape[0],
        "local_active_contractors": cont_df.shape[0],
        "ccure_active_employees_count": int(ccure_df[ccure_df['PersonnelType'].str.contains('employee', na=False).sum()) if 'PersonnelType' in ccure_df.columns else None
    }

    report_path = OUT_DIR / f"ccure_vs_local_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
    with pd.ExcelWriter(report_path, engine="xlsxwriter") as w:
        rows_ccure_only.to_excel(w, sheet_name="ccure_only_profiles", index=False)
        rows_local_only.to_excel(w, sheet_name="local_only_profiles", index=False)
        emp_df.to_excel(w, sheet_name="local_employees", index=False)
        cont_df.to_excel(w, sheet_name="local_contractors", index=False)
        ccure_df.to_excel(w, sheet_name="ccure_all", index=False)
        pd.DataFrame([summary]).to_excel(w, sheet_name="summary", index=False)

    print("Comparison finished.")
    print("Summary:", summary)
    print("Report saved to:", report_path)
    return {
        "in_ccure_not_local_count": len(in_ccure_not_local),
        "in_local_not_ccure_count": len(in_local_not_ccure),
        "report": str(report_path),
        "details": {
            "in_ccure_not_local": in_ccure_not_local[:200],
            "in_local_not_ccure": in_local_not_ccure[:200]
        }
    }

if __name__ == "__main__":
    r = compare()
    print(r)














# app.py
from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Body
from fastapi.responses import JSONResponse
import shutil, uuid, json
from settings import UPLOAD_DIR, OUTPUT_DIR
import os

app = FastAPI(title="Attendance Analytics")

@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = UPLOAD_DIR / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_employee_excel
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_employee_excel(dest)
    return {"status":"ok", "path": str(dest)}

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = UPLOAD_DIR / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_contractor_excel
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_contractor_excel(dest)
    return {"status":"ok", "path": str(dest)}

@app.post("/ingest/live-details")
async def ingest_live(request: Request):
    """
    Accepts:
      - Raw JSON array in body (preferred)
      - JSON object with {"details": [...]} in body
      - multipart/form-data where a form field 'details' contains a JSON string
    """
    details = None
    # Try direct JSON
    try:
        body = await request.json()
        if isinstance(body, dict) and 'details' in body:
            details = body['details']
        else:
            details = body
    except Exception:
        # not JSON, try form
        try:
            form = await request.form()
            if 'details' in form:
                raw = form['details']
                if isinstance(raw, str):
                    details = json.loads(raw)
                else:
                    try:
                        details = json.loads((await raw.read()).decode('utf-8'))
                    except Exception:
                        details = list(form.getlist('details'))
            else:
                # attempt first field
                first = None
                for v in form.values():
                    first = v
                    break
                if isinstance(first, str):
                    details = json.loads(first)
                else:
                    raise HTTPException(status_code=400, detail="No JSON payload found")
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Could not parse request body as JSON or form: {e}")

    if not isinstance(details, (list, tuple)):
        raise HTTPException(status_code=400, detail="Expected top-level array (JSON list) of detail objects")

    try:
        from compare_service import ingest_live_details_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    try:
        res = ingest_live_details_list(details)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to ingest details: {e}")

    if isinstance(res, dict):
        return {"status": "ok", **res}
    return {"status": "ok", "inserted": len(details)}


@app.get("/ingest/fetch-all")
def fetch_all_and_ingest():
    try:
        import region_clients
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"region_clients unavailable: {e}")

    details = region_clients.fetch_all_details()
    if not isinstance(details, list):
        raise HTTPException(status_code=500, detail="Unexpected data from region_clients.fetch_all_details")

    try:
        from compare_service import ingest_live_details_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    res = ingest_live_details_list(details)
    if isinstance(res, dict):
        return {"status":"ok", **res}
    return {"status":"ok", "inserted": len(details)}


@app.get("/reports/daily/{yyyymmdd}")
def daily_report(yyyymmdd: str):
    import datetime
    try:
        dt = datetime.datetime.strptime(yyyymmdd, "%Y%m%d").date()
    except Exception:
        raise HTTPException(status_code=400, detail="Date must be in YYYYMMDD format")

    try:
        from compare_service import compute_daily_attendance, compare_with_active
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    # compute attendance summary (will build attendance_summary rows)
    compute_daily_attendance(dt)

    # compare vs active and CCURE
    summary = compare_with_active(dt)
    return JSONResponse(summary)


# additional endpoints to fetch CCure stats directly (optional)
@app.get("/ccure/stats")
def ccure_stats():
    try:
        import ccure_client
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ccure_client import failed: {e}")
    try:
        stats = ccure_client.get_global_stats()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ccure_client.get_global_stats failed: {e}")
    return stats









# compare_service.py
import pandas as pd
import numpy as np
from datetime import datetime, date, timezone
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary
import re
from dateutil import parser as dateutil_parser
import traceback

# --- Helpers -----------------------------------------------------------------

def _to_native(value):
    if value is None:
        return None
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    if isinstance(value, (np.integer,)):
        return int(value)
    if isinstance(value, (np.floating,)):
        return float(value)
    if isinstance(value, (np.bool_, bool)):
        return bool(value)
    try:
        import datetime as _dt
        if isinstance(value, _dt.datetime):
            try:
                if value.tzinfo is not None:
                    utc = value.astimezone(timezone.utc)
                    return utc.replace(tzinfo=None).isoformat() + "Z"
                else:
                    return value.isoformat()
            except Exception:
                return str(value)
        if hasattr(value, 'isoformat'):
            try:
                return value.isoformat()
            except Exception:
                return str(value)
    except Exception:
        pass
    return value

def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        # drop empty placeholders
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    """Return normalized card-like string: digits only, no spaces, trim leading zeros."""
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        # remove non-digit characters
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        # strip leading zeros so '007' and '7' match; keep at least one digit
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        # remove punctuation, collapse whitespace
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

# timestamp parsing helpers
def _parse_timestamp_from_value(val):
    if val is None:
        return None
    import datetime as _dt
    # direct datetime
    if isinstance(val, _dt.datetime):
        dt = val
        try:
            if dt.tzinfo is not None:
                return dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            return dt
    # numeric epoch
    try:
        import numpy as _np
        if isinstance(val, (int, float, _np.integer, _np.floating)):
            v = int(val)
            # heuristics: millisecond vs second
            if v > 1e12:
                return _dt.fromtimestamp(v / 1000.0, tz=timezone.utc).replace(tzinfo=None)
            if v > 1e9:
                return _dt.fromtimestamp(v, tz=timezone.utc).replace(tzinfo=None)
    except Exception:
        pass
    # string parsing using dateutil
    if isinstance(val, str):
        s = val.strip()
        if s == "":
            return None
        try:
            # try iso / dateutil
            dt = dateutil_parser.parse(s)
            if dt.tzinfo is not None:
                dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            # fallback to custom formats
            fmts = ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M:%S.%f",
                    "%d/%m/%Y %H:%M:%S", "%d-%m-%Y %H:%M:%S",
                    "%Y-%m-%dT%H:%M:%S")
            for fmt in fmts:
                try:
                    return _dt.strptime(s, fmt)
                except Exception:
                    pass
    return None

def _extract_timestamp_from_detail(detail):
    # check many likely keys
    fields = [
        "LocaleMessageDateTime", "LocalMessageDateTime", "LocaleMessageTime", "LocalMessageTime",
        "LocaleMessageDate", "Timestamp", "timestamp", "Time", "LocaleTime", "LocalTime",
        "time", "date", "LocaleMessageDateTimeUtc", "LocalMessageDateTimeUtc",
        "Swipe_Time", "SwipeTime", "SwipeTimeLocal", "SwipeTimestamp", "SwipeDateTime"
    ]
    if isinstance(detail, dict):
        for k in fields:
            if k in detail:
                dt = _parse_timestamp_from_value(detail.get(k))
                if dt is not None:
                    return dt
        # scan all values for any parseable timestamp
        for v in detail.values():
            dt = _parse_timestamp_from_value(v)
            if dt is not None:
                return dt
    else:
        return _parse_timestamp_from_value(detail)
    return None

# --- Main functions ----------------------------------------------------------

def ingest_live_details_list(details_list):
    """Persist details_list into LiveSwipe. returns counts."""
    from db import SessionLocal as _SessionLocal
    inserted = 0
    skipped = 0
    with _SessionLocal() as db:
        for d in details_list:
            try:
                ts_parsed = _extract_timestamp_from_detail(d)
            except Exception:
                ts_parsed = None
            if ts_parsed is None:
                # skip rows without parseable timestamp
                skipped += 1
                continue

            # robust extraction of employee id and card fields (many alias names)
            emp = None
            for k in ("EmployeeID", "employee_id", "employeeId", "Employee Id", "EmpID", "Emp Id"):
                if isinstance(d, dict) and k in d:
                    emp = d.get(k)
                    break
            emp = _normalize_employee_key(emp)

            card = None
            for k in ("CardNumber", "card_number", "Card", "Card No", "CardNo", "Badge", "BadgeNo", "badge_number", "IPassID", "iPass ID"):
                if isinstance(d, dict) and k in d:
                    card = d.get(k)
                    break
            card = _normalize_card_like(card)

            full_name = None
            for k in ("ObjectName1", "FullName", "full_name", "EmpName", "Name"):
                if isinstance(d, dict) and k in d:
                    full_name = d.get(k)
                    break

            partition = None
            for k in ("PartitionName2", "PartitionName1", "Partition", "PartitionName", "Region"):
                if isinstance(d, dict) and k in d:
                    partition = d.get(k)
                    break

            floor = d.get("Floor") if isinstance(d, dict) else None
            door = None
            for k in ("Door", "DoorName", "door"):
                if isinstance(d, dict) and k in d:
                    door = d.get(k)
                    break

            region = d.get("__region") if isinstance(d, dict) and "__region" in d else d.get("Region") if isinstance(d, dict) else None

            try:
                rec = LiveSwipe(
                    timestamp=ts_parsed,
                    employee_id=emp,
                    card_number=card,
                    full_name=full_name,
                    partition=partition,
                    floor=floor,
                    door=door,
                    region=region,
                    raw=d
                )
                db.add(rec)
                inserted += 1
            except Exception:
                # skip insertion errors but continue
                db.rollback()
                skipped += 1
                continue
        db.commit()
    print(f"[ingest_live_details_list] inserted={inserted} skipped={skipped}")
    return {"inserted": inserted, "skipped_invalid_timestamp": skipped}


def compute_daily_attendance(target_date: date):
    """Build AttendanceSummary rows for target_date (upserts)."""
    with SessionLocal() as db:
        start = datetime.combine(target_date, datetime.min.time())
        end = datetime.combine(target_date, datetime.max.time())
        swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
        if not swipes:
            print(f"[compute_daily_attendance] no swipes for {target_date}")
            return []

        rows = []
        for s in swipes:
            rows.append({
                "id": s.id,
                "timestamp": s.timestamp,
                "employee_id": _normalize_employee_key(s.employee_id),
                "card_number": _normalize_card_like(s.card_number),
                "full_name": s.full_name,
                "partition": s.partition,
                "floor": s.floor,
                "door": s.door
            })
        df = pd.DataFrame(rows)
        if df.empty:
            print(f"[compute_daily_attendance] dataframe empty after rows -> {target_date}")
            return []

        # create grouping key: prefer employee_id, otherwise card_number
        df['key'] = df['employee_id'].fillna(df['card_number'])
        df = df[df['key'].notna()]
        if df.empty:
            print("[compute_daily_attendance] no usable keys after filling employee_id/card")
            return []

        grouped = df.groupby('key', dropna=False).agg(
            presence_count=('id', 'count'),
            first_seen=('timestamp', 'min'),
            last_seen=('timestamp', 'max'),
            full_name=('full_name', 'first'),
            partition=('partition', 'first'),
            card_number=('card_number', 'first')
        ).reset_index().rename(columns={'key': 'employee_id'})

        # upsert AttendanceSummary rows (merge)
        for _, row in grouped.iterrows():
            try:
                derived_obj = {
                    "partition": (row.get('partition') or None),
                    "full_name": (row.get('full_name') or None),
                    "card_number": (row.get('card_number') or None)
                }
                rec = AttendanceSummary(
                    employee_id=str(row['employee_id']) if pd.notna(row['employee_id']) else None,
                    date=target_date,
                    presence_count=int(row['presence_count']),
                    first_seen=row['first_seen'],
                    last_seen=row['last_seen'],
                    derived=derived_obj
                )
                db.merge(rec)
            except Exception as e:
                print("[compute_daily_attendance] upsert error:", e)
                continue
        db.commit()
        print(f"[compute_daily_attendance] built {len(grouped)} attendance keys for {target_date}")
        return grouped.to_dict(orient='records')


def compare_with_active(target_date: date):
    """Compare AttendanceSummary for date with ActiveEmployee & ActiveContractor and return json-safe dict."""
    from ccure_client import get_global_stats_or_none
    with SessionLocal() as db:
        att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == target_date).all()
        if not att_rows:
            att_df = pd.DataFrame(columns=["employee_id", "presence_count", "first_seen", "last_seen", "card_number", "partition", "full_name"])
        else:
            att_df = pd.DataFrame([{
                "employee_id": _normalize_employee_key(a.employee_id),
                "presence_count": a.presence_count,
                "first_seen": a.first_seen,
                "last_seen": a.last_seen,
                "card_number": _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None),
                "partition": (a.derived.get('partition') if (a.derived and isinstance(a.derived, dict)) else None),
                "full_name": (a.derived.get('full_name') if (a.derived and isinstance(a.derived, dict)) else None)
            } for a in att_rows])

        act_rows = db.query(ActiveEmployee).all()
        contractor_rows = db.query(ActiveContractor).all()

        # Build maps & active list
        act_list = []
        card_to_emp = {}
        name_to_emp = {}

        # Employees
        for e in act_rows:
            emp_id_norm = _normalize_employee_key(e.employee_id)
            # extract card-like from raw if present
            card_from_raw = None
            try:
                rr = e.raw_row or {}
                if isinstance(rr, dict):
                    ck_list = [
                        "CardNumber","card_number","Card","Card No","CardNo","IPassID","IpassID","iPass ID","IPASSID",
                        "Badge Number","BadgeNo","Badge"
                    ]
                    for ck in ck_list:
                        v = rr.get(ck)
                        if v:
                            ckey = _normalize_card_like(v)
                            if ckey:
                                card_from_raw = ckey
                                break
                    # fallback: scan all values for numeric candidate
                    if not card_from_raw:
                        for v in rr.values():
                            try:
                                tmp = _normalize_card_like(v)
                                if tmp and 3 <= len(tmp) <= 12:
                                    card_from_raw = tmp
                                    break
                            except Exception:
                                pass
            except Exception:
                card_from_raw = None

            act_list.append({
                "employee_id": emp_id_norm,
                "full_name": e.full_name,
                "location_city": e.location_city,
                "status": e.current_status,
                "card_number": card_from_raw
            })
            if emp_id_norm:
                card_to_emp[emp_id_norm] = emp_id_norm
            if card_from_raw:
                card_to_emp[card_from_raw] = emp_id_norm
            n = _normalize_name(e.full_name)
            if n:
                name_to_emp[n] = emp_id_norm

        # Contractors
        for c in contractor_rows:
            worker_id = _normalize_employee_key(c.worker_system_id)
            ipass = _normalize_employee_key(c.ipass_id)
            w_ipass = ("W" + ipass) if ipass and not str(ipass).startswith("W") else ipass
            primary_id = worker_id or ipass or None
            act_list.append({
                "employee_id": primary_id,
                "full_name": c.full_name,
                "location_city": c.location,
                "status": c.status,
                "card_number": None
            })
            if primary_id:
                card_to_emp[primary_id] = primary_id
            if ipass:
                card_to_emp[ipass] = primary_id
            if w_ipass:
                card_to_emp[w_ipass] = primary_id
            try:
                rr = c.raw_row or {}
                if isinstance(rr, dict):
                    for ck in ("Worker System Id","Worker System ID","iPass ID","IPASSID","CardNumber","card_number"):
                        if ck in rr and rr.get(ck):
                            key = _normalize_card_like(rr.get(ck))
                            if key:
                                card_to_emp[key] = primary_id
            except Exception:
                pass
            n = _normalize_name(c.full_name)
            if n:
                name_to_emp[n] = primary_id

        act_df = pd.DataFrame(act_list)

        # If no active rows, return attendance-only view
        if act_df.empty:
            if att_df.empty:
                return {"by_location": [], "merged": [], "ccure": get_global_stats_or_none()}
            att_df['partition'] = att_df.get('partition').fillna('Unknown')
            att_df['presence_count'] = att_df['presence_count'].fillna(0)
            att_df['present_today'] = att_df['presence_count'].apply(lambda x: bool(x and x != 0))
            loc_group = att_df.groupby('partition', dropna=False).agg(
                total_n=('employee_id', 'count'),
                present_n=('present_today', 'sum')
            ).reset_index().rename(columns={'partition':'location_city'})
            loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
            by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]
            merged_list = []
            for r in att_df.to_dict(orient='records'):
                merged_list.append({
                    "employee_id": _to_native(r.get('employee_id')),
                    "presence_count": _to_native(r.get('presence_count')),
                    "first_seen": _to_native(r.get('first_seen')),
                    "last_seen": _to_native(r.get('last_seen')),
                    "full_name": _to_native(r.get('full_name')),
                    "location_city": _to_native(r.get('partition')),
                    "present_today": _to_native(r.get('present_today'))
                })
            return {"by_location": by_location, "merged": merged_list, "ccure": get_global_stats_or_none()}

        # normalize columns
        act_df['employee_id'] = act_df['employee_id'].astype(object).apply(_normalize_employee_key)
        att_df['employee_id'] = att_df['employee_id'].astype(object).apply(_normalize_employee_key)
        act_df['card_number'] = act_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in act_df.columns else pd.Series([pd.NA]*len(act_df))
        att_df['card_number'] = att_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in att_df.columns else pd.Series([pd.NA]*len(att_df))

        # ensure card_to_emp includes act_df card_numbers
        for r in act_df.to_dict(orient='records'):
            c = r.get('card_number')
            eid = r.get('employee_id')
            if c and eid:
                card_to_emp[c] = eid
            if eid:
                # also map numeric-only forms of eid
                n = re.sub(r'\D','', str(eid))
                if n:
                    card_to_emp[n.lstrip('0') or n] = eid

        # mapping function tries multiple strategies
        emp_set = set([x for x in act_df['employee_id'].dropna().astype(str)])

        def numeric_variants(s):
            s = str(s)
            clean = re.sub(r'\D','', s)
            variants = set()
            if clean:
                variants.add(clean)
                variants.add(clean.lstrip('0') or clean)
                if not s.startswith('W'):
                    variants.add('W' + clean)
            return list(variants)

        def remap_att_key(row):
            primary = row.get('employee_id') or None
            card = row.get('card_number') or None

            primary_norm = _normalize_employee_key(primary)
            card_norm = _normalize_card_like(card)

            # 1) exact employee id exists in active list
            if primary_norm and primary_norm in emp_set:
                return primary_norm

            # 2) numeric-variants of primary may map to card_to_emp
            if primary_norm:
                for v in numeric_variants(primary_norm):
                    if v in card_to_emp:
                        return card_to_emp[v]
                if primary_norm in card_to_emp:
                    return card_to_emp[primary_norm]

            # 3) direct card mapping
            if card_norm:
                if card_norm in card_to_emp:
                    return card_to_emp[card_norm]
                if (card_norm.lstrip('0') or card_norm) in card_to_emp:
                    return card_to_emp[card_norm.lstrip('0') or card_norm]
                if ('W' + card_norm) in card_to_emp:
                    return card_to_emp['W' + card_norm]

            # 4) name matching fallback
            fname = _normalize_name(row.get('full_name') or row.get('full_name_att') or None)
            if fname and fname in name_to_emp:
                return name_to_emp[fname]

            # 5) last resort - return primary_norm (maybe non-mapped) so it still shows up
            return primary_norm or card_norm or None

        att_df['mapped_employee_id'] = att_df.apply(remap_att_key, axis=1)

        # drop original employee_id column to avoid duplicate label conflict
        att_merge_df = att_df.drop(columns=['employee_id'], errors='ignore').copy()

        # merge left: act_df left_on employee_id, right_on mapped_employee_id
        merged = pd.merge(
            act_df,
            att_merge_df,
            left_on='employee_id',
            right_on='mapped_employee_id',
            how='left',
            suffixes=('', '_att')
        )

        # fill and finalize
        merged['presence_count'] = merged.get('presence_count', pd.Series([0]*len(merged))).fillna(0)
        # ensure ints when possible
        def safe_int(v):
            try:
                if pd.isna(v):
                    return 0
                iv = int(float(v))
                return iv
            except Exception:
                return v
        merged['presence_count'] = merged['presence_count'].apply(safe_int)
        merged['present_today'] = merged['presence_count'].apply(lambda x: bool(x and x != 0))
        merged['location_city'] = merged.get('location_city').fillna('Unknown')

        # by_location
        loc_group = merged.groupby('location_city', dropna=False).agg(
            total_n=('employee_id', 'count'),
            present_n=('present_today', 'sum')
        ).reset_index()
        loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
        by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]

        merged_list = []
        for r in merged.to_dict(orient='records'):
            clean = {k:_to_native(v) for k,v in r.items()}
            # unify keys for clarity in API response
            clean['mapped_employee_id'] = clean.get('mapped_employee_id')
            clean['card_number_att'] = clean.get('card_number') or clean.get('card_number_att') or None
            # include status if present
            if 'status' not in clean:
                clean['status'] = None
            # ensure employee_id key exists
            if 'employee_id' not in clean:
                clean['employee_id'] = None
            merged_list.append(clean)

        # CCURE stats fetch (best-effort)
        ccure_stats = get_global_stats_or_none()

        # compare counts summary between CCure and Active sheets
        try:
            ccure_summary = ccure_stats or {}
            cc_total_profiles = ccure_summary.get('TotalProfiles')
            cc_active_profiles = ccure_summary.get('ActiveProfiles')
            cc_active_emps = ccure_summary.get('ActiveEmployees')
            cc_active_contractors = ccure_summary.get('ActiveContractors')
        except Exception:
            cc_total_profiles = cc_active_profiles = cc_active_emps = cc_active_contractors = None

        # local sheet counts
        active_emp_count = len(act_rows)
        active_contract_count = len(contractor_rows)

        diff = {
            "active_sheet_employee_count": active_emp_count,
            "active_sheet_contractor_count": active_contract_count,
            "ccure_active_employees": cc_active_emps,
            "ccure_active_contractors": cc_active_contractors,
            "delta_employees": (cc_active_emps - active_emp_count) if (isinstance(cc_active_emps, int) and isinstance(active_emp_count, int)) else None,
            "delta_contractors": (cc_active_contractors - active_contract_count) if (isinstance(cc_active_contractors, int) and isinstance(active_contract_count, int)) else None
        }

        result = {
            "by_location": by_location,
            "merged": merged_list,
            "ccure": ccure_stats,
            "count_comparison": diff
        }
        return result


# helper wrapper to fetch CCURE stats without raising
def get_global_stats_or_none():
    try:
        from ccure_client import get_global_stats
        return get_global_stats()
    except Exception:
        return None







# ccure_client.py
"""
Simple CCure client wrappers.
This module provides best-effort calls to the internal CCure API listed in your request.
If CCure is unreachable the functions will return None or empty results (they won't raise).
"""

import requests
from requests.exceptions import RequestException

BASE = "http://10.199.22.57:5001"

def _safe_get(path, params=None, timeout=8):
    try:
        r = requests.get(BASE + path, params=params, timeout=timeout)
        r.raise_for_status()
        return r.json()
    except RequestException:
        return None
    except ValueError:
        return None

def get_global_stats():
    """
    Attempts to call the CCure summary endpoint /api/employees (or endpoints you described).
    This function will try multiple candidate endpoints to build a totals dict.
    """
    # try /api/employees for full list (may be large) - some installs give summary at /api/employees/stats
    # fallback: /api/stats?details=...
    res = _safe_get("/api/employees")
    if isinstance(res, list):
        # build basic summary counts
        total_profiles = len(res)
        active_profiles = sum(1 for r in res if r.get("Employee_Status") and r.get("Employee_Status").lower() == "active")
        active_emps = sum(1 for r in res if r.get("PersonnelType") and r.get("PersonnelType").lower().startswith("employee") and (r.get("Employee_Status") or "").lower() == "active")
        active_contractors = sum(1 for r in res if r.get("PersonnelType") and r.get("PersonnelType").lower().startswith("contractor") and (r.get("Employee_Status") or "").lower() == "active")
        terminated_profiles = sum(1 for r in res if (r.get("Employee_Status") or "").lower() in ("deactive","inactive","deactivated","terminated"))
        return {
            "TotalProfiles": total_profiles,
            "ActiveProfiles": active_profiles,
            "ActiveEmployees": active_emps,
            "ActiveContractors": active_contractors,
            "TerminatedProfiles": terminated_profiles
        }
    # fallback to stats endpoint
    res2 = _safe_get("/api/stats", params={"details": "All", "page": 1, "limit": 1})
    if isinstance(res2, dict) and any(k in res2 for k in ("TotalProfiles","ActiveProfiles","ActiveEmployees")):
        return res2
    # try specific counts endpoints
    candidate_details = ["ActiveProfiles","ActiveEmployees","ActiveContractors","TerminatedProfiles","TerminatedEmployees","TerminatedContractors"]
    result = {}
    for d in candidate_details:
        rr = _safe_get("/api/stats", params={"details": d, "page": 1, "limit": 1})
        try:
            if isinstance(rr, dict) and 'total' in rr:
                result[d] = rr['total']
        except Exception:
            pass
    if result:
        return {
            "TotalProfiles": result.get("TotalProfiles"),
            "ActiveProfiles": result.get("ActiveProfiles"),
            "ActiveEmployees": result.get("ActiveEmployees"),
            "ActiveContractors": result.get("ActiveContractors"),
            "TerminatedProfiles": result.get("TerminatedProfiles"),
            "TerminatedEmployees": result.get("TerminatedEmployees"),
            "TerminatedContractors": result.get("TerminatedContractors"),
        }
    return None

# convenience wrapper that returns None on errors
def get_global_stats_or_none():
    try:
        return get_global_stats()
    except Exception:
        return None








# ingest_excel.py
import pandas as pd
from datetime import datetime
from sqlalchemy.exc import IntegrityError
from db import SessionLocal, engine
from models import Base, ActiveEmployee, ActiveContractor
from settings import UPLOAD_DIR
import uuid, os

# --- database setup: do NOT run create_all at import time ---
def init_db():
    """
    Create DB tables if they do not exist.
    Call this manually only when you want to initialize/repair the DB:
      python -c "from ingest_excel import init_db; init_db()"
    """
    from db import engine
    from models import Base
    Base.metadata.create_all(bind=engine)

def _first_present(row, candidates):
    for c in candidates:
        v = row.get(c)
        if v is not None and str(v).strip() != "":
            return v
    return None

def ingest_employee_excel(path, uploaded_by="system"):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    # robust mapping keys
    with SessionLocal() as db:
        for _, row in df.iterrows():
            emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id'])
            if emp_id:
                emp_id = str(emp_id).strip()
            if not emp_id:
                # skip rows without an employee id
                continue
            full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
            # robust current_status detection
            status_candidates = ['Current Status','Status','Employee Status','Employee_Status','Status (Current)','CurrentStatus']
            current_status = _first_present(row, status_candidates)
            email = _first_present(row, ["Employee's Email",'Email','Email Address'])
            location_city = _first_present(row, ['Location City','Location','Location Description','City'])
            rec = ActiveEmployee(
                employee_id=emp_id,
                full_name=full_name,
                email=email,
                location_city=location_city,
                location_desc=row.get('Location Description'),
                current_status=current_status,
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)  # upsert
                db.commit()
            except IntegrityError:
                db.rollback()
            except Exception:
                db.rollback()

def ingest_contractor_excel(path):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    with SessionLocal() as db:
        for _, row in df.iterrows():
            wsid = _first_present(row, ['Worker System Id','Worker System ID','Worker ID','WorkerSystemId'])
            if wsid:
                wsid = str(wsid).strip()
            if not wsid:
                continue
            ipass = _first_present(row, ['iPass ID','"W" iPass ID','IPassID','iPassID','Ipass ID'])
            full_name = _first_present(row, ['Full Name','FullName','Name'])
            rec = ActiveContractor(
                worker_system_id=wsid,
                ipass_id=ipass,
                full_name=full_name,
                vendor=_first_present(row, ['Vendor Company Name','Vendor']),
                location=_first_present(row, ['Worker Location','Location']),
                status=_first_present(row, ['Status','Current Status']),
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)
                db.commit()
            except IntegrityError:
                db.rollback()
            except Exception:
                db.rollback()

if __name__ == "__main__":
    # ingestion convenience: read all uploaded files
    for f in os.listdir(UPLOAD_DIR):
        p = UPLOAD_DIR / f
        if 'contractor' in f.lower() or 'contractor' in str(p).lower():
            ingest_contractor_excel(p)
        else:
            ingest_employee_excel(p)
    print("Ingestion completed.")







# region_clients.py
import requests
from requests.exceptions import RequestException

# Edit endpoints if your hosts/ports differ
endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/live-summary",
    "emea":  "http://10.199.22.57:3007/api/occupancy/live-summary",
    "laca":  "http://10.199.22.57:4000/api/occupancy/live-summary",
    "apac":  "http://10.199.22.57:3008/api/occupancy/live-summary"
}

def fetch_all_regions():
    """Return list of dicts: [{region: name, count: N}, ...]"""
    results = []
    for region, url in endpoints.items():
        try:
            r = requests.get(url, timeout=6)
            r.raise_for_status()
            data = r.json()
            realtime = data.get("realtime", {}) if isinstance(data, dict) else {}
            total = 0
            for site in realtime.values():
                try:
                    total += int(site.get("total", 0))
                except Exception:
                    pass
            results.append({"region": region, "count": total})
        except RequestException as e:
            # log to console for now
            print(f"[region_clients] error fetching {region} @ {url}: {e}")
            results.append({"region": region, "count": None})
    return results

def fetch_all_details():
    """
    Return flattened 'details' list across all regions (tagged with '__region').
    Returns an empty list if none available.
    """
    all_details = []
    for region, url in endpoints.items():
        try:
            r = requests.get(url, timeout=6)
            r.raise_for_status()
            data = r.json()
            details = data.get("details", []) if isinstance(data, dict) else []
            for d in details:
                d2 = dict(d)
                d2["__region"] = region
                all_details.append(d2)
        except RequestException as e:
            print(f"[region_clients] warning: cannot fetch details from {region}@{url}: {e}")
            continue
        except Exception as e:
            print(f"[region_clients] unexpected error for {region}: {e}")
            continue
    return all_details







# reports.py
import csv
from compare_service import compare_with_active
from datetime import date

def write_daily_location_csv(target_date: date, out_path: str):
    summary = compare_with_active(target_date)
    rows = summary.get("by_location", [])
    # write to CSV
    with open(out_path, "w", newline="", encoding="utf-8") as fh:
        writer = csv.DictWriter(fh, fieldnames=["location_city", "total_n", "present_n", "percent_present"])
        writer.writeheader()
        for r in rows:
            writer.writerow({
                "location_city": r.get("location_city"),
                "total_n": r.get("total_n"),
                "present_n": r.get("present_n"),
                "percent_present": r.get("percent_present")
            })
    return out_path








# check_counts.py
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary
from datetime import date, datetime

with SessionLocal() as db:
    print("Active employees:", db.query(ActiveEmployee).count())
    print("Active contractors:", db.query(ActiveContractor).count())
    print("Live swipe rows:", db.query(LiveSwipe).count())
    print("AttendanceSummary rows:", db.query(AttendanceSummary).count())






(.venv) pip install requests python-dateutil pandas sqlalchemy


# backup
copy attendance.db attendance.db.bak   # windows
# reinitialize tables (creates if missing)
python -c "from ingest_excel import init_db; init_db(); print('db init done')"


(.venv) uvicorn app:app --reload --host 0.0.0.0 --port 8000















We dont get Any Output 
our Basically Requrnment is Compare Live Summary data in deatils with Active Sheet and 
and need Compare Summary data with ACtive sheet and Prepare Summary like 
How many Percentage Employee Visited Today 
Globally , As well Region , Location Wise ..

2) How many people are nominated for Specif Region then City and their Attendance 

3) and Most important point is here 

Add 
http://10.199.22.57:5001/api/employees 
-- Basically this API Gives data Globally like How Many profile are Created in CCure and Their braekdown 
This API Gives Total Profile deatsils ,,,
[
  {
    "id": 2097208998,
    "EmpName": "-, Paulet Isabel",
    "EmployeeID": "328326",
    "PersonnelType": "Terminated Personnel",
    "Manager_Name": "Lisseth Guilarte de Araujo",
    "Manager_WU_ID": "0",
    "Profile_Disabled": true,
    "Total_Cards": 1,
    "Active_Cards": 1,
    "Employee_Status": "Deactive",
    "imageUrl": "/api/employees/2097208998/image"
  },
  {
    "id": 2097198977,
    "EmpName": "., Ankit",
    "EmployeeID": "320762",
    "PersonnelType": "Terminated Personnel",
    "Manager_Name": "Sharad Jadhav",
    "Manager_WU_ID": "0",
    "Profile_Disabled": true,
    "Total_Cards": 0,
    "Active_Cards": 1,
    "Employee_Status": "Deactive",
    "imageUrl": "/api/employees/2097198977/image"
  },
  {
    "id": 2097197204,
    "EmpName": "., Anushka",
    "EmployeeID": "319473",
    "PersonnelType": "Employee",
    "Manager_Name": "Rawat, Mayank",
    "Manager_WU_ID": "0",
    "Profile_Disabled": false,
    "Total_Cards": 1,
    "Active_Cards": 1,
    "Employee_Status": "Active",
    "imageUrl": "/api/employees/2097197204/image"
  },


their BreakDown is like this

{
  "TotalProfiles": 21030,
  "ActiveProfiles": 10179,
  "ActiveEmployees": 8640,
  "ActiveContractors": 665,
  "TerminatedProfiles": 10851,
  "TerminatedEmployees": 148,
  "TerminatedContractors": 143
}

we need to Compare this date With ACtive Employee and Contractor Sheet.

using this API -

http://10.199.22.57:5001/api/stats?details=ActiveProfiles&page=1&limit=50  -- This API Filter only ActiveProfiles and their details..

  "total": 10179,
  "page": 1,
  "limit": 50,
  "data": [
    {
      "id": 2097197204,
      "EmpName": "., Anushka",
      "EmployeeID": "319473",
      "PersonnelType": "Employee",
      "Manager_Name": "Rawat, Mayank",
      "Manager_WU_ID": "0",
      "Profile_Disabled": false,
      "Total_Cards": 1,
      "Active_Cards": 1,
      "Employee_Status": "Active",
      "imageUrl": "/api/employees/2097197204/image"
    },
    {
      "id": 2097203526,
      "EmpName": "., Diwakar",
      "EmployeeID": "324002",
      "PersonnelType": "Employee",
      "Manager_Name": "Charkha, Bhakti",
      "Manager_WU_ID": "0",
      "Profile_Disabled": false,
      "Total_Cards": 1,
      "Active_Cards": 1,
      "Employee_Status": "Active",
      "imageUrl": "/api/employees/2097203526/image"
    },



http://10.199.22.57:5001/api/stats?details=ActiveEmployees&page=1&limit=50  -- This API Give Active Employee Details...

{
  "total": 8640,
  "page": 1,
  "limit": 50,
  "data": [
    {
      "id": 2097197204,
      "EmpName": "., Anushka",
      "EmployeeID": "319473",
      "PersonnelType": "Employee",
      "Manager_Name": "Rawat, Mayank",
      "Manager_WU_ID": "0",
      "Profile_Disabled": false,
      "Total_Cards": 1,
      "Active_Cards": 1,
      "Employee_Status": "Active",
      "imageUrl": "/api/employees/2097197204/image"
    },



http://10.199.22.57:5001/api/stats?details=ActiveContractors&page=1&limit=50

{
  "total": 665,
  "page": 1,
  "limit": 50,
  "data": [
    {
      "id": 2097209941,
      "EmpName": "Abdalla, Meira",
      "EmployeeID": "W0026455",
      "PersonnelType": "Contractor",
      "Manager_Name": "Luis Rodriguez ",
      "Manager_WU_ID": "0",
      "Profile_Disabled": false,
      "Total_Cards": 2,
      "Active_Cards": 2,
      "Employee_Status": "Active",
      "imageUrl": "/api/employees/2097209941/image"
    },


http://10.199.22.57:5001/api/stats?details=TerminatedProfiles&page=1&limit=50

{
  "total": 10851,
  "page": 1,
  "limit": 50,
  "data": [
    {
      "id": 2097208998,
      "EmpName": "-, Paulet Isabel",
      "EmployeeID": "328326",
      "PersonnelType": "Terminated Personnel",
      "Manager_Name": "Lisseth Guilarte de Araujo",
      "Manager_WU_ID": "0",
      "Profile_Disabled": true,
      "Total_Cards": 1,
      "Active_Cards": 1,
      "Employee_Status": "Deactive",
      "imageUrl": "/api/employees/2097208998/image"
    },





http://10.199.22.57:5001/api/stats?details=TerminatedEmployees&page=1&limit=50

{
  "total": 148,
  "page": 1,
  "limit": 50,
  "data": [
    {
      "id": 2097164234,
      "EmpName": "Ambadipudi, Padmank",
      "EmployeeID": "1234567890",
      "PersonnelType": "Employee",
      "Manager_Name": "",
      "Manager_WU_ID": "0",
      "Profile_Disabled": true,
      "Total_Cards": 1,
      "Active_Cards": 0,
      "Employee_Status": "Deactive",
      "imageUrl": "/api/employees/2097164234/image"
    },



http://10.199.22.57:5001/api/stats?details=TerminatedContractors&page=1&limit=50
{
  "total": 143,
  "page": 1,
  "limit": 50,
  "data": [
    {
      "id": 2097204167,
      "EmpName": "Acuna Quintanilla, Lina",
      "EmployeeID": "W0012898",
      "PersonnelType": "Contractor",
      "Manager_Name": "Monica Mattacheo",
      "Manager_WU_ID": "0",
      "Profile_Disabled": true,
      "Total_Cards": 2,
      "Active_Cards": 0,
      "Employee_Status": "Deactive",
      "imageUrl": "/api/employees/2097204167/image"
    },
    {
      "id": 2097204849,
      "EmpName": "Acuna, Debora",
      "EmployeeID": "W0016750",
      "PersonnelType": "Contractor",
      "Manager_Name": "Luciano Santoro",
      "Manager_WU_ID": "0",
      "Profile_Disabled": true,
      "Total_Cards": 1,
      "Active_Cards": 0,
      "Employee_Status": "Deactive",
      "imageUrl": "/api/employees/2097204849/image"
    },



this are API details so Basically add this API and and do Comparision on Basis of 


so Compare this data With 
ex- 
AS Per API Responce 

  "TotalProfiles": 21030,
  "ActiveProfiles": 10186,
  "ActiveEmployees": 8643,
  "ActiveContractors": 669,
  "TerminatedProfiles": 10844,
  "TerminatedEmployees": 148,
  "TerminatedContractors": 143

As per Active Emplooyee Sheet - 8,500 employee
as per Contractor Sheet - 600 Conntractor 

then chcck Diffrance like Emplooyee Sheet - 8,500 display this Count 
API Display this Count then match their EMployee ID and other filed and see Why Count is mismatch 


also so mismatchg Count may be Temp badges , Visitors badges, Property managenent ...like 



we have one Comparision logic and see this logic also 


import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import pandas as pd
from config import db_config
import pyodbc
 
def fetch_sql_data():
    """Fetch personnel data from SQL database"""
    conn = db_config.get_sql_connection(
        db_config.DB_CONFIG['SQL_SERVER'],
        db_config.DB_CONFIG['SQL_DB'],
        db_config.DB_CONFIG['SQL_USER'],
        db_config.DB_CONFIG['SQL_PASSWORD']
    )
    if conn is None:
        print("SQL connection failed.")
        return None
    
    query = db_config.build_sql_query()
    sql_df = pd.read_sql(query, conn)
    conn.close()
    return sql_df
 
def load_excel_data():
    """Load Active Contractors and Active Employees data"""
    contractors_df = pd.read_excel(db_config.FILE_PATHS['excel_file_1'])
    employees_df = pd.read_excel(db_config.FILE_PATHS['excel_file_2'])
    return contractors_df, employees_df
 
def prepare_id_sets(contractors_df, employees_df):
    """Prepare unique IDs from Excel sheets"""
    # Convert all to string and get unique IDs from Contractor sheet
    contractor_ids = pd.concat([
        contractors_df['Worker System Id'].astype(str),
        contractors_df['iPass ID'].astype(str),
        contractors_df['"W" iPass ID'].astype(str)
    ]).unique()
 
    # Get unique Employee IDs from Employee Active sheet
    employee_ids = employees_df['Employee ID'].astype(str).unique()
 
    # Combine both into a single set for comparison
    all_excel_ids = set(contractor_ids).union(set(employee_ids))
 
    return all_excel_ids
 
def compare_and_export(sql_df, all_excel_ids):
    """Compare SQL personnel data with Excel IDs and export unmatched profiles"""
    sql_df['EmployeeID'] = sql_df['EmployeeID'].astype(str)
 
    # Find extra profiles in SQL
    extra_profiles_df = sql_df[~sql_df['EmployeeID'].isin(all_excel_ids)]
 
    if extra_profiles_df.empty:
        print("No extra profiles found in SQL.")
    else:
        # Write results to Excel
        with pd.ExcelWriter(db_config.FILE_PATHS['output_report'], engine='xlsxwriter') as writer:
            extra_profiles_df.to_excel(writer, sheet_name='Extra Profiles', index=False)
        print(f"Extra profiles exported to: {db_config.FILE_PATHS['output_report']}")
 
def run_comparison():
    """Run the complete comparison workflow"""
    print("Loading Excel files...")
    contractors_df, employees_df = load_excel_data()
 
    print("Fetching SQL data...")
    sql_df = fetch_sql_data()
    if sql_df is None:
        return
 
    print("Preparing ID sets...")
    all_excel_ids = prepare_id_sets(contractors_df, employees_df)
 
    print("Comparing and exporting unmatched profiles...")
    compare_and_export(sql_df, all_excel_ids)
 
    print("Comparison process completed successfully.")
 
# Run the script if executed directly
if __name__ == "__main__":
    run_comparison()


check only logic how data Compare 




3) http://localhost:8000/reports/daily/20250818

   "employee_id": "314217",
      "full_name": "Gibellini, Gonzalo Nicolás",
      "location_city": "Buenos Aires",
      "status": null,
      "card_number": null,
      "presence_count": 0,
      "first_seen": null,
      "last_seen": null,
      "card_number_att": null,
      "partition": null,
      "full_name_att": null,
      "mapped_employee_id": null,
      "present_today": false
    },
    {
      "employee_id": "314232",
      "full_name": "Mateu González, Alvaro Luis",
      "location_city": "Panama City",
      "status": null,
      "card_number": null,
      "presence_count": 0,
      "first_seen": null,
      "last_seen": null,
      "card_number_att": null,
      "partition": null,
      "full_name_att": null,
      "mapped_employee_id": null,
      "present_today": false
    },
    {
      "employee_id": "314234",
      "full_name": "Jimenez Meza, Nicole Jimena",
      "location_city": "Santa Ana",
      "status": null,
      "card_number": null,
      "presence_count": 0,
      "first_seen": null,
      "last_seen": null,




Also this API are still not match status card number Presence count any data 



so read below file carefully and fix that issue 


# app.py
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.responses import JSONResponse
import shutil, uuid, json
from settings import UPLOAD_DIR, OUTPUT_DIR
import os

app = FastAPI(title="Attendance Analytics")

@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = UPLOAD_DIR / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    # lazy import to avoid DB activity at import
    try:
        from ingest_excel import ingest_employee_excel
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_employee_excel(dest)
    return {"status":"ok", "path": str(dest)}

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    dest = UPLOAD_DIR / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_contractor_excel
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_contractor_excel(dest)
    return {"status":"ok", "path": str(dest)}

@app.post("/ingest/live-details")
async def ingest_live(request: Request):
    """
    Accepts:
      - Raw JSON array in body (preferred)
      - JSON object with {"details": [...]} in body
      - multipart/form-data where a form field 'details' contains a JSON string
    """
    details = None
    try:
        body = await request.json()
        if isinstance(body, dict) and 'details' in body:
            details = body['details']
        else:
            details = body
    except Exception:
        # not JSON, try form
        try:
            form = await request.form()
            if 'details' in form:
                raw = form['details']
                if isinstance(raw, str):
                    details = json.loads(raw)
                else:
                    try:
                        details = json.loads((await raw.read()).decode('utf-8'))
                    except Exception:
                        details = list(form.getlist('details'))
            else:
                # attempt first field
                first = None
                for v in form.values():
                    first = v
                    break
                if isinstance(first, str):
                    details = json.loads(first)
                else:
                    raise HTTPException(status_code=400, detail="No JSON payload found")
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Could not parse request body as JSON or form: {e}")

    if not isinstance(details, (list, tuple)):
        raise HTTPException(status_code=400, detail="Expected top-level array (JSON list) of detail objects")

    # lazy import of compare_service
    try:
        from compare_service import ingest_live_details_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    try:
        res = ingest_live_details_list(details)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to ingest details: {e}")

    if isinstance(res, dict):
        return {"status": "ok", **res}
    return {"status": "ok", "inserted": len(details)}

@app.get("/ingest/fetch-all")
def fetch_all_and_ingest():
    try:
        import region_clients
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"region_clients unavailable: {e}")

    details = region_clients.fetch_all_details()
    if not isinstance(details, list):
        raise HTTPException(status_code=500, detail="Unexpected data from region_clients.fetch_all_details")

    try:
        from compare_service import ingest_live_details_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    res = ingest_live_details_list(details)
    if isinstance(res, dict):
        return {"status":"ok", **res}
    return {"status":"ok", "inserted": len(details)}

@app.get("/reports/daily/{yyyymmdd}")
def daily_report(yyyymmdd: str):
    import datetime
    try:
        dt = datetime.datetime.strptime(yyyymmdd, "%Y%m%d").date()
    except Exception:
        raise HTTPException(status_code=400, detail="Date must be in YYYYMMDD format")

    try:
        from compare_service import compute_daily_attendance, compare_with_active
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"compare_service import failed: {e}")

    compute_daily_attendance(dt)
    summary = compare_with_active(dt)
    return JSONResponse(summary)






# check_counts.py
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary

with SessionLocal() as db:
    print("Active employees:", db.query(ActiveEmployee).count())
    print("Active contractors:", db.query(ActiveContractor).count())
    print("Live swipe rows:", db.query(LiveSwipe).count())
    print("AttendanceSummary rows:", db.query(AttendanceSummary).count())





# compare_service.py
import pandas as pd
import numpy as np
from datetime import datetime, date, timezone
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary
import re

# --- Helpers -----------------------------------------------------------------

def _to_native(value):
    if value is None:
        return None
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    if isinstance(value, (np.integer,)):
        return int(value)
    if isinstance(value, (np.floating,)):
        return float(value)
    if isinstance(value, (np.bool_, bool)):
        return bool(value)
    try:
        import datetime as _dt
        if isinstance(value, _dt.datetime):
            try:
                if value.tzinfo is not None:
                    utc = value.astimezone(timezone.utc)
                    return utc.replace(tzinfo=None).isoformat() + "Z"
                else:
                    return value.isoformat()
            except Exception:
                return str(value)
        if hasattr(value, 'isoformat'):
            try:
                return value.isoformat()
            except Exception:
                return str(value)
    except Exception:
        pass
    return value

def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    """Return normalized card-like string: digits only, no spaces, trim leading zeros."""
    if s is None:
        return None
    try:
        ss = str(s).strip()
        # remove non-digit characters (cards commonly numeric)
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        # strip leading zeros so '007' and '7' match
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        # remove extra spaces/punctuation
        t = re.sub(r'[\s]+', ' ', re.sub(r'[^\w\s]', '', t))
        return t
    except Exception:
        return None

# timestamp parsing helpers (unchanged, robust)
def _parse_timestamp_from_value(val):
    if val is None:
        return None
    from datetime import datetime as _dt
    if isinstance(val, _dt):
        dt = val
        try:
            if dt.tzinfo is not None:
                dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            try:
                return dt.replace(tzinfo=None)
            except Exception:
                return None
    try:
        import numpy as _np
        if isinstance(val, (int, float, _np.integer, _np.floating)):
            v = int(val)
            if v > 1e12:
                return _dt.fromtimestamp(v / 1000.0, tz=timezone.utc).replace(tzinfo=None)
            else:
                return _dt.fromtimestamp(v, tz=timezone.utc).replace(tzinfo=None)
    except Exception:
        pass
    if isinstance(val, str):
        s = val.strip()
        if s == "":
            return None
        try:
            if s.endswith("Z"):
                s2 = s.replace("Z", "+00:00")
                dt = _dt.fromisoformat(s2)
                if dt.tzinfo is not None:
                    return dt.astimezone(timezone.utc).replace(tzinfo=None)
                return dt
            try:
                dt = _dt.fromisoformat(s)
                if dt.tzinfo is not None:
                    return dt.astimezone(timezone.utc).replace(tzinfo=None)
                return dt
            except Exception:
                pass
            if s.isdigit():
                v = int(s)
                if v > 1e12:
                    return _dt.fromtimestamp(v / 1000.0, tz=timezone.utc).replace(tzinfo=None)
                else:
                    return _dt.fromtimestamp(v, tz=timezone.utc).replace(tzinfo=None)
            for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M:%S.%f",
                        "%d/%m/%Y %H:%M:%S", "%d-%m-%Y %H:%M:%S"):
                try:
                    dt = _dt.strptime(s, fmt)
                    return dt
                except Exception:
                    pass
        except Exception:
            pass
    return None

def _extract_timestamp_from_detail(detail):
    keys = [
        "LocaleMessageDateTime", "LocalMessageDateTime", "LocaleMessageTime", "LocalMessageTime",
        "LocaleMessageDate", "Timestamp", "timestamp", "Time", "LocaleTime", "LocalTime",
        "time", "date", "LocaleMessageDateTimeUtc", "LocalMessageDateTimeUtc"
    ]
    if not isinstance(detail, dict):
        return _parse_timestamp_from_value(detail)
    for k in keys:
        if k in detail:
            ts = detail.get(k)
            parsed = _parse_timestamp_from_value(ts)
            if parsed is not None:
                return parsed
    for v in detail.values():
        p = _parse_timestamp_from_value(v)
        if p is not None:
            return p
    return None

# --- Main functions ----------------------------------------------------------

def ingest_live_details_list(details_list):
    """Persist details_list into LiveSwipe. returns counts."""
    from db import SessionLocal as _SessionLocal
    inserted = 0
    skipped = 0
    with _SessionLocal() as db:
        for d in details_list:
            try:
                ts_parsed = _extract_timestamp_from_detail(d)
            except Exception:
                ts_parsed = None
            if ts_parsed is None:
                skipped += 1
                continue

            emp = _normalize_employee_key(d.get("EmployeeID") or d.get("employee_id") or d.get("employeeId"))
            card = _normalize_employee_key(d.get("CardNumber") or d.get("card_number") or d.get("Card"))
            full_name = d.get("ObjectName1") or d.get("FullName") or d.get("full_name")
            partition = d.get("PartitionName2") or d.get("PartitionName1") or d.get("Partition")
            floor = d.get("Floor") or d.get("floor")
            door = d.get("Door") or d.get("DoorName") or d.get("door")
            region = d.get("PartitionName2") or d.get("Region") or d.get("region")

            rec = LiveSwipe(
                timestamp=ts_parsed,
                employee_id=emp,
                card_number=card,
                full_name=full_name,
                partition=partition,
                floor=floor,
                door=door,
                region=region,
                raw=d
            )
            db.add(rec)
            inserted += 1
        db.commit()
    return {"inserted": inserted, "skipped_invalid_timestamp": skipped}

def compute_daily_attendance(target_date: date):
    """Build AttendanceSummary rows for target_date."""
    with SessionLocal() as db:
        start = datetime.combine(target_date, datetime.min.time())
        end = datetime.combine(target_date, datetime.max.time())
        swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
        if not swipes:
            return []

        rows = []
        for s in swipes:
            rows.append({
                "id": s.id,
                "timestamp": s.timestamp,
                "employee_id": _normalize_employee_key(s.employee_id),
                "card_number": _normalize_card_like(s.card_number),
                "full_name": s.full_name,
                "partition": s.partition,
                "floor": s.floor,
                "door": s.door
            })
        df = pd.DataFrame(rows)
        if df.empty:
            return []

        df['key'] = df['employee_id'].fillna(df['card_number'])
        df = df[df['key'].notna()]
        if df.empty:
            return []

        grouped = df.groupby('key', dropna=False).agg(
            presence_count=('id', 'count'),
            first_seen=('timestamp', 'min'),
            last_seen=('timestamp', 'max'),
            full_name=('full_name', 'first'),
            partition=('partition', 'first'),
            card_number=('card_number', 'first')
        ).reset_index().rename(columns={'key': 'employee_id'})

        for _, row in grouped.iterrows():
            try:
                derived_obj = {
                    "partition": (row.get('partition') or None),
                    "full_name": (row.get('full_name') or None),
                    "card_number": (row.get('card_number') or None)
                }
                rec = AttendanceSummary(
                    employee_id=str(row['employee_id']) if pd.notna(row['employee_id']) else None,
                    date=target_date,
                    presence_count=int(row['presence_count']),
                    first_seen=row['first_seen'],
                    last_seen=row['last_seen'],
                    derived=derived_obj
                )
                db.merge(rec)
            except Exception:
                continue
        db.commit()
        return grouped.to_dict(orient='records')

def compare_with_active(target_date: date):
    """Compare AttendanceSummary for date with ActiveEmployee & ActiveContractor and return json-safe dict."""
    with SessionLocal() as db:
        att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == target_date).all()
        if not att_rows:
            att_df = pd.DataFrame(columns=["employee_id", "presence_count", "first_seen", "last_seen", "card_number", "partition", "full_name"])
        else:
            att_df = pd.DataFrame([{
                "employee_id": _normalize_employee_key(a.employee_id),
                "presence_count": a.presence_count,
                "first_seen": a.first_seen,
                "last_seen": a.last_seen,
                "card_number": _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None),
                "partition": (a.derived.get('partition') if (a.derived and isinstance(a.derived, dict)) else None),
                "full_name": (a.derived.get('full_name') if (a.derived and isinstance(a.derived, dict)) else None)
            } for a in att_rows])

        act_rows = db.query(ActiveEmployee).all()
        contractor_rows = db.query(ActiveContractor).all()

        # build unified active table and card->employee map
        act_list = []
        card_to_emp = {}
        name_to_emp = {}

        # Employees
        for e in act_rows:
            emp_id_norm = _normalize_employee_key(e.employee_id)
            # attempt to extract card-like fields inside raw_row
            card_from_raw = None
            try:
                rr = e.raw_row or {}
                if isinstance(rr, dict):
                    for ck in ("CardNumber", "card_number", "Card", "Card No", "CardNo", "IPassID", "IpassID", "Ipass Id"):
                        if ck in rr and rr.get(ck):
                            card_from_raw = _normalize_card_like(rr.get(ck))
                            break
            except Exception:
                card_from_raw = None

            act_list.append({
                "employee_id": emp_id_norm,
                "full_name": e.full_name,
                "location_city": e.location_city,
                "status": e.current_status,
                "card_number": card_from_raw
            })
            if emp_id_norm:
                card_to_emp[emp_id_norm] = emp_id_norm
            if card_from_raw:
                card_to_emp[card_from_raw] = emp_id_norm
            n = _normalize_name(e.full_name)
            if n:
                name_to_emp[n] = emp_id_norm

        # Contractors
        for c in contractor_rows:
            worker_id = _normalize_employee_key(c.worker_system_id)
            ipass = _normalize_employee_key(c.ipass_id)
            w_ipass = ("W" + ipass) if ipass else None
            primary_id = worker_id or ipass or None
            act_list.append({
                "employee_id": primary_id,
                "full_name": c.full_name,
                "location_city": c.location,
                "status": c.status,
                "card_number": None
            })
            if primary_id:
                card_to_emp[primary_id] = primary_id
            if ipass:
                card_to_emp[ipass] = primary_id
            if w_ipass:
                card_to_emp[w_ipass] = primary_id
            try:
                rr = c.raw_row or {}
                if isinstance(rr, dict):
                    for ck in ("Worker System Id","Worker System ID","iPass ID","IPassID","CardNumber","card_number"):
                        if ck in rr and rr.get(ck):
                            key = _normalize_card_like(rr.get(ck))
                            if key:
                                card_to_emp[key] = primary_id
            except Exception:
                pass
            n = _normalize_name(c.full_name)
            if n:
                name_to_emp[n] = primary_id

        act_df = pd.DataFrame(act_list)

        # If no active rows, return attendance-only view
        if act_df.empty:
            if att_df.empty:
                return {"by_location": [], "merged": []}
            att_df['partition'] = att_df.get('partition').fillna('Unknown')
            att_df['presence_count'] = att_df['presence_count'].fillna(0)
            att_df['present_today'] = att_df['presence_count'].apply(lambda x: bool(x and x != 0))
            loc_group = att_df.groupby('partition', dropna=False).agg(
                total_n=('employee_id', 'count'),
                present_n=('present_today', 'sum')
            ).reset_index().rename(columns={'partition':'location_city'})
            loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
            by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]
            merged_list = []
            for r in att_df.to_dict(orient='records'):
                merged_list.append({
                    "employee_id": _to_native(r.get('employee_id')),
                    "presence_count": _to_native(r.get('presence_count')),
                    "first_seen": _to_native(r.get('first_seen')),
                    "last_seen": _to_native(r.get('last_seen')),
                    "full_name": _to_native(r.get('full_name')),
                    "location_city": _to_native(r.get('partition')),
                    "present_today": _to_native(r.get('present_today'))
                })
            return {"by_location": by_location, "merged": merged_list}

        # normalize columns
        act_df['employee_id'] = act_df['employee_id'].astype(object).apply(_normalize_employee_key)
        att_df['employee_id'] = att_df['employee_id'].astype(object).apply(_normalize_employee_key)
        act_df['card_number'] = act_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in act_df.columns else pd.Series([pd.NA]*len(act_df))
        att_df['card_number'] = att_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in att_df.columns else pd.Series([pd.NA]*len(att_df))

        # ensure card_to_emp includes act_df card_numbers
        for r in act_df.to_dict(orient='records'):
            c = r.get('card_number')
            eid = r.get('employee_id')
            if c and eid:
                card_to_emp[c] = eid
            if eid:
                # also map leading-zero-trimmed numeric forms
                n = re.sub(r'\D','', str(eid))
                if n:
                    card_to_emp[n.lstrip('0') or n] = eid

        # mapping function tries multiple strategies
        def remap_att_key(row):
            # try attendance.employee_id first (often actual emp id)
            primary = row.get('employee_id') or None
            # try card-number field now
            card = row.get('card_number') or None

            # normalized forms
            primary_norm = _normalize_employee_key(primary)
            card_norm = _normalize_card_like(card)

            # 1) exact employee id exists in active list
            if primary_norm and primary_norm in set(act_df['employee_id'].dropna().astype(str)):
                return primary_norm

            # 2) primary_norm could be numeric-like but actually a card; check card_to_emp
            if primary_norm:
                ptrim = re.sub(r'\D','', primary_norm)
                if ptrim:
                    ptrim_nz = ptrim.lstrip('0') or ptrim
                    if ptrim_nz in card_to_emp:
                        return card_to_emp[ptrim_nz]
                if primary_norm in card_to_emp:
                    return card_to_emp[primary_norm]

            # 3) card_norm directly maps
            if card_norm:
                if card_norm in card_to_emp:
                    return card_to_emp[card_norm]
                # try trimmed numeric
                trytrim = card_norm.lstrip('0') or card_norm
                if trytrim in card_to_emp:
                    return card_to_emp[trytrim]

            # 4) fallback: try matching by normalized full name (best-effort)
            fname = _normalize_name(row.get('full_name') or row.get('full_name_att') or None)
            if fname and fname in name_to_emp:
                return name_to_emp[fname]

            # 5) last resort: return original primary (so attendance still present)
            return primary_norm or card_norm or None

        att_df['mapped_employee_id'] = att_df.apply(remap_att_key, axis=1)

        # drop original employee_id column from att frame to avoid duplicate labels during merge
        att_merge_df = att_df.drop(columns=['employee_id'], errors='ignore').copy()

        # merge using left_on act_df.employee_id, right_on att_merge_df.mapped_employee_id
        merged = pd.merge(
            act_df,
            att_merge_df,
            left_on='employee_id',
            right_on='mapped_employee_id',
            how='left',
            suffixes=('', '_att')
        )

        # fill and finalize
        merged['presence_count'] = merged.get('presence_count', pd.Series([0]*len(merged))).fillna(0)
        merged['presence_count'] = merged['presence_count'].apply(lambda x: int(x) if (pd.notnull(x) and float(x).is_integer()) else x)
        merged['present_today'] = merged['presence_count'].apply(lambda x: bool(x and x != 0))
        merged['location_city'] = merged.get('location_city').fillna('Unknown')

        loc_group = merged.groupby('location_city', dropna=False).agg(
            total_n=('employee_id', 'count'),
            present_n=('present_today', 'sum')
        ).reset_index()
        loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
        by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]

        merged_list = []
        for r in merged.to_dict(orient='records'):
            clean = {k:_to_native(v) for k,v in r.items()}
            if 'employee_id' not in clean:
                clean['employee_id'] = None
            merged_list.append(clean)

        return {"by_location": by_location, "merged": merged_list}




C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\db.py

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from settings import DB_URL

engine = create_engine(DB_URL, connect_args={"check_same_thread": False} if DB_URL.startswith("sqlite") else {})
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
Base = declarative_base()





#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\ingest_excel.py
import pandas as pd
from datetime import datetime
from sqlalchemy.exc import IntegrityError
from db import SessionLocal, engine
from models import Base, ActiveEmployee, ActiveContractor
from settings import UPLOAD_DIR
import uuid, os

# --- database setup: do NOT run create_all at import time ---
def init_db():
    """
    Create DB tables if they do not exist.
    Call this manually only when you want to initialize/repair the DB:
      python -c "from ingest_excel import init_db; init_db()"
    """
    from db import engine
    from models import Base
    Base.metadata.create_all(bind=engine)
    
def normalize_colname(c):
    return ''.join(ch for ch in c).strip()

def ingest_employee_excel(path, uploaded_by="system"):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    important = ['Employee ID','Full Name','Location City','Location Description','Current Status','Employee Type','Employee\'s Email']
    # adapt column mapping if names differ
    with SessionLocal() as db:
        for _, row in df.iterrows():
            emp_id = (row.get('Employee ID') or row.get('EmployeeID') or '').strip()
            if not emp_id:
                continue
            full_name = (row.get('Full Name') or f"{row.get('First Name','')} {row.get('Last Name','')}").strip()
            rec = ActiveEmployee(
                employee_id=emp_id,
                full_name=full_name,
                email=row.get("Employee's Email"),
                location_city=row.get('Location City') or row.get('Location Description'),
                location_desc=row.get('Location Description'),
                current_status=row.get('Current Status'),
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)  # upsert logic with merge
                db.commit()
            except IntegrityError:
                db.rollback()

def ingest_contractor_excel(path):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    with SessionLocal() as db:
        for _, row in df.iterrows():
            wsid = (row.get('Worker System Id') or row.get('Worker System ID') or '').strip()
            if not wsid:
                continue
            rec = ActiveContractor(
                worker_system_id=wsid,
                ipass_id=row.get('iPass ID') or row.get('"W" iPass ID'),
                full_name=row.get('Full Name'),
                vendor=row.get('Vendor Company Name'),
                location=row.get('Worker Location'),
                status=row.get('Status'),
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)
                db.commit()
            except IntegrityError:
                db.rollback()

if __name__ == "__main__":
    # example usage
    for f in os.listdir(UPLOAD_DIR):
        p = UPLOAD_DIR / f
        if 'contractor' in f.lower():
            ingest_contractor_excel(p)
        else:
            ingest_employee_excel(p)
    print("Ingestion completed.")






#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\models.py
from sqlalchemy import Column, Integer, String, DateTime, JSON, Boolean, Date
from sqlalchemy import ForeignKey, UniqueConstraint
from sqlalchemy.orm import relationship
from db import Base

class ActiveEmployee(Base):
    __tablename__ = "active_employees"
    id = Column(Integer, primary_key=True)
    employee_id = Column(String, index=True, unique=True, nullable=False)
    full_name = Column(String, index=True)
    email = Column(String)
    location_city = Column(String, index=True)
    location_desc = Column(String)
    current_status = Column(String)
    raw_row = Column(JSON)  # store original row for reference
    uploaded_at = Column(DateTime)

class ActiveContractor(Base):
    __tablename__ = "active_contractors"
    id = Column(Integer, primary_key=True)
    worker_system_id = Column(String, index=True, unique=True, nullable=False)
    ipass_id = Column(String, index=True)
    full_name = Column(String, index=True)
    vendor = Column(String)
    location = Column(String)
    status = Column(String)
    raw_row = Column(JSON)
    uploaded_at = Column(DateTime)

class LiveSwipe(Base):
    __tablename__ = "live_swipes"
    id = Column(Integer, primary_key=True)
    timestamp = Column(DateTime, index=True)
    employee_id = Column(String, index=True, nullable=True)
    card_number = Column(String, index=True, nullable=True)
    full_name = Column(String)
    partition = Column(String, index=True)
    floor = Column(String)
    door = Column(String)
    region = Column(String, index=True)
    raw = Column(JSON)

class AttendanceSummary(Base):
    __tablename__ = "attendance_summary"
    id = Column(Integer, primary_key=True)
    employee_id = Column(String, index=True)
    date = Column(Date, index=True)
    presence_count = Column(Integer)
    first_seen = Column(DateTime)
    last_seen = Column(DateTime)
    derived = Column(JSON)  # extra stats




# region_clients.py
import requests
from requests.exceptions import RequestException

# Edit endpoints if your hosts/ports differ
endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/live-summary",
    "emea":  "http://10.199.22.57:3007/api/occupancy/live-summary",
    "laca":  "http://10.199.22.57:4000/api/occupancy/live-summary",
    "apac":  "http://10.199.22.57:3008/api/occupancy/live-summary"
}

def fetch_all_regions():
    """Return list of dicts: [{region: name, count: N}, ...]"""
    results = []
    for region, url in endpoints.items():
        try:
            r = requests.get(url, timeout=6)
            r.raise_for_status()
            data = r.json()
            realtime = data.get("realtime", {}) if isinstance(data, dict) else {}
            total = 0
            for site in realtime.values():
                try:
                    total += int(site.get("total", 0))
                except Exception:
                    pass
            results.append({"region": region, "count": total})
        except RequestException as e:
            # log to console for now
            print(f"[region_clients] error fetching {region} @ {url}: {e}")
            results.append({"region": region, "count": None})
    return results

def fetch_all_details():
    """
    Return flattened 'details' list across all regions (tagged with '__region').
    Returns an empty list if none available.
    """
    all_details = []
    for region, url in endpoints.items():
        try:
            r = requests.get(url, timeout=6)
            r.raise_for_status()
            data = r.json()
            details = data.get("details", []) if isinstance(data, dict) else []
            for d in details:
                d2 = dict(d)
                d2["__region"] = region
                all_details.append(d2)
        except RequestException as e:
            print(f"[region_clients] warning: cannot fetch details from {region}@{url}: {e}")
            continue
    return all_details




# reports.py
import csv
from compare_service import compare_with_active
from datetime import datetime, date

def write_daily_location_csv(target_date: date, out_path: str):
    summary = compare_with_active(target_date)
    rows = summary.get("by_location", [])
    # write to CSV
    with open(out_path, "w", newline="", encoding="utf-8") as fh:
        writer = csv.DictWriter(fh, fieldnames=["location_city", "total_n", "present_n", "percent_present"])
        writer.writeheader()
        for r in rows:
            writer.writerow({
                "location_city": r.get("location_city"),
                "total_n": r.get("total_n"),
                "present_n": r.get("present_n"),
                "percent_present": r.get("percent_present")
            })
    return out_path




#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\settings.py
import os
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent
UPLOAD_DIR = BASE_DIR / "data" / "raw_uploads"
OUTPUT_DIR = BASE_DIR / "data" / "outputs"
DB_URL = os.environ.get("ATT_DB_URL", "sqlite:///./attendance.db")  # change to postgres in prod
LIVE_API_URL = os.environ.get("LIVE_API_URL", "http://localhost:3008/api/occupancy/live-summary")

UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)



# ml/predict.py
from joblib import load
import os
import pandas as pd

MODEL_PATH = os.path.join(os.path.dirname(__file__), "isojob.joblib")

def score_features(df_features: pd.DataFrame):
    """
    Returns anomaly scores if model exists, otherwise returns None.
    df_features: DataFrame with same columns used during training (e.g. days_present, presence_rate)
    """
    if not os.path.exists(MODEL_PATH):
        return None
    clf = load(MODEL_PATH)
    preds = clf.predict(df_features)        # -1 anomaly, 1 normal
    scores = clf.decision_function(df_features)
    df = df_features.copy()
    df["pred"] = preds
    df["score"] = scores
    return df




#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\ml\train_model.py
import pandas as pd
from sklearn.ensemble import IsolationForest
from joblib import dump, load
from db import SessionLocal
from models import AttendanceSummary
from datetime import date, timedelta

def build_feature_table(last_n_days=30):
    with SessionLocal() as db:
        # pull attendance summary for last N days
        end = date.today()
        start = end - timedelta(days=last_n_days)
        q = db.query(AttendanceSummary).filter(AttendanceSummary.date >= start, AttendanceSummary.date <= end)
        df = pd.read_sql(q.statement, q.session.bind)
    if df.empty:
        return None
    # pivot: rows=employee_id, cols=date, values=presence_count>0
    df['present'] = df['presence_count'] > 0
    pivot = df.pivot_table(index='employee_id', columns='date', values='present', aggfunc='max', fill_value=0)
    pivot['days_present'] = pivot.sum(axis=1)
    pivot['presence_rate'] = pivot['days_present'] / last_n_days
    features = pivot[['days_present','presence_rate']].fillna(0)
    return features

def train_isolationforest(save_path="models/isojob.joblib"):
    features = build_feature_table()
    if features is None:
        raise RuntimeError("No data")
    clf = IsolationForest(contamination=0.05, random_state=42)
    clf.fit(features)
    dump(clf, save_path)
    return save_path

if __name__ == "__main__":
    print("Training...")
    print(train_isolationforest())

