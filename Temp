When i Update this APi we got Wrong data ...


Employee Name	Employee ID	Card	Date	Time	SwipeGap	Door	Direction	Zone	Note
-	314528	416096	2025-11-17	05:57:11	00:00:00	LACA CR A2 Main Lobby Door	InDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	07:07:39	-	LACA CR A2 Emergency Elevators Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	07:10:33	-	LACA CR A2 Main Lobby Door	InDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	07:12:54	-	LACA CR A2 Main Lobby Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	08:53:06	-	LACA CR A2 Main Lobby Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	09:39:46	-	LACA CR A2 Main Lobby Door	InDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	09:50:48	-	LACA CR A2 Emergency Elevators Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	09:51:13	-	LACA CR B1 Main Lobby Door	InDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	09:53:11	-	LACA CR B1 Main Lobby Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	09:53:55	-	LACA CR A2 Main Lobby Door	InDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	12:06:54	-	LACA CR A2 Main Lobby Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	12:07:04	-	LACA CR B2 Main Lobby Door	InDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	12:08:19	-	LACA CR B2 Main Lobby Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	12:40:48	-	LACA CR A2 Main Lobby Door	InDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	14:27:27	-	LACA CR A2 Main Lobby Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	14:33:35	-	LACA CR A2 Main Lobby Door	InDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	15:08:51	-	LACA CR A2 Main Lobby Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	15:09:02	-	LACA CR B2 Main Lobby Door	InDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	15:51:09	-	LACA CR B2 Main Lobby Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	15:51:19	-	LACA CR A2 Main Lobby Door	InDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	17:26:56	-	LACA CR A2 Main Lobby Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	17:33:01	-	LACA CR A2 Main Lobby Door	InDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)
-	314528	416096	2025-11-17	17:33:12	-	LACA CR A2 Emergency Elevators Door	OutDirection	-	- (swipes_crcosta-rica-partition_20251117.csv)


Employee name is not visible 
Swipe gap is not display 

Compare both previous and currentv API and Fix this issue carefully....




# @app.route('/record', methods=['GET'])
# def record():
#     try:
#         # --- BEGIN existing record() logic ---
#         from pathlib import Path
#         import pandas as pd
#         import math
#         import re
#         from datetime import datetime, date
#         try:
#             q = request.args.get('employee_id') or request.args.get('person_uid')
#         except Exception:
#             q = None
#         include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
#         city_param = request.args.get('city') or request.args.get('site') or 'pune'

#         # pick outdir consistently
#         try:
#             base_out = Path(DEFAULT_OUTDIR)
#         except Exception:
#             try:
#                 base_out = Path(OUTDIR)
#             except Exception:
#                 base_out = Path.cwd()

#         # helper safe wrappers (use existing ones if present)
#         def _safe_read(fp, **kwargs):
#             try:
#                 if '_safe_read_csv' in globals():
#                     return _safe_read_csv(fp)
#                 return pd.read_csv(fp, **kwargs)
#             except Exception:
#                 try:
#                     return pd.read_csv(fp, dtype=str, **{k: v for k, v in kwargs.items() if k != 'parse_dates'})
#                 except Exception:
#                     return pd.DataFrame()

#         def _to_python_scalar(v):
#             if pd.isna(v):
#                 return None
#             try:
#                 return v.item() if hasattr(v, 'item') else v
#             except Exception:
#                 return v

#         # 1) find trend CSVs (city-specific first)
#         def _slug(s):
#             return re.sub(r'[^a-z0-9]+', '_', str(s or '').strip().lower()).strip('_')

#         city_slug = _slug(city_param)
#         trend_glob = list(base_out.glob(f"trend_{city_slug}_*.csv"))
#         if not trend_glob:
#             trend_glob = list(base_out.glob("trend_*.csv"))
#         trend_glob = sorted(trend_glob, reverse=True)

#         df_list = []
#         for fp in trend_glob:
#             try:
#                 tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
#             except Exception:
#                 try:
#                     tmp = pd.read_csv(fp, dtype=str)
#                     if 'Date' in tmp.columns:
#                         tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
#                 except Exception:
#                     continue
#             df_list.append(tmp)
#         if df_list:
#             trends_df = pd.concat(df_list, ignore_index=True)
#             try:
#                 trends_df = _replace_placeholder_strings(trends_df)
#             except Exception:
#                 pass
#         else:
#             trends_df = pd.DataFrame()

#         # if no query param, return a small sample of trend rows (if any)
#         if q is None:
#             try:
#                 if not trends_df.empty and '_clean_sample_df' in globals():
#                     cleaned = _clean_sample_df(trends_df, max_rows=10)
#                 elif not trends_df.empty:
#                     cleaned = trends_df.head(10).to_dict(orient='records')
#                 else:
#                     cleaned = []
#             except Exception:
#                 cleaned = []
#             return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

#         q_str = str(q).strip()

#         # helper to normalise series values to comparable strings/numerics
#         def normalize_series(s):
#             if s is None:
#                 return pd.Series([''] * (len(trends_df) if not trends_df.empty else 0))
#             s = s.fillna('').astype(str).str.strip()
#             def _norm_val(v):
#                 if not v:
#                     return ''
#                 try:
#                     if '.' in v:
#                         fv = float(v)
#                         if math.isfinite(fv) and fv.is_integer():
#                             return str(int(fv))
#                 except Exception:
#                     pass
#                 return v
#             return s.map(_norm_val)

#         # find matching rows in trends_df
#         found_mask = pd.Series(False, index=trends_df.index) if not trends_df.empty else pd.Series(dtype=bool)
#         candidates_cols = ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12', 'EmployeeName')
#         for c in candidates_cols:
#             if c in trends_df.columns:
#                 try:
#                     ser = normalize_series(trends_df[c])
#                     found_mask = found_mask | (ser == q_str)
#                 except Exception:
#                     pass

#         # numeric fallback
#         if (found_mask is None) or (not found_mask.any() if len(found_mask) else True):
#             try:
#                 q_numeric = float(q_str)
#                 for c in ('EmployeeID', 'Int1'):
#                     if c in trends_df.columns:
#                         try:
#                             numser = pd.to_numeric(trends_df[c], errors='coerce')
#                             found_mask = found_mask | (numser == q_numeric)
#                         except Exception:
#                             pass
#             except Exception:
#                 pass

#         matched = trends_df[found_mask].copy() if not trends_df.empty else pd.DataFrame()
#         if matched.empty:
#             cleaned_matched = []
#         else:
#             try:
#                 cleaned_matched = _clean_sample_df(matched, max_rows=len(matched)) if '_clean_sample_df' in globals() else matched.to_dict(orient='records')
#             except Exception:
#                 cleaned_matched = matched.to_dict(orient='records')

#         # enrich matched rows where possible
#         try:
#             if cleaned_matched and '_enrich_with_personnel_info' in globals():
#                 agg_df = pd.DataFrame(cleaned_matched)
#                 agg_df = _enrich_with_personnel_info(agg_df, image_endpoint_template="/employee/{}/image")
#                 cleaned_matched = agg_df.to_dict(orient='records')
#         except Exception:
#             pass

#         # build list of dates to scan for swipe files (from matched rows)
#         dates_to_scan = set()
#         try:
#             for _, r in (matched.iterrows() if not matched.empty else []):
#                 try:
#                     if 'Date' in r and pd.notna(r['Date']):
#                         try:
#                             d = pd.to_datetime(r['Date']).date()
#                             dates_to_scan.add(d)
#                         except Exception:
#                             pass
#                     for col in ('FirstSwipe','LastSwipe'):
#                         if col in r and pd.notna(r[col]):
#                             try:
#                                 d = pd.to_datetime(r[col]).date()
#                                 dates_to_scan.add(d)
#                             except Exception:
#                                 pass
#                 except Exception:
#                     continue
#         except Exception:
#             pass
#         if not dates_to_scan:
#             dates_to_scan = {None}  # indicates scan all swipes files

#         # ---------- helper: find swipe files for a date (robust) ----------
#         def _find_swipes_for_date(date_obj=None):
#             try:
#                 include_shifted = True
#                 try:
#                     if city_slug and str(city_slug).strip().lower() == 'pune':
#                         include_shifted = False
#                 except Exception:
#                     include_shifted = True

#                 if '_find_swipe_files' in globals() and callable(globals().get('_find_swipe_files')):
#                     try:
#                         cand = _find_swipe_files(str(base_out), date_obj=date_obj, city_slug=city_slug if city_slug else None, include_shifted=include_shifted)
#                         if cand:
#                             return cand
#                     except Exception:
#                         logging.exception("_find_swipe_files helper failed; falling back to glob search.")

#                 files = []
#                 if date_obj is None:
#                     files = list(base_out.glob("swipes_*_*.csv")) + list(base_out.glob("swipes_*.csv")) + list(base_out.glob("*swipe*.csv"))
#                 else:
#                     ymd = date_obj.strftime('%Y-%m-%d')
#                     ymd2 = date_obj.strftime('%Y%m%d')
#                     cand1 = [p for p in base_out.glob("swipes_*_*.csv") if (ymd in p.name or ymd2 in p.name)]
#                     cand2 = [p for p in base_out.glob("swipes_*.csv") if (ymd in p.name or ymd2 in p.name)]
#                     files = cand1 + cand2
#                 files = sorted(list({p for p in files if p.exists()}), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
#                 if not include_shifted:
#                     files = [f for f in files if 'shift' not in f.name.lower()]
#                 return files
#             except Exception:
#                 logging.exception("Error while searching for swipe files for date=%s city=%s", date_obj, city_slug)
#                 return []


#         # ---------- scan swipe files for the target person (dates_to_scan computed earlier) ----------
#         raw_files_set = set()
#         raw_swipes_out = []
#         seen_keys = set()

#         def _append_row(out_row, source_name):
#             key = (
#                 str(out_row.get('Date') or ''),
#                 str(out_row.get('Time') or ''),
#                 str(out_row.get('Door') or '').strip(),
#                 str(out_row.get('Direction') or '').strip(),
#                 str(out_row.get('CardNumber') or out_row.get('Card') or '').strip()
#             )
#             if key in seen_keys:
#                 return
#             seen_keys.add(key)
#             out_row['_source'] = source_name
#             raw_swipes_out.append(out_row)

#         for d in dates_to_scan:
#             swipe_candidates = _find_swipes_for_date(d)
#             if d is not None and not swipe_candidates:
#                 swipe_candidates = _find_swipes_for_date(None)

#             for fp in swipe_candidates:
#                 raw_files_set.add(fp.name)
#                 try:
#                     sdf = _safe_read(fp, parse_dates=['LocaleMessageTime'])
#                 except Exception:
#                     try:
#                         sdf = _safe_read(fp)
#                     except Exception:
#                         continue
#                 if sdf is None or sdf.empty:
#                     continue

#                 # minimal column-normalization & masking (reuse original code patterns)
#                 cols_lower = {c.lower(): c for c in sdf.columns}
#                 tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
#                 emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
#                 name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
#                 card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
#                 door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
#                 dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
#                 note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
#                 person_uid_col = cols_lower.get('person_uid')

#                 try:
#                     mask = pd.Series(False, index=sdf.index)
#                 except Exception:
#                     mask = pd.Series([False] * len(sdf))

#                 try:
#                     if person_uid_col and person_uid_col in sdf.columns:
#                         mask = mask | (sdf[person_uid_col].astype(str).str.strip() == q_str)
#                 except Exception:
#                     pass
#                 try:
#                     if emp_col and emp_col in sdf.columns:
#                         mask = mask | (sdf[emp_col].astype(str).str.strip() == q_str)
#                 except Exception:
#                     pass

#                 if (not mask.any()) and emp_col and emp_col in sdf.columns:
#                     try:
#                         q_numeric = float(q_str)
#                         emp_numeric = pd.to_numeric(sdf[emp_col], errors='coerce')
#                         mask = mask | (emp_numeric == q_numeric)
#                     except Exception:
#                         pass

#                 if (not mask.any()) and name_col and name_col in sdf.columns:
#                     try:
#                         mask = mask | (sdf[name_col].astype(str).str.strip().str.lower() == q_str.lower())
#                     except Exception:
#                         pass

#                 if not mask.any():
#                     continue

#                 filtered = sdf[mask].copy()
#                 if filtered.empty:
#                     continue

#                 # ensure LocaleMessageTime parsed
#                 if tcol and tcol in filtered.columns:
#                     try:
#                         filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
#                     except Exception:
#                         pass
#                 else:
#                     for cand in ('MessageUTC','MessageTime','Timestamp','timestamp','Date'):
#                         if cand in filtered.columns:
#                             try:
#                                 filtered['LocaleMessageTime'] = pd.to_datetime(filtered[cand], errors='coerce')
#                                 tcol = 'LocaleMessageTime'
#                                 break
#                             except Exception:
#                                 pass

#                 if tcol and tcol in filtered.columns:
#                     try:
#                         filtered = filtered.sort_values(by=tcol)
#                     except Exception:
#                         pass

#                 if tcol and tcol in filtered.columns:
#                     try:
#                         # ensure sorted by timestamp
#                         filtered = filtered.sort_values(by=tcol)
#                     except Exception:
#                         pass

#                     # compute swipe gaps (seconds) between consecutive swipes
#                     try:
#                         filtered['_prev_ts'] = filtered[tcol].shift(1)
#                         filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
#                         # reset gap at day boundary or when previous is NaT
#                         try:
#                             cur_dates = filtered[tcol].dt.date
#                             prev_dates = cur_dates.shift(1)
#                             day_boundary_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
#                             filtered.loc[day_boundary_mask, '_swipe_gap_seconds'] = 0.0
#                         except Exception:
#                             # date boundary logic failed â€” keep computed gaps
#                             pass
#                     except Exception:
#                         filtered['_swipe_gap_seconds'] = 0.0
#                 else:
#                     # no timestamp column -> defaults
#                     filtered['_swipe_gap_seconds'] = 0.0

#                 # convert each filtered row to output dict and append
#                 for _, r in filtered.iterrows():
#                     out = {}
#                     out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in filtered.columns else (matched.iloc[0].get('EmployeeName') if not matched.empty else q_str)
#                     # EmployeeID
#                     emp_val = None
#                     if emp_col and emp_col in filtered.columns:
#                         emp_val = _to_python_scalar(r.get(emp_col))
#                     else:
#                         for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
#                             cl = cols_lower.get(cand.lower())
#                             if cl and cl in filtered.columns:
#                                 emp_val = _to_python_scalar(r.get(cl))
#                                 if emp_val not in (None, '', 'nan'):
#                                     break
#                         if emp_val in (None, '', 'nan'):
#                             emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
#                     try:
#                         if emp_val is not None:
#                             s = str(emp_val).strip()
#                             if '.' in s:
#                                 try:
#                                     f = float(s)
#                                     if math.isfinite(f) and f.is_integer():
#                                         s = str(int(f))
#                                 except Exception:
#                                     pass
#                             emp_val = s
#                     except Exception:
#                         pass
#                     out['EmployeeID'] = emp_val

#                     # CardNumber / Card
#                     card_val = None
#                     if card_col and card_col in filtered.columns:
#                         card_val = _to_python_scalar(r.get(card_col))
#                     else:
#                         for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
#                             cl = cols_lower.get(cand.lower())
#                             if cl and cl in filtered.columns:
#                                 card_val = _to_python_scalar(r.get(cl))
#                                 if card_val not in (None, '', 'nan'):
#                                     break
#                         if card_val in (None, '', 'nan'):
#                             card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
#                     try:
#                         if card_val is not None:
#                             card_val = str(card_val).strip()
#                     except Exception:
#                         pass
#                     out['CardNumber'] = card_val
#                     out['Card'] = card_val

#                     # timestamps -> Date / Time / LocaleMessageTime
#                     if tcol and tcol in filtered.columns:
#                         ts = r.get(tcol)
#                         try:
#                             ts_py = pd.to_datetime(ts)
#                             out['Date'] = ts_py.date().isoformat()
#                             out['Time'] = ts_py.time().isoformat()
#                             out['LocaleMessageTime'] = ts_py.isoformat()
#                         except Exception:
#                             txt = str(r.get(tcol))
#                             out['Date'] = txt[:10]
#                             out['Time'] = txt[11:19] if len(txt) >= 19 else txt
#                             out['LocaleMessageTime'] = txt
#                     else:
#                         out['Date'] = None
#                         out['Time'] = None
#                         out['LocaleMessageTime'] = None

#                     out['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds') or 0.0)
#                     out['SwipeGap'] = format_seconds_to_hms(out['SwipeGapSeconds'])

#                     out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
#                     out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else (_to_python_scalar(r.get('Direction')) if 'Direction' in r else None)
#                     out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
#                     try:
#                         out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else (map_door_to_zone(out['Door'], out['Direction']) if 'map_door_to_zone' in globals() else None)
#                     except Exception:
#                         out['Zone'] = None

#                     out['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
#                     out['_source_file'] = fp.name
#                     _append_row(out, fp.name)

#         raw_swipe_files = sorted(list(raw_files_set))

#         return jsonify({
#             "aggregated_rows": cleaned_matched,
#             "raw_swipe_files": raw_swipe_files,
#             "raw_swipes": raw_swipes_out
#         }), 200
#         # --- END existing record() logic ---
#     except Exception as e:
#         logging.exception("Unhandled exception in /record endpoint")
#         # Return JSON so frontend sees useful error instead of Flask returning None
#         return jsonify({"error": "internal server error in /record", "details": str(e)}), 500


@app.route('/record', methods=['GET'])
def record():
    try:
        # --- BEGIN existing record() logic ---
        from pathlib import Path
        import pandas as pd
        import math
        import re
        from datetime import datetime, date
        try:
            q = request.args.get('employee_id') or request.args.get('person_uid')
        except Exception:
            q = None
        include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
        city_param = request.args.get('city') or request.args.get('site') or 'pune'

        # pick outdir consistently
        try:
            base_out = Path(DEFAULT_OUTDIR)
        except Exception:
            try:
                base_out = Path(OUTDIR)
            except Exception:
                base_out = Path.cwd()

        # helper safe wrappers (use existing ones if present)
        def _safe_read(fp, **kwargs):
            try:
                if '_safe_read_csv' in globals():
                    return _safe_read_csv(fp)
                return pd.read_csv(fp, **kwargs)
            except Exception:
                try:
                    return pd.read_csv(fp, dtype=str, **{k: v for k, v in kwargs.items() if k != 'parse_dates'})
                except Exception:
                    return pd.DataFrame()

        def _to_python_scalar(v):
            if pd.isna(v):
                return None
            try:
                return v.item() if hasattr(v, 'item') else v
            except Exception:
                return v

        # 1) find trend CSVs (city-specific first)
        def _slug(s):
            return re.sub(r'[^a-z0-9]+', '_', str(s or '').strip().lower()).strip('_')

        city_slug = _slug(city_param)
        trend_glob = list(base_out.glob(f"trend_{city_slug}_*.csv"))
        if not trend_glob:
            trend_glob = list(base_out.glob("trend_*.csv"))
        trend_glob = sorted(trend_glob, reverse=True)

        df_list = []
        for fp in trend_glob:
            try:
                tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
            except Exception:
                try:
                    tmp = pd.read_csv(fp, dtype=str)
                    if 'Date' in tmp.columns:
                        tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                except Exception:
                    continue
            df_list.append(tmp)
        if df_list:
            trends_df = pd.concat(df_list, ignore_index=True)
            try:
                trends_df = _replace_placeholder_strings(trends_df)
            except Exception:
                pass
        else:
            trends_df = pd.DataFrame()

        # if no query param, return a small sample of trend rows (if any)
        if q is None:
            try:
                if not trends_df.empty and '_clean_sample_df' in globals():
                    cleaned = _clean_sample_df(trends_df, max_rows=10)
                elif not trends_df.empty:
                    cleaned = trends_df.head(10).to_dict(orient='records')
                else:
                    cleaned = []
            except Exception:
                cleaned = []
            return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

        q_str = str(q).strip()

        # helper to normalise series values to comparable strings/numerics
        def normalize_series(s):
            if s is None:
                return pd.Series([''] * (len(trends_df) if not trends_df.empty else 0))
            s = s.fillna('').astype(str).str.strip()
            def _norm_val(v):
                if not v:
                    return ''
                try:
                    if '.' in v:
                        fv = float(v)
                        if math.isfinite(fv) and fv.is_integer():
                            return str(int(fv))
                except Exception:
                    pass
                return v
            return s.map(_norm_val)

        # find matching rows in trends_df
        found_mask = pd.Series(False, index=trends_df.index) if not trends_df.empty else pd.Series(dtype=bool)
        candidates_cols = ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12', 'EmployeeName')
        for c in candidates_cols:
            if c in trends_df.columns:
                try:
                    ser = normalize_series(trends_df[c])
                    found_mask = found_mask | (ser == q_str)
                except Exception:
                    pass

        # numeric fallback
        if (found_mask is None) or (not found_mask.any() if len(found_mask) else True):
            try:
                q_numeric = float(q_str)
                for c in ('EmployeeID', 'Int1'):
                    if c in trends_df.columns:
                        try:
                            numser = pd.to_numeric(trends_df[c], errors='coerce')
                            found_mask = found_mask | (numser == q_numeric)
                        except Exception:
                            pass
            except Exception:
                pass

        matched = trends_df[found_mask].copy() if not trends_df.empty else pd.DataFrame()
        if matched.empty:
            cleaned_matched = []
        else:
            try:
                cleaned_matched = _clean_sample_df(matched, max_rows=len(matched)) if '_clean_sample_df' in globals() else matched.to_dict(orient='records')
            except Exception:
                cleaned_matched = matched.to_dict(orient='records')

        # enrich matched rows where possible
        try:
            if cleaned_matched and '_enrich_with_personnel_info' in globals():
                agg_df = pd.DataFrame(cleaned_matched)
                agg_df = _enrich_with_personnel_info(agg_df, image_endpoint_template="/employee/{}/image")
                cleaned_matched = agg_df.to_dict(orient='records')
        except Exception:
            pass

        # build list of dates to scan for swipe files (from matched rows)
        dates_to_scan = set()
        try:
            for _, r in (matched.iterrows() if not matched.empty else []):
                try:
                    if 'Date' in r and pd.notna(r['Date']):
                        try:
                            d = pd.to_datetime(r['Date']).date()
                            dates_to_scan.add(d)
                        except Exception:
                            pass
                    for col in ('FirstSwipe','LastSwipe'):
                        if col in r and pd.notna(r[col]):
                            try:
                                d = pd.to_datetime(r[col]).date()
                                dates_to_scan.add(d)
                            except Exception:
                                pass
                except Exception:
                    continue
        except Exception:
            pass
        if not dates_to_scan:
            dates_to_scan = {None}  # indicates scan all swipes files

        # ---------- helper: find swipe files for a date (robust) ----------
        def _find_swipes_for_date(date_obj=None):
            try:
                include_shifted = True
                try:
                    if city_slug and str(city_slug).strip().lower() == 'pune':
                        include_shifted = False
                except Exception:
                    include_shifted = True

                if '_find_swipe_files' in globals() and callable(globals().get('_find_swipe_files')):
                    try:
                        cand = _find_swipe_files(str(base_out), date_obj=date_obj, city_slug=city_slug if city_slug else None, include_shifted=include_shifted)
                        if cand:
                            return cand
                    except Exception:
                        logging.exception("_find_swipe_files helper failed; falling back to glob search.")

                files = []
                if date_obj is None:
                    files = list(base_out.glob("swipes_*_*.csv")) + list(base_out.glob("swipes_*.csv")) + list(base_out.glob("*swipe*.csv"))
                else:
                    ymd = date_obj.strftime('%Y-%m-%d')
                    ymd2 = date_obj.strftime('%Y%m%d')
                    cand1 = [p for p in base_out.glob("swipes_*_*.csv") if (ymd in p.name or ymd2 in p.name)]
                    cand2 = [p for p in base_out.glob("swipes_*.csv") if (ymd in p.name or ymd2 in p.name)]
                    files = cand1 + cand2
                files = sorted(list({p for p in files if p.exists()}), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
                if not include_shifted:
                    files = [f for f in files if 'shift' not in f.name.lower()]
                return files
            except Exception:
                logging.exception("Error while searching for swipe files for date=%s city=%s", date_obj, city_slug)
                return []


        # ---------- scan swipe files for the target person (dates_to_scan computed earlier) ----------
        raw_files_set = set()
        raw_swipes_out = []
        seen_keys = set()

        def _append_row_for_evidence(out_row, source_name):
            # avoid exact duplicate rows from same file
            key = (
                str(out_row.get('LocaleMessageTime') or ''),
                str(out_row.get('DateOnly') or ''),
                str(out_row.get('Swipe_Time') or ''),
                str(out_row.get('Door') or '').strip(),
                str(out_row.get('Direction') or '').strip(),
                str(out_row.get('CardNumber') or '').strip()
            )
            if key in seen_keys:
                return False
            seen_keys.add(key)
            out_row['_source'] = source_name
            raw_swipes_out.append(out_row)
            return True

        # helper to format datetime to requested display formats
        def _format_time_fields(ts):
            # ts is a pandas Timestamp or datetime or None
            if ts is None or (isinstance(ts, float) and math.isnan(ts)):
                return (None, None, None)
            try:
                dt = pd.to_datetime(ts)
            except Exception:
                return (None, None, None)
            try:
                locale_iso = dt.isoformat()
            except Exception:
                locale_iso = str(dt)
            try:
                date_only = dt.strftime("%d-%b-%y")  # e.g. 17-Nov-25
            except Exception:
                try:
                    date_only = dt.date().isoformat()
                except Exception:
                    date_only = None
            try:
                # 12-hour time with AM/PM, strip leading zero
                swipe_time = dt.strftime("%I:%M:%S %p").lstrip("0")
            except Exception:
                swipe_time = None
            return (locale_iso, date_only, swipe_time)

        # loop over each date we want to scan (these are violation dates if matched rows existed)
        for d in dates_to_scan:
            swipe_candidates = _find_swipes_for_date(d)
            if d is not None and not swipe_candidates:
                # fallback to scanning all swipe files if none found for the exact date pattern
                swipe_candidates = _find_swipes_for_date(None)

            for fp in swipe_candidates:
                try:
                    sdf = _safe_read(fp, parse_dates=['LocaleMessageTime'])
                except Exception:
                    try:
                        sdf = _safe_read(fp)
                    except Exception:
                        continue
                if sdf is None or sdf.empty:
                    continue

                # minimal column-normalization for detection (case-insensitive)
                cols_lower = {c.lower(): c for c in sdf.columns}
                tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or \
                       cols_lower.get('timestamp') or cols_lower.get('time') or None
                emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or \
                          cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or \
                           cols_lower.get('employee_name') or None
                card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or \
                           cols_lower.get('chuid') or cols_lower.get('value') or None
                door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
                dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
                admit_cols = [c for c in ('admitcode','admit','admit_code','admit_type','admitstatus') if c in cols_lower]
                admit_col = cols_lower.get(admit_cols[0]) if admit_cols else None
                personnel_col = cols_lower.get('personneltype') or cols_lower.get('personneltypename') or None
                location_col = cols_lower.get('partitionname2') or cols_lower.get('location') or cols_lower.get('partitionname') or None
                person_uid_col = cols_lower.get('person_uid')

                # build boolean mask for rows that match the query identifier q_str
                try:
                    mask = pd.Series(False, index=sdf.index)
                except Exception:
                    mask = pd.Series([False] * len(sdf))

                try:
                    if person_uid_col and person_uid_col in sdf.columns:
                        mask = mask | (sdf[person_uid_col].astype(str).str.strip() == q_str)
                except Exception:
                    pass
                try:
                    if emp_col and emp_col in sdf.columns:
                        mask = mask | (sdf[emp_col].astype(str).str.strip() == q_str)
                except Exception:
                    pass
                # numeric fallback for employee id
                if (not mask.any()) and emp_col and emp_col in sdf.columns:
                    try:
                        q_numeric = float(q_str)
                        emp_numeric = pd.to_numeric(sdf[emp_col], errors='coerce')
                        mask = mask | (emp_numeric == q_numeric)
                    except Exception:
                        pass
                # name fallback
                if (not mask.any()) and name_col and name_col in sdf.columns:
                    try:
                        mask = mask | (sdf[name_col].astype(str).str.strip().str.lower() == q_str.lower())
                    except Exception:
                        pass

                if not mask.any():
                    # no matching rows in this file -> skip adding this file
                    continue

                filtered = sdf[mask].copy()
                if filtered.empty:
                    continue

                # parse/normalize timestamp column if available
                if tcol and tcol in filtered.columns:
                    try:
                        filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                    except Exception:
                        pass
                else:
                    # attempt common fallback column names to produce a timestamp
                    for cand in ('MessageUTC', 'MessageTime', 'Timestamp', 'timestamp', 'Date'):
                        if cand in filtered.columns:
                            try:
                                filtered['LocaleMessageTime'] = pd.to_datetime(filtered[cand], errors='coerce')
                                tcol = 'LocaleMessageTime'
                                break
                            except Exception:
                                pass

                # sort by timestamp for consistent timeline order
                if tcol and tcol in filtered.columns:
                    try:
                        filtered = filtered.sort_values(by=tcol)
                    except Exception:
                        pass

                # For each matching swipe row, build the slim evidence record expected by frontend
                added_any = False
                for _, r in filtered.iterrows():
                    # timestamp conversions
                    ts_val = None
                    if tcol and tcol in filtered.columns:
                        ts_val = r.get(tcol)
                    else:
                        # fallback: try Date column
                        if 'Date' in filtered.columns:
                            ts_val = r.get('Date')
                    locale_iso, date_only, swipe_time = _format_time_fields(ts_val)

                    # EmployeeID: prefer emp_col, then Int1/Text12, then fallback to matched trends row
                    emp_val = None
                    try:
                        if emp_col and emp_col in filtered.columns:
                            emp_val = _to_python_scalar(r.get(emp_col))
                        else:
                            for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                                cl = cols_lower.get(cand.lower())
                                if cl and cl in filtered.columns:
                                    emp_val = _to_python_scalar(r.get(cl))
                                    if emp_val:
                                        break
                            if emp_val in (None, '', 'nan'):
                                emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
                    except Exception:
                        emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)

                    # ObjectName1 (human name)
                    obj_name = None
                    try:
                        if name_col and name_col in filtered.columns:
                            obj_name = _to_python_scalar(r.get(name_col))
                        elif 'ObjectName1' in filtered.columns:
                            obj_name = _to_python_scalar(r.get('ObjectName1'))
                        elif 'EmployeeName' in filtered.columns:
                            obj_name = _to_python_scalar(r.get('EmployeeName'))
                        else:
                            obj_name = _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else None)
                    except Exception:
                        obj_name = _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else None)

                    # PersonnelType
                    personnel_val = _to_python_scalar(r.get(personnel_col)) if (personnel_col and personnel_col in filtered.columns) else None
                    # Location / Partition
                    location_val = _to_python_scalar(r.get(location_col)) if (location_col and location_col in filtered.columns) else _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None

                    # CardNumber
                    card_val = None
                    try:
                        if card_col and card_col in filtered.columns:
                            card_val = _to_python_scalar(r.get(card_col))
                        else:
                            for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                                cl = cols_lower.get(cand.lower())
                                if cl and cl in filtered.columns:
                                    card_val = _to_python_scalar(r.get(cl))
                                    if card_val not in (None, '', 'nan'):
                                        break
                            if card_val in (None, '', 'nan'):
                                card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                    except Exception:
                        card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                    if card_val is not None:
                        try:
                            card_val = str(card_val).strip()
                        except Exception:
                            pass

                    # AdmitCode / Admit
                    admit_val = _to_python_scalar(r.get(admit_col)) if (admit_col and admit_col in filtered.columns) else None
                    # some logs store admit/rejection text in 'Note' or 'Rejection_Type'
                    if not admit_val:
                        for cand in ('Admit','AdmitCode','Admit_Type','Rejection_Type','Note','NoteType','Source'):
                            cl = cols_lower.get(cand.lower())
                            if cl and cl in filtered.columns:
                                admit_val = _to_python_scalar(r.get(cl))
                                if admit_val:
                                    break

                    # Direction & Door
                    direction_val = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in filtered.columns else None
                    door_val = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else _to_python_scalar(r.get('Door')) if 'Door' in filtered.columns else None

                    row_out = {
                        "LocaleMessageTime": locale_iso,
                        "DateOnly": date_only,
                        "Swipe_Time": swipe_time,
                        "EmployeeID": emp_val,
                        "ObjectName1": obj_name,
                        "PersonnelType": personnel_val,
                        "Location": location_val,
                        "CardNumber": card_val,
                        "AdmitCode": admit_val,
                        "Direction": direction_val,
                        "Door": door_val
                    }

                    added = _append_row_for_evidence(row_out, fp.name)
                    if added:
                        added_any = True

                # only add file name to available evidence list if we actually added rows from it
                if added_any:
                    raw_files_set.add(fp.name)

        raw_swipe_files = sorted(list(raw_files_set))

        return jsonify({
            "aggregated_rows": cleaned_matched,
            "raw_swipe_files": raw_swipe_files,
            "raw_swipes": raw_swipes_out
        }), 200

    except Exception as e:
        # Close the try-block above with a proper except handler to avoid SyntaxError
        logging.exception("Unhandled exception in /record endpoint")
        return jsonify({"error": "internal server error in /record", "details": str(e)}), 500
