Now everything is working correct ..
so in backend API add 
day wise swipe swtails ..
add first swipe 
last swipe 
door name 
card number 
partation Name 
personeel Type,
CompanyName,
CardNumber,
CompanyName
PrimaryLocation
add above details in API responce refer below below API responce carefully..
2) Alos ADD Export button in frontend page carefully..

ex- {
      "LocaleMessageTime": "2025-09-05T14:23:47.000Z",
      "Dateonly": "2025-09-05",
      "Swipe_Time": "14:23:47",
      "EmployeeID": "320399",
      "PersonGUID": "163F85A9-547C-4CF0-9BE8-6B695C2A2517",
      "ObjectName1": "Akuffo, Alex Yirenkyi",
      "Door": "EMEA_LT_VNO_GAMA_1st Flr_Stairway Entrance",
      "PersonnelType": "Employee",
      "CardNumber": "601166",
      "Text5": "Vilnius - Gama Business Center",
      "PartitionName2": "LT.Vilnius",
      "AdmitCode": "Admit",
      "Direction": "InDirection",
      "CompanyName": "WU Processing Lithuania, UAB",
      "PrimaryLocation": "Vilnius - Gama Business Center",





#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py

@app.get("/duration")
async def api_duration(
    date_param: Optional[str] = Query(None, alias="date", description="Target date YYYY-MM-DD. Defaults to today in Asia/Kolkata"),
    start_date: Optional[str] = Query(None, description="Start date for a range (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date for a range (YYYY-MM-DD)"),
    regions: Optional[str] = Query(None, description="Comma-separated list: apac,emea,laca,namer. Default: all"),
    city: Optional[str] = Query(None, description="Optional city/location filter (e.g. Pune). Case-insensitive, matches PartitionName2/PrimaryLocation/Door/EmployeeName"),
    outdir: Optional[str] = Query(None, description="Output directory for CSVs. Defaults to OUTPUT_DIR/duration_reports"),
    sample_rows: int = Query(10, ge=0, le=100, description="How many sample rows to include per region in response")
):
    try:
        # --- parse region list
        if regions:
            regions_list = [r.strip().lower() for r in regions.split(",") if r.strip()]
        else:
            regions_list = ["apac", "emea", "laca", "namer"]

        # --- parse output dir
        if outdir:
            outdir_path = Path(outdir)
        else:
            outdir_path = OUTPUT_DIR / "duration_reports"
        outdir_path.mkdir(parents=True, exist_ok=True)

        # --- determine date(s)
        def _parse_date(s: str) -> date:
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                return date.fromisoformat(s)

        if start_date and end_date:
            try:
                start_obj = _parse_date(start_date)
                end_obj = _parse_date(end_date)
            except Exception:
                raise HTTPException(status_code=400, detail="Invalid start_date or end_date format. Use YYYY-MM-DD.")
            if start_obj > end_obj:
                raise HTTPException(status_code=400, detail="start_date must be <= end_date")
            # safety: limit max days to avoid overloading
            max_days = 31
            days_count = (end_obj - start_obj).days + 1
            if days_count > max_days:
                raise HTTPException(status_code=400, detail=f"Date range too large (> {max_days} days). Please request a smaller range.")
            date_list = [start_obj + timedelta(days=i) for i in range(days_count)]
            range_mode = True
        else:
            # single-date: prefer explicit `date` query param, otherwise today's Asia/Kolkata
            if date_param:
                try:
                    target_date = _parse_date(date_param)
                except Exception:
                    raise HTTPException(status_code=400, detail="Invalid date format. Use YYYY-MM-DD.")
            else:
                tz = ZoneInfo("Asia/Kolkata")
                target_date = datetime.now(tz).date()
            date_list = [target_date]
            start_obj = end_obj = date_list[0]
            range_mode = False

        # --- import duration_report lazily
        try:
            import duration_report
        except Exception as e:
            logger.exception("Failed importing duration_report module")
            raise HTTPException(status_code=500, detail=f"duration module import failed: {e}")

        loop = asyncio.get_running_loop()

        # Helper serializer
        def _to_json_safe(v):
            try:
                if pd.isna(v):
                    return None
            except Exception:
                pass
            if isinstance(v, (datetime, date)):
                return v.isoformat()
            if hasattr(v, "isoformat") and not isinstance(v, str):
                try:
                    return v.isoformat()
                except Exception:
                    pass
            if isinstance(v, (int, float, bool)):
                return v
            return v

        # For each date, call run_for_date and collect results
        per_date_results = {}  # iso_date -> results dict returned by duration_report.run_for_date
        for single_date in date_list:
            try:
                task = loop.run_in_executor(None, duration_report.run_for_date, single_date, regions_list, str(outdir_path), city)
                per_date_results[single_date.isoformat()] = await asyncio.wait_for(task, timeout=COMPUTE_WAIT_TIMEOUT_SECONDS)
            except asyncio.TimeoutError:
                raise HTTPException(status_code=504, detail=f"Duration computation timed out for date {single_date.isoformat()}")
            except Exception as e:
                logger.exception("duration run_for_date failed for date %s", single_date)
                raise HTTPException(status_code=500, detail=f"duration run failed for {single_date.isoformat()}: {e}")

        # Aggregate results per region and per employee across dates
        resp = {
            "start_date": start_obj.isoformat(),
            "end_date": end_obj.isoformat(),
            "regions": {}
        }

        for r in regions_list:
            try:
                # Prepare date list strings
                dates_iso = [d.isoformat() for d in date_list]

                # employee_map keyed by person_uid (prefer) else by EmployeeID|EmployeeName
                employees: Dict[str, Dict[str, Any]] = {}
                date_rows = {}

                for iso_d, day_res in per_date_results.items():
                    # day_res is a dict: region -> {"swipes": df, "durations": df}
                    region_obj = day_res.get(r) if isinstance(day_res, dict) else None
                    durations_df = None
                    swipes_df = None
                    if isinstance(region_obj, dict):
                        swipes_df = region_obj.get("swipes")
                        durations_df = region_obj.get("durations")
                    elif isinstance(region_obj, pd.DataFrame):
                        durations_df = region_obj

                    # count rows for this date / region
                    rows_count = int(len(durations_df)) if durations_df is not None else 0
                    swipe_count = int(len(swipes_df)) if swipes_df is not None else 0
                    date_rows[iso_d] = {"rows": rows_count, "swipe_rows": swipe_count}

                    if durations_df is None or durations_df.empty:
                        continue

                    # normalize columns safety
                    for col in ("person_uid", "EmployeeID", "EmployeeName", "Duration", "DurationSeconds"):
                        if col not in durations_df.columns:
                            durations_df[col] = None

                    # iterate rows and populate map
                    for _, row in durations_df.iterrows():
                        person_uid = row.get("person_uid") or None
                        # fallback key
                        if not person_uid or pd.isna(person_uid):
                            key = f"{str(row.get('EmployeeID') or '').strip()}|{str(row.get('EmployeeName') or '').strip()}"
                            person_uid = key

                        if person_uid not in employees:
                            employees[person_uid] = {
                                "person_uid": person_uid,
                                "EmployeeID": None if pd.isna(row.get("EmployeeID")) else str(row.get("EmployeeID")),
                                "EmployeeName": None if pd.isna(row.get("EmployeeName")) else str(row.get("EmployeeName")),
                                "durations": {d: None for d in dates_iso},
                                "durations_seconds": {d: None for d in dates_iso},
                                "total_seconds_present_in_range": 0
                            }
                        # fill this date
                        dur_str = None if pd.isna(row.get("Duration")) else str(row.get("Duration"))
                        dur_secs = None
                        try:
                            v = row.get("DurationSeconds")
                            if pd.notna(v):
                                dur_secs = int(float(v))
                        except Exception:
                            dur_secs = None

                        employees[person_uid]["durations"][iso_d] = dur_str
                        employees[person_uid]["durations_seconds"][iso_d] = dur_secs
                        if dur_secs is not None:
                            employees[person_uid]["total_seconds_present_in_range"] += dur_secs

                # convert employees map to sorted list (sort by EmployeeName then EmployeeID)
                emp_list = list(employees.values())
                emp_list.sort(key=lambda x: (x.get("EmployeeName") or "").lower(), reverse=False)

                # Keep only top sample_rows in durations_sample (if requested) -- still return full employees list
                durations_sample = emp_list[:sample_rows] if sample_rows and sample_rows > 0 else []

                resp["regions"][r] = {
                    "dates": dates_iso,
                    "employees": emp_list,
                    "durations_sample": durations_sample,
                    "date_rows": date_rows
                }
            except Exception:
                logger.exception("Failed to aggregate range results for region %s", r)
                resp["regions"][r] = {"dates": [d.isoformat() for d in date_list], "employees": [], "durations_sample": [], "date_rows": {}}

        return JSONResponse(resp)
    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("api_duration (range) failed")
        raise HTTPException(status_code=500, detail=f"duration api error: {exc}")








# #C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py

# """
# duration_report.py

# Purpose:
#   - Connects to ACVSUJournal MSSQL databases for APAC, EMEA, LACA, NAMER regions (credentials provided by user)
#   - Extracts CardAdmitted swipe events for a specified date and computes daily duration per employee
#     using first swipe (min) and last swipe (max) timestamps for that date.
#   - Writes per-region CSV duration reports and returns pandas DataFrames for further use.
#   - Contains clear TODO hooks to later improve shift-aware logic.

# Notes & Usage:
#   - Place this file in:
#       C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics
#     (or any other folder).

#   - Requires Python packages: pyodbc, pandas, python-dateutil
#     Install (example):
#       pip install pyodbc pandas python-dateutil

#   - ODBC driver: adjust DRIVER in the connection string depending on the host environment.
#     Example uses 'ODBC Driver 17 for SQL Server' by default.
#     If you use a different driver (e.g. 18), change the DRIVER value.

#   - Example CLI usage:
#       python duration_report.py --date 2025-07-21 --regions apac,namer --outdir ./out_reports

#   - The script by default picks "today" in Asia/Kolkata timezone if --date is omitted.

# Security:
#   - This script contains credentials as provided by the user. In production, consider moving credentials
#     to environment variables or a secure vault.

# TODO (future improvements):
#   - Shift-aware logic: handle cases where employees have predefined shift windows spanning midnight,
#     and decide which swipes belong to which shift.
#   - Handle badge-in-only or badge-out-only cases (e.g., tele-getting or push-button exits)
#   - More advanced deduplication of multiple swipes in short windows.

# """

# import argparse
# import logging
# import os
# from datetime import datetime, timedelta, date
# from zoneinfo import ZoneInfo
# from pathlib import Path

# import pandas as pd

# # Optional: import pyodbc only when connecting (allows importing this module even without driver)
# try:
#     import pyodbc
# except Exception:
#     pyodbc = None

# # --------------------- Configuration ---------------------
# ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# REGION_CONFIG = {
#     "apac": {
#         "user": "GSOC_Test",
#         "password": "Westernccure@2025",
#         "server": "SRVWUPNQ0986V",
#         "database": "ACVSUJournal_00010029",
#         # partitions to include (from provided query)
#         "partitions": [
#             "APAC.Default", "CN.Beijing", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur"
#         ]
#     },
#     "emea": {
#         "user": "GSOC_Test",
#         "password": "Westernccure@2025",
#         "server": "SRVWUFRA0986V",
#         "database": "ACVSUJournal_00011028",  # note: user provided 00011028 in credentials block
#         "partitions": [
#             "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
#             "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
#         ]
#     },
#     "laca": {
#         "user": "GSOC_Test",
#         "password": "Westernccure@2025",
#         "server": "SRVWUSJO0986V",
#         "database": "ACVSUJournal_00010029",
#         "partitions": [
#             "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
#             "PA.Panama City", "PE.Lima"
#         ]
#     },
#     "namer": {
#         "user": "GSOC_Test",
#         "password": "Westernccure@2025",
#         "server": "SRVWUDEN0891V",
#         "database": "ACVSUJournal_00010029",
#         # For NAMER we'll filter by ObjectName2 patterns (HQ, Austin, Miami, NYC)
#         "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
#     }
# }


# # --------------------- SQL Builder ---------------------
# GENERIC_SQL_TEMPLATE = r"""
# SELECT
#     t1.[ObjectName1],
#     t1.[ObjectName2],
#     CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
#     t2.[PersonnelTypeID],
#     t3.[Name] AS PersonnelTypeName,
#     t1.ObjectIdentity1 AS EmployeeIdentity,
#     t1.PartitionName2,
#     DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
#     t1.MessageType
# FROM [{db}].[dbo].[ACVSUJournalLog] AS t1
# INNER JOIN [ACVSCore].[Access].[Personnel] AS t2 ON t1.ObjectIdentity1 = t2.GUID
# INNER JOIN [ACVSCore].[Access].[PersonnelType] AS t3 ON t2.PersonnelTypeID = t3.ObjectID
# WHERE t1.MessageType = 'CardAdmitted'
#   AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
#   {region_filter}
# ;
# """


# def build_region_query(region_key: str, target_date: date) -> str:
#     rc = REGION_CONFIG[region_key]
#     date_str = target_date.strftime("%Y-%m-%d")
#     region_filter = ""

#     if region_key in ("apac", "emea", "laca"):
#         partitions = rc.get("partitions", [])
#         # build partition IN clause
#         parts_sql = ", ".join(f"'{p}'" for p in partitions)
#         region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
#     elif region_key == "namer":
#         likes = rc.get("logical_like", [])
#         # build OR of LIKE patterns on ObjectName2
#         like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
#         region_filter = f"AND ({like_sql})"
#     else:
#         # no filter
#         region_filter = ""

#     return GENERIC_SQL_TEMPLATE.format(db=rc["database"], date=date_str, region_filter=region_filter)


# # --------------------- DB Utilities ---------------------
# def get_connection(region_key: str):
#     """Create and return a pyodbc connection for the region configuration.
#     Caller must ensure pyodbc is installed and the ODBC driver exists on the host.
#     """
#     if pyodbc is None:
#         raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

#     rc = REGION_CONFIG[region_key]
#     conn_str = (
#         f"DRIVER={{{ODBC_DRIVER}}};"
#         f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
#         "TrustServerCertificate=Yes;"
#     )
#     return pyodbc.connect(conn_str, autocommit=True)


# def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
#     """Fetch CardAdmitted swipes for the given region and target_date.
#     Returns a pandas DataFrame with columns including EmployeeID and LocaleMessageTime (UTC->locale adjusted).
#     """
#     sql = build_region_query(region_key, target_date)
#     logging.info("Built SQL for region %s, date %s", region_key, target_date)
#     if pyodbc is None:
#         # Return an empty DataFrame skeleton so the script can be inspected without DB driver installed.
#         logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
#         cols = ["ObjectName1", "ObjectName2", "EmployeeID", "PersonnelTypeID", "PersonnelTypeName",
#                 "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "MessageType"]
#         return pd.DataFrame(columns=cols)

#     conn = get_connection(region_key)
#     try:
#         df = pd.read_sql(sql, conn)
#     finally:
#         conn.close()
#     # Ensure LocaleMessageTime is datetime
#     if "LocaleMessageTime" in df.columns:
#         df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"])
#     return df


# # --------------------- Duration Calculation ---------------------
# def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
#     """Given swipe events DataFrame, compute first and last swipe per employee per day and duration.
#     Returns DataFrame with columns: EmployeeID, Date, FirstSwipe, LastSwipe, DurationSeconds, Duration (HH:MM:SS), CountSwipes
#     """
#     if swipes_df.empty:
#         return pd.DataFrame(columns=[
#             "EmployeeID", "Date", "FirstSwipe", "LastSwipe", "DurationSeconds", "Duration", "CountSwipes",
#             "PersonnelTypeName", "PartitionName2"
#         ])

#     df = swipes_df.copy()

#     # Normalize employee id and remove rows without EmployeeID
#     df = df[df["EmployeeID"].notna()].copy()
#     # Create date column (local date already applied in SQL)
#     df["Date"] = df["LocaleMessageTime"].dt.date

#     # Group by EmployeeID + Date (use EmployeeIdentity if you prefer)
#     group_cols = ["EmployeeID", "Date"]
#     agg = df.groupby(group_cols).agg(
#         FirstSwipe=pd.NamedAgg(column="LocaleMessageTime", aggfunc="min"),
#         LastSwipe=pd.NamedAgg(column="LocaleMessageTime", aggfunc="max"),
#         CountSwipes=pd.NamedAgg(column="LocaleMessageTime", aggfunc="count"),
#         PersonnelTypeName=pd.NamedAgg(column="PersonnelTypeName", aggfunc="first"),
#         PartitionName2=pd.NamedAgg(column="PartitionName2", aggfunc="first")
#     ).reset_index()

#     agg["DurationSeconds"] = (agg["LastSwipe"] - agg["FirstSwipe"]).dt.total_seconds()
#     # Negative durations (if any) will be clipped to 0
#     agg["DurationSeconds"] = agg["DurationSeconds"].clip(lower=0)

#     # Human readable HH:MM:SS
#     agg["Duration"] = agg["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))))
#     return agg


# # --------------------- Main Runner ---------------------
# def run_for_date(target_date: date, regions: list, outdir: str) -> dict:
#     """
#     Run duration reports for the requested regions on target_date.
#     Returns a dict of region->DataFrame (duration report).
#     """
#     outdir_path = Path(outdir)
#     outdir_path.mkdir(parents=True, exist_ok=True)

#     results = {}
#     for r in regions:
#         r = r.lower()
#         if r not in REGION_CONFIG:
#             logging.warning("Unknown region '%s' - skipping", r)
#             continue
#         logging.info("Fetching swipes for region %s on %s", r, target_date)
#         swipes = fetch_swipes_for_region(r, target_date)
#         durations = compute_daily_durations(swipes)
#         csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
#         durations.to_csv(csv_path, index=False)
#         logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
#         results[r] = durations
#     return results


# def parse_args():
#     p = argparse.ArgumentParser(description="Generate daily duration report from ACVSUJournal swipe logs.")
#     p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
#     p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
#                    default="apac,emea,laca,namer")
#     p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
#     return p.parse_args()


# if __name__ == "__main__":
#     logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
#     args = parse_args()

#     # Default date in Asia/Kolkata timezone
#     if args.date:
#         target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
#     else:
#         tz = ZoneInfo("Asia/Kolkata")
#         target_date = datetime.now(tz).date()

#     regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
#     outdir = args.outdir

#     logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
#     results = run_for_date(target_date, regions, outdir)

#     # Print a short summary
#     for r, df in results.items():
#         logging.info("Region %s: %d employees with computed durations", r, len(df))
#     logging.info("Completed. CSVs are in %s", Path(outdir).absolute())

















#C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\duration_report.py

"""
duration_report.py

Updated: includes Door, EmployeeName, CardNumber extraction (XML fallback),
CompanyName, PrimaryLocation, Direction, and per-swipe output alongside per-person durations.

Place into your backend directory and restart the server.
"""
import argparse
import logging
import os
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd

# Optional: import pyodbc only when connecting (allows importing this module even without driver)
try:
    import pyodbc
except Exception:
    pyodbc = None

# --------------------- Configuration ---------------------
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "database": "ACVSUJournal_00010029",
        "partitions": [
            "APAC.Default", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur"
        ]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "database": "ACVSUJournal_00011028",
        "partitions": [
            "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
            "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
        ]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "database": "ACVSUJournal_00010029",
        "partitions": [
            "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
            "PA.Panama City", "PE.Lima"
        ]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "database": "ACVSUJournal_00010029",
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# --------------------- SQL Builder ---------------------
# This SQL attempts to extract Card from XML and falls back to t2.Text12.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,                       -- Person name from ACVS
    t1.[ObjectName2] AS Door,                               -- Door / reader name
    CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    -- try extract card from xml CHUID/Card or CHUID element, then fallback to shred 'sc' and finally t2.Text12
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
INNER JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
INNER JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
;
"""


def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    return GENERIC_SQL_TEMPLATE.format(db=rc["database"], date=date_str, region_filter=region_filter)


# --------------------- DB Utilities ---------------------
def get_connection(region_key: str):
    """Create and return a pyodbc connection for the region configuration."""
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)


def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    """
    Fetch CardAdmitted swipes for the given region and target_date.
    Returns a pandas DataFrame with normalized column names:
      ['EmployeeName','Door','EmployeeID','CardNumber','PersonnelTypeName','EmployeeIdentity',
       'PartitionName2','LocaleMessageTime','MessageType','Direction','CompanyName','PrimaryLocation']
    If pyodbc is not available, returns an empty skeleton DataFrame with those columns.
    """
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "CardNumber", "PersonnelTypeName", "EmployeeIdentity",
        "PartitionName2", "LocaleMessageTime", "MessageType", "Direction", "CompanyName", "PrimaryLocation"
    ]
    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = get_connection(region_key)
    try:
        df = pd.read_sql(sql, conn)
    finally:
        conn.close()

    # Ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Parse datetime
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    return df[cols]


# --------------------- Duration Calculation ---------------------
def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute first and last swipe per person per day and duration.
    - uses EmployeeIdentity (GUID) as primary dedupe key
    - falls back to EmployeeID|CardNumber|EmployeeName when GUID missing
    - returns DataFrame with columns:
      person_uid, EmployeeIdentity, EmployeeID, EmployeeName, CardNumber,
      Date, FirstSwipe, LastSwipe, FirstDoor, LastDoor, CountSwipes,
      DurationSeconds, Duration, PersonnelTypeName, PartitionName2,
      CompanyName, PrimaryLocation, FirstDirection, LastDirection
    """
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()

    # Ensure expected columns present (avoid KeyError)
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # Parse datetime if required
    if df["LocaleMessageTime"].dtype == object:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    # Drop exact duplicate swipe records (same GUID, same timestamp, same card, same door)
    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    df = df.drop_duplicates(subset=dedupe_cols, keep="first")

    # Create Date column (uses local date from LocaleMessageTime)
    df["Date"] = df["LocaleMessageTime"].dt.date

    # Build person_uid: prefer EmployeeIdentity (GUID); otherwise concat EmployeeID|CardNumber|EmployeeName
    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    df["person_uid"] = df.apply(make_person_uid, axis=1)
    df = df[df["person_uid"].notna()].copy()

    # Group by person_uid + Date and compute aggregates
    def agg_for_group(g):
        g_sorted = g.sort_values("LocaleMessageTime")
        first = g_sorted.iloc[0]
        last = g_sorted.iloc[-1]

        # First/last directions might be missing or identical; preserve both
        first_dir = first.get("Direction")
        last_dir = last.get("Direction")

        return pd.Series({
            "person_uid": first["person_uid"],
            "EmployeeIdentity": first.get("EmployeeIdentity"),
            "EmployeeID": first.get("EmployeeID"),
            "EmployeeName": first.get("EmployeeName"),
            "CardNumber": first.get("CardNumber"),
            "Date": first["Date"],
            "FirstSwipe": first["LocaleMessageTime"],
            "LastSwipe": last["LocaleMessageTime"],
            "FirstDoor": first.get("Door"),
            "LastDoor": last.get("Door"),
            "CountSwipes": int(len(g_sorted)),
            "PersonnelTypeName": first.get("PersonnelTypeName"),
            "PartitionName2": first.get("PartitionName2"),
            "CompanyName": first.get("CompanyName"),
            "PrimaryLocation": first.get("PrimaryLocation"),
            "FirstDirection": first_dir,
            "LastDirection": last_dir
        })

    grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    # compute duration fields
    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    # Reorder columns and return
    return grouped[out_cols]


# --------------------- Main Runner ---------------------
def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    """
    Run duration reports for the requested regions on target_date.
    Returns a dict: region -> {"swipes": DataFrame, "durations": DataFrame}
    """
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        try:
            swipes = fetch_swipes_for_region(r, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", r)
            swipes = pd.DataFrame()

        # optional city filter: match PartitionName2 OR PrimaryLocation OR Door OR EmployeeName (case-insensitive)
        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                swipes = swipes[combined_mask].copy()
            else:
                # no matching columns -> keep original
                logging.debug("City filter provided but no matching columns to filter on for region %s", r)

        # compute durations (de-duplicated, per-person)
        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", r)
            durations = pd.DataFrame()

        # save CSVs
        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
        try:
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", r)
        try:
            # write swipes with a stable column order if possible
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", r)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
        results[r] = {"swipes": swipes, "durations": durations}

    return results


def parse_args():
    p = argparse.ArgumentParser(description="Generate daily duration report from ACVSUJournal swipe logs.")
    p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
    p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
                   default="apac,emea,laca,namer")
    p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
    p.add_argument("--city", help="Optional city/location filter (e.g. Pune)", default=None)
    return p.parse_args()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    args = parse_args()

    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    else:
        tz = ZoneInfo("Asia/Kolkata")
        target_date = datetime.now(tz).date()

    regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
    outdir = args.outdir

    logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
    results = run_for_date(target_date, regions, outdir, city=args.city)

    for r, obj in results.items():
        dur = obj.get("durations")
        logging.info("Region %s: %d persons with computed durations", r, len(dur) if dur is not None else 0)
    logging.info("Completed. CSVs are in %s", Path(outdir).absolute())





// src/pages/DurationPage.jsx
import React, { useState, useMemo } from "react";
import axios from "axios";
import {
  Box,
  Grid,
  Paper,
  Typography,
  TextField,
  Button,
  MenuItem,
  Select,
  InputLabel,
  FormControl,
  Table,
  TableHead,
  TableRow,
  TableCell,
  TableBody,
  TableContainer,
  CircularProgress,
  IconButton,
  Tooltip,
  Card,
  CardContent,
} from "@mui/material";
import DateRangeIcon from "@mui/icons-material/DateRange";
import SearchIcon from "@mui/icons-material/Search";
import DownloadIcon from "@mui/icons-material/CloudDownload";
import ClearIcon from "@mui/icons-material/Clear";

// Base API (change if your app uses different base)
// const API_BASE = process.env.REACT_APP_API_BASE || "http://localhost:8000";


const API_BASE = import.meta.env.VITE_API_BASE || import.meta.env.REACT_APP_API_BASE || "http://localhost:8000";

const REGIONS = [
  { value: "apac", label: "APAC" },
  { value: "emea", label: "EMEA" },
  { value: "laca", label: "LACA" },
  { value: "namer", label: "NAMER" },
];

function secondsToHMS(s) {
  if (s == null) return "";
  const sec = Number(s);
  if (!Number.isFinite(sec)) return "";
  const h = Math.floor(sec / 3600);
  const m = Math.floor((sec % 3600) / 60);
  const r = Math.floor(sec % 60);
  return `${h}:${String(m).padStart(2, "0")}:${String(r).padStart(2, "0")}`;
}

export default function DurationPage() {
  const [region, setRegion] = useState("apac");
  const [city, setCity] = useState("");
  const [startDate, setStartDate] = useState(""); // YYYY-MM-DD
  const [endDate, setEndDate] = useState("");
  const [singleDate, setSingleDate] = useState("");
  const [useRange, setUseRange] = useState(true);

  const [data, setData] = useState(null); // full API response
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState("");

  // Build query params & fetch
  const fetchDurations = async () => {
    setError("");
    setLoading(true);
    setData(null);

    try {
      const params = {};
      // if range selected and both dates provided -> range mode
      if (useRange && startDate && endDate) {
        params.start_date = startDate;
        params.end_date = endDate;
      } else if (!useRange && singleDate) {
        params.date = singleDate;
      } else {
        // incomplete selection: prefer singleDate if present otherwise use today's date
        if (singleDate) params.date = singleDate;
      }

      if (region) params.regions = region;
      if (city) params.city = city;

      const res = await axios.get(`${API_BASE}/duration`, { params, timeout: 120000 });
      setData(res.data);
    } catch (err) {
      console.error(err);
      setError(err?.response?.data?.detail || err.message || "Failed to fetch duration data");
    } finally {
      setLoading(false);
    }
  };

  // Dynamic columns and rows for selected region
  const regionObj = useMemo(() => {
    if (!data || !region) return null;
    return data.regions?.[region] || null;
  }, [data, region]);

  // CSV export (simple)
  const exportCsv = () => {
    if (!regionObj) return;
    const dates = regionObj.dates || [];
    const rows = regionObj.employees || [];

    // header
    const header = ["EmployeeID", "EmployeeName", "TotalSecondsPresentInRange", ...dates];
    const csvRows = [header.join(",")];

    rows.forEach((r) => {
      const row = [
        `"${(r.EmployeeID || "").replace(/"/g, '""')}"`,
        `"${(r.EmployeeName || "").replace(/"/g, '""')}"`,
        r.total_seconds_present_in_range ?? "",
        ...dates.map((d) => `"${(r.durations?.[d] ?? "")}"`),
      ];
      csvRows.push(row.join(","));
    });

    const blob = new Blob([csvRows.join("\n")], { type: "text/csv;charset=utf-8;" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url;
    const filename = `duration_${region}_${data?.start_date || ""}_to_${data?.end_date || ""}.csv`;
    a.download = filename;
    document.body.appendChild(a);
    a.click();
    a.remove();
    URL.revokeObjectURL(url);
  };

  // Render helpers
  const renderTable = () => {
    if (!regionObj) return <Typography>No data for selected region.</Typography>;

    const dates = regionObj.dates || [];
    const rows = regionObj.employees || [];

    return (
      <TableContainer component={Paper} sx={{ mt: 2 }}>
        <Table size="small">
          <TableHead>
            <TableRow>
              <TableCell><b>EmployeeID</b></TableCell>
              <TableCell><b>EmployeeName</b></TableCell>
              <TableCell align="right"><b>Total (hh:mm:ss)</b></TableCell>
              {dates.map((d) => (
                <TableCell key={d} align="center"><b>{d}</b></TableCell>
              ))}
            </TableRow>
          </TableHead>
          <TableBody>
            {rows.length === 0 ? (
              <TableRow>
                <TableCell colSpan={3 + dates.length} align="center">No employees in the response.</TableCell>
              </TableRow>
            ) : (
              rows.map((r) => (
                <TableRow key={r.person_uid || `${r.EmployeeID}-${r.EmployeeName}`}>
                  <TableCell>{r.EmployeeID || "-"}</TableCell>
                  <TableCell>{r.EmployeeName || "-"}</TableCell>
                  <TableCell align="right">{secondsToHMS(r.total_seconds_present_in_range)}</TableCell>
                  {dates.map((d) => (
                    <TableCell key={d} align="center">{r.durations?.[d] ?? "-"}</TableCell>
                  ))}
                </TableRow>
              ))
            )}
          </TableBody>
        </Table>
      </TableContainer>
    );
  };

  return (
    <Box sx={{ p: 3 }}>
      <Typography variant="h5" gutterBottom>
        Duration Reports
      </Typography>

      <Grid container spacing={2}>
        <Grid item xs={12} md={8}>
          <Card>
            <CardContent>
              <Grid container spacing={2} alignItems="center">
                <Grid item xs={12} md={4}>
                  <FormControl fullWidth>
                    <InputLabel id="region-label">Region</InputLabel>
                    <Select
                      labelId="region-label"
                      value={region}
                      label="Region"
                      onChange={(e) => setRegion(e.target.value)}
                    >
                      {REGIONS.map((r) => (
                        <MenuItem key={r.value} value={r.value}>{r.label}</MenuItem>
                      ))}
                    </Select>
                  </FormControl>
                </Grid>

                <Grid item xs={12} md={4}>
                  <TextField
                    fullWidth
                    label="City / Partition (optional)"
                    placeholder="e.g. CR.Costa Rica Partition"
                    value={city}
                    onChange={(e) => setCity(e.target.value)}
                  />
                </Grid>

                <Grid item xs={12} md={4} sx={{ display: "flex", gap: 1 }}>
                  <Button
                    startIcon={<DateRangeIcon />}
                    variant={useRange ? "contained" : "outlined"}
                    onClick={() => setUseRange(true)}
                  >
                    Range
                  </Button>
                  <Button
                    startIcon={<DateRangeIcon />}
                    variant={!useRange ? "contained" : "outlined"}
                    onClick={() => setUseRange(false)}
                  >
                    Single Day
                  </Button>
                </Grid>

                {useRange ? (
                  <>
                    <Grid item xs={12} md={4}>
                      <TextField
                        label="Start date"
                        type="date"
                        fullWidth
                        InputLabelProps={{ shrink: true }}
                        value={startDate}
                        onChange={(e) => setStartDate(e.target.value)}
                      />
                    </Grid>
                    <Grid item xs={12} md={4}>
                      <TextField
                        label="End date"
                        type="date"
                        fullWidth
                        InputLabelProps={{ shrink: true }}
                        value={endDate}
                        onChange={(e) => setEndDate(e.target.value)}
                      />
                    </Grid>
                  </>
                ) : (
                  <Grid item xs={12} md={4}>
                    <TextField
                      label="Date"
                      type="date"
                      fullWidth
                      InputLabelProps={{ shrink: true }}
                      value={singleDate}
                      onChange={(e) => setSingleDate(e.target.value)}
                    />
                  </Grid>
                )}

                <Grid item xs={12} md={4} sx={{ display: "flex", gap: 1 }}>
                  <Button
                    variant="contained"
                    startIcon={<SearchIcon />}
                    onClick={fetchDurations}
                    disabled={loading}
                  >
                    {loading ? "Loading..." : "Run"}
                  </Button>

                  <Button
                    variant="outlined"
                    startIcon={<ClearIcon />}
                    onClick={() => {
                      setStartDate("");
                      setEndDate("");
                      setSingleDate("");
                      setCity("");
                      setData(null);
                      setError("");
                    }}
                  >
                    Clear
                  </Button>

                  <Tooltip title="Export CSV (current region)">
                    <span>
                      <IconButton onClick={exportCsv} disabled={!regionObj || (regionObj.employees || []).length === 0}>
                        <DownloadIcon />
                      </IconButton>
                    </span>
                  </Tooltip>
                </Grid>
              </Grid>
            </CardContent>
          </Card>

          <Box sx={{ mt: 2 }}>
            <Paper sx={{ p: 2 }}>
              <Box sx={{ display: "flex", justifyContent: "space-between", alignItems: "center" }}>
                <Typography variant="subtitle1">
                  {data ? `Showing ${region.toUpperCase()} — ${data.start_date} → ${data.end_date}` : "No results yet"}
                </Typography>
                <Typography variant="caption" color="text.secondary">
                  Tip: use the CSV export for offline analysis
                </Typography>
              </Box>

              {loading && (
                <Box sx={{ display: "flex", justifyContent: "center", py: 4 }}>
                  <CircularProgress />
                </Box>
              )}

              {error && (
                <Typography color="error" sx={{ mt: 2 }}>
                  {error}
                </Typography>
              )}

              {!loading && !error && (
                <Box sx={{ mt: 2 }}>
                  {renderTable()}
                </Box>
              )}
            </Paper>
          </Box>
        </Grid>

        <Grid item xs={12} md={4}>
          <Card sx={{ position: "sticky", top: 16 }}>
            <CardContent>
              <Typography variant="h6">How it works</Typography>
              <Typography variant="body2" sx={{ mt: 1 }}>
                - Use Range for multiple days (start + end).<br/>
                - Use Single Day for a calendar date.<br/>
                - Region selects which region to query (APAC/EMEA/LACA/NAMER).<br/>
                - City filters by PartitionName2 / PrimaryLocation / Door / EmployeeName.<br/>
                - The table shows per-person durations per day and a total seconds value.
              </Typography>

              <Box sx={{ mt: 2 }}>
                <Typography variant="subtitle2">Example endpoints</Typography>
                <Typography variant="caption" display="block">
                  Single day: <code>/duration?date=2025-09-04&regions=apac</code>
                </Typography>
                <Typography variant="caption" display="block">
                  Range: <code>/duration?start_date=2025-09-01&end_date=2025-09-05&regions=apac&city=CR.Costa%20Rica%20Partition</code>
                </Typography>
              </Box>
            </CardContent>
          </Card>
        </Grid>
      </Grid>
    </Box>
  );
}



