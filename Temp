Now i run this refer below js each file line by line only for Employee image purpose dont make unnecessary chamges ..

All database Connection has already done so dont make new connection only refer logic for EMployee image and integrate logic in ap.py file carefully..


For Refrance 

// controllers/employeeImageController.js
const sql       = require('mssql');
const { getPool } = require('../config/db');

async function getEmployeeImage(req, res) {
  console.log(`ðŸŽ¯ [GET] /api/employees/${req.params.id}/image called`);
  try {
    const pool = await getPool();
    console.log('âœ… DB pool acquired for image');
    const result = await pool.request()
      .input('id', sql.BigInt, req.params.id)
      .query(`
        SELECT AI.Image AS ImageBuffer
        FROM ACVSCore.Access.Images AI
        WHERE AI.ParentId = @id
      `);
    console.log('âœ… Image query rows:', result.recordset.length);

    if (!result.recordset.length || !result.recordset[0].ImageBuffer) {
      console.warn('âš ï¸ No image found for ID', req.params.id);
      return res.status(404).send('Image not found');
    }

    const buf = result.recordset[0].ImageBuffer;
    const header = buf.toString('hex', 0, 4).toLowerCase();
    let contentType = 'application/octet-stream';
    if (header.startsWith('ffd8')) contentType = 'image/jpeg';
    else if (header.startsWith('89504e47')) contentType = 'image/png';
    else if (header.startsWith('47494638')) contentType = 'image/gif';

    console.log('âœ… Serving image, type:', contentType, 'size:', buf.length);
    res.setHeader('Content-Type', contentType);
    return res.send(buf);

  } catch (err) {
    console.error('âŒ Error in getEmployeeImage:', err.stack || err);
    return res.status(500).send('Internal Server Error');
  }
}

module.exports = { getEmployeeImage };






 
// controllers/employeeController.js
const { getPool } = require('../config/db');
 
const getEmployees = async (req, res) => {
  console.log('ðŸŽ¯ [GET] /api/employees called');
  try {
    const pool = await getPool();
    console.log('âœ… DB pool acquired');
 
    const result = await pool.request().query(`
      WITH EmpAgg AS (
        SELECT
          AP.ObjectID           AS EmployeeObjID,
          AP.Name               AS EmpName,
          CASE WHEN AP.Int1 = 0 OR AP.Int1 IS NULL
               THEN AP.Text12
               ELSE CAST(AP.Int1 AS NVARCHAR(50))
          END AS EmployeeID,
          ISNULL(PT.Name, '')   AS PersonnelType,
          AP.Text10             AS Manager_Name,
          AP.Int4               AS Manager_WU_ID,
          AP.Disabled           AS Profile_Disabled,
          ISNULL(AP.Text4, '')  AS CompanyName,
          ISNULL(AP.Text5, '')  AS PrimaryLocation,
          COUNT(ACR.Name)       AS Total_Cards,
 
          SUM(
            CASE
              WHEN (ACR.ExpirationDateTime IS NULL OR ACR.ExpirationDateTime >= GETDATE())
                AND (ACR.Disabled IS NULL OR ACR.Disabled = 0)
                AND (ACR.Lost IS NULL OR ACR.Lost = 0)
                AND (ACR.Stolen IS NULL OR ACR.Stolen = 0)
              THEN 1 ELSE 0
            END
          ) AS Active_Cards,
 
          CASE
            WHEN COUNT(ACR.Name) - SUM(
              CASE
                WHEN (ACR.ExpirationDateTime IS NULL OR ACR.ExpirationDateTime >= GETDATE())
                  AND (ACR.Disabled IS NULL OR ACR.Disabled = 0)
                  AND (ACR.Lost IS NULL OR ACR.Lost = 0)
                  AND (ACR.Stolen IS NULL OR ACR.Stolen = 0)
                THEN 1 ELSE 0
              END
            ) < 0 THEN 0
            ELSE COUNT(ACR.Name) - SUM(
              CASE
                WHEN (ACR.ExpirationDateTime IS NULL OR ACR.ExpirationDateTime >= GETDATE())
                  AND (ACR.Disabled IS NULL OR ACR.Disabled = 0)
                  AND (ACR.Lost IS NULL OR ACR.Lost = 0)
                  AND (ACR.Stolen IS NULL OR ACR.Stolen = 0)
                THEN 1 ELSE 0
              END
            )
          END AS Deactive_Cards,
 
          -- Active card numbers (comma separated)
          ISNULL(
            STUFF(
              (
                SELECT ', ' + COALESCE(CAST(ACR2.CardNumber AS NVARCHAR(100)), CAST(ACR2.ObjectID AS NVARCHAR(50)))
                FROM ACVSCore.Access.Credential ACR2
                WHERE ACR2.Name = AP.Name
                  AND (
                     (ACR2.ExpirationDateTime IS NULL OR ACR2.ExpirationDateTime >= GETDATE())
                     AND (ACR2.Disabled IS NULL OR ACR2.Disabled = 0)
                     AND (ACR2.Lost IS NULL OR ACR2.Lost = 0)
                     AND (ACR2.Stolen IS NULL OR ACR2.Stolen = 0)
                  )
                FOR XML PATH(''), TYPE
              ).value('.', 'NVARCHAR(MAX)')
            , 1, 2, '')
          , '') AS Active_Card_Numbers,
 
          -- Deactive card numbers (comma separated)
          ISNULL(
            STUFF(
              (
                SELECT ', ' + COALESCE(CAST(ACR3.CardNumber AS NVARCHAR(100)), CAST(ACR3.ObjectID AS NVARCHAR(50)))
                FROM ACVSCore.Access.Credential ACR3
                WHERE ACR3.Name = AP.Name
                  AND NOT (
                     (ACR3.ExpirationDateTime IS NULL OR ACR3.ExpirationDateTime >= GETDATE())
                     AND (ACR3.Disabled IS NULL OR ACR3.Disabled = 0)
                     AND (ACR3.Lost IS NULL OR ACR3.Lost = 0)
                     AND (ACR3.Stolen IS NULL OR ACR3.Stolen = 0)
                  )
                FOR XML PATH(''), TYPE
              ).value('.', 'NVARCHAR(MAX)')
            , 1, 2, '')
          , '') AS Deactive_Card_Numbers
 
        FROM ACVSCore.Access.Personnel AP
        LEFT JOIN ACVSCore.Access.Credential    ACR ON AP.Name = ACR.Name
        LEFT JOIN ACVSCore.Access.PersonnelType PT  ON PT.ObjectID = AP.PersonnelTypeID
        GROUP BY
          AP.ObjectID,
          AP.Name,
          AP.Int1,
          AP.Text12,
          PT.Name,
          AP.Text10,
          AP.Int4,
          AP.Disabled,
          AP.Text4,
          AP.Text5
      ),
      EmpAggWithStatus AS (
        SELECT
          EmployeeObjID,
          EmpName,
          EmployeeID,
          PersonnelType,
          Manager_Name,
          Manager_WU_ID,
          Profile_Disabled,
          CompanyName,
          PrimaryLocation,
          Total_Cards,
          Active_Cards,
          Deactive_Cards,
          Active_Card_Numbers,
          Deactive_Card_Numbers,
          CASE WHEN Profile_Disabled = 1 THEN 'Deactive'
               WHEN Profile_Disabled = 0 AND Active_Cards > 0 THEN 'Active'
               ELSE 'Deactive' END AS Employee_Status
        FROM EmpAgg
      )
      SELECT
        e.EmployeeObjID,
        e.EmpName,
        e.EmployeeID,
        e.PersonnelType,
        e.Manager_Name,
        e.Manager_WU_ID,
        e.Profile_Disabled,
        e.CompanyName,
        e.PrimaryLocation,
        e.Total_Cards,
        e.Active_Cards,
        e.Deactive_Cards,
        e.Active_Card_Numbers,
        e.Deactive_Card_Numbers,
        CASE WHEN img.Image IS NULL THEN 'No' ELSE 'Yes' END AS HasImage,
        e.Employee_Status,
        clr.ClearanceCount,
        clr.Clearances
      FROM EmpAggWithStatus e
      OUTER APPLY (
        -- image presence check (no binary)
        SELECT TOP (1) AI.Image
        FROM ACVSCore.Access.Images AI
        WHERE AI.ParentId = e.EmployeeObjID
          AND DATALENGTH(AI.Image) > 0
        ORDER BY AI.ObjectID DESC
      ) img
      OUTER APPLY (
        SELECT
          (SELECT COUNT(1) FROM ACVSCore.Access.PersonnelClearancePair APC_count WHERE APC_count.PersonnelID = e.EmployeeObjID) AS ClearanceCount,
          STUFF((
            SELECT ', ' + AC2.Name
            FROM ACVSCore.Access.PersonnelClearancePair APC2
            INNER JOIN ACVSCore.Access.Clearance AC2
              ON AC2.ObjectID = APC2.ClearanceID
            WHERE APC2.PersonnelID = e.EmployeeObjID
            FOR XML PATH(''), TYPE
          ).value('.', 'NVARCHAR(MAX)'), 1, 2, '') AS Clearances
      ) clr
      ORDER BY e.EmpName;
    `);
 
    console.log('âœ… Query executed, rows:', result.recordset.length);
 
    const employees = result.recordset.map(emp => ({
      id:                   emp.EmployeeObjID,
      EmpName:              emp.EmpName,
      EmployeeID:           emp.EmployeeID,
      PersonnelType:        emp.PersonnelType,
      Manager_Name:         emp.Manager_Name,
      Manager_WU_ID:        emp.Manager_WU_ID,
      Profile_Disabled:     Boolean(emp.Profile_Disabled),
      CompanyName:          emp.CompanyName,
      PrimaryLocation:      emp.PrimaryLocation,
      Total_Cards:          emp.Total_Cards,
      Active_Cards:         emp.Active_Cards,
      Deactive_Cards:       emp.Deactive_Cards,
      Active_Card_Numbers:  emp.Active_Card_Numbers || '',
      Deactive_Card_Numbers: emp.Deactive_Card_Numbers || '',
      Employee_Status:      emp.Employee_Status,
      imageUrl:             `/api/employees/${emp.EmployeeObjID}/image`,
      HasImage:             emp.HasImage === 'Yes',
      ClearanceCount:       emp.ClearanceCount || 0,
      Clearances:           emp.Clearances || ''
    }));
 
    return res.json(employees);
  } catch (err) {
    console.error('âŒ Error in getEmployees:', err.stack || err);
    return res.status(500).json({
      error: 'Internal Server Error',
      message: err.message
    });
  }
};
 
 
const getEmployeeStats = async (req, res) => {
  try {
    const pool = await getPool();
 
    // If no details param -> return the single-row summary (existing behaviour)
    const details = (req.query.details || '').trim();
    if (!details) {
      const summaryResult = await pool.request().query(`
        SELECT
          COUNT(*) AS TotalProfiles,
          SUM(CASE WHEN AP.Disabled = 0 THEN 1 ELSE 0 END) AS ActiveProfiles,
          SUM(CASE WHEN AP.Disabled = 0 AND PT.Name = 'Employee' THEN 1 ELSE 0 END) AS ActiveEmployees,
          SUM(CASE WHEN AP.Disabled = 0 AND PT.Name = 'Contractor' THEN 1 ELSE 0 END) AS ActiveContractors,
          SUM(CASE WHEN AP.Disabled = 1 THEN 1 ELSE 0 END) AS TerminatedProfiles,
          SUM(CASE WHEN AP.Disabled = 1 AND PT.Name = 'Employee' THEN 1 ELSE 0 END) AS TerminatedEmployees,
          SUM(CASE WHEN AP.Disabled = 1 AND PT.Name = 'Contractor' THEN 1 ELSE 0 END) AS TerminatedContractors
        FROM ACVSCore.Access.Personnel AP
        INNER JOIN ACVSCore.Access.PersonnelType PT ON PT.ObjectID = AP.PersonnelTypeID
      `);
      return res.json(summaryResult.recordset[0]);
    }
 
    // --- details listing mode (unchanged) ---
    // normalize details token
    const token = details.toLowerCase();
 
    // pagination
    let page = parseInt(req.query.page, 10) || 1;
    let limit = parseInt(req.query.limit, 10) || 50;
    if (page < 1) page = 1;
    if (limit < 1) limit = 1;
    if (limit > 500) limit = 500; // safety cap
 
    const offset = (page - 1) * limit;
 
    // optional personnel type filter (whitelist)
    const pTypeRaw = (req.query.pType || '').trim();
    const pType = (pTypeRaw.toLowerCase() === 'employee') ? 'Employee'
                : (pTypeRaw.toLowerCase() === 'contractor') ? 'Contractor'
                : null;
 
    // optional search q
    const qRaw = (req.query.q || '').trim();
    const q = qRaw.replace(/[%_]/g, ''); // basic sanitization (remove wildcard chars)
 
    // map details token to WHERE clauses
    const whereClauses = [];
 
    switch (token) {
      case 'totalprofiles':
      case 'total':
        // no extra where
        break;
      case 'activeprofiles':
      case 'active':
        whereClauses.push('EA.Profile_Disabled = 0');
        break;
      case 'terminatedprofiles':
      case 'terminated':
        whereClauses.push('EA.Profile_Disabled = 1');
        break;
      case 'activeemployees':
        whereClauses.push("EA.Profile_Disabled = 0 AND EA.PersonnelType = 'Employee'");
        break;
      case 'activecontractors':
        whereClauses.push("EA.Profile_Disabled = 0 AND EA.PersonnelType = 'Contractor'");
        break;
      case 'terminatedemployees':
        whereClauses.push("EA.Profile_Disabled = 1 AND EA.PersonnelType = 'Employee'");
        break;
      case 'terminatedcontractors':
        whereClauses.push("EA.Profile_Disabled = 1 AND EA.PersonnelType = 'Contractor'");
        break;
      default:
        return res.status(400).json({ error: 'Unsupported details value', supported: [
          'TotalProfiles','ActiveProfiles','TerminatedProfiles',
          'ActiveEmployees','ActiveContractors','TerminatedEmployees','TerminatedContractors'
        ]});
    }
 
    if (pType) {
      whereClauses.push(`EA.PersonnelType = '${pType}'`);
    }
 
    if (q) {
      // search against EmpName and EmployeeID (case-insensitive)
      const safeQ = q.replace(/'/g, "''");
      whereClauses.push(`(EA.EmpName LIKE '%${safeQ}%' OR EA.EmployeeID LIKE '%${safeQ}%')`);
    }
 
    const whereSql = whereClauses.length ? `WHERE ${whereClauses.join(' AND ')}` : '';
 
    // Build CTE that aggregates cards per person (same logic as getEmployees)
    const cte = `
      WITH EmpAgg AS (
        SELECT
          AP.ObjectID           AS EmployeeObjID,
          AP.Name               AS EmpName,
          CASE WHEN AP.Int1 = 0 OR AP.Int1 IS NULL THEN AP.Text12 ELSE CAST(AP.Int1 AS NVARCHAR(50)) END AS EmployeeID,
          ISNULL(PT.Name, '')   AS PersonnelType,
          AP.Text10             AS Manager_Name,
          AP.Int4               AS Manager_WU_ID,
          AP.Disabled           AS Profile_Disabled,
          COUNT(ACR.Name)       AS Total_Cards,
          SUM(
            CASE
              WHEN (ACR.ExpirationDateTime IS NULL OR ACR.ExpirationDateTime >= GETDATE())
                AND (ACR.Disabled IS NULL OR ACR.Disabled = 0)
                AND (ACR.Lost IS NULL OR ACR.Lost = 0)
                AND (ACR.Stolen IS NULL OR ACR.Stolen = 0)
              THEN 1 ELSE 0
            END
          ) AS Active_Cards
        FROM ACVSCore.Access.Personnel AP
        LEFT JOIN ACVSCore.Access.Credential ACR ON AP.Name = ACR.Name
        LEFT JOIN ACVSCore.Access.PersonnelType PT ON PT.ObjectID = AP.PersonnelTypeID
        GROUP BY
          AP.ObjectID, AP.Name, AP.Int1, AP.Text12, PT.Name, AP.Text10, AP.Int4, AP.Disabled
      ),
      EmpAggWithStatus AS (
        SELECT
          EmployeeObjID,
          EmpName,
          EmployeeID,
          PersonnelType,
          Manager_Name,
          Manager_WU_ID,
          Profile_Disabled,
          Total_Cards,
          Active_Cards,
          CASE WHEN Profile_Disabled = 1 THEN 'Deactive'
               WHEN Profile_Disabled = 0 AND Active_Cards > 0 THEN 'Active'
               ELSE 'Deactive' END AS Employee_Status
        FROM EmpAgg
      )
    `;
 
    // count query
    const countSql = `${cte} SELECT COUNT(*) AS totalCount FROM EmpAggWithStatus EA ${whereSql};`;
 
    // data query with pagination
    const dataSql = `
      ${cte}
      SELECT
        EA.EmployeeObjID AS id,
        EA.EmpName,
        EA.EmployeeID,
        EA.PersonnelType,
        EA.Manager_Name,
        EA.Manager_WU_ID,
        EA.Profile_Disabled,
        EA.Total_Cards,
        EA.Active_Cards,
        EA.Employee_Status
      FROM EmpAggWithStatus EA
      ${whereSql}
      ORDER BY EA.EmpName
      OFFSET ${offset} ROWS FETCH NEXT ${limit} ROWS ONLY;
    `;
 
    // execute both queries (count then data)
    const countResult = await pool.request().query(countSql);
    const totalCount = (countResult.recordset[0] && countResult.recordset[0].totalCount) ? countResult.recordset[0].totalCount : 0;
 
    const dataResult = await pool.request().query(dataSql);
    const rows = dataResult.recordset || [];
 
    const employees = rows.map(emp => ({
      id:               emp.id,
      EmpName:          emp.EmpName,
      EmployeeID:       emp.EmployeeID,
      PersonnelType:    emp.PersonnelType,
      Manager_Name:     emp.Manager_Name,
      Manager_WU_ID:    emp.Manager_WU_ID,
      Profile_Disabled: Boolean(emp.Profile_Disabled),
      Total_Cards:      emp.Total_Cards,
      Active_Cards:     emp.Active_Cards,
      Employee_Status:  emp.Employee_Status,
      imageUrl:         `/api/employees/${emp.id}/image`
    }));
 
    return res.json({
      total: totalCount,
      page,
      limit,
      data: employees
    });
 
  } catch (err) {
    console.error('âŒ getEmployeeStats error', err.stack || err);
    return res.status(500).json({ error: 'Failed to get stats', message: err.message });
  }
};
 
module.exports = {
  getEmployees,
  getEmployeeStats
};
 
 





 
// frontend/src/components/EmployeeCard.jsx
import React, { useState, useRef, useEffect } from 'react';
import { FaUser, FaIdBadge, FaUserTie, FaIdCard, FaCheckCircle, FaClone, FaMapMarkerAlt } from 'react-icons/fa';
import CurrentLocation from './CurrentLocation';
import { HiOutlineBuildingOffice2 } from "react-icons/hi2";
import './EmployeeCard.css';
 
export default function EmployeeCard({ emp }) {
  // ---- Hooks: MUST be called unconditionally ----
  const [showMore, setShowMore] = useState(false);
  const [showCardsPopup, setShowCardsPopup] = useState(false);
  const [showClearancePopup, setShowClearancePopup] = useState(false);
  const cardsRef = useRef(null);
  const clearanceRef = useRef(null);
 
  // close popups on outside click (unconditional hook)
  useEffect(() => {
    function handleDocClick(e) {
      if (showCardsPopup && cardsRef.current && !cardsRef.current.contains(e.target)) {
        setShowCardsPopup(false);
      }
      if (showClearancePopup && clearanceRef.current && !clearanceRef.current.contains(e.target)) {
        setShowClearancePopup(false);
      }
    }
    document.addEventListener('mousedown', handleDocClick);
    return () => document.removeEventListener('mousedown', handleDocClick);
  }, [showCardsPopup, showClearancePopup]);
 
  // helper to convert comma string -> array
  const toList = (csv) => {
    if (!csv) return [];
    return csv.split(',').map(s => s.trim()).filter(Boolean);
  };
 
  // Early return AFTER hooks (so hooks are always called in same order)
  if (!emp) return null;
 
  const rawStatus = emp.Employee_Status || 'Deactive';
  const normalizedStatus = rawStatus.trim().toLowerCase();
  const isRedStatus =
    normalizedStatus === 'deactive' || normalizedStatus === 'terminated';
 
  return (
    <div
      className="employee-card-container"
      data-status={normalizedStatus}
      style={{
        background: isRedStatus
          ? '#f55847'
          : 'linear-gradient(180deg, #131318 0%, #1d1d26 100%)',
      }}
    >
      {/* Left column */}
      <div className="left-col">
        <div className="photo-ring">
          <img
            className="photo"
            src={emp.imageUrl ? `http://localhost:5001${emp.imageUrl}` : '/images/no-photo.jpg'}
            alt={emp.EmpName || 'Employee photo'}
            onError={(e) => {
              e.target.onerror = null;
              e.target.src = '/images/no-photo.jpg';
            }}
          />
        </div>
 
        <div className="photo-meta">
          <div className="emp-name">{emp.EmpName || 'â€”'}</div>
          <div className="emp-role">{emp.PersonnelType || 'Employee'}</div>
          <div className={`status-pill ${normalizedStatus}`}>{rawStatus}</div>
        </div>
      </div>
 
      {/* Separator */}
      <div className="separator" aria-hidden />
 
      {/* Right column */}
      <div className="right-col">
        <h3 className="profile-title">Profile Details</h3>
 
        <table className="details-table" aria-label="employee details">
          <tbody>
            {/* Always visible */}
            <tr>
              <td className="label "><FaUser color='#FFDD00' /> Name</td>
              <td className="value v-color">{emp.EmpName || 'â€”'}</td>
            </tr>
            <tr>
              <td className="label"><FaIdBadge color='#FFDD00' /> Employee ID</td>
              <td className="value v-color">{emp.EmployeeID || 'â€”'}</td>
            </tr>
            <tr>
              <td className="label"><FaUserTie color='#FFDD00' /> Manager</td>
              <td className="value v-color">{emp.Manager_Name || 'â€”'}</td>
            </tr>
 
            {/* Active Cards: displays Total_Cards (requested) */}
            <tr
              className="clickable-row"
              ref={cardsRef}
            >
              <td className="label"><FaIdCard /> Total Cards</td>
              <td
                className="value v-color clickable-cell"
                onClick={(e) => { e.stopPropagation(); setShowCardsPopup(prev => !prev); }}
                title="Click to view card details"
                role="button"
                tabIndex={0}
                onKeyDown={(e) => { if (e.key === 'Enter' || e.key === ' ') { setShowCardsPopup(prev => !prev); } }}
              >
                {emp.Total_Cards ?? 0}
              </td>
 
 
 
              {/* {showCardsPopup && (
                <td className="popup-td" colSpan="2">
                  <div className="popup-card fancy-popup">
                    <div className="popup-header">
                      <strong>Card Details</strong>
                      <button
                        className="popup-close fancy-close"
                        onClick={() => setShowCardsPopup(false)}
                        title="Close"
                      >
                        ðŸ•“
                      </button>
                    </div>
 
                    <div className="popup-body">
                      <div><strong>Total Cards:</strong> {emp.Total_Cards ?? 0}</div>
                      <div><strong>Active Cards:</strong> {emp.Active_Cards ?? 0}</div>
                      <div><strong>Deactive Cards:</strong> {emp.Deactive_Cards ?? 0}</div>
 
                      <div style={{ marginTop: 10 }}>
                        <strong>Active Card Numbers</strong>
                        <ul className="popup-list">
                          {toList(emp.Active_Card_Numbers).length === 0 && (<li>â€”</li>)}
                          {toList(emp.Active_Card_Numbers).map((c, i) => <li key={`ac-${i}`}>{c}</li>)}
                        </ul>
                      </div>
 
                      <div style={{ marginTop: 10 }}>
                        <strong>Deactive Card Numbers</strong>
                        <ul className="popup-list">
                          {toList(emp.Deactive_Card_Numbers).length === 0 && (<li>â€”</li>)}
                          {toList(emp.Deactive_Card_Numbers).map((c, i) => <li key={`dc-${i}`}>{c}</li>)}
                        </ul>
                      </div>
                    </div>
                  </div>
                </td>
              )} */}
 
 
              {showCardsPopup && (
                <div className="popup-overlay" onClick={() => setShowCardsPopup(false)}>
                  <div
                    className="popup-card fancy-popup"
                    onClick={(e) => e.stopPropagation()} // prevent closing when clicking inside
                  >
                    <div className="popup-header">
                      <strong>Card Details</strong>
                      <button
                        className="popup-close fancy-close"
                        onClick={() => setShowCardsPopup(false)}
                        title="Close"
                      >
                       <i class="fas fa-window-close"></i>
                      </button>
                    </div>
 
                    <div className="popup-body">
                      <div><strong>Total Cards:</strong> {emp.Total_Cards ?? 0}</div>
                      <div><strong>Active Cards:</strong> {emp.Active_Cards ?? 0}</div>
                      <div><strong>Deactive Cards:</strong> {emp.Deactive_Cards ?? 0}</div>
 
                      <div style={{ marginTop: 10 }}>
                        <strong>Active Card Numbers</strong>
                        <ul className="popup-list">
                          {toList(emp.Active_Card_Numbers).length === 0 && (<li>â€”</li>)}
                          {toList(emp.Active_Card_Numbers).map((c, i) => <li key={`ac-${i}`}>{c}</li>)}
                        </ul>
                      </div>
 
                      <div style={{ marginTop: 10 }}>
                        <strong>Deactive Card Numbers</strong>
                        <ul className="popup-list">
                          {toList(emp.Deactive_Card_Numbers).length === 0 && (<li>â€”</li>)}
                          {toList(emp.Deactive_Card_Numbers).map((c, i) => <li key={`dc-${i}`}>{c}</li>)}
                        </ul>
                      </div>
                    </div>
                  </div>
                </div>
              )}
 
            </tr>
 
            {/* Clearance row: shows ClearanceCount and clickable to open details */}
            {/* Clearance row: shows ClearanceCount and clickable to open details */}
            <tr className="clickable-row" ref={clearanceRef}>
              <td className="label">
                <FaCheckCircle color="#FFDD00" /> Clearance
              </td>
              <td
                className="value v-color clickable-cell"
                onClick={(e) => {
                  e.stopPropagation();
                  setShowClearancePopup((prev) => !prev);
                }}
                title="Click to view clearance details"
                role="button"
                tabIndex={0}
                onKeyDown={(e) => {
                  if (e.key === 'Enter' || e.key === ' ') {
                    setShowClearancePopup((prev) => !prev);
                  }
                }}
              >
                {emp.ClearanceCount ?? 0}
              </td>
            </tr>
 
            {/* âœ… Centered Clearance Popup */}
            {showClearancePopup && (
              <div className="popup-overlay" onClick={() => setShowClearancePopup(false)}>
                <div
                  className="popup-card fancy-popup"
                  onClick={(e) => e.stopPropagation()} // prevent closing when clicking inside
                >
                  <div className="popup-header">
                    <strong>Clearance Details</strong>
                    <button
                      className="popup-close fancy-close"
                      onClick={() => setShowClearancePopup(false)}
                      title="Close"
                    >
                      ðŸ•“
                    </button>
                  </div>
 
                  <div className="popup-body">
                    <div>
                      <strong>Clearance Count:</strong> {emp.ClearanceCount ?? 0}
                    </div>
                    <div style={{ marginTop: 8 }}>
                      <strong>Clearances</strong>
                      <ul className="popup-list">
                        {(!emp.Clearances || emp.Clearances.trim() === '') && <li>â€”</li>}
                        {emp.Clearances &&
                          emp.Clearances.split(',').map((c, i) => (
                            <li key={`clr-${i}`}>{c.trim()}</li>
                          ))}
                      </ul>
                    </div>
                  </div>
                </div>
              </div>
            )}
 
            <tr>
              <td className="label"><HiOutlineBuildingOffice2 color='#FFDD00' /> Company Name</td>
              <td className="value v-color">{emp.CompanyName || 'â€”'}</td>
            </tr>
            <tr>
              <td className="label"><FaMapMarkerAlt color='#FFDD00' /> Primary Location</td>
              <td className="value v-color">{emp.PrimaryLocation || 'â€”'}</td>
            </tr>
 
            <tr>
              <td colSpan="2" style={{ paddingTop: 1 }}>
                <CurrentLocation empId={emp.id ?? emp.EmployeeID} showMore={showMore} />
              </td>
            </tr>
 
            {/* Hidden until showMore is true */}
            {/* {showMore && (
              <>
                <tr>
                  <td className="label"><FaClone /> Total Cards</td>
                  <td className="value">{emp.Total_Cards ?? 0}</td>
                </tr>
              </>
            )} */}
 
 
 
          </tbody>
        </table>
 
        {/* Toggle Button */}
        <button
          className="show-more-btn"
          onClick={() => setShowMore((prev) => !prev)}
        >
          {showMore ? 'Show Less' : 'Show More'}
        </button>
      </div>
    </div>
  );
}
 
 
 
 



Refer above logic carefully and fix this issue strivkly....




 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 # backend/app.py
from flask import Flask, jsonify, request, send_from_directory, jsonify, send_file
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re
import io
import base64
import os
import difflib
from io import BytesIO
from flask import send_file  # add if not present
from pathlib import Path
from typing import Optional, List, Dict, Any
from duration_report import REGION_CONFIG
from datetime import date, timedelta, datetime
from flask import jsonify, request
import logging
logging.basicConfig(level=logging.INFO)
#from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR
from trend_runner import run_trend_for_date, build_monthly_training, _enrich_with_personnel_info
from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE


# app = Flask(__name__)



def _safe_read_csv(fp):
    try:
        return pd.read_csv(fp, parse_dates=['LocaleMessageTime'], low_memory=False)
    except Exception:
        try:
            return pd.read_csv(fp, low_memory=False)
        except Exception:
            return pd.DataFrame()


# ---------- Ensure outputs directory exists early (so OVERRIDES_FILE can be defined safely) ----------

BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)

OUTDIR = DEFAULT_OUTDIR

OVERRIDES_FILE = DEFAULT_OUTDIR / "overrides.csv"


def _load_overrides():
    if not OVERRIDES_FILE.exists():
        return {}
    try:
        df = pd.read_csv(OVERRIDES_FILE, dtype=str)
        out = {}
        for _, r in df.iterrows():
            emp = str(r.get('EmployeeID') or r.get('person_uid') or '').strip()
            if not emp:
                continue
            out[emp] = {
                'level': str(r.get('OverrideLevel') or '').strip(),
                'reason': str(r.get('Reason') or '').strip(),
                'ts': str(r.get('Timestamp') or '').strip()
            }
        return out
    except Exception:
        logging.exception("Failed reading overrides file")
        return {}

def _save_override(employee_key, level, reason):
    now = datetime.now().isoformat()
    row = {'EmployeeID': employee_key, 'OverrideLevel': level, 'Reason': reason or '', 'Timestamp': now}
    try:
        if OVERRIDES_FILE.exists():
            df = pd.read_csv(OVERRIDES_FILE, dtype=str)
            # pandas.DataFrame.append is deprecated -> use concat
            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        else:
            df = pd.DataFrame([row])
        df.to_csv(OVERRIDES_FILE, index=False)
        return True
    except Exception:
        logging.exception("Failed to save override")
        return False

def _slug_city(s):
    """
    Convert a city/site string into a safe slug: lowercase, alphanumeric+hyphen
    """
    if not s:
        return ''
    # Remove special chars, spaces to hyphens, lower
    slug = re.sub(r'[^\w\s-]', '', str(s)).strip().lower()
    slug = re.sub(r'[\s_]+', '-', slug)
    return slug


# --- Use REGION_CONFIG servers to talk to ACVSCore (no separate ACVSCORE_DB_CONFIG) ---
# ODBC driver variable is already defined later: ODBC_DRIVER (safe to reference only at runtime)

_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20

def _get_acvscore_conn():
    """
    Try to connect to ACVSCore by reusing credentials from REGION_CONFIG.
    Loops through REGION_CONFIG entries and attempts:
      1) SQL auth (UID/PWD) to database "ACVSCore" using region server + credentials
      2) If SQL auth fails on that server, try Trusted_Connection (Windows auth) as a fallback
    If that fails, optionally attempt ACVSCORE_DB_CONFIG if defined (safe: checked via globals()).
    Returns first successful pyodbc connection or None.
    Implements a short backoff after recent failure to reduce log noise.
    """
    try:
        import pyodbc
    except Exception:
        logging.exception("pyodbc not installed; ACVSCore lookups unavailable.")
        return None

    # basic backoff: skip attempts if we just failed recently
    from datetime import datetime
    now = datetime.now().timestamp()
    last = _acvscore_backoff.get("ts")
    if last and _acvscore_backoff.get("failed") and (now - last) < _ACVSCORE_BACKOFF_SECONDS:
        logging.debug("Skipping ACVSCore connection attempt (backoff active).")
        return None

    # iterate region servers (use the same credentials defined in REGION_CONFIG)
    tried = []
    for region_key, rc in (REGION_CONFIG or {}).items():
        server = rc.get("server")
        user = rc.get("user")
        pwd = rc.get("password")
        if not server:
            continue

        # Try SQL auth first if credentials present
        if user and pwd:
            tried.append(f"{region_key}@{server}(sql)")
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={server};DATABASE=ACVSCore;UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;"
            )
            try:
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (sql auth).", server, region_key)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("SQL auth to %s failed: %s", server, e)

        # Try Trusted Connection fallback on same server
        tried.append(f"{region_key}@{server}(trusted)")
        conn_str_trusted = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={server};DATABASE=ACVSCore;Trusted_Connection=yes;"
            "TrustServerCertificate=Yes;"
        )
        try:
            conn = pyodbc.connect(conn_str_trusted, autocommit=True, timeout=5)
            logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (trusted connection).", server, region_key)
            _acvscore_backoff["ts"] = None
            _acvscore_backoff["failed"] = False
            return conn
        except Exception as e:
            logging.debug("Trusted connection to %s failed: %s", server, e)
            continue

    # Fallback: if a global ACVSCORE_DB_CONFIG exists, try it (safe check)
    try:
        if 'ACVSCORE_DB_CONFIG' in globals() and isinstance(globals().get('ACVSCORE_DB_CONFIG'), dict):
            cfg = globals().get('ACVSCORE_DB_CONFIG')
            server = cfg.get('server')
            user = cfg.get('user')
            pwd = cfg.get('password')
            database = cfg.get('database', 'ACVSCore')
            tried.append(f"ACVSCORE_DB_CONFIG@{server}")
            try:
                conn_str = (
                    f"DRIVER={{{ODBC_DRIVER}}};"
                    f"SERVER={server};DATABASE={database};UID={user};PWD={pwd};"
                    "TrustServerCertificate=Yes;"
                )
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore using ACVSCORE_DB_CONFIG (%s).", server)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("ACVSCORE_DB_CONFIG connection failed: %s", e)
    except Exception:
        # defensive: do not propagate any errors from fallback logic
        logging.debug("ACVSCORE_DB_CONFIG fallback not available or failed.")

    # record failure to backoff
    _acvscore_backoff["ts"] = now
    _acvscore_backoff["failed"] = True
    logging.error("Failed to connect to ACVSCore using REGION_CONFIG servers. Tried: %s", tried)
    return None


# ODBC driver (keep existing env-based driver)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data



def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    out: Dict[str, Any] = {}
    logging.info("get_personnel_info: lookup called with candidate_identifier=%s", candidate_identifier)
    if candidate_identifier is None:
        logging.debug("get_personnel_info: no candidate provided")
        return out
    conn = _get_acvscore_conn()
    if conn is None:
        logging.info("get_personnel_info: ACVSCore connection unavailable (skipping DB lookup)")
        return out
    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        # avoid passing non-GUID strings to the GUID parameter (prevents SQL Server conversion errors)
        cand_guid = cand if _looks_like_guid(cand) else None
        params = (cand, cand_guid, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()
        if row:
            # columns: ObjectID, GUID, Name, EmailAddress, ManagerEmail
            try:
                out['ObjectID'] = row[0]
                out['GUID'] = row[1]
                out['Name'] = row[2]
                # canonical email fields (provide aliases for downstream code)
                email_val = row[3] if len(row) > 3 else None
                out['EmailAddress'] = email_val or None
                out['EmployeeEmail'] = email_val or None
                out['Email'] = email_val or None
                out['ManagerEmail'] = row[4] if len(row) > 4 else None
            except Exception:
                out = {
                    'ObjectID': row[0] if len(row) > 0 else None,
                    'GUID': row[1] if len(row) > 1 else None,
                    'Name': row[2] if len(row) > 2 else None,
                    'EmailAddress': row[3] if len(row) > 3 else None,
                    'EmployeeEmail': row[3] if len(row) > 3 else None,
                    'Email': row[3] if len(row) > 3 else None,
                    'ManagerEmail': row[4] if len(row) > 4 else None
                }
            logging.info("get_personnel_info: found personnel row for candidate=%s -> ObjectID=%s Email=%s",
                         candidate_identifier, out.get('ObjectID'), out.get('EmailAddress'))
        else:
            logging.debug("get_personnel_info: no personnel row found for candidate=%s", candidate_identifier)
    except Exception:
        logging.exception("Failed personnel lookup for candidate: %s", candidate_identifier)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out



def get_person_image_bytes(parent_id) -> Optional[bytes]:
    logging.info("get_person_image_bytes: lookup for ParentId=%s", parent_id)
    # 1) try DB (as before) â€” keep behaviour if available
    try:
        conn = _get_acvscore_conn()
        if conn is not None:
            try:
                cur = conn.cursor()
                sql = """
                    SELECT TOP 1 AI.Image
                    FROM ACVSCore.Access.Images AI
                    WHERE AI.ParentId = ?
                      AND DATALENGTH(AI.Image) > 0
                    ORDER BY AI.ObjectID DESC
                """
                cur.execute(sql, (str(parent_id),))
                row = cur.fetchone()
                if row and row[0] is not None:
                    logging.info("get_person_image_bytes: image found in DB for ParentId=%s (len=%d)", parent_id, len(row[0]) if row[0] else 0)
                    try:
                        b = bytes(row[0])
                        return b
                    except Exception:
                        return row[0]
            except Exception:
                logging.exception("Failed to fetch image for ParentId=%s via DB", parent_id)
            finally:
                try:
                    cur.close()
                except Exception:
                    pass
                try:
                    conn.close()
                except Exception:
                    pass
    except Exception:
        logging.debug("ACVSCore DB unavailable for image lookup; will try filesystem fallbacks for ParentId=%s", parent_id)

    # 2) Try filesystem fallbacks under DEFAULT_OUTDIR (use typical file extensions)
    try:
        # ensure DEFAULT_OUTDIR exists and convert parent_id to safe filename
        cand_ids = []
        if parent_id is None:
            return None
        pid_raw = str(parent_id).strip()
        # add raw and numeric-only variants
        cand_ids.append(pid_raw)
        try:
            # if numeric-like, add int form
            if '.' in pid_raw:
                f = float(pid_raw)
                if f.is_integer():
                    cand_ids.append(str(int(f)))
        except Exception:
            pass
        # also try stripped non-alphanumeric variants
        cand_ids = list(dict.fromkeys(cand_ids))

        for c in cand_ids:
            for folder in (Path(DEFAULT_OUTDIR) / "images", Path(DEFAULT_OUTDIR), Path(".")):
                if not folder.exists():
                    continue
                for ext in (".jpg", ".jpeg", ".png", ".bmp", ".gif", ".webp"):
                    fp = folder / (f"{c}{ext}")
                    logging.debug("get_person_image_bytes: checking path %s", fp)
                    if fp.exists() and fp.is_file():
                        logging.info("get_person_image_bytes: loaded image file %s", fp)
                        try:
                            return fp.read_bytes()
                        except Exception:
                            continue
                # also try files where parent_id might be part of filename
                for fp in folder.glob(f"*{c}*"):
                    logging.debug("get_person_image_bytes: checking glob match %s", fp)
                    if fp.is_file():
                        try:
                            b = fp.read_bytes()
                            if b:
                                logging.info("get_person_image_bytes: loaded image via glob %s", fp)
                                return b
                        except Exception:
                            continue
    except Exception:
        logging.exception("Filesystem image lookup failed for ParentId=%s", parent_id)

    # nothing found
    return None


# ---------- New route to serve employee image ----------
# We'll import send_file later where used; define route after app created.

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

# send_file is needed for Excel responses
from flask import send_file
try:
    # optional import; used for styling
    from openpyxl import load_workbook
    from openpyxl.styles import Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        # convert floats NaN handled above
        return x
    try:
        # fallback to string
        return str(x)
    except Exception:
        return None


_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False


# Helper: format seconds to HH:MM:SS
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return None
        # guard against floats and NaN
        s = int(float(seconds))
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return None


# Placeholder tokens (keep consistent with trend_runner expectations)
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', 'â€”', 'â€“', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


def _resolve_field_from_record(record: dict, candidate_tokens: list):
    """
    Search a single row `record` (dict) for likely columns listed in candidate_tokens.
    Return first non-placeholder value found (converted to Python scalar), else None.
    """
    if record is None:
        return None

    # 1) exact key matches (case-sensitive & common casing)
    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)

    # 2) case-insensitive contains match
    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)

    # 3) xml / value parsing fallback for CardNumber
    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue

    # 4) final fallback: first non-placeholder value
    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)

    return None


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Clean a dataframe for JSON output (convert NaN -> None, pandas types -> native, format datetimes).
    """
    if df is None or df.empty:
        return []
    df = df.copy()

    # remove duplicate suffix columns
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    # Date normalization
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    # Datetime columns to ISO strings
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # Replace NaN/inf -> None
    df = df.where(pd.notnull(df), None)

    # Convert records to safe Python types
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        # Typical fields
        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        # ----- Schema-aware fallback resolution -----
        if not emp_id:
            emp_tokens = ['Int1', 'Text12', 'EmployeeID', 'empid', 'id']
            resolved_emp = _resolve_field_from_record(r, emp_tokens)
            if resolved_emp is not None:
                try:
                    s = str(resolved_emp).strip()
                    # remove trailing .0 for floats
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    if _looks_like_guid(s):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = s
                        emp_id = s
                except Exception:
                    if _looks_like_guid(resolved_emp):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = resolved_emp
                        emp_id = resolved_emp

        # Prefer Credential.CardNumber / CHUID / Card as CardNumber when missing â€” reject GUIDs/placeholders
        if out.get('CardNumber') in (None, '', 'nan'):
            card_tokens = ['CardNumber', 'CHUID', 'Card', 'card_no', 'cardnum']
            resolved_card = _resolve_field_from_record(r, card_tokens)
            if resolved_card is not None:
                try:
                    cs = str(resolved_card).strip()
                    if _looks_like_guid(cs) or _is_placeholder_str(cs):
                        out['CardNumber'] = None
                    else:
                        out['CardNumber'] = cs
                except Exception:
                    out['CardNumber'] = None

        # final safety: ensure EmployeeID/CardNumber are not GUID-like tokens
        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None
        if 'CardNumber' in out and isinstance(out['CardNumber'], str) and _looks_like_guid(out['CardNumber']):
            out['CardNumber'] = None

        # If EmployeeName empty or looks like a GUID, prefer EmployeeID (human id) over GUIDs
        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            if emp_id not in (None, '', 'nan') and not _looks_like_guid(emp_id):
                out['EmployeeName'] = str(emp_id)
            else:
                out['EmployeeName'] = None

        cleaned.append(out)
    return cleaned

# ----- Helpers added to match commented (Pune) file functionality but multi-city-aware -----

def _replace_placeholder_strings(obj):
    """
    If obj is a DataFrame, replace known placeholder strings with None (NaN).
    If obj is a scalar/string, return None for placeholder strings else return obj.
    """
    if obj is None:
        return obj
    try:
        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
            for col in df.columns:
                try:
                    # Replace placeholder strings (case-insensitive)
                    df[col] = df[col].apply(lambda x: None if _is_placeholder_str(x) else x)
                except Exception:
                    continue
            return df
        else:
            # scalar
            return None if _is_placeholder_str(obj) else obj
    except Exception:
        return obj

def _normalize_id_local(v):
    """
    Normalize an identifier for robust matching/counting:
    - treat NaN/None/empty as None
    - strip and convert float-like integers to integer strings
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == '' or s.lower() == 'nan':
        return None
    try:
        if '.' in s:
            fv = float(s)
            if math.isfinite(fv) and fv.is_integer():
                s = str(int(fv))
    except Exception:
        pass
    return s





# def _find_swipe_files(outdir: Path, date_obj: Optional[date] = None, city_slug: Optional[str] = None):
#     """
#     Robust swipe-file discovery.
#     - Supports filenames like:
#         - swipes_YYYYMMDD.csv
#         - swipes_<city>_YYYYMMDD.csv
#         - <region>_swipes_YYYYMMDD.csv
#         - <region>_swipes_*.csv
#         - any file containing '_swipes_' or starting with 'swipes'
#         - fallback: any file that ends with _YYYYMMDD.csv
#     - If date_obj is None, returns recent files that look like swipe files.
#     - Returns list of Path objects sorted by mtime (newest first).
#     """
#     p = Path(outdir)
#     files_set = set()
#     try:
#         # Normalize city slug for matching
#         city_slug_l = (city_slug or "").lower().strip()

#         def add_glob(pattern):
#             try:
#                 for fp in p.glob(pattern):
#                     if fp.is_file():
#                         files_set.add(fp)
#             except Exception:
#                 pass

#         if date_obj is None:
#             # recent swipe-like files
#             add_glob("*_swipes_*.csv")        # region_swipes_YYYY or region_swipes_any.csv
#             add_glob("swipes_*.csv")         # swipes_YYYY or swipes_city_YYYY
#             add_glob("*swipes*.csv")         # permissive
#             add_glob("*_swipes.csv")
#             # also include any file that includes 'swipe' (some exporters use 'swipe' singular)
#             add_glob("*swipe*.csv")
#             # city-specific guesses
#             if city_slug_l:
#                 add_glob(f"*{city_slug_l}*_swipes_*.csv")
#                 add_glob(f"*{city_slug_l}*swipes*.csv")
#                 add_glob(f"*{city_slug_l}*.csv")
#         else:
#             target = date_obj.strftime("%Y%m%d")
#             # common patterns observed in pipeline
#             patterns = [
#                 f"*_{target}.csv",                 # anything ending _YYYYMMDD.csv
#                 f"*_swipes_{target}.csv",         # region_swipes_YYYYMMDD.csv or swipes_YYYYMMDD
#                 f"swipes*_{target}.csv",
#                 f"swipes_{target}.csv",
#                 f"*swipes*_{target}.csv",
#                 f"*{city_slug_l}*_{target}.csv",
#                 f"*{city_slug_l}*swipes*_{target}.csv",
#                 f"*{city_slug_l}_{target}.csv"
#             ]
#             for pat in patterns:
#                 add_glob(pat)

#         # final fallback: any CSV in folder that contains '_swipe' (case-insensitive)
#         try:
#             for fp in p.iterdir():
#                 if not fp.is_file():
#                     continue
#                 name = fp.name.lower()
#                 if ('_swipe' in name) or ('swipe' in name and name.endswith('.csv')):
#                     files_set.add(fp)
#         except Exception:
#             pass

#     except Exception:
#         logging.exception("Error while searching for swipe files in %s", outdir)

#     # sort by modification time (most recent first)
#     files = sorted(list(files_set), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
#     return files



def _find_swipe_files(outdir: Path, date_obj: Optional[date] = None, city_slug: Optional[str] = None, include_shifted: bool = True):
    """
    Robust swipe-file discovery.
    - If include_shifted is False, files with 'shift' in the filename (e.g. _shifted) are excluded.
    - Supports various filename patterns; if date_obj is None returns recent swipe-like files.
    - Returns list of Path objects sorted by mtime (newest first).
    """
    p = Path(outdir)
    files_set = set()
    try:
        city_slug_l = (city_slug or "").lower().strip()

        def add_glob(pattern):
            try:
                for fp in p.glob(pattern):
                    if fp.is_file():
                        files_set.add(fp)
            except Exception:
                pass

        if date_obj is None:
            add_glob("*_swipes_*.csv")
            add_glob("swipes_*.csv")
            add_glob("*swipes*.csv")
            add_glob("*_swipes.csv")
            add_glob("*swipe*.csv")
            if city_slug_l:
                add_glob(f"*{city_slug_l}*_swipes_*.csv")
                add_glob(f"*{city_slug_l}*swipes*.csv")
                add_glob(f"*{city_slug_l}*.csv")
        else:
            target = date_obj.strftime("%Y%m%d")
            patterns = [
                f"*_{target}.csv",
                f"*_swipes_{target}.csv",
                f"swipes*_{target}.csv",
                f"swipes_{target}.csv",
                f"*swipes*_{target}.csv",
                f"*{city_slug_l}*_{target}.csv",
                f"*{city_slug_l}*swipes*_{target}.csv",
                f"*{city_slug_l}_{target}.csv"
            ]
            for pat in patterns:
                add_glob(pat)

        # fallback: any CSV containing 'swipe'/'swipes' in the name
        try:
            for fp in p.iterdir():
                if not fp.is_file():
                    continue
                name = fp.name.lower()
                if ('_swipe' in name) or ('swipe' in name and name.endswith('.csv')):
                    files_set.add(fp)
        except Exception:
            pass

    except Exception:
        logging.exception("Error while searching for swipe files in %s", outdir)

    # Filter out shifted files if requested
    files = sorted(list(files_set), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
    if not include_shifted:
        files = [f for f in files if 'shift' not in f.name.lower()]

    return files


# -----------------------
# Routes
# -----------------------




@app.route('/')
def root():
    return "Trend Analysis API â€” Multi-city"

@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.get_json(force=True) or {}
        else:
            try:
                params = request.form.to_dict() or {}
            except Exception:
                params = {}

    date_str = (params.get('date') or params.get('Date') or '').strip() or None
    start_str = (params.get('start') or params.get('Start') or '').strip() or None
    end_str = (params.get('end') or params.get('End') or '').strip() or None

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    regions_param = params.get('regions') or params.get('region') or ''
    if regions_param:
        regions = [r.strip().lower() for r in re.split(r'[;,|]', str(regions_param)) if r.strip()]
    else:
        try:
            regions = [k.lower() for k in list(REGION_CONFIG.keys())]
        except Exception:
            regions = ['apac']

    valid_regions = []
    for r in regions:
        if r in (REGION_CONFIG or {}):
            valid_regions.append(r)
        else:
            logging.debug("Requested region '%s' not in REGION_CONFIG - skipping", r)
    if not valid_regions:
        valid_regions = [k.lower() for k in REGION_CONFIG.keys()] if REGION_CONFIG else ['apac']
    params['_regions_to_run'] = valid_regions

    city_param = params.get('city') or params.get('site') or params.get('site_name') or None
    city_slug = _slug_city(city_param) if city_param else None
    params['_city'] = city_slug

    combined_rows = []
    files = []

    # ---------------------------
    # Run trend for each requested date
    # ---------------------------
    for d in dates:
        try:
            if run_trend_for_date is None:
                raise RuntimeError("run_trend_for_date helper not available in trend_runner")
            try:
                df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug)
            except TypeError:
                try:
                    df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
                except Exception:
                    # Last-resort: try duration_report fallback if available
                    try:
                        from duration_report import run_for_date as _dr_run_for_date
                        region_results = _dr_run_for_date(d, valid_regions, str(DEFAULT_OUTDIR), city_param)
                        combined_list = []
                        for rkey, res in (region_results or {}).items():
                            try:
                                df_dur = res.get('durations')
                                if df_dur is not None and not df_dur.empty:
                                    combined_list.append(df_dur)
                            except Exception:
                                continue
                        df = pd.concat(combined_list, ignore_index=True) if combined_list else pd.DataFrame()
                    except Exception:
                        raise
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_{city_slug}_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        if df is None or (hasattr(df, 'empty') and df.empty):
            continue

        try:
            df = _replace_placeholder_strings(df)
        except Exception:
            pass

        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        combined_rows.append(df)

    # *** Important: combine after loop to avoid UnboundLocalError and extra repeated concat inside loop ***
    try:
        combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()
    except Exception:
        combined_df = pd.DataFrame()

    try:
        if not combined_df.empty:
            if 'person_uid' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['person_uid'].dropna().astype(str).nunique())
            elif 'EmployeeID' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['EmployeeID'].dropna().astype(str).nunique())
            else:
                raw_unique_person_uids = int(len(combined_df))
        else:
            raw_unique_person_uids = 0
    except Exception:
        raw_unique_person_uids = int(len(combined_df)) if combined_df is not None else 0

    try:
        if not combined_df.empty and 'IsFlagged' in combined_df.columns:
            flagged_df = combined_df[combined_df['IsFlagged'] == True].copy()
        else:
            flagged_df = pd.DataFrame()
    except Exception:
        flagged_df = pd.DataFrame()

    try:
        analysis_count = int(raw_unique_person_uids)
    except Exception:
        analysis_count = int(len(combined_df)) if combined_df is not None else 0

    try:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = float((flagged_count / analysis_count * 100.0) if analysis_count and analysis_count > 0 else 0.0)
    except Exception:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = 0.0

    try:
        # If we have flagged rows, return ALL flagged rows (strict)
        if flagged_df is not None and not flagged_df.empty:
            sample_source = flagged_df
            # return exactly flagged_count rows (no hidden head(10) truncation)
            samples = _clean_sample_df(sample_source, max_rows=int(len(flagged_df)))
        else:
            # old behaviour: show a small sample of combined_df
            sample_source = combined_df
            samples = _clean_sample_df(sample_source.head(10), max_rows=10) if sample_source is not None and not sample_source.empty else []
    except Exception:
        samples = []

    # -----------------------------
    # NEW: Enrich the sample rows with EmployeeEmail and imageUrl using trend_runner helper
    # -----------------------------
    try:
        if isinstance(samples, list) and samples:
            try:
                base = (request.url_root or request.host_url).rstrip('/')
            except Exception:
                base = ''
            try:
                # build a small dataframe and call trend_runner enrichment helper
                tmp_df = pd.DataFrame(samples)
                # pass a fully-qualified image endpoint template so frontend img src works directly
                template = "/employee/{}/image"
                if base:
                    # result will be like: http://host:port/employee/<id>/image
                    template = f"{base}/employee/{{}}/image"
                enriched = _enrich_with_personnel_info(tmp_df, image_endpoint_template=template)
                # ensure Python native types and safe values via cleaning function
                enriched_clean = _clean_sample_df(enriched, max_rows=len(enriched))
                samples = enriched_clean
            except Exception:
                logging.exception("Failed to enrich /run sample with personnel info (non-fatal).")
    except Exception:
        logging.exception("Sample enrichment failed (non-fatal).")

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "aggregated_rows_total_raw": int(len(combined_df)),
        "aggregated_unique_persons": int(analysis_count),
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "flagged_rate_percent": float(flagged_rate_pct),
        "files": files,
         "sample": (samples if isinstance(samples, list) else samples),
       # "sample": (samples[:10] if isinstance(samples, list) else samples),
        "reasons_count": {},
        "risk_counts": {},
        #"flagged_persons": (samples if samples else []),
         "flagged_persons": (samples if samples else []),
        "_raw_unique_person_uids": int(raw_unique_person_uids),
        "regions_run": params.get('_regions_to_run', []),
        "city_used": city_slug
    }

    return jsonify(resp)



@app.route('/latest', methods=['GET'])
def latest_results():
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]

    start_date_iso = None
    end_date_iso = None
    try:
        m = re.search(r'(\d{8})', latest.name)
        if m:
            ymd = m.group(1)
            dt = datetime.strptime(ymd, "%Y%m%d").date()
            start_date_iso = dt.isoformat()
            end_date_iso = dt.isoformat()
    except Exception:
        start_date_iso = None
        end_date_iso = None

    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)

    df = _replace_placeholder_strings(df)

    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in df.columns), None)

    def _norm_val_for_latest(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                fv = float(s)
                if math.isfinite(fv) and fv.is_integer():
                    s = str(int(fv))
        except Exception:
            pass
        return s

    if id_col is None:
        unique_persons = int(len(df))
    else:
        ids_series = df[id_col].apply(_norm_val_for_latest) if id_col in df.columns else pd.Series([None]*len(df))
        if id_col != 'person_uid' and 'person_uid' in df.columns:
            ids_series = ids_series.fillna(df['person_uid'].astype(str).replace('nan','').replace('None',''))
        unique_persons = int(len(set([x for x in ids_series.unique() if x])))

    # build initial sample (list of dicts)
    sample = _clean_sample_df(df, max_rows=5)  # returns list




    # --- Enrich sample rows with EmployeeEmail and imageUrl (best-effort) ---
    try:
        if isinstance(sample, list) and sample:
            # compute base once
            try:
                base = (request.url_root or request.host_url).rstrip('/')
            except Exception:
                base = ''

            for s in sample:
                # ensure keys exist (consistent shape)
                s.setdefault('EmployeeEmail', None)
                s.setdefault('imageUrl', None)
                s.setdefault('HasImage', False)

                # pick a lookup token (EmployeeID / person_uid / EmployeeName / EmployeeIdentity)
                lookup_token = (
                    s.get('EmployeeID')
                    or s.get('person_uid')
                    or s.get('EmployeeName')
                    or s.get('EmployeeIdentity')
                )

                pi = {}
                if lookup_token:
                    try:
                        pi = get_personnel_info(lookup_token) or {}
                    except Exception:
                        pi = {}

                # Prefer personnel DB email if available
                if pi:
                    email = pi.get('EmailAddress') or pi.get('EmployeeEmail') or pi.get('Email') or None
                    if email:
                        s['EmployeeEmail'] = email

                    # prefer ObjectID / GUID for images
                    objid = pi.get('ObjectID') or pi.get('GUID') or None
                    if objid:
                        # provide relative image path (frontend resolves with API_BASE)
                        s['imageUrl'] = f"/employee/{objid}/image"
                        try:
                            b = get_person_image_bytes(objid)
                            s['HasImage'] = True if b else False
                        except Exception:
                            s['HasImage'] = False

                # fallback: look up email from the CSV rows read (df) if still missing
                if not s.get('EmployeeEmail') and isinstance(df, pd.DataFrame):
                    try:
                        match_mask = pd.Series(False, index=df.index)
                        if s.get('person_uid') and 'person_uid' in df.columns:
                            match_mask |= df['person_uid'].astype(str).str.strip() == str(s.get('person_uid')).strip()
                        if s.get('EmployeeID') and 'EmployeeID' in df.columns:
                            match_mask |= df['EmployeeID'].astype(str).str.strip() == str(s.get('EmployeeID')).strip()

                        if match_mask.any():
                            idx = df[match_mask].index[0]
                            for col in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                                if col in df.columns:
                                    val = df.at[idx, col]
                                    if val not in (None, '', 'nan'):
                                        s['EmployeeEmail'] = val
                                        break
                    except Exception:
                        pass

                # Ensure we at least provide a consistent image route (use EmployeeID/person_uid if no ObjectID)
                if not s.get('imageUrl'):
                    empid = s.get('EmployeeID') or s.get('person_uid')
                    if empid:
                        s['imageUrl'] = f"/employee/{empid}/image"
                        # don't try to check bytes here (avoid extra DB hit) â€” HasImage remains False if unknown

    except Exception:
        # if enrichment fails, continue (sample remains as-is)
        pass


    resp = {
        
        "file": latest.name,
        "rows_raw": int(len(df)),
        "rows": unique_persons,
        "sample": sample,
        "start_date": start_date_iso,
        "end_date": end_date_iso,
        "city": city_slug
    }
    return jsonify(resp)




#new endpoint



# @app.route('/record', methods=['GET'])
# def record():
#     try:
#         # --- BEGIN existing record() logic ---
#         from pathlib import Path
#         import pandas as pd
#         import math
#         import re
#         from datetime import datetime, date
#         try:
#             q = request.args.get('employee_id') or request.args.get('person_uid')
#         except Exception:
#             q = None
#         include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
#         city_param = request.args.get('city') or request.args.get('site') or 'pune'

#         # pick outdir consistently
#         try:
#             base_out = Path(DEFAULT_OUTDIR)
#         except Exception:
#             try:
#                 base_out = Path(OUTDIR)
#             except Exception:
#                 base_out = Path.cwd()

#         # helper safe wrappers (use existing ones if present)
#         def _safe_read(fp, **kwargs):
#             try:
#                 if '_safe_read_csv' in globals():
#                     return _safe_read_csv(fp)
#                 return pd.read_csv(fp, **kwargs)
#             except Exception:
#                 try:
#                     return pd.read_csv(fp, dtype=str, **{k: v for k, v in kwargs.items() if k != 'parse_dates'})
#                 except Exception:
#                     return pd.DataFrame()

#         def _to_python_scalar(v):
#             if pd.isna(v):
#                 return None
#             try:
#                 return v.item() if hasattr(v, 'item') else v
#             except Exception:
#                 return v

#         # 1) find trend CSVs (city-specific first)
#         def _slug(s):
#             return re.sub(r'[^a-z0-9]+', '_', str(s or '').strip().lower()).strip('_')

#         city_slug = _slug(city_param)
#         trend_glob = list(base_out.glob(f"trend_{city_slug}_*.csv"))
#         if not trend_glob:
#             trend_glob = list(base_out.glob("trend_*.csv"))
#         trend_glob = sorted(trend_glob, reverse=True)

#         df_list = []
#         for fp in trend_glob:
#             try:
#                 tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
#             except Exception:
#                 try:
#                     tmp = pd.read_csv(fp, dtype=str)
#                     if 'Date' in tmp.columns:
#                         tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
#                 except Exception:
#                     continue
#             df_list.append(tmp)
#         if df_list:
#             trends_df = pd.concat(df_list, ignore_index=True)
#             try:
#                 trends_df = _replace_placeholder_strings(trends_df)
#             except Exception:
#                 pass
#         else:
#             trends_df = pd.DataFrame()

#         # if no query param, return a small sample of trend rows (if any)
#         if q is None:
#             try:
#                 if not trends_df.empty and '_clean_sample_df' in globals():
#                     cleaned = _clean_sample_df(trends_df, max_rows=10)
#                 elif not trends_df.empty:
#                     cleaned = trends_df.head(10).to_dict(orient='records')
#                 else:
#                     cleaned = []
#             except Exception:
#                 cleaned = []
#             return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

#         q_str = str(q).strip()

#         # helper to normalise series values to comparable strings/numerics
#         def normalize_series(s):
#             if s is None:
#                 return pd.Series([''] * (len(trends_df) if not trends_df.empty else 0))
#             s = s.fillna('').astype(str).str.strip()
#             def _norm_val(v):
#                 if not v:
#                     return ''
#                 try:
#                     if '.' in v:
#                         fv = float(v)
#                         if math.isfinite(fv) and fv.is_integer():
#                             return str(int(fv))
#                 except Exception:
#                     pass
#                 return v
#             return s.map(_norm_val)

#         # find matching rows in trends_df
#         found_mask = pd.Series(False, index=trends_df.index) if not trends_df.empty else pd.Series(dtype=bool)
#         candidates_cols = ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12', 'EmployeeName')
#         for c in candidates_cols:
#             if c in trends_df.columns:
#                 try:
#                     ser = normalize_series(trends_df[c])
#                     found_mask = found_mask | (ser == q_str)
#                 except Exception:
#                     pass

#         # numeric fallback
#         if (found_mask is None) or (not found_mask.any() if len(found_mask) else True):
#             try:
#                 q_numeric = float(q_str)
#                 for c in ('EmployeeID', 'Int1'):
#                     if c in trends_df.columns:
#                         try:
#                             numser = pd.to_numeric(trends_df[c], errors='coerce')
#                             found_mask = found_mask | (numser == q_numeric)
#                         except Exception:
#                             pass
#             except Exception:
#                 pass

#         matched = trends_df[found_mask].copy() if not trends_df.empty else pd.DataFrame()
#         if matched.empty:
#             cleaned_matched = []
#         else:
#             try:
#                 cleaned_matched = _clean_sample_df(matched, max_rows=len(matched)) if '_clean_sample_df' in globals() else matched.to_dict(orient='records')
#             except Exception:
#                 cleaned_matched = matched.to_dict(orient='records')

#         # enrich matched rows where possible
#         try:
#             if cleaned_matched and '_enrich_with_personnel_info' in globals():
#                 agg_df = pd.DataFrame(cleaned_matched)
#                 agg_df = _enrich_with_personnel_info(agg_df, image_endpoint_template="/employee/{}/image")
#                 cleaned_matched = agg_df.to_dict(orient='records')
#         except Exception:
#             pass

#         # build list of dates to scan for swipe files (from matched rows)
#         dates_to_scan = set()
#         try:
#             for _, r in (matched.iterrows() if not matched.empty else []):
#                 try:
#                     if 'Date' in r and pd.notna(r['Date']):
#                         try:
#                             d = pd.to_datetime(r['Date']).date()
#                             dates_to_scan.add(d)
#                         except Exception:
#                             pass
#                     for col in ('FirstSwipe','LastSwipe'):
#                         if col in r and pd.notna(r[col]):
#                             try:
#                                 d = pd.to_datetime(r[col]).date()
#                                 dates_to_scan.add(d)
#                             except Exception:
#                                 pass
#                 except Exception:
#                     continue
#         except Exception:
#             pass
#         if not dates_to_scan:
#             dates_to_scan = {None}  # indicates scan all swipes files

#         # ---------- helper: find swipe files for a date (robust) ----------
#         def _find_swipes_for_date(date_obj=None):
#             try:
#                 include_shifted = True
#                 try:
#                     if city_slug and str(city_slug).strip().lower() == 'pune':
#                         include_shifted = False
#                 except Exception:
#                     include_shifted = True

#                 if '_find_swipe_files' in globals() and callable(globals().get('_find_swipe_files')):
#                     try:
#                         cand = _find_swipe_files(str(base_out), date_obj=date_obj, city_slug=city_slug if city_slug else None, include_shifted=include_shifted)
#                         if cand:
#                             return cand
#                     except Exception:
#                         logging.exception("_find_swipe_files helper failed; falling back to glob search.")

#                 files = []
#                 if date_obj is None:
#                     files = list(base_out.glob("swipes_*_*.csv")) + list(base_out.glob("swipes_*.csv")) + list(base_out.glob("*swipe*.csv"))
#                 else:
#                     ymd = date_obj.strftime('%Y-%m-%d')
#                     ymd2 = date_obj.strftime('%Y%m%d')
#                     cand1 = [p for p in base_out.glob("swipes_*_*.csv") if (ymd in p.name or ymd2 in p.name)]
#                     cand2 = [p for p in base_out.glob("swipes_*.csv") if (ymd in p.name or ymd2 in p.name)]
#                     files = cand1 + cand2
#                 files = sorted(list({p for p in files if p.exists()}), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
#                 if not include_shifted:
#                     files = [f for f in files if 'shift' not in f.name.lower()]
#                 return files
#             except Exception:
#                 logging.exception("Error while searching for swipe files for date=%s city=%s", date_obj, city_slug)
#                 return []


#         # ---------- scan swipe files for the target person (dates_to_scan computed earlier) ----------
#         raw_files_set = set()
#         raw_swipes_out = []
#         seen_keys = set()

#         def _append_row(out_row, source_name):
#             key = (
#                 str(out_row.get('Date') or ''),
#                 str(out_row.get('Time') or ''),
#                 str(out_row.get('Door') or '').strip(),
#                 str(out_row.get('Direction') or '').strip(),
#                 str(out_row.get('CardNumber') or out_row.get('Card') or '').strip()
#             )
#             if key in seen_keys:
#                 return
#             seen_keys.add(key)
#             out_row['_source'] = source_name
#             raw_swipes_out.append(out_row)

#         for d in dates_to_scan:
#             swipe_candidates = _find_swipes_for_date(d)
#             if d is not None and not swipe_candidates:
#                 swipe_candidates = _find_swipes_for_date(None)

#             for fp in swipe_candidates:
#                 raw_files_set.add(fp.name)
#                 try:
#                     sdf = _safe_read(fp, parse_dates=['LocaleMessageTime'])
#                 except Exception:
#                     try:
#                         sdf = _safe_read(fp)
#                     except Exception:
#                         continue
#                 if sdf is None or sdf.empty:
#                     continue

#                 # minimal column-normalization & masking (reuse original code patterns)
#                 cols_lower = {c.lower(): c for c in sdf.columns}
#                 tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
#                 emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
#                 name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
#                 card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
#                 door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
#                 dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
#                 note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
#                 person_uid_col = cols_lower.get('person_uid')

#                 try:
#                     mask = pd.Series(False, index=sdf.index)
#                 except Exception:
#                     mask = pd.Series([False] * len(sdf))

#                 try:
#                     if person_uid_col and person_uid_col in sdf.columns:
#                         mask = mask | (sdf[person_uid_col].astype(str).str.strip() == q_str)
#                 except Exception:
#                     pass
#                 try:
#                     if emp_col and emp_col in sdf.columns:
#                         mask = mask | (sdf[emp_col].astype(str).str.strip() == q_str)
#                 except Exception:
#                     pass

#                 if (not mask.any()) and emp_col and emp_col in sdf.columns:
#                     try:
#                         q_numeric = float(q_str)
#                         emp_numeric = pd.to_numeric(sdf[emp_col], errors='coerce')
#                         mask = mask | (emp_numeric == q_numeric)
#                     except Exception:
#                         pass

#                 if (not mask.any()) and name_col and name_col in sdf.columns:
#                     try:
#                         mask = mask | (sdf[name_col].astype(str).str.strip().str.lower() == q_str.lower())
#                     except Exception:
#                         pass

#                 if not mask.any():
#                     continue

#                 filtered = sdf[mask].copy()
#                 if filtered.empty:
#                     continue

#                 # ensure LocaleMessageTime parsed
#                 if tcol and tcol in filtered.columns:
#                     try:
#                         filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
#                     except Exception:
#                         pass
#                 else:
#                     for cand in ('MessageUTC','MessageTime','Timestamp','timestamp','Date'):
#                         if cand in filtered.columns:
#                             try:
#                                 filtered['LocaleMessageTime'] = pd.to_datetime(filtered[cand], errors='coerce')
#                                 tcol = 'LocaleMessageTime'
#                                 break
#                             except Exception:
#                                 pass

#                 if tcol and tcol in filtered.columns:
#                     try:
#                         filtered = filtered.sort_values(by=tcol)
#                     except Exception:
#                         pass

#                 if tcol and tcol in filtered.columns:
#                     try:
#                         # ensure sorted by timestamp
#                         filtered = filtered.sort_values(by=tcol)
#                     except Exception:
#                         pass

#                     # compute swipe gaps (seconds) between consecutive swipes
#                     try:
#                         filtered['_prev_ts'] = filtered[tcol].shift(1)
#                         filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
#                         # reset gap at day boundary or when previous is NaT
#                         try:
#                             cur_dates = filtered[tcol].dt.date
#                             prev_dates = cur_dates.shift(1)
#                             day_boundary_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
#                             filtered.loc[day_boundary_mask, '_swipe_gap_seconds'] = 0.0
#                         except Exception:
#                             # date boundary logic failed â€” keep computed gaps
#                             pass
#                     except Exception:
#                         filtered['_swipe_gap_seconds'] = 0.0
#                 else:
#                     # no timestamp column -> defaults
#                     filtered['_swipe_gap_seconds'] = 0.0

#                 # convert each filtered row to output dict and append
#                 for _, r in filtered.iterrows():
#                     out = {}
#                     out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in filtered.columns else (matched.iloc[0].get('EmployeeName') if not matched.empty else q_str)
#                     # EmployeeID
#                     emp_val = None
#                     if emp_col and emp_col in filtered.columns:
#                         emp_val = _to_python_scalar(r.get(emp_col))
#                     else:
#                         for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
#                             cl = cols_lower.get(cand.lower())
#                             if cl and cl in filtered.columns:
#                                 emp_val = _to_python_scalar(r.get(cl))
#                                 if emp_val not in (None, '', 'nan'):
#                                     break
#                         if emp_val in (None, '', 'nan'):
#                             emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
#                     try:
#                         if emp_val is not None:
#                             s = str(emp_val).strip()
#                             if '.' in s:
#                                 try:
#                                     f = float(s)
#                                     if math.isfinite(f) and f.is_integer():
#                                         s = str(int(f))
#                                 except Exception:
#                                     pass
#                             emp_val = s
#                     except Exception:
#                         pass
#                     out['EmployeeID'] = emp_val

#                     # CardNumber / Card
#                     card_val = None
#                     if card_col and card_col in filtered.columns:
#                         card_val = _to_python_scalar(r.get(card_col))
#                     else:
#                         for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
#                             cl = cols_lower.get(cand.lower())
#                             if cl and cl in filtered.columns:
#                                 card_val = _to_python_scalar(r.get(cl))
#                                 if card_val not in (None, '', 'nan'):
#                                     break
#                         if card_val in (None, '', 'nan'):
#                             card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
#                     try:
#                         if card_val is not None:
#                             card_val = str(card_val).strip()
#                     except Exception:
#                         pass
#                     out['CardNumber'] = card_val
#                     out['Card'] = card_val

#                     # timestamps -> Date / Time / LocaleMessageTime
#                     if tcol and tcol in filtered.columns:
#                         ts = r.get(tcol)
#                         try:
#                             ts_py = pd.to_datetime(ts)
#                             out['Date'] = ts_py.date().isoformat()
#                             out['Time'] = ts_py.time().isoformat()
#                             out['LocaleMessageTime'] = ts_py.isoformat()
#                         except Exception:
#                             txt = str(r.get(tcol))
#                             out['Date'] = txt[:10]
#                             out['Time'] = txt[11:19] if len(txt) >= 19 else txt
#                             out['LocaleMessageTime'] = txt
#                     else:
#                         out['Date'] = None
#                         out['Time'] = None
#                         out['LocaleMessageTime'] = None

#                     out['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds') or 0.0)
#                     out['SwipeGap'] = format_seconds_to_hms(out['SwipeGapSeconds'])

#                     out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
#                     out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else (_to_python_scalar(r.get('Direction')) if 'Direction' in r else None)
#                     out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
#                     try:
#                         out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else (map_door_to_zone(out['Door'], out['Direction']) if 'map_door_to_zone' in globals() else None)
#                     except Exception:
#                         out['Zone'] = None

#                     out['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
#                     out['_source_file'] = fp.name
#                     _append_row(out, fp.name)

#         raw_swipe_files = sorted(list(raw_files_set))

#         return jsonify({
#             "aggregated_rows": cleaned_matched,
#             "raw_swipe_files": raw_swipe_files,
#             "raw_swipes": raw_swipes_out
#         }), 200
#         # --- END existing record() logic ---
#     except Exception as e:
#         logging.exception("Unhandled exception in /record endpoint")
#         # Return JSON so frontend sees useful error instead of Flask returning None
#         return jsonify({"error": "internal server error in /record", "details": str(e)}), 500

@app.route('/record', methods=['GET'])
def record():
    try:
        # --- BEGIN existing record() logic ---
        from pathlib import Path
        import pandas as pd
        import math
        import re
        from datetime import datetime, date
        try:
            q = request.args.get('employee_id') or request.args.get('person_uid')
        except Exception:
            q = None
        include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
        city_param = request.args.get('city') or request.args.get('site') or 'pune'

        # pick outdir consistently
        try:
            base_out = Path(DEFAULT_OUTDIR)
        except Exception:
            try:
                base_out = Path(OUTDIR)
            except Exception:
                base_out = Path.cwd()

        # helper safe wrappers (use existing ones if present)
        def _safe_read(fp, **kwargs):
            try:
                if '_safe_read_csv' in globals():
                    return _safe_read_csv(fp)
                return pd.read_csv(fp, **kwargs)
            except Exception:
                try:
                    return pd.read_csv(fp, dtype=str, **{k: v for k, v in kwargs.items() if k != 'parse_dates'})
                except Exception:
                    return pd.DataFrame()

        def _to_python_scalar(v):
            if pd.isna(v):
                return None
            try:
                return v.item() if hasattr(v, 'item') else v
            except Exception:
                return v

        # 1) find trend CSVs (city-specific first)
        def _slug(s):
            return re.sub(r'[^a-z0-9]+', '_', str(s or '').strip().lower()).strip('_')

        city_slug = _slug(city_param)
        trend_glob = list(base_out.glob(f"trend_{city_slug}_*.csv"))
        if not trend_glob:
            trend_glob = list(base_out.glob("trend_*.csv"))
        trend_glob = sorted(trend_glob, reverse=True)

        df_list = []
        for fp in trend_glob:
            try:
                tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
            except Exception:
                try:
                    tmp = pd.read_csv(fp, dtype=str)
                    if 'Date' in tmp.columns:
                        tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                except Exception:
                    continue
            df_list.append(tmp)
        if df_list:
            trends_df = pd.concat(df_list, ignore_index=True)
            try:
                trends_df = _replace_placeholder_strings(trends_df)
            except Exception:
                pass
        else:
            trends_df = pd.DataFrame()

        # if no query param, return a small sample of trend rows (if any)
        if q is None:
            try:
                if not trends_df.empty and '_clean_sample_df' in globals():
                    cleaned = _clean_sample_df(trends_df, max_rows=10)
                elif not trends_df.empty:
                    cleaned = trends_df.head(10).to_dict(orient='records')
                else:
                    cleaned = []
            except Exception:
                cleaned = []
            return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

        q_str = str(q).strip()

        # helper to normalise series values to comparable strings/numerics
        def normalize_series(s):
            if s is None:
                return pd.Series([''] * (len(trends_df) if not trends_df.empty else 0))
            s = s.fillna('').astype(str).str.strip()
            def _norm_val(v):
                if not v:
                    return ''
                try:
                    if '.' in v:
                        fv = float(v)
                        if math.isfinite(fv) and fv.is_integer():
                            return str(int(fv))
                except Exception:
                    pass
                return v
            return s.map(_norm_val)

        # find matching rows in trends_df
        found_mask = pd.Series(False, index=trends_df.index) if not trends_df.empty else pd.Series(dtype=bool)
        candidates_cols = ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12', 'EmployeeName')
        for c in candidates_cols:
            if c in trends_df.columns:
                try:
                    ser = normalize_series(trends_df[c])
                    found_mask = found_mask | (ser == q_str)
                except Exception:
                    pass

        # numeric fallback
        if (found_mask is None) or (not found_mask.any() if len(found_mask) else True):
            try:
                q_numeric = float(q_str)
                for c in ('EmployeeID', 'Int1'):
                    if c in trends_df.columns:
                        try:
                            numser = pd.to_numeric(trends_df[c], errors='coerce')
                            found_mask = found_mask | (numser == q_numeric)
                        except Exception:
                            pass
            except Exception:
                pass

        matched = trends_df[found_mask].copy() if not trends_df.empty else pd.DataFrame()
        if matched.empty:
            cleaned_matched = []
        else:
            try:
                cleaned_matched = _clean_sample_df(matched, max_rows=len(matched)) if '_clean_sample_df' in globals() else matched.to_dict(orient='records')
            except Exception:
                cleaned_matched = matched.to_dict(orient='records')

        # enrich matched rows where possible
        try:
            if cleaned_matched and '_enrich_with_personnel_info' in globals():
                agg_df = pd.DataFrame(cleaned_matched)
                agg_df = _enrich_with_personnel_info(agg_df, image_endpoint_template="/employee/{}/image")
                cleaned_matched = agg_df.to_dict(orient='records')
        except Exception:
            pass

        # build list of dates to scan for swipe files (from matched rows)
        dates_to_scan = set()
        try:
            for _, r in (matched.iterrows() if not matched.empty else []):
                try:
                    if 'Date' in r and pd.notna(r['Date']):
                        try:
                            d = pd.to_datetime(r['Date']).date()
                            dates_to_scan.add(d)
                        except Exception:
                            pass
                    for col in ('FirstSwipe','LastSwipe'):
                        if col in r and pd.notna(r[col]):
                            try:
                                d = pd.to_datetime(r[col]).date()
                                dates_to_scan.add(d)
                            except Exception:
                                pass
                except Exception:
                    continue
        except Exception:
            pass
        if not dates_to_scan:
            dates_to_scan = {None}  # indicates scan all swipes files

        # ---------- helper: find swipe files for a date (robust) ----------
        def _find_swipes_for_date(date_obj=None):
            try:
                include_shifted = True
                try:
                    if city_slug and str(city_slug).strip().lower() == 'pune':
                        include_shifted = False
                except Exception:
                    include_shifted = True

                if '_find_swipe_files' in globals() and callable(globals().get('_find_swipe_files')):
                    try:
                        cand = _find_swipe_files(str(base_out), date_obj=date_obj, city_slug=city_slug if city_slug else None, include_shifted=include_shifted)
                        if cand:
                            return cand
                    except Exception:
                        logging.exception("_find_swipe_files helper failed; falling back to glob search.")

                files = []
                if date_obj is None:
                    files = list(base_out.glob("swipes_*_*.csv")) + list(base_out.glob("swipes_*.csv")) + list(base_out.glob("*swipe*.csv"))
                else:
                    ymd = date_obj.strftime('%Y-%m-%d')
                    ymd2 = date_obj.strftime('%Y%m%d')
                    cand1 = [p for p in base_out.glob("swipes_*_*.csv") if (ymd in p.name or ymd2 in p.name)]
                    cand2 = [p for p in base_out.glob("swipes_*.csv") if (ymd in p.name or ymd2 in p.name)]
                    files = cand1 + cand2
                files = sorted(list({p for p in files if p.exists()}), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
                if not include_shifted:
                    files = [f for f in files if 'shift' not in f.name.lower()]
                return files
            except Exception:
                logging.exception("Error while searching for swipe files for date=%s city=%s", date_obj, city_slug)
                return []


        # ---------- scan swipe files for the target person (dates_to_scan computed earlier) ----------
        raw_files_set = set()
        raw_swipes_out = []
        seen_keys = set()

        def _append_row_for_evidence(out_row, source_name):
            # avoid exact duplicate rows from same file
            key = (
                str(out_row.get('LocaleMessageTime') or ''),
                str(out_row.get('DateOnly') or ''),
                str(out_row.get('Swipe_Time') or ''),
                str(out_row.get('Door') or '').strip(),
                str(out_row.get('Direction') or '').strip(),
                str(out_row.get('CardNumber') or '').strip()
            )
            if key in seen_keys:
                return False
            seen_keys.add(key)
            out_row['_source'] = source_name
            raw_swipes_out.append(out_row)
            return True

        # helper to format datetime to requested display formats
        def _format_time_fields(ts):
            # ts is a pandas Timestamp or datetime or None
            if ts is None or (isinstance(ts, float) and math.isnan(ts)):
                return (None, None, None)
            try:
                dt = pd.to_datetime(ts)
            except Exception:
                return (None, None, None)
            try:
                locale_iso = dt.isoformat()
            except Exception:
                locale_iso = str(dt)
            try:
                date_only = dt.strftime("%d-%b-%y")  # e.g. 17-Nov-25
            except Exception:
                try:
                    date_only = dt.date().isoformat()
                except Exception:
                    date_only = None
            try:
                # 12-hour time with AM/PM, strip leading zero
                swipe_time = dt.strftime("%I:%M:%S %p").lstrip("0")
            except Exception:
                swipe_time = None
            return (locale_iso, date_only, swipe_time)

        # loop over each date we want to scan (these are violation dates if matched rows existed)
        for d in dates_to_scan:
            swipe_candidates = _find_swipes_for_date(d)
            if d is not None and not swipe_candidates:
                # fallback to scanning all swipe files if none found for the exact date pattern
                swipe_candidates = _find_swipes_for_date(None)

            for fp in swipe_candidates:
                try:
                    sdf = _safe_read(fp, parse_dates=['LocaleMessageTime'])
                except Exception:
                    try:
                        sdf = _safe_read(fp)
                    except Exception:
                        continue
                if sdf is None or sdf.empty:
                    continue

                # minimal column-normalization for detection (case-insensitive)
                cols_lower = {c.lower(): c for c in sdf.columns}
                tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or \
                       cols_lower.get('timestamp') or cols_lower.get('time') or None
                emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or \
                          cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or \
                           cols_lower.get('employee_name') or None
                card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or \
                           cols_lower.get('chuid') or cols_lower.get('value') or None
                door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
                dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
                admit_cols = [c for c in ('admitcode','admit','admit_code','admit_type','admitstatus') if c in cols_lower]
                admit_col = cols_lower.get(admit_cols[0]) if admit_cols else None
                personnel_col = cols_lower.get('personneltype') or cols_lower.get('personneltypename') or None
                location_col = cols_lower.get('partitionname2') or cols_lower.get('location') or cols_lower.get('partitionname') or None
                person_uid_col = cols_lower.get('person_uid')

                # build boolean mask for rows that match the query identifier q_str
                try:
                    mask = pd.Series(False, index=sdf.index)
                except Exception:
                    mask = pd.Series([False] * len(sdf))

                try:
                    if person_uid_col and person_uid_col in sdf.columns:
                        mask = mask | (sdf[person_uid_col].astype(str).str.strip() == q_str)
                except Exception:
                    pass
                try:
                    if emp_col and emp_col in sdf.columns:
                        mask = mask | (sdf[emp_col].astype(str).str.strip() == q_str)
                except Exception:
                    pass
                # numeric fallback for employee id
                if (not mask.any()) and emp_col and emp_col in sdf.columns:
                    try:
                        q_numeric = float(q_str)
                        emp_numeric = pd.to_numeric(sdf[emp_col], errors='coerce')
                        mask = mask | (emp_numeric == q_numeric)
                    except Exception:
                        pass
                # name fallback
                if (not mask.any()) and name_col and name_col in sdf.columns:
                    try:
                        mask = mask | (sdf[name_col].astype(str).str.strip().str.lower() == q_str.lower())
                    except Exception:
                        pass

                if not mask.any():
                    # no matching rows in this file -> skip adding this file
                    continue

                filtered = sdf[mask].copy()
                if filtered.empty:
                    continue

                # parse/normalize timestamp column if available
                if tcol and tcol in filtered.columns:
                    try:
                        filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                    except Exception:
                        pass
                else:
                    # attempt common fallback column names to produce a timestamp
                    for cand in ('MessageUTC', 'MessageTime', 'Timestamp', 'timestamp', 'Date'):
                        if cand in filtered.columns:
                            try:
                                filtered['LocaleMessageTime'] = pd.to_datetime(filtered[cand], errors='coerce')
                                tcol = 'LocaleMessageTime'
                                break
                            except Exception:
                                pass

                # sort by timestamp for consistent timeline order
                if tcol and tcol in filtered.columns:
                    try:
                        filtered = filtered.sort_values(by=tcol)
                    except Exception:
                        pass

                    # --- compute swipe gaps (preserve previous logic) ---
                    try:
                        filtered['_prev_ts'] = filtered[tcol].shift(1)
                        filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                        # reset gap at day boundary or when previous is NaT
                        try:
                            cur_dates = filtered[tcol].dt.date
                            prev_dates = cur_dates.shift(1)
                            day_boundary_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
                            filtered.loc[day_boundary_mask, '_swipe_gap_seconds'] = 0.0
                        except Exception:
                            pass
                    except Exception:
                        filtered['_swipe_gap_seconds'] = 0.0
                else:
                    # no timestamp column -> defaults
                    filtered['_swipe_gap_seconds'] = 0.0

                # For each matching swipe row, build the slim evidence record expected by frontend
                added_any = False
                for _, r in filtered.iterrows():
                    # timestamp conversions
                    ts_val = None
                    if tcol and tcol in filtered.columns:
                        ts_val = r.get(tcol)
                    else:
                        # fallback: try Date column
                        if 'Date' in filtered.columns:
                            ts_val = r.get('Date')
                    locale_iso, date_only, swipe_time = _format_time_fields(ts_val)

                    # EmployeeID: prefer emp_col, then Int1/Text12, then fallback to matched trends row
                    emp_val = None
                    try:
                        if emp_col and emp_col in filtered.columns:
                            emp_val = _to_python_scalar(r.get(emp_col))
                        else:
                            for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                                cl = cols_lower.get(cand.lower())
                                if cl and cl in filtered.columns:
                                    emp_val = _to_python_scalar(r.get(cl))
                                    if emp_val:
                                        break
                            if emp_val in (None, '', 'nan'):
                                emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
                    except Exception:
                        emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)

                    # ObjectName1 / EmployeeName (human name)
                    obj_name = None
                    try:
                        if name_col and name_col in filtered.columns:
                            obj_name = _to_python_scalar(r.get(name_col))
                        elif 'ObjectName1' in filtered.columns:
                            obj_name = _to_python_scalar(r.get('ObjectName1'))
                        elif 'EmployeeName' in filtered.columns:
                            obj_name = _to_python_scalar(r.get('EmployeeName'))
                        else:
                            obj_name = _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else None)
                    except Exception:
                        obj_name = _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else None)

                    # PersonnelType
                    personnel_val = _to_python_scalar(r.get(personnel_col)) if (personnel_col and personnel_col in filtered.columns) else None
                    # Location / Partition
                    location_val = _to_python_scalar(r.get(location_col)) if (location_col and location_col in filtered.columns) else _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None

                    # CardNumber
                    card_val = None
                    try:
                        if card_col and card_col in filtered.columns:
                            card_val = _to_python_scalar(r.get(card_col))
                        else:
                            for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                                cl = cols_lower.get(cand.lower())
                                if cl and cl in filtered.columns:
                                    card_val = _to_python_scalar(r.get(cl))
                                    if card_val not in (None, '', 'nan'):
                                        break
                            if card_val in (None, '', 'nan'):
                                card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                    except Exception:
                        card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                    if card_val is not None:
                        try:
                            card_val = str(card_val).strip()
                        except Exception:
                            pass

                    # AdmitCode / Note
                    admit_val = _to_python_scalar(r.get(admit_col)) if (admit_col and admit_col in filtered.columns) else None
                    # some logs store admit/rejection text in 'Note' or 'Rejection_Type'
                    if not admit_val:
                        for cand in ('Admit','AdmitCode','Admit_Type','Rejection_Type','Note','NoteType','Source'):
                            cl = cols_lower.get(cand.lower())
                            if cl and cl in filtered.columns:
                                admit_val = _to_python_scalar(r.get(cl))
                                if admit_val:
                                    break

                    # Direction & Door
                    direction_val = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in filtered.columns else None
                    door_val = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else _to_python_scalar(r.get('Door')) if 'Door' in filtered.columns else None

                    # Zone: prefer precomputed _zone, else map using map_door_to_zone if available
                    zone_val = None
                    try:
                        if '_zone' in r and r.get('_zone') not in (None, '', 'nan'):
                            zone_val = _to_python_scalar(r.get('_zone'))
                        else:
                            if 'map_door_to_zone' in globals():
                                try:
                                    zone_val = map_door_to_zone(door_val, direction_val)
                                except Exception:
                                    zone_val = None
                    except Exception:
                        zone_val = None

                    # Swipe gap
                    try:
                        swipe_gap_seconds = float(r.get('_swipe_gap_seconds') or 0.0)
                    except Exception:
                        swipe_gap_seconds = 0.0
                    swipe_gap_str = format_seconds_to_hms(swipe_gap_seconds)

                    # build output row: include EmployeeName (frontend expects this), plus legacy keys
                    row_out = {
                        "EmployeeName": obj_name,
                        "ObjectName1": obj_name,
                        "EmployeeID": emp_val,
                        "CardNumber": card_val,
                        "Card": card_val,
                        "LocaleMessageTime": locale_iso,
                        "DateOnly": date_only,
                        "Date": date_only,
                        "Time": swipe_time,
                        "Swipe_Time": swipe_time,
                        "SwipeGapSeconds": swipe_gap_seconds,
                        "SwipeGap": swipe_gap_str,
                        "Door": door_val,
                        "Direction": direction_val,
                        "Zone": zone_val,
                        "Note": admit_val,
                        "PersonnelType": personnel_val,
                        "Location": location_val,
                        "PartitionName2": _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None,
                        "_source_file": fp.name
                    }

                    added = _append_row_for_evidence(row_out, fp.name)
                    if added:
                        added_any = True

                # only add file name to available evidence list if we actually added rows from it
                if added_any:
                    raw_files_set.add(fp.name)

        raw_swipe_files = sorted(list(raw_files_set))

        return jsonify({
            "aggregated_rows": cleaned_matched,
            "raw_swipe_files": raw_swipe_files,
            "raw_swipes": raw_swipes_out
        }), 200

    except Exception as e:
        # Close the try-block above with a proper except handler to avoid SyntaxError
        logging.exception("Unhandled exception in /record endpoint")
        return jsonify({"error": "internal server error in /record", "details": str(e)}), 500






# @app.route('/record/export', methods=['GET'])
# def export_record_excel():
#     q = request.args.get('employee_id') or request.args.get('person_uid')
#     date_str = request.args.get('date')
#     city_param = request.args.get('city') or request.args.get('site') or 'pune'
#     city_slug = _slug_city(city_param)

#     if not q:
#         return jsonify({"error":"employee_id or person_uid is required"}), 400

#     p = Path(DEFAULT_OUTDIR)
#     files_to_scan = []
#     if date_str:
#         try:
#             dd = pd.to_datetime(date_str).date()
            
#             files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=dd, city_slug=city_slug, include_shifted=False if city_slug == 'pune' else True)
#         except Exception:
#             return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400
#     else:
#         files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=city_slug)
#     if not files_to_scan:
#         # show any available swipe-style files to help frontend debugging
#         avail = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=None, include_shifted=False)

#         avail_names = [f.name for f in avail] if avail else []
#         logging.info(
#             "export_record_excel: no files matched for date=%s city=%s; available swipe files=%s",
#             date_str, city_slug, avail_names
#         )
#         return jsonify({
#             "error": "no raw swipe files found for requested date / outputs",
#             "available_swipe_files": avail_names
#         }), 404

#     all_rows = []
#     for fp in files_to_scan:
#         try:
#             raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
#         except Exception:
#             try:
#                 raw_df = pd.read_csv(fp, dtype=str)
#             except Exception:
#                 continue

#         raw_df = _replace_placeholder_strings(raw_df)
#         cols_lower = {c.lower(): c for c in raw_df.columns}
#         tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
#         emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
#         name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
#         card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
#         door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
#         dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
#         note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
#         person_uid_col = cols_lower.get('person_uid')

#         mask = pd.Series(False, index=raw_df.index)
#         if person_uid_col and person_uid_col in raw_df.columns:
#             mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
#         if emp_col and emp_col in raw_df.columns:
#             mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
#         if not mask.any() and emp_col and emp_col in raw_df.columns:
#             try:
#                 q_numeric = float(q)
#                 emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
#                 mask = mask | (emp_numeric == q_numeric)
#             except Exception:
#                 pass
#         if not mask.any() and name_col and name_col in raw_df.columns:
#             mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())

#         if not mask.any():
#             continue

#         filtered = raw_df[mask].copy()
#         if filtered.empty:
#             continue

#         if tcol and tcol in filtered.columns:
#             try:
#                 filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
#             except Exception:
#                 pass

#         if tcol and tcol in filtered.columns:
#             filtered = filtered.sort_values(by=tcol)
#             filtered['_prev_ts'] = filtered[tcol].shift(1)
#             try:
#                 filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
#             except Exception:
#                 filtered['_swipe_gap_seconds'] = 0.0
#         else:
#             filtered['_swipe_gap_seconds'] = 0.0

#         try:
#             if door_col and door_col in filtered.columns:
#                 if dir_col and dir_col in filtered.columns:
#                     filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
#                 else:
#                     filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
#             else:
#                 filtered['_zone'] = filtered.get('PartitionName2', None)
#         except Exception:
#             filtered['_zone'] = None

#         for _, r in filtered.iterrows():
#             row = {}
#             row['EmployeeName'] = _to_python_scalar(r.get(name_col)) if (name_col and name_col in filtered.columns) else None
#             emp_val = None
#             if emp_col and emp_col in filtered.columns:
#                 emp_val = _to_python_scalar(r.get(emp_col))
#             else:
#                 for cand in ('int1','text12','employeeid','employee_identity','employeeidentity'):
#                     if cand in cols_lower and cols_lower[cand] in filtered.columns:
#                         emp_val = _to_python_scalar(r.get(cols_lower[cand]))
#                         if emp_val:
#                             break
#             row['EmployeeID'] = emp_val
#             row['Card'] = _to_python_scalar(r.get(card_col)) if (card_col and card_col in filtered.columns) else None

#             if tcol and tcol in filtered.columns:
#                 ts = r.get(tcol)
#                 try:
#                     ts_py = pd.to_datetime(ts)
#                     row['Date'] = ts_py.date().isoformat()
#                     row['Time'] = ts_py.time().isoformat()
#                     row['LocaleMessageTime'] = ts_py.isoformat()
#                 except Exception:
#                     txt = str(r.get(tcol))
#                     row['Date'] = txt[:10]
#                     row['Time'] = txt[11:19] if len(txt) >= 19 else None
#                     row['LocaleMessageTime'] = txt
#             else:
#                 row['Date'] = None
#                 row['Time'] = None
#                 row['LocaleMessageTime'] = None

#             row['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
#             row['SwipeGap'] = format_seconds_to_hms(row['SwipeGapSeconds'])
#             row['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
#             row['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else None
#             row['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
#             try:
#                 zone_val = r.get('_zone') if '_zone' in r else None
#                 if zone_val is None:
#                     zone_val = map_door_to_zone(row['Door'], row['Direction'])
#                 row['Zone'] = _to_python_scalar(zone_val)
#             except Exception:
#                 row['Zone'] = None
#             row['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
#             row['_source_file'] = fp.name
#             all_rows.append(row)

#     if not all_rows:
#         return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

#     df_out = pd.DataFrame(all_rows)
#     details_cols = ['EmployeeName','EmployeeID','Door','Direction','Zone','Date','LocaleMessageTime','SwipeGapSeconds','PartitionName2','_source_file']
#     timeline_cols = ['EmployeeName','EmployeeID','Card','Date','Time','SwipeGapSeconds','Door','Direction','Zone','Note','_source_file']

#     details_df = df_out[[c for c in details_cols if c in df_out.columns]].copy()
#     timeline_df = df_out[[c for c in timeline_cols if c in df_out.columns]].copy()

#     output = io.BytesIO()
#     try:
#         with pd.ExcelWriter(output, engine='openpyxl') as writer:
#             details_df.to_excel(writer, sheet_name='Details â€” Evidence', index=False)
#             timeline_df.to_excel(writer, sheet_name='Swipe timeline', index=False)
#             writer.save()
#             output.seek(0)
#     except Exception as e:
#         logging.exception("Failed to create Excel: %s", e)
#         return jsonify({"error":"failed to create excel"}), 500

#     if OPENPYXL_AVAILABLE:
#         try:
#             wb = load_workbook(output)
#             thin = Side(border_style="thin", color="000000")
#             thick = Side(border_style="medium", color="000000")
#             for ws in wb.worksheets:
#                 header = ws[1]
#                 for cell in header:
#                     cell.font = Font(bold=True)
#                     cell.alignment = Alignment(horizontal="center", vertical="center")
#                     cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
#                 for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
#                     for cell in row:
#                         cell.alignment = Alignment(horizontal="center", vertical="center")
#                         cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
#                 for col in ws.columns:
#                     max_len = 0
#                     col_letter = col[0].column_letter
#                     for cell in col:
#                         try:
#                             v = str(cell.value) if cell.value is not None else ""
#                         except Exception:
#                             v = ""
#                         if len(v) > max_len:
#                             max_len = len(v)
#                     width = min(max(10, max_len + 2), 50)
#                     ws.column_dimensions[col_letter].width = width
#             out2 = io.BytesIO()
#             wb.save(out2)
#             out2.seek(0)
#             return send_file(out2, as_attachment=True,
#                              download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
#                              mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
#         except Exception:
#             logging.exception("Excel styling failed, returning raw file")
#             output.seek(0)
#             return send_file(output, as_attachment=True,
#                              download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
#                              mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
#     else:
#         output.seek(0)
#         return send_file(output, as_attachment=True,
#                          download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
#                          mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")




@app.route('/record/export', methods=['GET'])
def export_record_excel():
    q = request.args.get('employee_id') or request.args.get('person_uid')
    date_str = request.args.get('date')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    if not q:
        return jsonify({"error":"employee_id or person_uid is required"}), 400

    q_str = str(q).strip()

    # Helper: load trend CSV(s) and build set of flagged (id, date) tuples
    def _load_flagged_map(target_date=None):
        flagged_pairs = set()
        try:
            p = Path(DEFAULT_OUTDIR)
            candidates = []
            if target_date:
                # try city/date specific trend file first
                ymd = target_date.strftime('%Y%m%d')
                f = p / f"trend_{city_slug}_{ymd}.csv"
                if f.exists():
                    candidates = [f]
            if not candidates:
                # fallback to any trend_*.csv in outputs
                candidates = sorted(p.glob("trend_*.csv"), reverse=True)
            for fp in candidates:
                try:
                    tdf = pd.read_csv(fp)
                except Exception:
                    try:
                        tdf = pd.read_csv(fp, dtype=str)
                    except Exception:
                        continue
                if tdf is None or tdf.empty:
                    continue
                tdf = _replace_placeholder_strings(tdf)
                # ensure Date is normalized to iso date string
                if 'Date' in tdf.columns:
                    try:
                        tdf['Date'] = pd.to_datetime(tdf['Date'], errors='coerce').dt.date
                        tdf['DateISO'] = tdf['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
                    except Exception:
                        tdf['DateISO'] = tdf['Date'].astype(str)
                else:
                    tdf['DateISO'] = None
                # find flagged rows (IsFlagged True or AnomalyScore >= threshold)
                if 'IsFlagged' in tdf.columns:
                    sel = tdf[tdf['IsFlagged'] == True]
                else:
                    # fallback: consider AnomalyScore >= ANOMALY_THRESHOLD as flagged
                    if 'AnomalyScore' in tdf.columns:
                        try:
                            sel = tdf[pd.to_numeric(tdf['AnomalyScore'], errors='coerce').fillna(0) >= ANOMALY_THRESHOLD]
                        except Exception:
                            sel = pd.DataFrame()
                    else:
                        sel = pd.DataFrame()

                if sel is None or sel.empty:
                    continue

                for _, rr in sel.iterrows():
                    date_iso = None
                    try:
                        date_iso = rr.get('DateISO') or (pd.to_datetime(rr.get('Date'), errors='coerce').date().isoformat() if rr.get('Date') is not None else None)
                    except Exception:
                        date_iso = None
                    # collect EmployeeID and person_uid if present
                    for idcol in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'Int1', 'Text12', 'CardNumber'):
                        try:
                            if idcol in rr and rr.get(idcol) not in (None, '', float('nan')):
                                val = str(rr.get(idcol)).strip()
                                if val:
                                    flagged_pairs.add((idcol, val, date_iso))
                                    # also add date-agnostic tuple for easy contains checks
                                    flagged_pairs.add((idcol, val, None))
                        except Exception:
                            continue
            return flagged_pairs
        except Exception:
            return set()

    # parse date param for targeted checking (optional)
    target_date_obj = None
    if date_str:
        try:
            target_date_obj = pd.to_datetime(date_str).date()
        except Exception:
            return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400

    flagged_map = _load_flagged_map(target_date_obj)

    # Quick check: ensure the requested employee/q is flagged for the requested date (or flagged at all)
    def _is_q_flagged(qtoken, date_iso=None):
        if not qtoken:
            return False
        # check across multiple id columns recorded in trend files
        for idcol in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'Int1', 'Text12', 'CardNumber'):
            if (idcol, qtoken, date_iso) in flagged_map or (idcol, qtoken, None) in flagged_map:
                return True
        return False

    # if date provided require flagged on that date; otherwise accept flagged any-date
    q_flagged = _is_q_flagged(q_str, target_date_obj.isoformat() if target_date_obj else None)
    if not q_flagged:
        # if not flagged with exact id, try numeric-normalized attempts (strip trailing .0 etc)
        try:
            if '.' in q_str:
                fq = None
                try:
                    f = float(q_str)
                    if math.isfinite(f) and f.is_integer():
                        fq = str(int(f))
                except Exception:
                    fq = None
                if fq and _is_q_flagged(fq, target_date_obj.isoformat() if target_date_obj else None):
                    q_flagged = True
        except Exception:
            pass

    if not q_flagged:
        return jsonify({"error": "employee not flagged (no evidence rows for requested employee/date)"}), 404

    # find swipe files to scan
    p = Path(DEFAULT_OUTDIR)
    files_to_scan = []
    if target_date_obj:
        files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=target_date_obj, city_slug=city_slug, include_shifted=False if city_slug == 'pune' else True)
    else:
        files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=city_slug)
    if not files_to_scan:
        avail = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=None, include_shifted=False)
        avail_names = [f.name for f in avail] if avail else []
        logging.info("export_record_excel: no files matched for date=%s city=%s; available swipe files=%s", date_str, city_slug, avail_names)
        return jsonify({"error": "no raw swipe files found for requested date / outputs", "available_swipe_files": avail_names}), 404

    all_rows = []
    for fp in files_to_scan:
        try:
            raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
        except Exception:
            try:
                raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

        raw_df = _replace_placeholder_strings(raw_df)
        cols_lower = {c.lower(): c for c in raw_df.columns}
        tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
        emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
        name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
        card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
        door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
        dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
        admit_cols = [c for c in ('admitcode','admit','admit_code','admit_type','admitstatus') if c in cols_lower]
        admit_col = cols_lower.get(admit_cols[0]) if admit_cols else None
        personnel_col = cols_lower.get('personneltype') or cols_lower.get('personneltypename') or None
        location_col = cols_lower.get('partitionname2') or cols_lower.get('location') or cols_lower.get('partitionname') or None
        person_uid_col = cols_lower.get('person_uid')

        mask = pd.Series(False, index=raw_df.index)
        if person_uid_col and person_uid_col in raw_df.columns:
            mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == q_str)
        if emp_col and emp_col in raw_df.columns:
            mask = mask | (raw_df[emp_col].astype(str).str.strip() == q_str)
        if not mask.any() and emp_col and emp_col in raw_df.columns:
            try:
                q_numeric = float(q_str)
                emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                mask = mask | (emp_numeric == q_numeric)
            except Exception:
                pass
        if not mask.any() and name_col and name_col in raw_df.columns:
            try:
                mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == q_str.lower())
            except Exception:
                pass

        if not mask.any():
            continue

        filtered = raw_df[mask].copy()
        if filtered.empty:
            continue

        if tcol and tcol in filtered.columns:
            try:
                filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
            except Exception:
                pass

        if tcol and tcol in filtered.columns:
            filtered = filtered.sort_values(by=tcol)
            filtered['_prev_ts'] = filtered[tcol].shift(1)
            try:
                filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
            except Exception:
                filtered['_swipe_gap_seconds'] = 0.0
        else:
            filtered['_swipe_gap_seconds'] = 0.0

        try:
            if door_col and door_col in filtered.columns:
                if dir_col and dir_col in filtered.columns:
                    filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                else:
                    filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
            else:
                filtered['_zone'] = filtered.get('PartitionName2', None)
        except Exception:
            filtered['_zone'] = None

        for _, r in filtered.iterrows():
            # compute timestamp variants
            ts_val = None
            if tcol and tcol in filtered.columns:
                ts_val = r.get(tcol)
            else:
                if 'Date' in filtered.columns:
                    ts_val = r.get('Date')
            try:
                dt = pd.to_datetime(ts_val)
                locale_iso = dt.isoformat() if pd.notna(dt) else None
            except Exception:
                locale_iso = str(ts_val) if ts_val is not None else None
            try:
                date_only = dt.strftime("%d-%b-%y") if pd.notna(dt) else None
            except Exception:
                date_only = None
            try:
                swipe_time = dt.strftime("%I:%M:%S %p").lstrip("0") if pd.notna(dt) else None
            except Exception:
                swipe_time = None

            # EmployeeID resolution
            emp_val = None
            try:
                if emp_col and emp_col in filtered.columns:
                    emp_val = _to_python_scalar(r.get(emp_col))
                else:
                    for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                        cl = cols_lower.get(cand.lower())
                        if cl and cl in filtered.columns:
                            emp_val = _to_python_scalar(r.get(cl))
                            if emp_val:
                                break
            except Exception:
                emp_val = None
            if emp_val is not None:
                try:
                    s = str(emp_val).strip()
                    if '.' in s:
                        try:
                            f = float(s)
                            if math.isfinite(f) and f.is_integer():
                                s = str(int(f))
                        except Exception:
                            pass
                    emp_val = s
                except Exception:
                    pass

            # ObjectName1 / EmployeeName
            obj_name = None
            try:
                if name_col and name_col in filtered.columns:
                    obj_name = _to_python_scalar(r.get(name_col))
                elif 'ObjectName1' in filtered.columns:
                    obj_name = _to_python_scalar(r.get('ObjectName1'))
                elif 'EmployeeName' in filtered.columns:
                    obj_name = _to_python_scalar(r.get('EmployeeName'))
            except Exception:
                obj_name = None

            personnel_val = _to_python_scalar(r.get(personnel_col)) if (personnel_col and personnel_col in filtered.columns) else None
            location_val = _to_python_scalar(r.get(location_col)) if (location_col and location_col in filtered.columns) else (_to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None)

            # Card Number
            card_val = None
            try:
                if card_col and card_col in filtered.columns:
                    card_val = _to_python_scalar(r.get(card_col))
                else:
                    for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                        cl = cols_lower.get(cand.lower())
                        if cl and cl in filtered.columns:
                            card_val = _to_python_scalar(r.get(cl))
                            if card_val not in (None, '', 'nan'):
                                break
                if card_val is not None:
                    card_val = str(card_val).strip()
            except Exception:
                card_val = None

            admit_val = _to_python_scalar(r.get(admit_col)) if (admit_col and admit_col in filtered.columns) else None
            if not admit_val:
                for cand in ('Admit','AdmitCode','Admit_Type','Rejection_Type','Note','NoteType','Source'):
                    cl = cols_lower.get(cand.lower())
                    if cl and cl in filtered.columns:
                        admit_val = _to_python_scalar(r.get(cl))
                        if admit_val:
                            break

            direction_val = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in filtered.columns else None
            door_val = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else _to_python_scalar(r.get('Door')) if 'Door' in filtered.columns else None

            row_out = {
                "LocaleMessageTime": locale_iso,
                "DateOnly": date_only,
                "Swipe_Time": swipe_time,
                "EmployeeID": emp_val,
                "ObjectName1": obj_name,
                "PersonnelType": personnel_val,
                "Location": location_val,
                "CardNumber": card_val,
                "AdmitCode": admit_val,
                "Direction": direction_val,
                "Door": door_val,
                "_source_file": fp.name
            }

            all_rows.append(row_out)

    if not all_rows:
        return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

    df_out = pd.DataFrame(all_rows)

    # Further restrict to only rows that match a flagged trend row on the same Date & EmployeeID/person_uid
    # Build flagged lookup using trend CSV(s) again but now keyed by EmployeeID/Date
    flagged_dates = set()
    try:
        p = Path(DEFAULT_OUTDIR)
        # load relevant trend csv(s)
        trend_candidates = []
        if target_date_obj:
            fp_try = p / f"trend_{city_slug}_{target_date_obj.strftime('%Y%m%d')}.csv"
            if fp_try.exists():
                trend_candidates = [fp_try]
        if not trend_candidates:
            trend_candidates = sorted(p.glob("trend_*.csv"), reverse=True)
        for tf in trend_candidates:
            try:
                tdf = pd.read_csv(tf)
            except Exception:
                try:
                    tdf = pd.read_csv(tf, dtype=str)
                except Exception:
                    continue
            if tdf is None or tdf.empty:
                continue
            tdf = _replace_placeholder_strings(tdf)
            if 'IsFlagged' in tdf.columns:
                tsel = tdf[tdf['IsFlagged'] == True]
            else:
                if 'AnomalyScore' in tdf.columns:
                    try:
                        tsel = tdf[pd.to_numeric(tdf['AnomalyScore'], errors='coerce').fillna(0) >= ANOMALY_THRESHOLD]
                    except Exception:
                        tsel = pd.DataFrame()
                else:
                    tsel = pd.DataFrame()
            if tsel is None or tsel.empty:
                continue
            for _, tt in tsel.iterrows():
                try:
                    dval = None
                    if 'Date' in tt and pd.notna(tt.get('Date')):
                        try:
                            dval = pd.to_datetime(tt.get('Date')).date().isoformat()
                        except Exception:
                            dval = str(tt.get('Date'))
                    # collect candidate ids
                    for idcol in ('EmployeeID','person_uid','EmployeeIdentity','Int1','Text12','CardNumber'):
                        if idcol in tt and tt.get(idcol) not in (None, '', float('nan')):
                            try:
                                ival = str(tt.get(idcol)).strip()
                                if ival:
                                    flagged_dates.add((idcol, ival, dval))
                                    flagged_dates.add((idcol, ival, None))
                            except Exception:
                                continue
                except Exception:
                    continue
    except Exception:
        flagged_dates = set()

    # Keep rows where employee/date is present in flagged_dates
    def _row_is_flagged(r):
        emp = r.get('EmployeeID') or ''
        date_only = r.get('DateOnly')
        # try iso date from DateOnly (it is in DD-MMM-YY), so translate to iso for matching if possible
        date_iso = None
        try:
            if date_only:
                # parse using day-month-year short form
                date_iso = pd.to_datetime(date_only, format="%d-%b-%y", errors='coerce')
                if pd.notna(date_iso):
                    date_iso = date_iso.date().isoformat()
                else:
                    date_iso = None
        except Exception:
            date_iso = None
        # match by EmployeeID & date or EmployeeID with any date
        if emp and ((('EmployeeID', emp, date_iso) in flagged_dates) or (('EmployeeID', emp, None) in flagged_dates)):
            return True
        # also try matching by person_uid if employee string includes emp:/uid: patterns
        for idcol in ('person_uid', 'EmployeeIdentity', 'Int1', 'Text12', 'CardNumber'):
            if ((idcol, q_str, date_iso) in flagged_dates) or ((idcol, q_str, None) in flagged_dates):
                return True
        # fallback: if q matched and we earlier accepted q_flagged, accept rows for q
        try:
            if str(q_str) and (str(q_str) == str(emp) or str(q_str) == str(r.get('CardNumber'))):
                return True
        except Exception:
            pass
        return False

    df_filtered = df_out[df_out.apply(_row_is_flagged, axis=1)].copy()
    if df_filtered.empty:
        return jsonify({"error":"no flagged swipe rows found for the requested employee/date"}), 404

    # final column ordering and ensure only the columns requested
    final_cols = ["LocaleMessageTime","DateOnly","Swipe_Time","EmployeeID","ObjectName1","PersonnelType","Location","CardNumber","AdmitCode","Direction","Door"]
    # ensure all final_cols exist in df_filtered (create missing as None)
    for c in final_cols:
        if c not in df_filtered.columns:
            df_filtered[c] = None
    df_final = df_filtered[final_cols].copy()

    # write excel with single sheet "Evidence" (strict columns only)
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df_final.to_excel(writer, sheet_name='Evidence', index=False)
            writer.save()
            output.seek(0)
    except Exception as e:
        logging.exception("Failed to create Excel: %s", e)
        return jsonify({"error":"failed to create excel"}), 500

    if OPENPYXL_AVAILABLE:
        try:
            wb = load_workbook(output)
            thin = Side(border_style="thin", color="000000")
            thick = Side(border_style="medium", color="000000")
            for ws in wb.worksheets:
                header = ws[1]
                for cell in header:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                        cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
                for col in ws.columns:
                    max_len = 0
                    col_letter = col[0].column_letter
                    for cell in col:
                        try:
                            v = str(cell.value) if cell.value is not None else ""
                        except Exception:
                            v = ""
                        if len(v) > max_len:
                            max_len = len(v)
                    width = min(max(10, max_len + 2), 50)
                    ws.column_dimensions[col_letter].width = width
            out2 = io.BytesIO()
            wb.save(out2)
            out2.seek(0)
            return send_file(out2, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        except Exception:
            logging.exception("Excel styling failed, returning raw file")
            output.seek(0)
            return send_file(output, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    else:
        output.seek(0)
        return send_file(output, as_attachment=True,
                         download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                         mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")



@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)


@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        if build_monthly_training is None:
            raise RuntimeError("build_monthly_training not available")
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500




# chatbot helpers (kept mostly as-is)
try:
    from trend_runner import _read_past_trend_csvs, _normalize_id_val, SCENARIO_EXPLANATIONS
except Exception:
    _read_past_trend_csvs = None
    _normalize_id_val = None
    SCENARIO_EXPLANATIONS = {}

def _load_latest_trend_df(outdir: Path, city: str = "pune"):
    city_slug = _slug_city(city)
    csvs = sorted(outdir.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(outdir.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return None, None
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    df = _replace_placeholder_strings(df)
    return df, latest.name

def _find_person_rows(identifier: str, days: int = 90, outdir: Path = DEFAULT_OUTDIR):
    if _normalize_id_val:
        norm = _normalize_id_val(identifier)
    else:
        norm = str(identifier).strip()
        if '.' in norm:
            try:
                f = float(norm)
                if f.is_integer():
                    norm = str(int(f))
            except Exception:
                pass
    today = datetime.now().date()
    try:
        if _read_past_trend_csvs:
            past = _read_past_trend_csvs(str(outdir), days, today)

            
        else:
            files = sorted(Path(outdir).glob("trend_pune_*.csv"), reverse=True)
            dfs = []
            cutoff = today - timedelta(days=days)
            for fp in files:
                try:
                    tmp = pd.read_csv(fp, parse_dates=['Date'])
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                    tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                    dfs.append(tmp)
                except Exception:
                    try:
                        tmp = pd.read_csv(fp, dtype=str)
                        if 'Date' in tmp.columns:
                            tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                            tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                            dfs.append(tmp)
                    except Exception:
                        continue
            past = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()





    except Exception:
        past = pd.DataFrame()

    if past is None or past.empty:
        return pd.DataFrame()

    past = _replace_placeholder_strings(past)
    match_mask = pd.Series(False, index=past.index)
    for col in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
        if col in past.columns:
            try:
                match_mask = match_mask | (past[col].astype(str).fillna('').str.strip() == str(norm).strip())
            except Exception:
                continue

    if not match_mask.any():
        try:
            qnum = float(norm)
            for col in ('EmployeeID','Int1'):
                if col in past.columns:
                    try:
                        numcol = pd.to_numeric(past[col], errors='coerce')
                        match_mask = match_mask | (numcol == qnum)
                    except Exception:
                        continue
        except Exception:
            pass

    if not match_mask.any() and 'EmployeeName' in past.columns:
        names = past['EmployeeName'].dropna().astype(str).unique().tolist()
        close = difflib.get_close_matches(str(identifier), names, n=5, cutoff=0.7)
        if close:
            match_mask = match_mask | past['EmployeeName'].astype(str).isin(close)

    return past[match_mask].copy()

def _explain_scenario_code(code):
    if not code:
        return None
    code = str(code).strip()
    if code in SCENARIO_EXPLANATIONS:
        try:
            fn = SCENARIO_EXPLANATIONS.get(code)
            try:
                txt = fn({})
                return txt
            except Exception:
                return code.replace("_", " ").replace(">= ", "â‰¥ ")
        except Exception:
            return code.replace("_", " ").replace(">= ", "â‰¥ ")
    return code.replace("_", " ").replace(">=", "â‰¥")

def _map_score_to_label_fallback(score: float):
    try:
        s = float(score)
    except Exception:
        return (0.0, "Low")
    if s >= 0.75:
        return (s, "High")
    if s >= 0.4:
        return (s, "Medium")
    return (s, "Low")

@app.route('/chatbot/query', methods=['POST'])
def chatbot_query():
    payload = request.get_json(force=True)
    q = (payload.get('q') or '').strip()
    if not q:
        return jsonify({"error":"query text 'q' required"}), 400
    lang = payload.get('lang')
    q_l = q.lower().strip()

    if re.search(r"\bwho is (high|low) risk\b", q_l) or re.search(r"\b(high|low) risk (people|persons|people) (today)?\b", q_l):
        want = 'high' if 'high' in q_l else 'low' if 'low' in q_l else None
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer": "No trend data available.", "evidence": []})
        if 'RiskLevel' not in df.columns:
            if 'RiskScore' in df.columns:
                def _map_rs(s):
                    try:
                        if pd.isna(s):
                            return 'Low'
                    except Exception:
                        pass
                    try:
                        if 'map_score_to_label' in globals() and callable(globals().get('map_score_to_label')):
                            try:
                                return globals().get('map_score_to_label')(float(s))[1]
                            except Exception:
                                pass
                        return _map_score_to_label_fallback(float(s))[1]
                    except Exception:
                        return 'Low'
                df['RiskLevel'] = df['RiskScore'].apply(lambda s: _map_rs(s))
            else:
                df['RiskLevel'] = df.get('RiskLevel', 'Low')
        if want == 'high':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'high']
        elif want == 'low':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'low']
        else:
            sel = df
        names = sel['EmployeeName'].dropna().astype(str).unique().tolist()
        if not names:
            ans = f"No {want} risk persons found in the latest data." if want else "No persons found."
            return jsonify({"answer": ans, "evidence": []})
        else:
            ans = f"{want.capitalize()} risk persons today: " + ", ".join(names[:40])
            sample = _clean_sample_df(sel.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bexplain\s+([A-Za-z0-9_\-]+)\b.*", q_l)
    if m:
        code = m.group(1).strip()
        explanation = _explain_scenario_code(code)
        ans = f"Explanation for '{code}': {explanation}"
        return jsonify({"answer": ans, "evidence": []})

    if 'trend details' in q_l or 'top reasons' in q_l or 'trend details for today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'Reasons' in df.columns:
            reasons = {}
            for v in df['Reasons'].dropna().astype(str):
                for part in re.split(r'[;,\|]', v):
                    key = part.strip()
                    if key and not _is_placeholder_str(key):
                        reasons[key] = reasons.get(key, 0) + 1
            top = sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:10]
            if not top:
                return jsonify({"answer":"No reason counts available today.","evidence":[]})
            ans = "Top reasons today: " + ", ".join([f"{k} ({c})" for k,c in top])
            sample = []
            try:
                top_reasons = [k for k,_ in top]
                mask = df['Reasons'].astype(str).apply(lambda s: any(tr in s for tr in top_reasons))
                sample_df = df[mask].head(10)
                sample = _clean_sample_df(sample_df, max_rows=10)
            except Exception:
                sample = []
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bshow (?:me )?([A-Za-z0-9\-\:\s]+?) (?:for )?(?:last )?(\d+)\s*days\b", q_l)
    if not m:
        m = re.match(r".*\b(show|display)\s+(?:me\s+)?([A-Za-z0-9\-\:\s]+?)\s+last\s+(\d+)\s*days\b", q_l)
    if m:
        if len(m.groups()) == 2:
            identifier, days = m.group(1).strip(), int(m.group(2))
        else:
            identifier = m.group(1).strip()
            days = int(m.group(2))
        rows = _find_person_rows(identifier, days=days, outdir=DEFAULT_OUTDIR)
        if rows is None or rows.empty:
            return jsonify({"answer": f"No records found for '{identifier}' in last {days} days.", "evidence": []})
        flagged = rows[rows.get('IsFlagged', False) == True] if 'IsFlagged' in rows.columns else pd.DataFrame()
        flagged_count = int(len(flagged))
        total_days = int(len(rows))
        latest_row = rows.sort_values('Date', ascending=False).iloc[0].to_dict()
        name = latest_row.get('EmployeeName') or latest_row.get('person_uid') or latest_row.get('EmployeeID')
        ans = f"Found {total_days} day(s) for {name} in the last {days} days. Flagged days: {flagged_count}."
        sample = _clean_sample_df(rows.sort_values('Date', ascending=False).head(10), max_rows=10)
        return jsonify({"answer": ans, "evidence": sample})

    if 'present today' in q_l or 'who is present today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'PresentToday' in df.columns:
            present = df[df['PresentToday'] == True]
            names = present['EmployeeName'].dropna().unique().tolist()
            ans = f"Present today: {', '.join(names[:40]) if names else 'None'}"
            sample = _clean_sample_df(present.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})
        else:
            return jsonify({"answer":"PresentToday field not available in latest trends.","evidence":[]})

    hint = "I can answer: 'Who is high risk today', 'Who is low risk today', 'Show me <EmployeeID|Name> last 90 days', 'Explain <scenario_code>', 'Trend details for today â€” top reasons'."
    return jsonify({"answer": f"I can help with trend & risk questions. I recognized: {q}. Try: {hint}", "evidence":[]})





@app.route('/employee/<empid>/image', methods=['GET'])
def serve_employee_image(empid):
    if empid is None:
        return jsonify({"error": "employee id required"}), 400
    try:
        img_bytes = get_person_image_bytes(empid)

        # If no image found, return a small inline SVG placeholder so <img> can still render
        if not img_bytes:
            svg = (
                '<svg xmlns="http://www.w3.org/2000/svg" width="160" height="160">'
                '<rect fill="#eef2f7" width="100%" height="100%"/>'
                '<text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle" fill="#64748b" font-size="18">'
                'No image</text></svg>'
            )
            bio = io.BytesIO(svg.encode('utf-8'))
            bio.seek(0)
            resp = send_file(bio, mimetype='image/svg+xml')
            resp.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            # explicit CORS header for image resources (helps cross-origin <img> loads)
            resp.headers['Access-Control-Allow-Origin'] = '*'
            return resp

        # detect content type heuristically (jpeg/png/bmp)
        header = img_bytes[:8] if isinstance(img_bytes, (bytes, bytearray)) else bytes(img_bytes)[:8]
        content_type = 'application/octet-stream'
        try:
            if len(header) >= 2 and header[0] == 0xFF and header[1] == 0xD8:
                content_type = 'image/jpeg'
            elif header.startswith(b'\x89PNG\r\n\x1a\n'):
                content_type = 'image/png'
            elif header.startswith(b'BM'):
                content_type = 'image/bmp'
            else:
                content_type = 'application/octet-stream'
        except Exception:
            content_type = 'application/octet-stream'

        bio = io.BytesIO(img_bytes if isinstance(img_bytes, (bytes, bytearray)) else bytes(img_bytes))
        bio.seek(0)
        resp = send_file(bio, mimetype=content_type)
        resp.headers['Cache-Control'] = 'private, max-age=300'
        # explicit CORS header for image resources (helps cross-origin <img> loads)
        resp.headers['Access-Control-Allow-Origin'] = '*'
        return resp
    except Exception:
        logging.exception("Error serving image for employee %s", empid)
        return jsonify({"error": "failed to serve image"}), 500




# run
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)


 
 





# backend/duration_report.py
from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np
import hashlib

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name (override with environment variable if needed)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
try:
    from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    # fallback â€” keep behaviour if config file unavailable
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

    def map_door_to_zone(door: object, direction: object = None) -> str:
        """
        Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
        """
        try:
            if door is None:
                return None
            s = str(door).strip()
            if not s:
                return None
            s_l = s.lower()
            if direction and isinstance(direction, str):
                d = direction.strip().lower()
                if "out" in d:
                    return OUT_OF_OFFICE_ZONE
                if "in" in d:
                    return "Reception Area"
            if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
                return OUT_OF_OFFICE_ZONE
            return "Working Area"
        except Exception:
            return None

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "SG.Singapore", "PH.Manila","IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023"
        ],
        "partitions": ["LT.Vilnius","IT.Rome","UK.London","IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition","PA.Panama City","PE.Lima", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["US.CO.OBS", "USA/Canada Default", "US.FL.Miami", "US.NYC"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# GENERIC SQL template - we keep AdjustedMessageTime in SELECT for debugging but the Python
# logic in this file no longer depends on it for date assignment.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND t3.[Name] = 'Employee'
  {date_condition}
  {region_filter}
"""

# Helpers
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

# GUID / placeholders helpers
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', 'â€”', 'â€“', 'none', 'null'])

def _looks_like_guid(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
        except Exception:
            continue
        if not s:
            continue
        if _is_placeholder_str(s):
            continue
        if _looks_like_guid(s):
            continue
        return s
    return None

def _canonical_person_uid_from_row(row):
    """
    Produce canonical person_uid in the form:
      - 'emp:<employeeid>' (if a sensible non-GUID EmployeeID present),
      - 'uid:<EmployeeIdentity>' (if present),
      - 'name:<sha1 10chars>' fallback when name present,
      - otherwise None.
    """
    empid = None
    for cand in ('EmployeeID', 'Int1', 'Text12'):
        if cand in row and row.get(cand) not in (None, '', float('nan')):
            empid = row.get(cand)
            break
    empident = row.get("EmployeeIdentity", None)
    name = row.get("EmployeeName", None)

    # normalize empid numeric floats -> ints
    if empid is not None:
        try:
            s = str(empid).strip()
            if '.' in s:
                f = float(s)
                if f.is_integer():
                    s = str(int(f))
            if s and not _looks_like_guid(s) and not _is_placeholder_str(s):
                return f"emp:{s}"
        except Exception:
            pass

    if empident not in (None, '', float('nan')):
        try:
            si = str(empident).strip()
            if si:
                return f"uid:{si}"
        except Exception:
            pass

    if name not in (None, '', float('nan')):
        try:
            sn = str(name).strip()
            if sn and not _looks_like_guid(sn) and not _is_placeholder_str(sn):
                h = hashlib.sha1(sn.lower().encode('utf8')).hexdigest()[:10]
                return f"name:{h}"
        except Exception:
            pass

    return None

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        partitions = rc.get("partitions", [])
        if partitions:
            parts_sql = ", ".join(f"'{p}'" for p in partitions)
            region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
        else:
            likes = rc.get("logical_like", [])
            if likes:
                like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
                region_filter = f"AND ({like_sql})"
            else:
                region_filter = ""
    else:
        region_filter = ""

    # NOTE: AdjustedMessageTime / 2AM boundary logic removed from date selection.
    date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
        "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
        "Direction", "CompanyName", "PrimaryLocation"
    ]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    # ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Dates parsing
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # Keep AdjustedMessageTime if present for debugging but we do NOT use it for date boundaries anymore.
    try:
        if "AdjustedMessageTime" in df.columns:
            df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
        else:
            df["AdjustedMessageTime"] = pd.NaT
    except Exception:
        df["AdjustedMessageTime"] = pd.NaT

    # defensive: make text fields strings (avoid object type surprises)
    for tcol in ("Door", "PartitionName2", "PersonnelTypeName", "EmployeeName", "CompanyName", "PrimaryLocation"):
        if tcol in df.columns:
            df[tcol] = df[tcol].fillna("").astype(str)

    # Filter: only Employees (defensive; the SQL template already requests t3.Name = 'Employee')
    try:
        if "PersonnelTypeName" in df.columns:
            df = df[df["PersonnelTypeName"].str.strip().str.lower() == "employee"].copy()
    except Exception:
        logging.debug("Could not apply PersonnelTypeName filter for region %s", region_key)

    # canonical person_uid (consistent with trend_runner)
    def make_person_uid(row):
        try:
            return _canonical_person_uid_from_row(row)
        except Exception:
            # fallback: try simple concatenation if canonical fails
            try:
                eid = row.get("EmployeeIdentity")
                if pd.notna(eid) and str(eid).strip() != "":
                    return str(eid).strip()
            except Exception:
                pass
            parts = []
            for c in ("EmployeeID", "CardNumber", "EmployeeName"):
                try:
                    v = row.get(c)
                    if v not in (None, '', float('nan')):
                        parts.append(str(v).strip())
                except Exception:
                    continue
            return "|".join(parts) if parts else None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)

    # APAC partition normalization (more robust)
    if region_key == "apac" and not df.empty:
        def normalize_apac_partition(row):
            """
            Minimal / strict APAC partition mapping (only adjust Taguig / Quezon / KL cases).
            - APAC_PI_*  -> Taguig City
            - APAC_PH_*  -> Quezon City
            - APAC_MY_KL* or APAC_MY + KL -> MY.Kuala Lumpur
            If none match, fall back to previously-known tokens or keep original PartitionName2.
            """
            door = str(row.get("Door") or "") or ""
            part = str(row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()

            # 1) Strict: APAC_PI => Taguig City
            # Matches examples like: "APAC_PI_Manila_DR_MainEntrance"
            if re.search(r'\bAPAC[_\-]?PI\b', d) or re.search(r'\bAPAC[_\-]?PI[_\-]', d):
                return "Taguig City"

            # 2) Strict: APAC_PH => Quezon City
            # Matches examples like: "APAC_PH_Manila_7th Floor_Open Office Door 2-721"
            if re.search(r'\bAPAC[_\-]?PH\b', d) or re.search(r'\bAPAC[_\-]?PH[_\-]', d):
                return "Quezon City"

            # 3) Strict: Kuala Lumpur patterns
            # Accept variants like:
            #   "APAC_MY_KL_MAIN ENTRANCE DOOR", "APAC_MY_KL", "APAC_MY KL MAIN", "APAC_MY_KUALA-LUMPUR"
            # We require APAC + MY + (KL or KUALA) tokens in the door/partition string.
            # if ("APAC" in d) and ("MY" in d) and (
            #     "KL" in d or
            #     "KUALA" in d or
            #     re.search(r'KUALA[^A-Z0-9]*LUMPUR', d)
            # ):
            #     return "MY.Kuala Lumpur"


            # 3) Strict: Kuala Lumpur patterns (robust to underscores, dashes, spaces, extra text)
            # Matches: APAC_MY_KL_MAIN ENTRANCE DOOR, APAC-MY-KL, APAC MY KUALA-LUMPUR, etc.
            if re.search(r'APAC[^A-Z0-9]*MY[^A-Z0-9]*(?:KL\b|KUALA\b|KUALA[^A-Z0-9]*LUMPUR)', d):
                return "MY.Kuala Lumpur"



            # --- Minimal remaining token map (kept small & safe) ---
            token_map = {
                "APAC_IN_PUN": "Pune",
                "APAC_PUN": "Pune",
                "VIS_PUN": "Pune",
                "VIS_PUN_177": "Pune",
                "PUN": "Pune",
                "APAC_IN_HYD": "IN.HYD",
                "APAC_HYD": "IN.HYD",
                "HYD": "IN.HYD",
                "IN.HYD": "IN.HYD",
                "SG": "SG.Singapore",
                "SINGAPORE": "SG.Singapore",
                
            }

            # Prefer explicit tokens already present in PartitionName2
            for key, canonical in token_map.items():
                if key in p:
                    return canonical

            # Check door/partition tokens for any remaining known tokens
            toks = [t for t in re.split(r'[^A-Z0-9]+', d + " " + p) if t]
            for t in toks:
                if t in token_map:
                    return token_map[t]

            # If PartitionName2 already has a meaningful value, keep it
            if p and p.strip():
                return part

            # Unknown -> return empty string (caller applies strict masking)
            return ""

            # helper: split door/partition into alphanumeric tokens (keeps mixed tokens like VIS_PUN)
            def make_tokens(s: str):
                toks = [t for t in re.split(r'[^A-Z0-9\-]+', s or "") if t]
                return toks

            # 1) If PartitionName2 already contains a known canonical token, prefer it.
            for key, canonical in token_map.items():
                if key in p:
                    return canonical

            # 2) Tokenize Door and PartitionName2 and check tokens (exact token match)
            door_tokens = make_tokens(d)
            part_tokens = make_tokens(p)
            all_tokens = door_tokens + part_tokens
            for t in all_tokens:
                if t in token_map:
                    return token_map[t]

            # 3) Substring / regex fallbacks -- more permissive
            # Taguig-specific patterns
            if re.search(r'\bTAGUIG\b', d) or re.search(r'\bTAGUIG\b', p):
                return "Taguig City"
            # Quezon / Manila
            if re.search(r'\bQUEZON\b', d) or re.search(r'\bQUEZON\b', p) or re.search(r'\bMANILA\b', d) or re.search(r'\bMANILA\b', p):
                return "Quezon City"
            # Pune / PUN
            if re.search(r'\bPUN(E)?\b', d) or re.search(r'\bPUN(E)?\b', p):
                return "Pune"
            # Hyderabad
            if re.search(r'\bHYD\b', d) or re.search(r'\bHYD\b', p):
                return "IN.HYD"
            # Singapore
            if re.search(r'\bSINGAPORE\b', d) or re.search(r'\bSG\b', d) or re.search(r'\bSINGAPORE\b', p):
                return "SG.Singapore"
            # Kuala Lumpur / MY
            if re.search(r'\bKUALA\b', d) or re.search(r'\bKUALA\b', p) or re.search(r'\bMY\b', d) or re.search(r'\bMY\b', p) or re.search(r'KUALA.?LUMPUR', d):
                return "MY.Kuala Lumpur"

            # 4) If PartitionName2 is a non-empty meaningful value, keep it (no change)
            if p and p.strip():
                return part

            # 5) Unknown: return empty string (caller can filter strictly if needed)
            return ""

        # apply mapping and log the mapping summary for debugging
        df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)
        try:
            vc = df["PartitionName2"].value_counts(dropna=False).to_dict()
            logging.info("APAC PartitionName2 mapping counts example: %s", {k: vc.get(k, 0) for k in list(vc)[:10]})
        except Exception:
            logging.debug("APAC partition mapping counts unavailable")


    # NAMER: normalize PartitionName2 and add LogicalLocation per previous behaviour
    if region_key == "namer" and not df.empty:
        def namer_partition_and_logical(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()
            normalized = part
            logical = "Other"

            if ("US.CO.HQ" in d) or ("HQ" in d and "HQ" in d[:20]) or ("DENVER" in d) or (p == "US.CO.OBS"):
                normalized = "US.CO.OBS"
                logical = "Denver-HQ"
            elif "AUSTIN" in d or "AUSTIN TX" in d or p == "USA/CANADA DEFAULT":
                normalized = "USA/Canada Default"
                logical = "Austin Texas"
            elif "MIAMI" in d or p == "US.FL.MIAMI":
                normalized = "US.FL.Miami"
                logical = "Miami"
            elif "NYC" in d or "NEW YORK" in d or p == "US.NYC":
                normalized = "US.NYC"
                logical = "New York"
            else:
                if p == "US.CO.OBS":
                    normalized = "US.CO.OBS"; logical = "Denver-HQ"
                elif p == "USA/CANADA DEFAULT":
                    normalized = "USA/Canada Default"; logical = "Austin Texas"
                elif p == "US.FL.MIAMI":
                    normalized = "US.FL.Miami"; logical = "Miami"
                elif p == "US.NYC":
                    normalized = "US.NYC"; logical = "New York"
                else:
                    normalized = part
                    logical = "Other"
            return pd.Series({"PartitionName2": normalized, "LogicalLocation": logical})

        mapped = df.apply(namer_partition_and_logical, axis=1)
        df["PartitionName2"] = mapped["PartitionName2"].astype(str)
        df["LogicalLocation"] = mapped["LogicalLocation"].astype(str)

    # ensure PartitionName2 column exists as string
    if "PartitionName2" not in df.columns:
        df["PartitionName2"] = ""

    # ensure LogicalLocation exists
    if "LogicalLocation" not in df.columns:
        df["LogicalLocation"] = ""

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]

# ---------------------------------------------------------------------
# compute_daily_durations (single robust implementation)
# ---------------------------------------------------------------------
def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "DurationMinutes", "DurationDisplay", "DurationHMS",
        "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes if present
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # drop near-duplicate swipes (round to seconds)
    try:
        df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
        dedupe_subset = ["person_uid", "_lts_rounded", "CardNumber", "Door"]
        df = df.drop_duplicates(subset=dedupe_subset, keep="first").copy()
    except Exception:
        dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
        try:
            df = df.drop_duplicates(subset=dedupe_cols, keep="first")
        except Exception:
            pass

    # Date assignment (STRICT: only use LocaleMessageTime.date())
    try:
        df["Date"] = df["LocaleMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure person_uid (keep existing canonicalization where present)
    df["person_uid"] = df.apply(
        lambda row: row.get("person_uid")
        if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
        else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
        axis=1
    )
    df = df[df["person_uid"].notna()].copy()

    # Group and aggregate â€” pick first non-empty EmployeeName and EmployeeID
    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", lambda s: _pick_first_non_guid_value(s)),
            EmployeeName=("EmployeeName", lambda s: _pick_first_non_guid_value(s)),
            CardNumber=("CardNumber", lambda s: _pick_first_non_guid_value(s)),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            empid = _pick_first_non_guid_value(g_sorted["EmployeeID"]) if "EmployeeID" in g_sorted else first.get("EmployeeID")
            ename = _pick_first_non_guid_value(g_sorted["EmployeeName"]) if "EmployeeName" in g_sorted else first.get("EmployeeName")
            cnum = _pick_first_non_guid_value(g_sorted["CardNumber"]) if "CardNumber" in g_sorted else first.get("CardNumber")
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": empid,
                "EmployeeName": ename,
                "CardNumber": cnum,
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    # # compute duration using LocaleMessageTime first/last (wall-clock)
    # grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    # grouped["Duration"] = grouped["DurationSeconds"].apply(
    #     lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    # )



        # compute duration using LocaleMessageTime first/last (wall-clock)
    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)

    # Format Duration as total hours:minutes (e.g. "02:13" or "26:05")
    def _seconds_to_hhmm(seconds_val):
        try:
            if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
                return None
            total = int(round(float(seconds_val)))
            hours = total // 3600
            minutes = (total % 3600) // 60
            # hours may be >24; keep full hours
            return f"{hours}:{minutes:02d}"
        except Exception:
            return None

    grouped["Duration"] = grouped["DurationSeconds"].apply(_seconds_to_hhmm)


    # helper: format minutes to "Hh Mm"
    def _format_minutes_to_hhmm(seconds_val):
        try:
            if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
                return None
            total_minutes = int(round(float(seconds_val) / 60.0))
            h = total_minutes // 60
            m = total_minutes % 60
            return f"{h}h {m}m"
        except Exception:
            return None

    # Duration in integer minutes (useful where UI currently shows '689 min')
    # grouped["DurationMinutes"] = grouped["DurationSeconds"].apply(lambda s: int(round(s/60)) if pd.notna(s) else None)

    # Human readable hours display, e.g. "11h 29m"
    grouped["DurationDisplay"] = grouped["DurationSeconds"].apply(_format_minutes_to_hhmm)

    # Also provide a standard H:MM:SS string
    grouped["DurationHMS"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    # ensure all output cols exist
    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    # drop helper column if present
    if "_lts_rounded" in grouped.columns:
        try:
            grouped = grouped.drop(columns=["_lts_rounded"])
        except Exception:
            pass
    if "_prev_ts" in grouped.columns:
        try:
            grouped = grouped.drop(columns=["_prev_ts"])
        except Exception:
            pass

    return grouped[out_cols]






def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    # Defensive: normalize incoming regions list
    try:
        requested_regions = [r.lower() for r in (regions or []) if r]
    except Exception:
        requested_regions = []

    # If user provided a city/site, try to map that city to one or more region keys
    def _normalize_token(s: str) -> str:
        return re.sub(r'[^a-z0-9]', '', str(s or '').strip().lower())

    if city:
        city_raw = str(city).strip()
        city_norm = _normalize_token(city_raw)

        # find regions whose partitions or logical_like look like this city
        matched_regions = []
        for rkey, rc in (REGION_CONFIG or {}).items():
            parts = rc.get("partitions", []) or []
            likes = rc.get("logical_like", []) or []
            tokens = set()
            for p in parts:
                if not p:
                    continue
                tokens.add(_normalize_token(p))
                # also split on punctuation/dot and add parts (e.g. "LT.Vilnius" -> "vilnius")
                for part_piece in re.split(r'[.\-/\s]', str(p)):
                    if part_piece:
                        tokens.add(_normalize_token(part_piece))
            for lk in likes:
                tokens.add(_normalize_token(lk))
            # also include server/database names as a fallback
            if city_norm and city_norm in tokens:
                matched_regions.append(rkey)

        # If we could map city to specific region(s), only run those
        if matched_regions:
            requested_regions = [m for m in matched_regions]

    # fallback to all regions in config if none requested
    try:
        if not requested_regions:
            requested_regions = [k.lower() for k in list(REGION_CONFIG.keys())]
    except Exception:
        requested_regions = ['apac']

    results: Dict[str, Any] = {}
    for r in requested_regions:
        if not r:
            continue
        rkey = r.lower()
        if rkey not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", rkey, target_date)
        try:
            swipes = fetch_swipes_for_region(rkey, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", rkey)
            swipes = pd.DataFrame()

        # If a city was requested, apply a strict (but defensive) city filter
        if city and not swipes.empty:
            city_raw = str(city).strip()
            city_norm = _normalize_token(city_raw)

            alt_tokens = set()
            alt_tokens.add(city_raw)
            alt_tokens.add(city_raw.replace('-', ' '))
            alt_tokens.add(city_raw.replace('_', ' '))
            alt_tokens.add(city_raw.replace('.', ' '))
            alt_tokens.add(city_raw.replace(' ', '-'))
            alt_tokens.update({t.title() for t in list(alt_tokens)})
            if city_norm:
                alt_tokens.add(city_norm)

            def _norm_for_cmp(s):
                try:
                    if s is None:
                        return ''
                    return re.sub(r'[^a-z0-9]', '', str(s).strip().lower())
                except Exception:
                    return ''

            # precompute normalized columns (safe defaults)
            try:
                part_norm = swipes["PartitionName2"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "PartitionName2" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
                door_norm = swipes["Door"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "Door" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
                pl_norm = swipes["PrimaryLocation"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "PrimaryLocation" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
            except Exception:
                part_norm = pd.Series([''] * len(swipes), index=swipes.index)
                door_norm = pd.Series([''] * len(swipes), index=swipes.index)
                pl_norm = pd.Series([''] * len(swipes), index=swipes.index)

            # Build mask: start False
            mask = pd.Series(False, index=swipes.index)



            # 1) Strict: prefer PartitionName2 exact matches (canonical names from normalize)
            for t in alt_tokens:
                t_norm = _norm_for_cmp(t)
                if not t_norm:
                    continue
                try:
                    mask = mask | (part_norm == t_norm)
                except Exception:
                    continue

            # 2) For rows where PartitionName2 is empty / unknown, allow Door token contains match (permissive)
            try:
                no_part_mask = part_norm.fillna('').astype(str) == ''
                if no_part_mask.any():
                    door_mask = pd.Series(False, index=swipes.index)
                    for t in alt_tokens:
                        t_norm = _norm_for_cmp(t)
                        if not t_norm:
                            continue
                        door_mask = door_mask | door_norm.str.contains(t_norm, na=False)
                    # only accept door matches when partition is empty
                    mask = mask | (door_mask & no_part_mask)
            except Exception:
                logging.debug("Door-based fallback match failed for city filter in region %s", rkey)

            # 3) Allow PrimaryLocation match ONLY for rows with empty PartitionName2 and not already matched
            try:
                remaining_mask = ~mask
                pl_mask = pd.Series(False, index=swipes.index)
                for t in alt_tokens:
                    t_norm = _norm_for_cmp(t)
                    if not t_norm:
                        continue
                    pl_mask = pl_mask | pl_norm.str.contains(t_norm, na=False)
                mask = mask | (pl_mask & (part_norm.fillna('') == '') & remaining_mask)
            except Exception:
                logging.debug("PrimaryLocation fallback match failed for city filter in region %s", rkey)



            # --- Special-case matching for MY.Kuala Lumpur requested by user ---
            # Many KL door names are like "APAC_MY_KL_MAIN ENTRANCE DOOR" which normalize to
            # 'apacmyklmainentrancedoor' and won't contain 'mykualalumpur' as a contiguous token.
            # So when user asked for MY.Kuala Lumpur, accept any door that contains APAC + MY + (KL | KUALA | KUALALUMPUR).
            try:
                # city_norm is already normalized (non-alphanum removed). compare against expected KL token
                if city_norm in ("mykualalumpur", "mykuala", "kualalumpur", "kuala"):
                    kl_mask = (
                        door_norm.str.contains("apac", na=False)
                        & door_norm.str.contains("my", na=False)
                        & (
                            door_norm.str.contains("kl", na=False)
                            | door_norm.str.contains("kuala", na=False)
                            | door_norm.str.contains("kualalumpur", na=False)
                        )
                    )
                    # Accept KL door matches even if PartitionName2 is empty (safe & strict)
                    mask = mask | kl_mask
            except Exception:
                logging.debug("KL special-case matching failed for city filter")



            # 4) Finally, also check Door and EmployeeName contains across all rows (additional permissive matches)
            try:
                for col in ("Door", "EmployeeName"):
                    if col in swipes.columns:
                        col_norm = swipes[col].fillna("").astype(str).str.lower().apply(_norm_for_cmp)
                        col_mask = pd.Series(False, index=swipes.index)
                        for t in alt_tokens:
                            t_norm = _norm_for_cmp(t)
                            if not t_norm:
                                continue
                            col_mask = col_mask | col_norm.str.contains(t_norm, na=False)
                        mask = mask | col_mask
            except Exception:
                logging.debug("Door/EmployeeName contains-match fallback failed for city filter in region %s", rkey)

            # Apply the mask strictly
            before = len(swipes)
            swipes = swipes[mask].copy()
            logging.info("City filter '%s' applied for region %s: rows before=%d after=%d", city_raw, rkey, before, len(swipes))

        # compute durations for this region
        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", rkey)
            durations = pd.DataFrame()

        # frontend-friendly display columns for swipes
        try:
            if "LocaleMessageTime" in swipes.columns:
                swipes["LocaleMessageTime"] = pd.to_datetime(swipes["LocaleMessageTime"], errors="coerce")
                swipes["DateOnly"] = swipes["LocaleMessageTime"].dt.date
                swipes["Time"] = swipes["LocaleMessageTime"].dt.strftime("%H:%M:%S")
            else:
                if "Date" in swipes.columns and "Time" in swipes.columns:
                    swipes["DateOnly"] = swipes["Date"]
        except Exception:
            logging.debug("Frontend display enrichment failed for region %s", rkey)

        if "AdjustedMessageTime" not in swipes.columns:
            swipes["AdjustedMessageTime"] = pd.NaT

        # write outputs
        try:
            csv_path = outdir_path / f"{rkey}_duration_{target_date.strftime('%Y%m%d')}.csv"
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", rkey)
        try:
            swipes_csv_path = outdir_path / f"{rkey}_swipes_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", rkey)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", rkey, csv_path if 'csv_path' in locals() else '<unknown>', len(durations) if durations is not None else 0)
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", rkey, swipes_csv_path if 'swipes_csv_path' in locals() else '<unknown>', len(swipes) if swipes is not None else 0)

        results[rkey] = {"swipes": swipes, "durations": durations}

    return results


# end of file







