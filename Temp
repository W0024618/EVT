(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python app.py
WARNING:root:Historical profile file current_analysis.csv not found in candidate locations.
 * Serving Flask app 'app'
 * Debug mode: on
INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8002
 * Running on http://10.199.47.235:8002
INFO:werkzeug:Press CTRL+C to quit
INFO:werkzeug: * Restarting with stat
WARNING:root:Historical profile file current_analysis.csv not found in candidate locations.
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 134-209-644
ERROR:root:Failed fetching swipes for region apac
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 898, in run_for_date
    swipes = fetch_swipes_for_region(r, target_date)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 712, in fetch_swipes_for_region    
    sql = build_region_query(region_key, target_date)
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py", line 678, in build_region_query
    candidates = _get_candidate_databases(rc)
                 ^^^^^^^^^^^^^^^^^^^^^^^^
NameError: name '_get_candidate_databases' is not defined
WARNING:root:run_trend_for_date: no features computed
INFO:werkzeug:127.0.0.1 - - [11/Nov/2025 11:51:23] "GET /run?start=2025-11-07&end=2025-11-07&full=true HTTP/1.1" 200 -





When i Update duration file we got below error so check both file carefully and fix the issue and share me Fully updated file so i can 
easily swap file each other...





# # backend/duration_report.py
# from __future__ import annotations

# import logging
# import os
# import re
# import warnings
# from datetime import date, datetime, timedelta
# from pathlib import Path
# from typing import Optional, List, Dict, Any

# import pandas as pd

# try:
#     import pyodbc
# except Exception:
#     pyodbc = None

# # ODBC driver name (override with environment variable if needed)
# ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# # Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
# try:
#     from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
# except Exception:
#     # fallback — keep behaviour if config file unavailable
#     BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
#     OUT_OF_OFFICE_ZONE = "Out of office"

#     def map_door_to_zone(door: object, direction: object = None) -> str:
#         """
#         Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
#         (This is only used if config.door_zone can't be imported.)
#         """
#         try:
#             if door is None:
#                 return None
#             s = str(door).strip()
#             if not s:
#                 return None
#             s_l = s.lower()
#             # fallback: direction-based inference
#             if direction and isinstance(direction, str):
#                 d = direction.strip().lower()
#                 if "out" in d:
#                     return OUT_OF_OFFICE_ZONE
#                 if "in" in d:
#                     # assume reception/working
#                     return "Reception Area"
#             # heuristic fallback
#             if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
#                 return OUT_OF_OFFICE_ZONE
#             # else treat as working area
#             return "Working Area"
#         except Exception:
#             return None

# # REGION configuration - databases list used to build UNION queries
# REGION_CONFIG = {
#     "apac": {
#         "user": "GSOC_Test",
#         "password": "Westernccuredb@2026",
#         "server": "SRVWUPNQ0986V",
#         "databases": [
#             "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
#             "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
#         ],
#         "partitions": ["APAC.Default", "SG.Singapore","JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur", "IN.HYD"]
#     },
#     "emea": {
#         "user": "GSOC_Test",
#         "password": "Westernccuredb@2026",
#         "server": "SRVWUFRA0986V",
#         "databases": [
#             "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
#             "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
#             "ACVSUJournal_00011023"
#         ],
#         "partitions": ["LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
#     },
#     "laca": {
#         "user": "GSOC_Test",
#         "password": "Westernccuredb@2026",
#         "server": "SRVWUSJO0986V",
#         "databases": [
#             "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
#             "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
#         ],
#         "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City"]
#     },
#     "namer": {
#         "user": "GSOC_Test",
#         "password": "Westernccuredb@2026",
#         "server": "SRVWUDEN0891V",
#         "databases": [
#             "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
#             "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
#         ],
#         "partitions": ["Denver", "Austin Texas", "Miami", "New York"],
#         "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
#     }
# }



# GENERIC_SQL_TEMPLATE = r"""
# SELECT
#     t1.[ObjectName1] AS EmployeeName,
#     t1.[ObjectName2] AS Door,
#     -- prefer contractor text12 else Int1 (match original logic)
#     CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
#     t2.[Int1] AS Int1,
#     t2.[Text12] AS Text12,
#     -- expose raw XML & shredded value so downstream code can attempt robust extraction
#     t_xml.XmlMessage AS XmlMessage,
#     sc.value AS XmlShredValue,
#     -- CardNumber: try CHUID/Card inside XmlMessage, fallback to shredded value, then text12/int1
#     COALESCE(
#       TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
#       TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
#       sc.value,
#       NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
#       t2.[Text12]
#     ) AS CardNumber,
#     t3.[Name] AS PersonnelTypeName,
#     t1.ObjectIdentity1 AS EmployeeIdentity,
#     t1.PartitionName2,
#     DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
#     t1.MessageType,
#     t5d.value AS Direction,
#     t2.Text4 AS CompanyName,
#     t2.Text5 AS PrimaryLocation
# FROM [{db}].dbo.ACVSUJournalLog AS t1
# LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
# LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
# LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
#   ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
# LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
#   ON t1.XmlGUID = t_xml.GUID
# LEFT JOIN (
#   SELECT GUID, value
#   FROM [{db}].dbo.ACVSUJournalLogxmlShred
#   WHERE Name IN ('Card','CHUID')
# ) AS sc
#   ON t1.XmlGUID = sc.GUID
# WHERE t1.MessageType = 'CardAdmitted'
#   AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
#   {region_filter}
# """
# # Helpers
# def _split_db_name(dbname: str):
#     m = re.match(r"^(.*?)(\d+)$", dbname)
#     if not m:
#         return dbname, None
#     return m.group(1), m.group(2)

# def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
#     prefix, digits = _split_db_name(db_base)
#     if digits is None:
#         return [db_base]
#     width = len(digits)
#     try:
#         cur = int(digits)
#     except Exception:
#         return [db_base]
#     out = []
#     for i in range(last_n):
#         num = cur - i
#         if num < 0:
#             break
#         out.append(f"{prefix}{str(num).zfill(width)}")
#     return out

# def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
#     if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
#         return rc["databases"]
#     base_db = rc.get("database")
#     if not base_db:
#         return []
#     last_n = int(rc.get("last_n_databases", 1) or 1)
#     if last_n <= 1:
#         return [base_db]
#     return _expand_databases_from_base(base_db, last_n)

# def _connect_master(rc: Dict[str, Any]):
#     if pyodbc is None:
#         logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
#         return None
#     try:
#         conn_str = (
#             f"DRIVER={{{ODBC_DRIVER}}};"
#             f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
#             "TrustServerCertificate=Yes;"
#         )
#         return pyodbc.connect(conn_str, autocommit=True)
#     except Exception:
#         logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
#         return None

# def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
#     if not candidates:
#         return []
#     master_conn = _connect_master(rc)
#     if master_conn is None:
#         logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
#         return candidates
#     try:
#         exists = []
#         cursor = master_conn.cursor()
#         for db in candidates:
#             try:
#                 cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
#                 row = cursor.fetchone()
#                 if row and row[0] and int(row[0]) > 0:
#                     exists.append(db)
#             except Exception:
#                 logging.exception("Error checking existence for database %s", db)
#         cursor.close()
#         logging.info("Databases present for server %s: %s", rc.get("server"), exists)
#         return exists if exists else candidates
#     finally:
#         try:
#             master_conn.close()
#         except Exception:
#             pass

# def build_region_query(region_key: str, target_date: date) -> str:
#     rc = REGION_CONFIG[region_key]
#     date_str = target_date.strftime("%Y-%m-%d")
#     region_filter = ""

#     if region_key in ("apac", "emea", "laca"):
#         partitions = rc.get("partitions", [])
#         parts_sql = ", ".join(f"'{p}'" for p in partitions)
#         region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
#     elif region_key == "namer":
#         likes = rc.get("logical_like", [])
#         like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
#         region_filter = f"AND ({like_sql})"
#     else:
#         region_filter = ""

#     candidates = _get_candidate_databases(rc)
#     if not candidates:
#         candidates = [rc.get("database")]

#     valid_dbs = _filter_existing_databases(rc, candidates)

#     union_parts = []
#     for dbname in valid_dbs:
#         union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter))

#     if not union_parts:
#         dbname = rc.get("database")
#         return GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter)

#     sql = "\nUNION ALL\n".join(union_parts)
#     return sql
    
# # DB connection & fetch
# def get_connection(region_key: str):
#     if pyodbc is None:
#         raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

#     rc = REGION_CONFIG[region_key]
#     # use first database in list if present
#     db = rc.get("databases", [rc.get("database")])[0]
#     conn_str = (
#         f"DRIVER={{{ODBC_DRIVER}}};"
#         f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
#         "TrustServerCertificate=Yes;"
#     )
#     return pyodbc.connect(conn_str, autocommit=True)

# def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
#     sql = build_region_query(region_key, target_date)
#     logging.info("Built SQL for region %s, date %s", region_key, target_date)
#     cols = [
#     "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
#     "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "MessageType",
#     "Direction", "CompanyName", "PrimaryLocation"
# ]

#     if pyodbc is None:
#         logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
#         return pd.DataFrame(columns=cols)

#     conn = None
#     try:
#         conn = get_connection(region_key)
#         with warnings.catch_warnings():
#             warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
#             df = pd.read_sql(sql, conn)
#     except Exception:
#         logging.exception("Failed to run query for region %s", region_key)
#         df = pd.DataFrame(columns=cols)
#     finally:
#         try:
#             if conn is not None:
#                 conn.close()
#         except Exception:
#             pass

#     for c in cols:
#         if c not in df.columns:
#             df[c] = None

#     try:
#         df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
#     except Exception:
#         df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

#     # maintain person_uid same as compute logic
#     def make_person_uid(row):
#         eid = row.get("EmployeeIdentity")
#         if pd.notna(eid) and str(eid).strip() != "":
#             return str(eid).strip()
#         pieces = [
#             (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
#             (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
#             (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
#         ]
#         joined = "|".join([p for p in pieces if p])
#         return joined or None

#     if not df.empty:
#         df['person_uid'] = df.apply(make_person_uid, axis=1)

#     return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]

# # compute durations (unchanged largely)
# def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
#     out_cols = [
#         "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
#         "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
#         "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
#         "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
#     ]

#     if swipes_df is None or swipes_df.empty:
#         return pd.DataFrame(columns=out_cols)

#     df = swipes_df.copy()
#     expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
#                 "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
#     for col in expected:
#         if col not in df.columns:
#             df[col] = None

#     if df["LocaleMessageTime"].dtype == object:
#         df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

#     dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
#     df = df.drop_duplicates(subset=dedupe_cols, keep="first")

#     df["Date"] = df["LocaleMessageTime"].dt.date

#     df["person_uid"] = df.apply(
#         lambda row: row["person_uid"]
#         if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
#         else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
#         axis=1
#     )
#     df = df[df["person_uid"].notna()].copy()

#     try:
#         df = df.sort_values("LocaleMessageTime")
#         grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
#             FirstSwipe=("LocaleMessageTime", "first"),
#             LastSwipe=("LocaleMessageTime", "last"),
#             FirstDoor=("Door", "first"),
#             LastDoor=("Door", "last"),
#             CountSwipes=("LocaleMessageTime", "count"),
#             EmployeeIdentity=("EmployeeIdentity", "first"),
#             EmployeeID=("EmployeeID", "first"),
#             EmployeeName=("EmployeeName", "first"),
#             CardNumber=("CardNumber", "first"),
#             PersonnelTypeName=("PersonnelTypeName", "first"),
#             PartitionName2=("PartitionName2", "first"),
#             CompanyName=("CompanyName", "first"),
#             PrimaryLocation=("PrimaryLocation", "first"),
#             FirstDirection=("Direction", "first"),
#             LastDirection=("Direction", "last")
#         ).reset_index()
#     except Exception:
#         def agg_for_group(g):
#             g_sorted = g.sort_values("LocaleMessageTime")
#             first = g_sorted.iloc[0]
#             last = g_sorted.iloc[-1]
#             return pd.Series({
#                 "person_uid": first["person_uid"],
#                 "EmployeeIdentity": first.get("EmployeeIdentity"),
#                 "EmployeeID": first.get("EmployeeID"),
#                 "EmployeeName": first.get("EmployeeName"),
#                 "CardNumber": first.get("CardNumber"),
#                 "Date": first["Date"],
#                 "FirstSwipe": first["LocaleMessageTime"],
#                 "LastSwipe": last["LocaleMessageTime"],
#                 "FirstDoor": first.get("Door"),
#                 "LastDoor": last.get("Door"),
#                 "CountSwipes": int(len(g_sorted)),
#                 "PersonnelTypeName": first.get("PersonnelTypeName"),
#                 "PartitionName2": first.get("PartitionName2"),
#                 "CompanyName": first.get("CompanyName"),
#                 "PrimaryLocation": first.get("PrimaryLocation"),
#                 "FirstDirection": first.get("Direction"),
#                 "LastDirection": last.get("Direction")
#             })
#         grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

#     grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
#     grouped["Duration"] = grouped["DurationSeconds"].apply(
#         lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
#     )

#     for c in out_cols:
#         if c not in grouped.columns:
#             grouped[c] = None

#     return grouped[out_cols]

# # runner helper: run_for_date
# def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
#     outdir_path = Path(outdir)
#     outdir_path.mkdir(parents=True, exist_ok=True)

#     results: Dict[str, Any] = {}
#     for r in regions:
#         r = r.lower()
#         if r not in REGION_CONFIG:
#             logging.warning("Unknown region '%s' - skipping", r)
#             continue
#         logging.info("Fetching swipes for region %s on %s", r, target_date)
#         try:
#             swipes = fetch_swipes_for_region(r, target_date)
#         except Exception:
#             logging.exception("Failed fetching swipes for region %s", r)
#             swipes = pd.DataFrame()

#         # optional city filter (defensive)
#         if city and not swipes.empty:
#             city_l = str(city).strip().lower()
#             mask_parts = []
#             for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
#                 if col in swipes.columns:
#                     # normalize to string and test contains
#                     mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
#             if mask_parts:
#                 combined_mask = mask_parts[0]
#                 for m in mask_parts[1:]:
#                     combined_mask = combined_mask | m
#                 before = len(swipes)
#                 swipes = swipes[combined_mask].copy()
#                 logging.info("City filter '%s' applied: rows before=%d after=%d", city, before, len(swipes))
#             else:
#                 # fallback: we expected a location column but none present - skip city filter
#                 logging.warning("City filter requested (%s) but no location columns present in swipes; skipping city filter", city)


#         try:
#             durations = compute_daily_durations(swipes)
#         except Exception:
#             logging.exception("Failed computing durations for region %s", r)
#             durations = pd.DataFrame()

#         csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
#         swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
#         try:
#             durations.to_csv(csv_path, index=False)
#         except Exception:
#             logging.exception("Failed writing durations CSV for %s", r)
#         try:
#             swipes.to_csv(swipes_csv_path, index=False)
#         except Exception:
#             logging.exception("Failed writing swipes CSV for %s", r)

#         logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
#         logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
#         results[r] = {"swipes": swipes, "durations": durations}

#     return results
















#C:\Users\W0024618\Desktop\Trend Analysis\backend\duration_report.py

from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name (override with environment variable if needed)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
try:
    from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    # fallback — keep behaviour if config file unavailable
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

    def map_door_to_zone(door: object, direction: object = None) -> str:
        """
        Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
        (This is only used if config.door_zone can't be imported.)
        """
        try:
            if door is None:
                return None
            s = str(door).strip()
            if not s:
                return None
            s_l = s.lower()
            # fallback: direction-based inference
            if direction and isinstance(direction, str):
                d = direction.strip().lower()
                if "out" in d:
                    return OUT_OF_OFFICE_ZONE
                if "in" in d:
                    # assume reception/working
                    return "Reception Area"
            # heuristic fallback
            if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
                return OUT_OF_OFFICE_ZONE
            # else treat as working area
            return "Working Area"
        except Exception:
            return None

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "SG.Singapore","JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur", "IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023"
        ],
        "partitions": ["LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["Denver", "Austin Texas", "Miami", "New York"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}


# --- Note: Added AdjustedMessageTime into the generic template, date condition templated as {date_condition}
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  {date_condition}
  {region_filter}
"""

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    # Build date_condition:
    # - For APAC we must fetch LocaleMessageTime rows for target_date AND target_date + 1 day
    #   because AdjustedMessageTime = LocaleMessageTime - 2 hours; some swipes near midnight on next day
    #   belong to the shifted date group.
    if region_key == "apac":
        next_date_str = (target_date + timedelta(days=1)).strftime("%Y-%m-%d")
        date_condition = (
            "AND (CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d1}' "
            "OR CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d2}')"
            .format(d1=date_str, d2=next_date_str)
        )
    else:
        date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

    
# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    # use first database in list if present
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
    "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
    "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
    "Direction", "CompanyName", "PrimaryLocation"
]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    for c in cols:
        if c not in df.columns:
            df[c] = None

    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # NEW: convert AdjustedMessageTime (may be NULL for non-Pune rows)
    try:
        if "AdjustedMessageTime" in df.columns:
            df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
        else:
            df["AdjustedMessageTime"] = pd.NaT
    except Exception:
        df["AdjustedMessageTime"] = pd.to_datetime(df.get("AdjustedMessageTime").astype(str), errors="coerce") if "AdjustedMessageTime" in df.columns else pd.NaT

    # maintain person_uid same as compute logic
    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]

# compute durations (unchanged largely) but Date assignment updated to use AdjustedMessageTime for APAC.Default
def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "AdjustedMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    if df["LocaleMessageTime"].dtype == object:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    if "AdjustedMessageTime" in df.columns and df["AdjustedMessageTime"].dtype == object:
        df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")

    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    df = df.drop_duplicates(subset=dedupe_cols, keep="first")

    # Date assignment:
    # - For Pune (PartitionName2 == 'APAC.Default') and if AdjustedMessageTime exists, use adjusted date (shifted date)
    # - Otherwise use LocaleMessageTime.date()
    try:
        # default to LocaleMessageTime date
        df["Date"] = df["LocaleMessageTime"].dt.date
        # mask for APAC.Default rows with valid AdjustedMessageTime
        mask = (df.get("PartitionName2") == "APAC.Default") & (pd.notna(df.get("AdjustedMessageTime")))
        if mask.any():
            df.loc[mask, "Date"] = df.loc[mask, "AdjustedMessageTime"].dt.date
    except Exception:
        # fallback: naive conversion
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    df["person_uid"] = df.apply(
        lambda row: row["person_uid"]
        if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
        else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
        axis=1
    )
    df = df[df["person_uid"].notna()].copy()

    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", "first"),
            EmployeeName=("EmployeeName", "first"),
            CardNumber=("CardNumber", "first"),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": first.get("EmployeeID"),
                "EmployeeName": first.get("EmployeeName"),
                "CardNumber": first.get("CardNumber"),
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(
        lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    )

    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    return grouped[out_cols]

# runner helper: run_for_date
def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        try:
            swipes = fetch_swipes_for_region(r, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", r)
            swipes = pd.DataFrame()

        # optional city filter (defensive)
        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    # normalize to string and test contains
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                before = len(swipes)
                swipes = swipes[combined_mask].copy()
                logging.info("City filter '%s' applied: rows before=%d after=%d", city, before, len(swipes))
            else:
                # fallback: we expected a location column but none present - skip city filter
                logging.warning("City filter requested (%s) but no location columns present in swipes; skipping city filter", city)


        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", r)
            durations = pd.DataFrame()

        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
        try:
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", r)
        try:
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", r)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
        results[r] = {"swipes": swipes, "durations": durations}

    return results



