def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "DurationMinutes", "DurationDisplay", "DurationHMS",
        "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes if present
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # drop near-duplicate swipes (round to seconds)
    try:
        df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
        dedupe_subset = ["person_uid", "_lts_rounded", "CardNumber", "Door"]
        df = df.drop_duplicates(subset=dedupe_subset, keep="first").copy()
    except Exception:
        dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
        try:
            df = df.drop_duplicates(subset=dedupe_cols, keep="first")
        except Exception:
            pass

    # Date assignment (STRICT: only use LocaleMessageTime.date())
    try:
        df["Date"] = df["LocaleMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure person_uid (keep existing canonicalization where present)
    df["person_uid"] = df.apply(
        lambda row: row.get("person_uid")
        if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
        else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
        axis=1
    )
    df = df[df["person_uid"].notna()].copy()

    # Group and aggregate — pick first non-empty EmployeeName and EmployeeID
    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", lambda s: _pick_first_non_guid_value(s)),
            EmployeeName=("EmployeeName", lambda s: _pick_first_non_guid_value(s)),
            CardNumber=("CardNumber", lambda s: _pick_first_non_guid_value(s)),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            empid = _pick_first_non_guid_value(g_sorted["EmployeeID"]) if "EmployeeID" in g_sorted else first.get("EmployeeID")
            ename = _pick_first_non_guid_value(g_sorted["EmployeeName"]) if "EmployeeName" in g_sorted else first.get("EmployeeName")
            cnum = _pick_first_non_guid_value(g_sorted["CardNumber"]) if "CardNumber" in g_sorted else first.get("CardNumber")
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": empid,
                "EmployeeName": ename,
                "CardNumber": cnum,
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    # compute duration using LocaleMessageTime first/last (wall-clock)
    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(
        lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    )

    # helper: format minutes to "Hh Mm"
    def _format_minutes_to_hhmm(seconds_val):
        try:
            if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
                return None
            total_minutes = int(round(float(seconds_val) / 60.0))
            h = total_minutes // 60
            m = total_minutes % 60
            return f"{h}h {m}m"
        except Exception:
            return None

    # Duration in integer minutes (useful where UI currently shows '689 min')
    grouped["DurationMinutes"] = grouped["DurationSeconds"].apply(lambda s: int(round(s/60)) if pd.notna(s) else None)

    # Human readable hours display, e.g. "11h 29m"
    grouped["DurationDisplay"] = grouped["DurationSeconds"].apply(_format_minutes_to_hhmm)

    # Also provide a standard H:MM:SS string
    grouped["DurationHMS"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    # ensure all output cols exist
    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    # drop helper column if present
    if "_lts_rounded" in grouped.columns:
        try:
            grouped = grouped.drop(columns=["_lts_rounded"])
        except Exception:
            pass
    if "_prev_ts" in grouped.columns:
        try:
            grouped = grouped.drop(columns=["_prev_ts"])
        except Exception:
            pass

    return grouped[out_cols]















SyntaxError: expected 'except' or 'finally' block
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python app.py
>>
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 2319
    if tcol and tcol in filtered.columns:
SyntaxError: expected 'except' or 'finally' block
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> 



Now We got indentation error 



Fix Duration error 





# backend/duration_report.py
from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np
import hashlib

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name (override with environment variable if needed)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
try:
    from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    # fallback — keep behaviour if config file unavailable
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

    def map_door_to_zone(door: object, direction: object = None) -> str:
        """
        Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
        """
        try:
            if door is None:
                return None
            s = str(door).strip()
            if not s:
                return None
            s_l = s.lower()
            if direction and isinstance(direction, str):
                d = direction.strip().lower()
                if "out" in d:
                    return OUT_OF_OFFICE_ZONE
                if "in" in d:
                    return "Reception Area"
            if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
                return OUT_OF_OFFICE_ZONE
            return "Working Area"
        except Exception:
            return None

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "SG.Singapore", "PH.Manila","IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023"
        ],
        "partitions": ["LT.Vilnius","IT.Rome","UK.London","IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition","PA.Panama City","PE.Lima", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["US.CO.OBS", "USA/Canada Default", "US.FL.Miami", "US.NYC"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# GENERIC SQL template - we keep AdjustedMessageTime in SELECT for debugging but the Python
# logic in this file no longer depends on it for date assignment.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND t3.[Name] = 'Employee'
  {date_condition}
  {region_filter}
"""

# Helpers
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

# GUID / placeholders helpers
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _looks_like_guid(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
        except Exception:
            continue
        if not s:
            continue
        if _is_placeholder_str(s):
            continue
        if _looks_like_guid(s):
            continue
        return s
    return None

def _canonical_person_uid_from_row(row):
    """
    Produce canonical person_uid in the form:
      - 'emp:<employeeid>' (if a sensible non-GUID EmployeeID present),
      - 'uid:<EmployeeIdentity>' (if present),
      - 'name:<sha1 10chars>' fallback when name present,
      - otherwise None.
    """
    empid = None
    for cand in ('EmployeeID', 'Int1', 'Text12'):
        if cand in row and row.get(cand) not in (None, '', float('nan')):
            empid = row.get(cand)
            break
    empident = row.get("EmployeeIdentity", None)
    name = row.get("EmployeeName", None)

    # normalize empid numeric floats -> ints
    if empid is not None:
        try:
            s = str(empid).strip()
            if '.' in s:
                f = float(s)
                if f.is_integer():
                    s = str(int(f))
            if s and not _looks_like_guid(s) and not _is_placeholder_str(s):
                return f"emp:{s}"
        except Exception:
            pass

    if empident not in (None, '', float('nan')):
        try:
            si = str(empident).strip()
            if si:
                return f"uid:{si}"
        except Exception:
            pass

    if name not in (None, '', float('nan')):
        try:
            sn = str(name).strip()
            if sn and not _looks_like_guid(sn) and not _is_placeholder_str(sn):
                h = hashlib.sha1(sn.lower().encode('utf8')).hexdigest()[:10]
                return f"name:{h}"
        except Exception:
            pass

    return None

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        partitions = rc.get("partitions", [])
        if partitions:
            parts_sql = ", ".join(f"'{p}'" for p in partitions)
            region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
        else:
            likes = rc.get("logical_like", [])
            if likes:
                like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
                region_filter = f"AND ({like_sql})"
            else:
                region_filter = ""
    else:
        region_filter = ""

    # NOTE: AdjustedMessageTime / 2AM boundary logic removed from date selection.
    date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
        "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
        "Direction", "CompanyName", "PrimaryLocation"
    ]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    # ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Dates parsing
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # Keep AdjustedMessageTime if present for debugging but we do NOT use it for date boundaries anymore.
    try:
        if "AdjustedMessageTime" in df.columns:
            df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
        else:
            df["AdjustedMessageTime"] = pd.NaT
    except Exception:
        df["AdjustedMessageTime"] = pd.NaT

    # defensive: make text fields strings (avoid object type surprises)
    for tcol in ("Door", "PartitionName2", "PersonnelTypeName", "EmployeeName", "CompanyName", "PrimaryLocation"):
        if tcol in df.columns:
            df[tcol] = df[tcol].fillna("").astype(str)

    # Filter: only Employees (defensive; the SQL template already requests t3.Name = 'Employee')
    try:
        if "PersonnelTypeName" in df.columns:
            df = df[df["PersonnelTypeName"].str.strip().str.lower() == "employee"].copy()
    except Exception:
        logging.debug("Could not apply PersonnelTypeName filter for region %s", region_key)

    # canonical person_uid (consistent with trend_runner)
    def make_person_uid(row):
        try:
            return _canonical_person_uid_from_row(row)
        except Exception:
            # fallback: try simple concatenation if canonical fails
            try:
                eid = row.get("EmployeeIdentity")
                if pd.notna(eid) and str(eid).strip() != "":
                    return str(eid).strip()
            except Exception:
                pass
            parts = []
            for c in ("EmployeeID", "CardNumber", "EmployeeName"):
                try:
                    v = row.get(c)
                    if v not in (None, '', float('nan')):
                        parts.append(str(v).strip())
                except Exception:
                    continue
            return "|".join(parts) if parts else None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)

    # APAC partition normalization (light touch)
    if region_key == "apac" and not df.empty:
        def normalize_apac_partition(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part or ""
            if "APAC_PI" in d or d.startswith("APAC_PI"):
                return "Taguig City"
            if "APAC_PH" in d or d.startswith("APAC_PH"):
                return "Quezon City"
            if "PUN" in d or "PUNE" in d or ("APAC.DEFAULT" in p.upper() or p.upper().strip() == "APAC.DEFAULT"):
                return "Pune"
            if "APAC_MY" in d or "MY.KUALA" in p.upper() or "KUALA" in d:
                return "MY.Kuala Lumpur"
            if "IN.HYD" in p.upper() or "HYD" in d:
                return "IN.HYD"
            if "SG.SINGAPORE" in p or "SINGAPORE" in d:
                return "SG.Singapore"
            return part
        df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)

    # NAMER: normalize PartitionName2 and add LogicalLocation per previous behaviour
    if region_key == "namer" and not df.empty:
        def namer_partition_and_logical(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()
            normalized = part
            logical = "Other"

            if ("US.CO.HQ" in d) or ("HQ" in d and "HQ" in d[:20]) or ("DENVER" in d) or (p == "US.CO.OBS"):
                normalized = "US.CO.OBS"
                logical = "Denver-HQ"
            elif "AUSTIN" in d or "AUSTIN TX" in d or p == "USA/CANADA DEFAULT":
                normalized = "USA/Canada Default"
                logical = "Austin Texas"
            elif "MIAMI" in d or p == "US.FL.MIAMI":
                normalized = "US.FL.Miami"
                logical = "Miami"
            elif "NYC" in d or "NEW YORK" in d or p == "US.NYC":
                normalized = "US.NYC"
                logical = "New York"
            else:
                if p == "US.CO.OBS":
                    normalized = "US.CO.OBS"; logical = "Denver-HQ"
                elif p == "USA/CANADA DEFAULT":
                    normalized = "USA/Canada Default"; logical = "Austin Texas"
                elif p == "US.FL.MIAMI":
                    normalized = "US.FL.Miami"; logical = "Miami"
                elif p == "US.NYC":
                    normalized = "US.NYC"; logical = "New York"
                else:
                    normalized = part
                    logical = "Other"
            return pd.Series({"PartitionName2": normalized, "LogicalLocation": logical})

        mapped = df.apply(namer_partition_and_logical, axis=1)
        df["PartitionName2"] = mapped["PartitionName2"].astype(str)
        df["LogicalLocation"] = mapped["LogicalLocation"].astype(str)

    # ensure PartitionName2 column exists as string
    if "PartitionName2" not in df.columns:
        df["PartitionName2"] = ""

    # ensure LogicalLocation exists
    if "LogicalLocation" not in df.columns:
        df["LogicalLocation"] = ""

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]

# ---------------------------------------------------------------------
# compute_daily_durations (single robust implementation)
# ---------------------------------------------------------------------
def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes if present
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # drop near-duplicate swipes (round to seconds)
    try:
        df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
        dedupe_subset = ["person_uid", "_lts_rounded", "CardNumber", "Door"]
        df = df.drop_duplicates(subset=dedupe_subset, keep="first").copy()
    except Exception:
        dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
        try:
            df = df.drop_duplicates(subset=dedupe_cols, keep="first")
        except Exception:
            pass

    # Date assignment (STRICT: only use LocaleMessageTime.date())
    try:
        df["Date"] = df["LocaleMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure person_uid (keep existing canonicalization where present)
    df["person_uid"] = df.apply(
        lambda row: row.get("person_uid")
        if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
        else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
        axis=1
    )
    df = df[df["person_uid"].notna()].copy()

    # Group and aggregate — pick first non-empty EmployeeName and EmployeeID
    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", lambda s: _pick_first_non_guid_value(s)),
            EmployeeName=("EmployeeName", lambda s: _pick_first_non_guid_value(s)),
            CardNumber=("CardNumber", lambda s: _pick_first_non_guid_value(s)),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            empid = _pick_first_non_guid_value(g_sorted["EmployeeID"]) if "EmployeeID" in g_sorted else first.get("EmployeeID")
            ename = _pick_first_non_guid_value(g_sorted["EmployeeName"]) if "EmployeeName" in g_sorted else first.get("EmployeeName")
            cnum = _pick_first_non_guid_value(g_sorted["CardNumber"]) if "CardNumber" in g_sorted else first.get("CardNumber")
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": empid,
                "EmployeeName": ename,
                "CardNumber": cnum,
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    # compute duration using LocaleMessageTime first/last (wall-clock)
    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(
        lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    )


# -- add friendly duration fields (minutes + human display) --
def _format_minutes_to_hhmm(s):
    try:
        if s is None or (isinstance(s, float) and np.isnan(s)):
            return None
        total_minutes = int(round(float(s) / 60.0))
        h = total_minutes // 60
        m = total_minutes % 60
        return f"{h}h {m}m"
    except Exception:
        return None

# Duration in integer minutes (useful where UI currently shows '689 min')
grouped["DurationMinutes"] = grouped["DurationSeconds"].apply(lambda s: int(round(s/60)) if pd.notna(s) else None)

# Human readable hours display, e.g. "11h 29m"
grouped["DurationDisplay"] = grouped["DurationSeconds"].apply(_format_minutes_to_hhmm)

# Keep existing Duration as hh:mm:ss (backwards-compatible)
# optionally also provide a standard H:MM:SS string
grouped["DurationHMS"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)


    # ensure all output cols exist
    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    # drop helper column if present
    if "_lts_rounded" in grouped.columns:
        try:
            grouped = grouped.drop(columns=["_lts_rounded"])
        except Exception:
            pass

    return grouped[out_cols]

# ---------------------------------------------------------------------
# run_for_date (already used by trend_runner)
# ---------------------------------------------------------------------
def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    # Defensive: normalize incoming regions list
    try:
        requested_regions = [r.lower() for r in (regions or []) if r]
    except Exception:
        requested_regions = []

    # If user provided a city/site, try to map that city to one or more region keys
    def _normalize_token(s: str) -> str:
        return re.sub(r'[^a-z0-9]', '', str(s or '').strip().lower())

    if city:
        city_raw = str(city).strip()
        city_norm = _normalize_token(city_raw)

        # find regions whose partitions or logical_like look like this city
        matched_regions = []
        for rkey, rc in (REGION_CONFIG or {}).items():
            parts = rc.get("partitions", []) or []
            likes = rc.get("logical_like", []) or []
            tokens = set()
            for p in parts:
                if not p:
                    continue
                tokens.add(_normalize_token(p))
                # also split on punctuation/dot and add parts (e.g. "LT.Vilnius" -> "vilnius")
                for part_piece in re.split(r'[.\-/\s]', str(p)):
                    if part_piece:
                        tokens.add(_normalize_token(part_piece))
            for lk in likes:
                tokens.add(_normalize_token(lk))
            # also include server/database names as a fallback
            if city_norm and city_norm in tokens:
                matched_regions.append(rkey)

        # If we could map city to specific region(s), only run those
        if matched_regions:
            requested_regions = [m for m in matched_regions]

    # fallback to all regions in config if none requested
    try:
        if not requested_regions:
            requested_regions = [k.lower() for k in list(REGION_CONFIG.keys())]
    except Exception:
        requested_regions = ['apac']

    results: Dict[str, Any] = {}
    for r in requested_regions:
        if not r:
            continue
        rkey = r.lower()
        if rkey not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", rkey, target_date)
        try:
            swipes = fetch_swipes_for_region(rkey, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", rkey)
            swipes = pd.DataFrame()

        # optional city filter (defensive)
        if city and not swipes.empty:
            city_raw = str(city).strip()
            city_norm = _normalize_token(city_raw)

            alt_tokens = set()
            alt_tokens.add(city_raw)
            alt_tokens.add(city_raw.replace('-', ' '))
            alt_tokens.add(city_raw.replace('_', ' '))
            alt_tokens.add(city_raw.replace('.', ' '))
            alt_tokens.add(city_raw.replace(' ', '-'))
            alt_tokens.update({t.title() for t in list(alt_tokens)})
            if city_norm:
                alt_tokens.add(city_norm)

            def _norm_for_cmp(s):
                try:
                    if s is None:
                        return ''
                    return re.sub(r'[^a-z0-9]', '', str(s).strip().lower())
                except Exception:
                    return ''

            mask = pd.Series(False, index=swipes.index)
            # check PartitionName2 (exact / normalized)
            if "PartitionName2" in swipes.columns:
                try:
                    part_norm = swipes["PartitionName2"].fillna("").astype(str).str.lower().apply(_norm_for_cmp)
                    for t in alt_tokens:
                        t_norm = _norm_for_cmp(t)
                        if not t_norm:
                            continue
                        mask = mask | (part_norm == t_norm)
                except Exception:
                    logging.debug("PartitionName2 normalization failed for city filter in region %s", rkey)

            # check PrimaryLocation with contains
            if "PrimaryLocation" in swipes.columns:
                try:
                    pl_norm = swipes["PrimaryLocation"].fillna("").astype(str).str.lower().apply(_norm_for_cmp)
                    for t in alt_tokens:
                        t_norm = _norm_for_cmp(t)
                        if not t_norm:
                            continue
                        mask = mask | pl_norm.str.contains(t_norm, na=False)
                except Exception:
                    logging.debug("PrimaryLocation normalization failed for city filter in region %s", rkey)

            # check Door and EmployeeName columns (contains match)
            for col in ("Door", "EmployeeName"):
                if col in swipes.columns:
                    try:
                        col_norm = swipes[col].fillna("").astype(str).str.lower().apply(_norm_for_cmp)
                        for t in alt_tokens:
                            t_norm = _norm_for_cmp(t)
                            if not t_norm:
                                continue
                            mask = mask | col_norm.str.contains(t_norm, na=False)
                    except Exception:
                        continue

            if mask.any():
                before = len(swipes)
                swipes = swipes[mask].copy()
                logging.info("City filter '%s' applied for region %s: rows before=%d after=%d", city_raw, rkey, before, len(swipes))
            else:
                logging.info("City filter '%s' applied for region %s: rows before=%d after=%d (no matches)", city_raw, rkey, len(swipes), len(swipes))

        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", rkey)
            durations = pd.DataFrame()

        # frontend-friendly display columns for swipes
        try:
            if "LocaleMessageTime" in swipes.columns:
                swipes["LocaleMessageTime"] = pd.to_datetime(swipes["LocaleMessageTime"], errors="coerce")
                swipes["DateOnly"] = swipes["LocaleMessageTime"].dt.date
                swipes["Time"] = swipes["LocaleMessageTime"].dt.strftime("%H:%M:%S")
            else:
                if "Date" in swipes.columns and "Time" in swipes.columns:
                    swipes["DateOnly"] = swipes["Date"]
        except Exception:
            logging.debug("Frontend display enrichment failed for region %s", rkey)

        if "AdjustedMessageTime" not in swipes.columns:
            swipes["AdjustedMessageTime"] = pd.NaT

        # write outputs
        try:
            csv_path = outdir_path / f"{rkey}_duration_{target_date.strftime('%Y%m%d')}.csv"
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", rkey)
        try:
            swipes_csv_path = outdir_path / f"{rkey}_swipes_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", rkey)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", rkey, csv_path if 'csv_path' in locals() else '<unknown>', len(durations) if durations is not None else 0)
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", rkey, swipes_csv_path if 'swipes_csv_path' in locals() else '<unknown>', len(swipes) if swipes is not None else 0)

        results[rkey] = {"swipes": swipes, "durations": durations}

    return results


# end of file








alos check Api error and fix this



@app.route('/record', methods=['GET'])
def record():
    try:
        # --- BEGIN existing record() logic ---
        from pathlib import Path
        import pandas as pd
        import math
        import re
        from datetime import datetime, date
        try:
            q = request.args.get('employee_id') or request.args.get('person_uid')
        except Exception:
            q = None
        include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
        city_param = request.args.get('city') or request.args.get('site') or 'pune'

        # pick outdir consistently
        try:
            base_out = Path(DEFAULT_OUTDIR)
        except Exception:
            try:
                base_out = Path(OUTDIR)
            except Exception:
                base_out = Path.cwd()

        # helper safe wrappers (use existing ones if present)
        def _safe_read(fp, **kwargs):
            try:
                if '_safe_read_csv' in globals():
                    return _safe_read_csv(fp)
                return pd.read_csv(fp, **kwargs)
            except Exception:
                try:
                    return pd.read_csv(fp, dtype=str, **{k: v for k, v in kwargs.items() if k != 'parse_dates'})
                except Exception:
                    return pd.DataFrame()

        def _to_python_scalar(v):
            if pd.isna(v):
                return None
            try:
                return v.item() if hasattr(v, 'item') else v
            except Exception:
                return v

        # 1) find trend CSVs (city-specific first)
        def _slug(s):
            return re.sub(r'[^a-z0-9]+', '_', str(s or '').strip().lower()).strip('_')

        city_slug = _slug(city_param)
        trend_glob = list(base_out.glob(f"trend_{city_slug}_*.csv"))
        if not trend_glob:
            trend_glob = list(base_out.glob("trend_*.csv"))
        trend_glob = sorted(trend_glob, reverse=True)

        df_list = []
        for fp in trend_glob:
            try:
                tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
            except Exception:
                try:
                    tmp = pd.read_csv(fp, dtype=str)
                    if 'Date' in tmp.columns:
                        tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                except Exception:
                    continue
            df_list.append(tmp)
        if df_list:
            trends_df = pd.concat(df_list, ignore_index=True)
            try:
                trends_df = _replace_placeholder_strings(trends_df)
            except Exception:
                pass
        else:
            trends_df = pd.DataFrame()

        # if no query param, return a small sample of trend rows (if any)
        if q is None:
            try:
                if not trends_df.empty and '_clean_sample_df' in globals():
                    cleaned = _clean_sample_df(trends_df, max_rows=10)
                elif not trends_df.empty:
                    cleaned = trends_df.head(10).to_dict(orient='records')
                else:
                    cleaned = []
            except Exception:
                cleaned = []
            return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

        q_str = str(q).strip()

        # helper to normalise series values to comparable strings/numerics
        def normalize_series(s):
            if s is None:
                return pd.Series([''] * (len(trends_df) if not trends_df.empty else 0))
            s = s.fillna('').astype(str).str.strip()
            def _norm_val(v):
                if not v:
                    return ''
                try:
                    if '.' in v:
                        fv = float(v)
                        if math.isfinite(fv) and fv.is_integer():
                            return str(int(fv))
                except Exception:
                    pass
                return v
            return s.map(_norm_val)

        # find matching rows in trends_df
        found_mask = pd.Series(False, index=trends_df.index) if not trends_df.empty else pd.Series(dtype=bool)
        candidates_cols = ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12', 'EmployeeName')
        for c in candidates_cols:
            if c in trends_df.columns:
                try:
                    ser = normalize_series(trends_df[c])
                    found_mask = found_mask | (ser == q_str)
                except Exception:
                    pass

        # numeric fallback
        if (found_mask is None) or (not found_mask.any() if len(found_mask) else True):
            try:
                q_numeric = float(q_str)
                for c in ('EmployeeID', 'Int1'):
                    if c in trends_df.columns:
                        try:
                            numser = pd.to_numeric(trends_df[c], errors='coerce')
                            found_mask = found_mask | (numser == q_numeric)
                        except Exception:
                            pass
            except Exception:
                pass

        matched = trends_df[found_mask].copy() if not trends_df.empty else pd.DataFrame()
        if matched.empty:
            cleaned_matched = []
        else:
            try:
                cleaned_matched = _clean_sample_df(matched, max_rows=len(matched)) if '_clean_sample_df' in globals() else matched.to_dict(orient='records')
            except Exception:
                cleaned_matched = matched.to_dict(orient='records')

        # enrich matched rows where possible
        try:
            if cleaned_matched and '_enrich_with_personnel_info' in globals():
                agg_df = pd.DataFrame(cleaned_matched)
                agg_df = _enrich_with_personnel_info(agg_df, image_endpoint_template="/employee/{}/image")
                cleaned_matched = agg_df.to_dict(orient='records')
        except Exception:
            pass

        # build list of dates to scan for swipe files (from matched rows)
        dates_to_scan = set()
        try:
            for _, r in (matched.iterrows() if not matched.empty else []):
                try:
                    if 'Date' in r and pd.notna(r['Date']):
                        try:
                            d = pd.to_datetime(r['Date']).date()
                            dates_to_scan.add(d)
                        except Exception:
                            pass
                    for col in ('FirstSwipe','LastSwipe'):
                        if col in r and pd.notna(r[col]):
                            try:
                                d = pd.to_datetime(r[col]).date()
                                dates_to_scan.add(d)
                            except Exception:
                                pass
                except Exception:
                    continue
        except Exception:
            pass
        if not dates_to_scan:
            dates_to_scan = {None}  # indicates scan all swipes files

        # ---------- helper: find swipe files for a date (robust) ----------
        def _find_swipes_for_date(date_obj=None):
            try:
                include_shifted = True
                try:
                    if city_slug and str(city_slug).strip().lower() == 'pune':
                        include_shifted = False
                except Exception:
                    include_shifted = True

                if '_find_swipe_files' in globals() and callable(globals().get('_find_swipe_files')):
                    try:
                        cand = _find_swipe_files(str(base_out), date_obj=date_obj, city_slug=city_slug if city_slug else None, include_shifted=include_shifted)
                        if cand:
                            return cand
                    except Exception:
                        logging.exception("_find_swipe_files helper failed; falling back to glob search.")

                files = []
                if date_obj is None:
                    files = list(base_out.glob("swipes_*_*.csv")) + list(base_out.glob("swipes_*.csv")) + list(base_out.glob("*swipe*.csv"))
                else:
                    ymd = date_obj.strftime('%Y-%m-%d')
                    ymd2 = date_obj.strftime('%Y%m%d')
                    cand1 = [p for p in base_out.glob("swipes_*_*.csv") if (ymd in p.name or ymd2 in p.name)]
                    cand2 = [p for p in base_out.glob("swipes_*.csv") if (ymd in p.name or ymd2 in p.name)]
                    files = cand1 + cand2
                files = sorted(list({p for p in files if p.exists()}), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
                if not include_shifted:
                    files = [f for f in files if 'shift' not in f.name.lower()]
                return files
            except Exception:
                logging.exception("Error while searching for swipe files for date=%s city=%s", date_obj, city_slug)
                return []

        # ---------- scan swipe files for the target person (dates_to_scan computed earlier) ----------
        raw_files_set = set()
        raw_swipes_out = []
        seen_keys = set()

        def _append_row(out_row, source_name):
            key = (
                str(out_row.get('Date') or ''),
                str(out_row.get('Time') or ''),
                str(out_row.get('Door') or '').strip(),
                str(out_row.get('Direction') or '').strip(),
                str(out_row.get('CardNumber') or out_row.get('Card') or '').strip()
            )
            if key in seen_keys:
                return
            seen_keys.add(key)
            out_row['_source'] = source_name
            raw_swipes_out.append(out_row)

        for d in dates_to_scan:
            swipe_candidates = _find_swipes_for_date(d)
            if d is not None and not swipe_candidates:
                swipe_candidates = _find_swipes_for_date(None)

            for fp in swipe_candidates:
                raw_files_set.add(fp.name)
                try:
                    sdf = _safe_read(fp, parse_dates=['LocaleMessageTime'])
                except Exception:
                    try:
                        sdf = _safe_read(fp)
                    except Exception:
                        continue
                if sdf is None or sdf.empty:
                    continue

                # minimal column-normalization & masking (reuse original code patterns)
                cols_lower = {c.lower(): c for c in sdf.columns}
                tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
                emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
                card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
                door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
                dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
                note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
                person_uid_col = cols_lower.get('person_uid')

                try:
                    mask = pd.Series(False, index=sdf.index)
                except Exception:
                    mask = pd.Series([False] * len(sdf))

                try:
                    if person_uid_col and person_uid_col in sdf.columns:
                        mask = mask | (sdf[person_uid_col].astype(str).str.strip() == q_str)
                except Exception:
                    pass
                try:
                    if emp_col and emp_col in sdf.columns:
                        mask = mask | (sdf[emp_col].astype(str).str.strip() == q_str)
                except Exception:
                    pass

                if (not mask.any()) and emp_col and emp_col in sdf.columns:
                    try:
                        q_numeric = float(q_str)
                        emp_numeric = pd.to_numeric(sdf[emp_col], errors='coerce')
                        mask = mask | (emp_numeric == q_numeric)
                    except Exception:
                        pass

                if (not mask.any()) and name_col and name_col in sdf.columns:
                    try:
                        mask = mask | (sdf[name_col].astype(str).str.strip().str.lower() == q_str.lower())
                    except Exception:
                        pass

                if not mask.any():
                    continue

                filtered = sdf[mask].copy()
                if filtered.empty:
                    continue

                # ensure LocaleMessageTime parsed
                if tcol and tcol in filtered.columns:
                    try:
                        filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                    except Exception:
                        pass
                else:
                    for cand in ('MessageUTC','MessageTime','Timestamp','timestamp','Date'):
                        if cand in filtered.columns:
                            try:
                                filtered['LocaleMessageTime'] = pd.to_datetime(filtered[cand], errors='coerce')
                                tcol = 'LocaleMessageTime'
                                break
                            except Exception:
                                pass

                if tcol and tcol in filtered.columns:
                    try:
                        filtered = filtered.sort_values(by=tcol)
                    except Exception:
                        pass



                    # === compute swipe gaps (add/replace right after sorting filtered by tcol) ===
if tcol and tcol in filtered.columns:
    try:
        # ensure sorted
        filtered = filtered.sort_values(by=tcol)
    except Exception:
        pass

    try:
        # previous timestamp
        filtered['_prev_ts'] = filtered[tcol].shift(1)
        # gap seconds between consecutive swipes (float)
        filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
        # reset gap at day boundary (if previous date differs) or if prev is NaT
        try:
            cur_dates = filtered[tcol].dt.date
            prev_dates = cur_dates.shift(1)
            day_boundary_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
            filtered.loc[day_boundary_mask, '_swipe_gap_seconds'] = 0.0
        except Exception:
            # if date boundary logic fails, keep the computed gaps
            pass
    except Exception:
        filtered['_swipe_gap_seconds'] = 0.0
else:
    filtered['_swipe_gap_seconds'] = 0.0
# === end gap computation ===





                # convert each filtered row to output dict and append
                for _, r in filtered.iterrows():
                    out = {}
                    out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in filtered.columns else (matched.iloc[0].get('EmployeeName') if not matched.empty else q_str)
                    # EmployeeID
                    emp_val = None
                    if emp_col and emp_col in filtered.columns:
                        emp_val = _to_python_scalar(r.get(emp_col))
                    else:
                        for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                            cl = cols_lower.get(cand.lower())
                            if cl and cl in filtered.columns:
                                emp_val = _to_python_scalar(r.get(cl))
                                if emp_val not in (None, '', 'nan'):
                                    break
                        if emp_val in (None, '', 'nan'):
                            emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
                    try:
                        if emp_val is not None:
                            s = str(emp_val).strip()
                            if '.' in s:
                                try:
                                    f = float(s)
                                    if math.isfinite(f) and f.is_integer():
                                        s = str(int(f))
                                except Exception:
                                    pass
                            emp_val = s
                    except Exception:
                        pass
                    out['EmployeeID'] = emp_val

                    # CardNumber / Card
                    card_val = None
                    if card_col and card_col in filtered.columns:
                        card_val = _to_python_scalar(r.get(card_col))
                    else:
                        for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                            cl = cols_lower.get(cand.lower())
                            if cl and cl in filtered.columns:
                                card_val = _to_python_scalar(r.get(cl))
                                if card_val not in (None, '', 'nan'):
                                    break
                        if card_val in (None, '', 'nan'):
                            card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                    try:
                        if card_val is not None:
                            card_val = str(card_val).strip()
                    except Exception:
                        pass
                    out['CardNumber'] = card_val
                    out['Card'] = card_val

                    # timestamps -> Date / Time / LocaleMessageTime
                    if tcol and tcol in filtered.columns:
                        ts = r.get(tcol)
                        try:
                            ts_py = pd.to_datetime(ts)
                            out['Date'] = ts_py.date().isoformat()
                            out['Time'] = ts_py.time().isoformat()
                            out['LocaleMessageTime'] = ts_py.isoformat()
                        except Exception:
                            txt = str(r.get(tcol))
                            out['Date'] = txt[:10]
                            out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                            out['LocaleMessageTime'] = txt
                    else:
                        out['Date'] = None
                        out['Time'] = None
                        out['LocaleMessageTime'] = None



                    # out['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
                    # try:
                    #     s = int(out['SwipeGapSeconds'])
                    #     hh = s // 3600
                    #     mm = (s % 3600) // 60
                    #     ss = s % 60
                    #     out['SwipeGap'] = f"{hh:02d}:{mm:02d}:{ss:02d}"
                    # except Exception:
                    #     out['SwipeGap'] = None

                    out['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds') or 0.0)
                    out['SwipeGap'] = format_seconds_to_hms(out['SwipeGapSeconds'])

                    out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
                    out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else (_to_python_scalar(r.get('Direction')) if 'Direction' in r else None)
                    out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
                    try:
                        out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else (map_door_to_zone(out['Door'], out['Direction']) if 'map_door_to_zone' in globals() else None)
                    except Exception:
                        out['Zone'] = None

                    out['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
                    out['_source_file'] = fp.name
                    _append_row(out, fp.name)

        raw_swipe_files = sorted(list(raw_files_set))

        return jsonify({
            "aggregated_rows": cleaned_matched,
            "raw_swipe_files": raw_swipe_files,
            "raw_swipes": raw_swipes_out
        }), 200
        # --- END existing record() logic ---
    except Exception as e:
        logging.exception("Unhandled exception in /record endpoint")
        # Return JSON so frontend sees useful error instead of Flask returning None
        return jsonify({"error": "internal server error in /record", "details": str(e)}), 500

