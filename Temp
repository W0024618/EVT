C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py


# app.py
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.responses import JSONResponse
import shutil, uuid, json
from settings import UPLOAD_DIR, OUTPUT_DIR
from ingest_excel import ingest_employee_excel, ingest_contractor_excel
from compare_service import ingest_live_details_list, compute_daily_attendance, compare_with_active
import os

app = FastAPI(title="Attendance Analytics")

@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = UPLOAD_DIR / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    ingest_employee_excel(dest)
    return {"status":"ok", "path": str(dest)}

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    dest = UPLOAD_DIR / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    ingest_contractor_excel(dest)
    return {"status":"ok", "path": str(dest)}



@app.post("/ingest/live-details")
async def ingest_live(request: Request):
    """
    Accepts:
      - Raw JSON array in body (preferred)
      - JSON object with {"details": [...]} in body
      - multipart/form-data where a form field 'details' contains a JSON string
    """
    # 1) Try to parse JSON body first
    details = None
    try:
        body = await request.json()
        if isinstance(body, dict) and 'details' in body:
            details = body['details']
        else:
            details = body
    except Exception:
        # not a JSON body, try form data
        try:
            form = await request.form()
            if 'details' in form:
                raw = form['details']
                if isinstance(raw, str):
                    details = json.loads(raw)
                else:
                    # file upload or other; try to read if file-like
                    try:
                        details = json.loads((await raw.read()).decode('utf-8'))
                    except Exception:
                        details = list(form.getlist('details'))
            else:
                # try first field if it looks like JSON string
                first = None
                for v in form.values():
                    first = v
                    break
                if isinstance(first, str):
                    details = json.loads(first)
                else:
                    raise HTTPException(status_code=400, detail="No JSON payload found")
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Could not parse request body as JSON or form: {e}")

    if not isinstance(details, (list, tuple)):
        raise HTTPException(status_code=400, detail="Expected top-level array (JSON list) of detail objects")

    try:
        ingest_live_details_list(details)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to ingest details: {e}")

    return {"status": "ok", "inserted": len(details)}





@app.get("/ingest/fetch-all")
def fetch_all_and_ingest():
    """
    Convenience: call region_clients.fetch_all_details() and ingest them.
    Uses local import to avoid circular import issues.
    """
    try:
        import region_clients
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"region_clients unavailable: {e}")

    details = region_clients.fetch_all_details()
    if not isinstance(details, list):
        raise HTTPException(status_code=500, detail="Unexpected data from region_clients.fetch_all_details")
    ingest_live_details_list(details)
    return {"status":"ok", "inserted": len(details)}




@app.get("/reports/daily/{yyyymmdd}")
def daily_report(yyyymmdd: str):
    import datetime
    try:
        dt = datetime.datetime.strptime(yyyymmdd, "%Y%m%d").date()
    except Exception:
        raise HTTPException(status_code=400, detail="Date must be in YYYYMMDD format")
    compute_daily_attendance(dt)
    summary = compare_with_active(dt)
    # summary is JSON-safe (compare_service ensures conversion)
    return JSONResponse(summary)





C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\compare_service.py

# compare_service.py
import pandas as pd
import numpy as np
from datetime import datetime, date
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary

def _to_native(value):
    """Convert numpy/pandas types and datetimes to plain Python types."""
    if value is None:
        return None
    # pandas NA or numpy nan
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    # numpy numbers
    if isinstance(value, (np.integer,)):
        return int(value)
    if isinstance(value, (np.floating,)):
        # float('nan') already handled above
        return float(value)
    if isinstance(value, (np.bool_, bool)):
        return bool(value)
    # pandas Timestamp or datetime
    try:
        import datetime as _dt
        if isinstance(value, _dt.datetime) or hasattr(value, 'isoformat'):
            try:
                return value.isoformat()
            except Exception:
                return str(value)
    except Exception:
        pass
    return value

def ingest_live_details_list(details_list):
    """Persist details_list (array of detail dicts) into LiveSwipe table."""
    from db import SessionLocal as _SessionLocal
    with _SessionLocal() as db:
        for d in details_list:
            ts = d.get("LocaleMessageTime")
            try:
                ts_parsed = datetime.fromisoformat(ts.replace("Z", "+00:00")) if ts else datetime.utcnow()
            except Exception:
                ts_parsed = datetime.utcnow()
            rec = LiveSwipe(
                timestamp=ts_parsed,
                employee_id=(d.get("EmployeeID") or "").strip() or None,
                card_number=(d.get("CardNumber") or "").strip() or None,
                full_name=d.get("ObjectName1"),
                partition=d.get("PartitionName2") or d.get("PartitionName1"),
                floor=d.get("Floor"),
                door=d.get("Door") or d.get("DoorName"),
                region=d.get("PartitionName2"),
                raw=d
            )
            db.add(rec)
        db.commit()

def compute_daily_attendance(target_date: date):
    """Build AttendanceSummary rows for target_date by reading LiveSwipe rows on that date."""
    with SessionLocal() as db:
        start = datetime.combine(target_date, datetime.min.time())
        end = datetime.combine(target_date, datetime.max.time())
        swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
        if not swipes:
            return []

        rows = []
        for s in swipes:
            rows.append({
                "id": s.id,
                "timestamp": s.timestamp,
                "employee_id": s.employee_id,
                "card_number": s.card_number,
                "full_name": s.full_name,
                "partition": s.partition,
                "floor": s.floor,
                "door": s.door
            })
        df = pd.DataFrame(rows)
        if df.empty:
            return []

        # Key: prefer employee_id, fallback to card_number
        df['key'] = df['employee_id'].fillna(df['card_number'])
        grouped = df.groupby('key').agg(
            presence_count=('id', 'count'),
            first_seen=('timestamp', 'min'),
            last_seen=('timestamp', 'max'),
            full_name=('full_name', 'first'),
            partition=('partition', 'first')
        ).reset_index()

        # Upsert AttendanceSummary
        for _, row in grouped.iterrows():
            rec = AttendanceSummary(
                employee_id=row['key'],
                date=target_date,
                presence_count=int(row['presence_count']),
                first_seen=row['first_seen'],
                last_seen=row['last_seen'],
                derived={"partition": row['partition'], "full_name": row['full_name']}
            )
            db.merge(rec)
        db.commit()
        return grouped.to_dict(orient='records')

def compare_with_active(target_date: date):
    """
    Compare AttendanceSummary (for target_date) with ActiveEmployee table.
    Returns a dict with clean Python types, no NaN values (JSON-safe).
    """
    with SessionLocal() as db:
        # Attendance summary rows for date
        att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == target_date).all()
        if not att_rows:
            att_df = pd.DataFrame(columns=["employee_id", "presence_count", "first_seen", "last_seen"])
        else:
            att_df = pd.DataFrame([{
                "employee_id": a.employee_id,
                "presence_count": a.presence_count,
                "first_seen": a.first_seen,
                "last_seen": a.last_seen,
                **(a.derived or {})
            } for a in att_rows])

        # Active employees
        act_rows = db.query(ActiveEmployee).all()
        if not act_rows:
            act_df = pd.DataFrame(columns=["employee_id", "full_name", "location_city", "status"])
        else:
            act_df = pd.DataFrame([{
                "employee_id": e.employee_id,
                "full_name": e.full_name,
                "location_city": e.location_city,
                "status": e.current_status
            } for e in act_rows])

        # Ensure key column exists
        if 'employee_id' not in act_df.columns:
            act_df['employee_id'] = pd.NA
        if 'employee_id' not in att_df.columns:
            att_df['employee_id'] = pd.NA

        # Merge left (active employees -> attendance)
        merged = pd.merge(act_df, att_df, on='employee_id', how='left')

        # Fill presence_count and booleans safely
        if 'presence_count' in merged.columns:
            merged['presence_count'] = merged['presence_count'].fillna(0)
            # convert any floats like 0.0 to int where safe
            merged['presence_count'] = merged['presence_count'].apply(lambda x: int(x) if (pd.notnull(x) and float(x).is_integer()) else x)
        else:
            merged['presence_count'] = 0

        merged['present_today'] = merged['presence_count'].apply(lambda x: bool(x and x != 0))

        # Location summary
        merged['location_city'] = merged.get('location_city').fillna('Unknown')
        loc_group = merged.groupby('location_city', dropna=False).agg(
            total_n=('employee_id', 'count'),
            present_n=('present_today', 'sum')
        ).reset_index()

        # percent_present safe (avoid division by zero/NaN)
        def safe_percent(row):
            try:
                if row['total_n'] and row['total_n'] > 0:
                    return round((row['present_n'] / row['total_n']) * 100, 2)
            except Exception:
                pass
            return 0.0
        loc_group['percent_present'] = loc_group.apply(safe_percent, axis=1)

        # Convert to JSON-safe native Python types
        by_location = []
        for r in loc_group.to_dict(orient='records'):
            clean = {k: _to_native(v) for k, v in r.items()}
            # keep location_city key name consistent
            if 'location_city' not in clean and 'index' in clean:
                clean['location_city'] = clean.pop('index')
            by_location.append(clean)

        merged_list = []
        for r in merged.to_dict(orient='records'):
            clean = {}
            for k, v in r.items():
                clean[k] = _to_native(v)
            # ensure keys present
            if 'employee_id' not in clean:
                clean['employee_id'] = None
            merged_list.append(clean)

        return {"by_location": by_location, "merged": merged_list}






