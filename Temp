our Issue is 
1)we Got Failed to load CCURE averages this error again and again need to fix this issue 
2) Duration report continoulsy in loading state need to fix both issue how we can improve this performanve ..


Keep another logic as it is and resolve this issue and give me updated file carefully...





# app.py (updated, drop-in replacement)
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import shutil
import uuid
import json
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
from zoneinfo import ZoneInfo
import re
import asyncio
from typing import Optional, Dict, Any, List
import sys

# pandas required for duration serialization and CSV reading
import pandas as pd

# --- DB / models imports (kept for endpoints that still use DB like headcount/attendance fallback) ---
from db import SessionLocal
from models import LiveSwipe, AttendanceSummary

# Settings fallback
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"

RAW_UPLOADS_DIR = DATA_DIR / "raw_uploads"
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# ----------------- GLOBAL TIMEOUTS (UNIFY) -----------------
REGION_TIMEOUT_SECONDS = 20
COMPUTE_WAIT_TIMEOUT_SECONDS = 60   # increased to be safer for slower DB / endpoints
COMPUTE_SYNC_TIMEOUT_SECONDS = 90
# ----------------------------------------------------------

_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

_broadcaster_clients = set()


def broadcast_ccure_update(payload: dict):
    if not _broadcaster_clients:
        return
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = None
    for q in list(_broadcaster_clients):
        try:
            if loop and loop.is_running():
                loop.call_soon_threadsafe(q.put_nowait, payload)
            else:
                q.put_nowait(payload)
        except Exception:
            logger.exception("Failed to push payload to SSE client (will remove client)")
            try:
                _broadcaster_clients.discard(q)
            except Exception:
                pass


async def _sse_event_generator(client_queue: asyncio.Queue):
    try:
        while True:
            payload = await client_queue.get()
            try:
                data = json.dumps(payload, default=str)
            except Exception:
                data = json.dumps({"error": "serialization error", "payload": str(payload)})
            yield f"data: {data}\n\n"
    finally:
        try:
            _broadcaster_clients.discard(client_queue)
        except Exception:
            pass
        return


@app.get("/ccure/stream")
async def ccure_stream():
    q = asyncio.Queue()
    _broadcaster_clients.add(q)
    generator = _sse_event_generator(q)
    headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    return StreamingResponse(generator, media_type="text/event-stream", headers=headers)


def _guess_region_from_text(txt: str) -> str:
    if not txt:
        return "unknown"
    s = str(txt).strip().lower()
    s = re.sub(r"[,\-/()]", " ", s)
    if any(k in s for k in ("pune","quezon city","taguig city","bengaluru","hyderabad","chennai","manila","singapore","hong kong","beijing","shanghai","jakarta","kuala","osaka","tokyo","seoul","bangkok")):
        return "apac"
    if any(k in s for k in ("london","dublin","paris","frankfurt","amsterdam","stockholm","cape town","johannesburg","berlin","brussels","madrid","rome","milan")):
        return "emea"
    if any(k in s for k in ("mexico","bogota","buenos","santiago","sao","salvador","lima","caracas")):
        return "laca"
    if any(k in s for k in ("denver","new york","ny","chicago","toronto","vancouver","los angeles","san francisco","boston","houston","atlanta","miami")):
        return "namer"
    return "unknown"


@app.get("/headcount")
def api_headcount():
    try:
        totals = {"apac": 0, "emea": 0, "laca": 0, "namer": 0, "unknown": 0}
        with SessionLocal() as db:
            try:
                today = date.today()
                rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                if rows:
                    for r in rows:
                        try:
                            partition = None
                            if getattr(r, "derived", None) and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                            loc = partition or "unknown"
                            region = _guess_region_from_text(loc)
                            totals[region] = totals.get(region, 0) + 1
                        except Exception:
                            totals["unknown"] += 1
                else:
                    start = datetime.combine(today, datetime.min.time())
                    end = datetime.combine(today, datetime.max.time())
                    swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
                    for s in swipes:
                        loc = getattr(s, "partition", None) or "unknown"
                        region = _guess_region_from_text(loc)
                        totals[region] = totals.get(region, 0) + 1
            except Exception:
                logger.exception("Failed to compute headcount regions")
        out = {
            "apac": int(totals.get("apac", 0)),
            "emea": int(totals.get("emea", 0)),
            "laca": int(totals.get("laca", 0)),
            "namer": int(totals.get("namer", 0))
        }
        return JSONResponse(out)
    except Exception as exc:
        logger.exception("api_headcount failed")
        raise HTTPException(status_code=500, detail=f"headcount error: {exc}")


# ---------- small helpers ----------
def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None


def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None


def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None


# ---------- build_ccure_averages (fallback) ------------------------------
def build_ccure_averages(start_date: Optional[str] = None, end_date: Optional[str] = None) -> Dict[str, Any]:
    """
    Fail-safe averages computation using AttendanceSummary / region endpoints.
    Always returns stable structure.
    """
    try:
        def _parse_date_param(s):
            if not s:
                return None
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                try:
                    return date.fromisoformat(s)
                except Exception:
                    return None

        today = date.today()
        start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
        end_obj = _parse_date_param(end_date) if end_date else today
        if start_obj is None or end_obj is None or start_obj > end_obj:
            start_obj = today - timedelta(days=6)
            end_obj = today

        live_emp = 0
        live_contr = 0
        unknown_count = 0
        live_total_reported = 0
        live_total_details = 0

        # 1) try DB rows for today to compute live counts
        try:
            with SessionLocal() as db:
                try:
                    att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                except Exception:
                    logger.exception("Failed to query AttendanceSummary for today's rows")
                    att_rows = []

                if att_rows:
                    seen_keys = set()
                    def classify_from_derived(derived):
                        try:
                            if not derived or not isinstance(derived, dict):
                                return "contractor"
                            for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                                v = derived.get(k)
                                if v and "employee" in str(v).strip().lower():
                                    return "employee"
                            for k in ("Employee_Status","Employee Status","Status"):
                                v = derived.get(k)
                                if v and "terminated" in str(v).strip().lower():
                                    return "employee"
                            return "contractor"
                        except Exception:
                            return "contractor"

                    for a in att_rows:
                        try:
                            key = _normalize_employee_key(getattr(a, "employee_id", None))
                        except Exception:
                            key = None
                        if not key:
                            try:
                                key = _normalize_card_like(getattr(a, "derived", {}) and (a.derived.get('card_number') if isinstance(a.derived, dict) else None))
                            except Exception:
                                key = None
                        if not key:
                            unknown_count += 1
                            continue
                        if key in seen_keys:
                            continue
                        seen_keys.add(key)
                        cls = classify_from_derived(getattr(a, "derived", None))
                        if cls == "employee":
                            live_emp += 1
                        else:
                            live_contr += 1

                    live_total_reported = live_emp + live_contr + unknown_count
                    live_total_details = len(att_rows)
        except Exception:
            logger.exception("DB session error while computing live counts (falling back to region_clients)")

        # 2) fallback to region_clients if DB didn't yield details
        if live_total_details == 0:
            try:
                import region_clients
                details = []
                try:
                    details = region_clients.fetch_all_details(timeout=REGION_TIMEOUT_SECONDS) or []
                except TypeError:
                    # some implementations don't accept timeout kw
                    try:
                        details = region_clients.fetch_all_details()
                    except Exception:
                        details = []
                except Exception:
                    logger.exception("region_clients.fetch_all_details raised")
                    details = []

                if details:
                    for d in details:
                        try:
                            cls = "contractor"
                            for k in ("PersonnelType","personnelType","personnel_type","Personnel Type","Type","personnel"):
                                v = d.get(k)
                                if v and "employee" in str(v).strip().lower():
                                    cls = "employee"
                                    break
                            if cls == "employee":
                                live_emp += 1
                            else:
                                live_contr += 1
                            live_total_details += 1
                        except Exception:
                            continue
                    live_total_reported = live_emp + live_contr
                else:
                    # try live region totals
                    try:
                        regions = region_clients.fetch_all_regions(timeout=REGION_TIMEOUT_SECONDS) or []
                        for r in regions:
                            try:
                                c = r.get("count")
                                if isinstance(c, (int, float)):
                                    live_total_reported += int(c)
                            except Exception:
                                continue
                    except Exception:
                        logger.exception("region_clients.fetch_all_regions failed")
            except Exception:
                logger.exception("region_clients import/usage failed in build_ccure_averages")

        # 3) compute avg_range using AttendanceSummary (range)
        avg_range = None
        try:
            with SessionLocal() as db:
                q = db.query(AttendanceSummary.date, AttendanceSummary.employee_id, AttendanceSummary.presence_count)\
                      .filter(AttendanceSummary.date >= start_obj, AttendanceSummary.date <= end_obj).all()
                by_date = {}
                for row in q:
                    try:
                        # row may be ORM object or tuple-like
                        rdate = getattr(row, "date", None) or (row[0] if isinstance(row, (list, tuple)) and len(row) > 0 else None)
                        empid = getattr(row, "employee_id", None) or (row[1] if isinstance(row, (list, tuple)) and len(row) > 1 else None)
                        presence_val = getattr(row, "presence_count", None) or (row[2] if isinstance(row, (list, tuple)) and len(row) > 2 else None)
                        if rdate is None or empid is None:
                            continue
                        key = (empid or "").strip()
                        if key == "":
                            continue
                        by_date.setdefault(rdate, set())
                        try:
                            if int(presence_val or 0) > 0:
                                by_date[rdate].add(key)
                        except Exception:
                            by_date[rdate].add(key)
                    except Exception:
                        continue

                days_count = (end_obj - start_obj).days + 1
                daily_counts = []
                for i in range(days_count):
                    d = start_obj + timedelta(days=i)
                    daily_counts.append(len(by_date.get(d, set())))
                if days_count and any(daily_counts):
                    avg_range = int(round(sum(daily_counts) / float(days_count)))
        except Exception:
            logger.exception("Failed computing range average from AttendanceSummary (will try region history)")

        # 4) fallback: region history aggregated
        if avg_range is None:
            try:
                import region_clients
                entries = region_clients.fetch_all_history(timeout=REGION_TIMEOUT_SECONDS) or []
                agg = {}
                for e in entries:
                    try:
                        dstr = e.get("date")
                        if not dstr:
                            continue
                        region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                        emp = _safe_int(region_obj.get("Employee")) if region_obj else _safe_int(e.get("Employee"))
                        con = _safe_int(region_obj.get("Contractor")) if region_obj else _safe_int(e.get("Contractor"))
                        tot = _safe_int(region_obj.get("total")) if region_obj else _safe_int(e.get("total"))
                        if emp is None and con is None and tot is None:
                            continue
                        if tot is None:
                            tot = (emp or 0) + (con or 0)
                        agg.setdefault(dstr, {"total": 0, "count": 0})
                        agg[dstr]["total"] += tot or 0
                        agg[dstr]["count"] += 1
                    except Exception:
                        continue
                per_date_totals = []
                days_count = (end_obj - start_obj).days + 1
                for i in range(days_count):
                    dcheck = (start_obj + timedelta(days=i)).isoformat()
                    if dcheck in agg and agg[dcheck]["count"] > 0:
                        per_day_avg = float(agg[dcheck]["total"]) / float(agg[dcheck]["count"])
                        per_date_totals.append(per_day_avg)
                if per_date_totals:
                    avg_range = int(round(sum(per_date_totals) / float(len(per_date_totals))))
            except Exception:
                logger.exception("Failed computing avg_range from region history in fallback")

        # 5) try ccure_client global stats
        cc_active_emps = None
        cc_active_contractors = None
        try:
            import ccure_client
            if hasattr(ccure_client, "get_global_stats"):
                st = ccure_client.get_global_stats() or {}
                a = st.get("ActiveEmployees") or st.get("active_employees") or None
                b = st.get("ActiveContractors") or st.get("active_contractors") or None
                if a is not None and str(a).strip() != "":
                    cc_active_emps = int(a)
                if b is not None and str(b).strip() != "":
                    cc_active_contractors = int(b)
        except Exception:
            logger.debug("ccure_client.get_global_stats not available", exc_info=True)

        emp_pct = None
        contr_pct = None
        overall_pct = None
        try:
            if isinstance(cc_active_emps, int) and cc_active_emps > 0:
                emp_pct = round((live_emp / float(cc_active_emps)) * 100.0, 2)
            if isinstance(cc_active_contractors, int) and cc_active_contractors > 0:
                contr_pct = round((live_contr / float(cc_active_contractors)) * 100.0, 2)
            if isinstance(cc_active_emps, int) and isinstance(cc_active_contractors, int) and (cc_active_emps + cc_active_contractors) > 0:
                overall_pct = round(((live_emp + live_contr) / float(cc_active_emps + cc_active_contractors)) * 100.0, 2)
        except Exception:
            emp_pct = contr_pct = overall_pct = None

        resp = {
            "date": today.isoformat(),
            "notes": f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}" if (start_date or end_date) else None,
            "live_today": {
                "employee": int(live_emp),
                "contractor": int(live_contr),
                "total_reported": int(live_total_reported) if live_total_reported is not None else None,
                "total_from_details": int(live_total_details) if live_total_details is not None else None
            },
            "ccure_active": {
                "active_employees": cc_active_emps,
                "active_contractors": cc_active_contractors,
                "ccure_active_employees_reported": cc_active_emps,
                "ccure_active_contractors_reported": cc_active_contractors
            },
            "averages": {
                "employee_pct": emp_pct,
                "contractor_pct": contr_pct,
                "overall_pct": overall_pct,
                "avg_headcount_last_7_days": avg_range,
                "head_emp_pct_vs_ccure_today": emp_pct,
                "head_contractor_pct_vs_ccure_today": contr_pct,
                "headcount_overall_pct_vs_ccure_today": overall_pct,
                "history_avg_overall_last_7_days": avg_range
            }
        }

        return resp
    except Exception:
        logger.exception("build_ccure_averages failed unexpectedly - returning minimal structure")
        # return minimal structure instead of raising so callers get a stable shape
        return {
            "date": date.today().isoformat(),
            "notes": None,
            "live_today": {"employee": 0, "contractor": 0, "total_reported": 0, "total_from_details": 0},
            "ccure_active": {"active_employees": None, "active_contractors": None, "ccure_active_employees_reported": None, "ccure_active_contractors_reported": None},
            "averages": {"employee_pct": None, "contractor_pct": None, "overall_pct": None, "avg_headcount_last_7_days": None, "history_avg_overall_last_7_days": None}
        }


# -------------------------
# Upload endpoints (store uploads under data/ and data/raw_uploads/ and rotate)
# -------------------------
ALLOWED_EXT = (".xls", ".xlsx", ".csv")


def _remove_old_files_for_kind(kind: str):
    try:
        for p in DATA_DIR.iterdir():
            if p.is_file() and f"active_{kind}" in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old canonical file %s", p)
        for p in RAW_UPLOADS_DIR.iterdir():
            if p.is_file() and kind in p.name.lower():
                try:
                    p.unlink()
                except Exception:
                    logger.warning("Could not remove old raw file %s", p)
    except Exception:
        logger.exception("error while removing old files for kind=%s", kind)


def _save_upload_and_rotate(upload_file: UploadFile, kind: str) -> dict:
    fname = Path(upload_file.filename).name
    ext = Path(fname).suffix.lower()
    if ext not in ALLOWED_EXT:
        raise HTTPException(status_code=400, detail="Only .xls .xlsx .csv allowed")

    _remove_old_files_for_kind(kind)

    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    raw_name = f"{ts}_{kind}_{fname}"
    raw_path = RAW_UPLOADS_DIR / raw_name

    try:
        with raw_path.open("wb") as out_f:
            shutil.copyfileobj(upload_file.file, out_f)
    finally:
        try:
            upload_file.file.close()
        except Exception:
            pass

    canonical_path = DATA_DIR / f"active_{kind}{ext}"
    try:
        shutil.copy(raw_path, canonical_path)
    except Exception:
        logger.exception("Failed to write canonical copy for %s", canonical_path)

    info = {
        "raw_saved": str(raw_path),
        "canonical_saved": str(canonical_path),
        "original_filename": fname,
        "size_bytes": raw_path.stat().st_size if raw_path.exists() else None
    }
    logger.info("Uploaded %s: %s", kind, info)
    return info


@app.post("/api/upload/active-employees")
async def upload_active_employees_api(file: UploadFile = File(...)):
    return await upload_active_employees(file)


@app.post("/api/upload/active-contractors")
async def upload_active_contractors_api(file: UploadFile = File(...)):
    return await upload_active_contractors(file)


@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="employee")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("employee upload failed")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    if not file:
        raise HTTPException(status_code=400, detail="no file provided")
    try:
        info = _save_upload_and_rotate(file, kind="contractor")
        return JSONResponse({"status": "ok", "detail": info})
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("contractor upload failed")
        raise HTTPException(status_code=500, detail=str(e))


# ---------- map detailed -> compact (used when compute returns detailed) ----
def _map_detailed_to_resp(detailed: Dict[str, Any]) -> Dict[str, Any]:
    live_h = detailed.get("live_headcount", {}) or {}
    head_h = detailed.get("headcount", {}) or {}
    ccure_active_obj = detailed.get("ccure_active", {}) or {}
    averages_obj = detailed.get("averages", {}) or {}

    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    live_employee = int(live_h.get("employee") or head_h.get("employee") or 0)
    live_contractor = int(live_h.get("contractor") or head_h.get("contractor") or 0)
    total_reported = int(
        live_h.get("currently_present_total")
        or head_h.get("total_visited_today")
        or (live_employee + live_contractor)
        or 0
    )
    total_from_details = int(head_h.get("total_visited_today") or 0)

    mapped_headcount = {
        "total_visited_today": int(head_h.get("total_visited_today") or 0),
        "employee": int(head_h.get("employee") or 0),
        "contractor": int(head_h.get("contractor") or 0),
        "by_location": head_h.get("by_location") or {}
    }

    resp = {
        "date": detailed.get("date"),
        "notes": detailed.get("notes"),
        "live_today": {
            "employee": live_employee,
            "contractor": live_contractor,
            "total_reported": total_reported,
            "total_from_details": total_from_details
        },
        "headcount": mapped_headcount,
        "live_headcount": live_h,
        "ccure_active": {
            "active_employees": ccure_active_obj.get("ccure_active_employees_reported")
                             or ccure_active_obj.get("active_employees"),
            "active_contractors": ccure_active_obj.get("ccure_active_contractors_reported")
                               or ccure_active_obj.get("active_contractors"),
            "ccure_active_employees_reported": ccure_active_obj.get("ccure_active_employees_reported"),
            "ccure_active_contractors_reported": ccure_active_obj.get("ccure_active_contractors_reported")
        },
        "averages": averages_obj
    }
    return resp


def _build_verify_like_summary_from_mapped(mapped: Dict[str, Any], include_raw: bool = False) -> Dict[str, Any]:
    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    cc = mapped.get("ccure_active", {}) or {}
    head = mapped.get("headcount", {}) or {}
    live_head = mapped.get("live_headcount", {}) or {}
    averages = mapped.get("averages", {}) or {}

    cc_emp = to_int(cc.get("ccure_active_employees_reported") or cc.get("active_employees"))
    cc_con = to_int(cc.get("ccure_active_contractors_reported") or cc.get("active_contractors"))

    head_total = to_int(head.get("total_visited_today") or mapped.get("live_today", {}).get("total_from_details"))
    head_emp = to_int(head.get("employee") or mapped.get("live_today", {}).get("employee"))
    head_con = to_int(head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    live_total = to_int(live_head.get("currently_present_total") or mapped.get("live_today", {}).get("total_reported"))
    live_emp = to_int(live_head.get("employee") or mapped.get("live_today", {}).get("employee"))
    live_con = to_int(live_head.get("contractor") or mapped.get("live_today", {}).get("contractor"))

    history_emp_avg = averages.get("history_avg_employee_last_7_days")
    history_con_avg = averages.get("history_avg_contractor_last_7_days")
    history_overall_avg = averages.get("history_avg_overall_last_7_days")

    def pct(n, d):
        try:
            if n is None or d is None:
                return None
            if float(d) == 0:
                return None
            return round((float(n) / float(d)) * 100.0, 2)
        except Exception:
            return None

    summary = {
        "date": mapped.get("date"),
        "ccure_reported": {
            "employees": cc_emp,
            "contractors": cc_con,
            "total_reported": (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None
        },
        "headcount_attendance_summary": {
            "total_visited_today": head_total,
            "employee": head_emp,
            "contractor": head_con,
        },
        "live_headcount_region_clients": {
            "currently_present_total": live_total,
            "employee": live_emp,
            "contractor": live_con,
        },
        "percentages_vs_ccure": {
            "head_employee_pct_vs_ccure_today": pct(head_emp, cc_emp),
            "head_contractor_pct_vs_ccure_today": pct(head_con, cc_con),
            "head_overall_pct_vs_ccure": pct(head_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "live_employee_pct_vs_ccure_today": pct(live_emp, cc_emp),
            "live_contractor_pct_vs_ccure_today": pct(live_con, cc_con),
            "live_overall_pct_vs_ccure_today": pct(live_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "history_employee_pct_vs_ccure": pct(history_emp_avg, cc_emp),
            "history_contractor_pct_vs_ccure": pct(history_con_avg, cc_con),
            "history_overall_pct_vs_ccure": pct(history_overall_avg, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None)
        },
        "averages": {
            "history_avg_employee_last_7_days": history_emp_avg,
            "history_avg_contractor_last_7_days": history_con_avg,
            "history_avg_overall_last_7_days": history_overall_avg,
            "avg_headcount_last_7_days_db": averages.get("avg_headcount_last_7_days") or averages.get("avg_headcount_last_7_days_db"),
            "avg_headcount_per_site_last_7_days": averages.get("avg_headcount_per_site_last_7_days"),
            "employee_pct": averages.get("employee_pct"),
            "contractor_pct": averages.get("contractor_pct"),
            "overall_pct": averages.get("overall_pct"),
            **({k: v for k, v in averages.items() if k not in (
                "history_avg_employee_last_7_days",
                "history_avg_contractor_last_7_days",
                "history_avg_overall_last_7_days",
                "avg_headcount_last_7_days",
                "avg_headcount_last_7_days_db",
                "avg_headcount_per_site_last_7_days",
                "employee_pct","contractor_pct","overall_pct"
            )})
        },
        "notes": mapped.get("notes")
    }

    summary["headcount_details"] = {
        "total_visited_today": head_total,
        "employee": head_emp,
        "contractor": head_con,
        "by_location": head.get("by_location") if isinstance(head.get("by_location"), dict) else {}
    }
    summary["live_headcount_details"] = {
        "currently_present_total": live_total,
        "employee": live_emp,
        "contractor": live_con,
        "by_location": live_head.get("by_location") if isinstance(live_head.get("by_location"), dict) else {}
    }

    summary["ccure_active"] = {
        "active_employees": cc.get("active_employees") or cc.get("ccure_active_employees_reported"),
        "active_contractors": cc.get("active_contractors") or cc.get("ccure_active_contractors_reported"),
        "ccure_active_employees_reported": cc.get("ccure_active_employees_reported"),
        "ccure_active_contractors_reported": cc.get("ccure_active_contractors_reported")
    }

    return summary


# ---------- /ccure/verify (synchronous) -----
@app.get("/ccure/verify")
def ccure_verify(
    raw: bool = Query(False, description="if true, include the raw compute payload for debugging"),
    start_date: Optional[str] = Query(None, description="YYYY-MM-DD start date (inclusive)"),
    end_date: Optional[str] = Query(None, description="YYYY-MM-DD end date (inclusive)")
):
    try:
        detailed = None
        try:
            # try the faster compute path first (if available)
            from ccure_compare_service import compute_visit_averages  # may raise
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
        except Exception:
            logger.exception("compute_visit_averages() failed inside /ccure/verify; falling back to build_ccure_averages()")

        if isinstance(detailed, dict):
            mapped = _map_detailed_to_resp(detailed)
            summary = _build_verify_like_summary_from_mapped(mapped, include_raw=raw)
            if raw:
                summary["raw"] = detailed
            return JSONResponse(summary)
        else:
            fallback = build_ccure_averages(start_date, end_date)
            mapped_fallback = {
                "date": fallback.get("date"),
                "notes": fallback.get("notes"),
                "live_today": fallback.get("live_today", {}),
                "headcount": {
                    "total_visited_today": fallback.get("live_today", {}).get("total_from_details") or fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "live_headcount": {
                    "currently_present_total": fallback.get("live_today", {}).get("total_reported"),
                    "employee": fallback.get("live_today", {}).get("employee"),
                    "contractor": fallback.get("live_today", {}).get("contractor"),
                    "by_location": {}
                },
                "ccure_active": fallback.get("ccure_active", {}),
                "averages": fallback.get("averages", {})
            }

            summary = _build_verify_like_summary_from_mapped(mapped_fallback, include_raw=raw)
            if raw:
                summary["raw"] = mapped_fallback

            return JSONResponse(summary)
    except Exception as e:
        logger.exception("ccure_verify failed")
        raise HTTPException(status_code=500, detail=f"ccure verify error: {e}")


# ---------- /ccure/compare endpoints (unchanged) ----------
@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from data_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)


@app.get("/ccure/export_uploaded")
def ccure_export_uploaded():
    try:
        from data_compare_service import export_uploaded_sheets
    except Exception as e:
        logger.exception("data_compare_service import failed for export_uploaded")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    fname = export_uploaded_sheets()
    if not fname:
        raise HTTPException(status_code=500, detail="failed to export uploaded sheets")
    return JSONResponse({"status": "ok", "report_path": fname})


@app.get("/ccure/compare_v2")
def ccure_compare_v2(
    mode: str = Query("full"),
    stats_detail: str = Query("ActiveProfiles"),
    limit_list: int = Query(200, ge=1, le=5000),
    export: bool = Query(False),
    region_filter: Optional[str] = Query(None, description="Region code e.g. APAC"),
    location_city: Optional[str] = Query(None, description="Location city e.g. Pune"),
    location_state: Optional[str] = Query(None, description="Location state/province"),
    location_description: Optional[str] = Query(None, description="Location Description"),
    week_ref_date: Optional[str] = Query(None, description="YYYY-MM-DD for week (Mon-Fri)")
):
    try:
        from data_compare_service_v2 import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("data_compare_service_v2 import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")

    res = compare_ccure_vs_sheets(
        mode=mode,
        stats_detail=stats_detail,
        limit_list=limit_list,
        export=export,
        region_filter=region_filter,
        location_city=location_city,
        location_state=location_state,
        location_description=location_description,
        week_ref_date=week_ref_date
    )
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)


@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = OUTPUT_DIR / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(str(full),
                            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            filename=safe_name)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")


# ---------------------------
# NEW: Duration API endpoint
# ---------------------------
@app.get("/duration")
async def api_duration(
    date_param: Optional[str] = Query(None, alias="date", description="Target date YYYY-MM-DD. Defaults to today in Asia/Kolkata"),
    start_date: Optional[str] = Query(None, description="Start date for a range (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date for a range (YYYY-MM-DD)"),
    regions: Optional[str] = Query(None, description="Comma-separated list: apac,emea,laca,namer. Default: all"),
    city: Optional[str] = Query(None, description="Optional city/location filter (e.g. Pune). Case-insensitive"),
    outdir: Optional[str] = Query(None, description="Output directory for CSVs. Defaults to OUTPUT_DIR/duration_reports"),
    sample_rows: int = Query(10, ge=0, le=200, description="How many sample rows to include per region in response"),
    compliance_target: int = Query(3, ge=1, le=7, description="Compliance target days (e.g. 3 or 5). Default 3")
):
    """
    Returns per-region duration aggregates and swipe-level details with weekly compliance & duration category splits.
    This endpoint is defensive: if duration_report or DB is slow/unavailable, returns partial results with diagnostic messages.
    """
    try:
        # parse regions / outdir
        if regions:
            regions_list = [r.strip().lower() for r in regions.split(",") if r.strip()]
        else:
            regions_list = ["apac", "emea", "laca", "namer"]

        if outdir:
            outdir_path = Path(outdir)
        else:
            outdir_path = OUTPUT_DIR / "duration_reports"
        outdir_path.mkdir(parents=True, exist_ok=True)

        def _parse_date(s: str) -> date:
            try:
                return datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                return date.fromisoformat(s)

        if start_date and end_date:
            start_obj = _parse_date(start_date)
            end_obj = _parse_date(end_date)
            if start_obj > end_obj:
                raise HTTPException(status_code=400, detail="start_date must be <= end_date")
            max_days = 92
            days_count = (end_obj - start_obj).days + 1
            if days_count > max_days:
                raise HTTPException(status_code=400, detail=f"Date range too large (> {max_days} days). Please request a smaller range.")
            date_list = [start_obj + timedelta(days=i) for i in range(days_count)]
            range_mode = True
        else:
            if date_param:
                target_date = _parse_date(date_param)
            else:
                tz = ZoneInfo("Asia/Kolkata")
                target_date = datetime.now(tz).date()
            date_list = [target_date]
            start_obj = end_obj = date_list[0]
            range_mode = False

        try:
            import duration_report
        except Exception as e:
            logger.exception("Failed importing duration_report module")
            raise HTTPException(status_code=500, detail=f"duration module import failed: {e}")

        loop = asyncio.get_running_loop()

        def _to_json_safe(v):
            try:
                if pd.isna(v):
                    return None
            except Exception:
                pass
            if isinstance(v, (datetime, date)):
                return v.isoformat()
            if hasattr(v, "isoformat") and not isinstance(v, str):
                try:
                    return v.isoformat()
                except Exception:
                    pass
            try:
                if isinstance(v, (int, float, bool)):
                    return v
            except Exception:
                pass
            try:
                return str(v)
            except Exception:
                return None

        per_date_results: Dict[str, Any] = {}
        # run duration_report.run_for_date for each date concurrently but guarded by a per-date timeout
        for single_date in date_list:
            try:
                # schedule on threadpool
                fut = loop.run_in_executor(None, duration_report.run_for_date, single_date, regions_list, str(outdir_path), city)
                result = await asyncio.wait_for(fut, timeout=COMPUTE_WAIT_TIMEOUT_SECONDS)
                # result expected: dict(region -> {"swipes": DataFrame, "durations": DataFrame})
                per_date_results[single_date.isoformat()] = result if isinstance(result, dict) else {}
            except asyncio.TimeoutError:
                logger.exception("Duration computation timed out for date %s", single_date)
                # store diagnostic so front-end can show partial failure
                per_date_results[single_date.isoformat()] = {"__error": f"timeout after {COMPUTE_WAIT_TIMEOUT_SECONDS}s"}
            except Exception as e:
                logger.exception("duration run_for_date failed for date %s: %s", single_date, e)
                per_date_results[single_date.isoformat()] = {"__error": f"{e}"}

        dates_iso = [d.isoformat() for d in date_list]
        date_objs = [d for d in date_list]
        week_starts = sorted({(d - timedelta(days=d.weekday())) for d in date_objs})

        resp: Dict[str, Any] = {
            "start_date": start_obj.isoformat(),
            "end_date": end_obj.isoformat(),
            "regions": {},
            "diagnostics": {"per_date_status": {}}
        }

        # helper: if a returned value is a filesystem path to CSV(s), try reading with pandas
        def _coerce_result_region(region_val):
            # Accept dict with DataFrames or CSV paths (strings). Return dict {'durations': DataFrame, 'swipes': DataFrame}
            out = {"durations": pd.DataFrame(), "swipes": pd.DataFrame()}
            if not region_val:
                return out
            if isinstance(region_val, dict):
                dur = region_val.get("durations")
                sw = region_val.get("swipes")
                # if dur/sw are strings -> treat as CSV paths
                try:
                    if isinstance(dur, str):
                        p = Path(dur)
                        if p.exists():
                            if p.suffix.lower() == ".csv":
                                out["durations"] = pd.read_csv(p, parse_dates=["LocaleMessageTime"], dtype=str, error_bad_lines=False)
                            else:
                                out["durations"] = pd.read_csv(p, parse_dates=["LocaleMessageTime"], dtype=str, error_bad_lines=False)
                        else:
                            out["durations"] = pd.DataFrame()
                    elif isinstance(dur, pd.DataFrame):
                        out["durations"] = dur.copy()
                except Exception:
                    logger.exception("Failed to coerce durations into DataFrame (dur may be %s)", type(dur))

                try:
                    if isinstance(sw, str):
                        p = Path(sw)
                        if p.exists():
                            out["swipes"] = pd.read_csv(p, parse_dates=["LocaleMessageTime"], dtype=str, error_bad_lines=False)
                        else:
                            out["swipes"] = pd.DataFrame()
                    elif isinstance(sw, pd.DataFrame):
                        out["swipes"] = sw.copy()
                except Exception:
                    logger.exception("Failed to coerce swipes into DataFrame (sw may be %s)", type(sw))

                # also handle case region_val itself is a DataFrame (legacy)
                if isinstance(region_val, pd.DataFrame):
                    out["durations"] = region_val.copy()
                return out

            # unexpected types
            return out

        for r in regions_list:
            try:
                employees_map: Dict[str, Dict[str, Any]] = {}
                swipes_by_date: Dict[str, List[Dict[str, Any]]] = {}
                date_rows = {}

                # aggregate per-date
                for iso_d, day_res in per_date_results.items():
                    # check for error recorded earlier
                    if isinstance(day_res, dict) and "__error" in day_res:
                        resp["diagnostics"]["per_date_status"][iso_d] = day_res["__error"]
                        # mark empty row counts for this date
                        date_rows.setdefault(iso_d, {"rows": 0, "swipe_rows": 0})
                        swipes_by_date.setdefault(iso_d, [])
                        continue

                    region_obj = day_res.get(r) if isinstance(day_res, dict) else None
                    coerced = _coerce_result_region(region_obj)
                    durations_df = coerced.get("durations")
                    swipes_df = coerced.get("swipes")

                    rows_count = int(len(durations_df)) if isinstance(durations_df, pd.DataFrame) else 0
                    swipe_count = int(len(swipes_df)) if isinstance(swipes_df, pd.DataFrame) else 0
                    date_rows[iso_d] = {"rows": rows_count, "swipe_rows": swipe_count}

                    # SWIPES -> convert to list of dicts (safe)
                    if isinstance(swipes_df, pd.DataFrame) and not swipes_df.empty:
                        for c in ["LocaleMessageTime", "EmployeeName", "Door", "EmployeeID", "CardNumber", "PartitionName2", "PersonnelTypeName", "CompanyName", "PrimaryLocation", "Direction", "EmployeeIdentity", "MessageType", "Text5"]:
                            if c not in swipes_df.columns:
                                swipes_df[c] = None
                        swipe_records = []
                        for _, srow in swipes_df.iterrows():
                            dt = srow.get("LocaleMessageTime")
                            iso_ts = None
                            if pd.notna(dt):
                                try:
                                    iso_ts = pd.to_datetime(dt).isoformat()
                                except Exception:
                                    try:
                                        iso_ts = datetime.fromtimestamp(float(dt)).isoformat()
                                    except Exception:
                                        iso_ts = str(dt)
                            swipe_records.append({
                                "LocaleMessageTime": iso_ts,
                                "Dateonly": iso_d,
                                "Swipe_Time": (pd.to_datetime(srow.get("LocaleMessageTime")).time().isoformat() if pd.notna(srow.get("LocaleMessageTime")) else None),
                                "EmployeeID": srow.get("EmployeeID") if srow.get("EmployeeID") is not None else None,
                                "PersonGUID": srow.get("EmployeeIdentity") or srow.get("EmployeeIdentity"),
                                "ObjectName1": srow.get("EmployeeName"),
                                "Door": srow.get("Door"),
                                "PersonnelType": srow.get("PersonnelTypeName") or srow.get("PersonnelType"),
                                "CardNumber": srow.get("CardNumber"),
                                "Text5": srow.get("PrimaryLocation") or srow.get("Text5"),
                                "PartitionName2": srow.get("PartitionName2"),
                                "AdmitCode": srow.get("AdmitCode") or srow.get("MessageType"),
                                "Direction": srow.get("Direction"),
                                "CompanyName": srow.get("CompanyName"),
                                "PrimaryLocation": srow.get("PrimaryLocation") or srow.get("Text5"),
                            })
                        swipes_by_date[iso_d] = swipe_records
                    else:
                        swipes_by_date.setdefault(iso_d, [])

                    # DURATIONS -> iterate rows and aggregate per person
                    if isinstance(durations_df, pd.DataFrame) and not durations_df.empty:
                        # ensure columns exist
                        for col in ["person_uid", "EmployeeID", "EmployeeName", "CardNumber", "Date",
                                    "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor",
                                    "Duration", "DurationSeconds", "PersonnelTypeName", "PartitionName2",
                                    "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"]:
                            if col not in durations_df.columns:
                                durations_df[col] = None

                        for _, drow in durations_df.iterrows():
                            try:
                                person_uid = drow.get("person_uid")
                                if pd.isna(person_uid) or person_uid is None or str(person_uid).strip() == "":
                                    person_uid = f"{_to_json_safe(drow.get('EmployeeID'))}|{_to_json_safe(drow.get('EmployeeName'))}"

                                if person_uid not in employees_map:
                                    employees_map[person_uid] = {
                                        "person_uid": person_uid,
                                        "EmployeeID": _to_json_safe(drow.get("EmployeeID")),
                                        "EmployeeName": _to_json_safe(drow.get("EmployeeName")),
                                        "CardNumber": _to_json_safe(drow.get("CardNumber")),
                                        "durations": {d: None for d in dates_iso},
                                        "durations_seconds": {d: None for d in dates_iso},
                                        "total_seconds_present_in_range": 0,
                                        # keep internal First/Last but we'll remove them before returning
                                        "FirstSwipe": None,
                                        "LastSwipe": None,
                                        "FirstDoor": _to_json_safe(drow.get("FirstDoor")),
                                        "LastDoor": _to_json_safe(drow.get("LastDoor")),
                                        "PersonnelType": _to_json_safe(drow.get("PersonnelTypeName") or drow.get("PersonnelType")),
                                        "PartitionName2": _to_json_safe(drow.get("PartitionName2")),
                                        "CompanyName": _to_json_safe(drow.get("CompanyName")),
                                        "PrimaryLocation": _to_json_safe(drow.get("PrimaryLocation")),
                                        "FirstDirection": _to_json_safe(drow.get("FirstDirection")),
                                        "LastDirection": _to_json_safe(drow.get("LastDirection")),
                                    }

                                dur_str = None if pd.isna(drow.get("Duration")) else str(drow.get("Duration"))
                                dur_secs = None
                                try:
                                    v = drow.get("DurationSeconds")
                                    if pd.notna(v):
                                        dur_secs = int(float(v))
                                except Exception:
                                    dur_secs = None

                                employees_map[person_uid]["durations"][iso_d] = dur_str
                                employees_map[person_uid]["durations_seconds"][iso_d] = dur_secs
                                if dur_secs is not None:
                                    employees_map[person_uid]["total_seconds_present_in_range"] += dur_secs

                                # First/Last swipe times: keep earliest first, latest last
                                try:
                                    fs = drow.get("FirstSwipe")
                                    ls = drow.get("LastSwipe")
                                    if pd.notna(fs):
                                        fs_dt = pd.to_datetime(fs)
                                        cur_fs = employees_map[person_uid].get("FirstSwipe")
                                        if cur_fs is None:
                                            employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                        else:
                                            if pd.to_datetime(cur_fs) > fs_dt:
                                                employees_map[person_uid]["FirstSwipe"] = _to_json_safe(fs_dt)
                                    if pd.notna(ls):
                                        ls_dt = pd.to_datetime(ls)
                                        cur_ls = employees_map[person_uid].get("LastSwipe")
                                        if cur_ls is None:
                                            employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                                        else:
                                            if pd.to_datetime(cur_ls) < ls_dt:
                                                employees_map[person_uid]["LastSwipe"] = _to_json_safe(ls_dt)
                                except Exception:
                                    pass
                            except Exception:
                                logger.exception("Failed processing duration row for region %s date %s", r, iso_d)

                # Build list sorted by name
                emp_list = list(employees_map.values())
                emp_list.sort(key=lambda x: ((x.get("EmployeeName") or "") or "").lower())

                # SHIFT-FIX: attempt reconstruction when days look anomalous
                SHIFT_GAP_SECONDS = 6 * 3600        # gap > 6 hours -> new session
                SHIFT_MIN_FIX_SECONDS = 4 * 3600    # attempt fix if a day duration < 4h
                SHIFT_MAX_FIX_SECONDS = 20 * 3600   # or a day duration > 20h

                def _parse_swipe_ts(swipe_rec):
                    ts = swipe_rec.get("LocaleMessageTime")
                    if not ts:
                        return None
                    try:
                        return pd.to_datetime(ts)
                    except Exception:
                        try:
                            return datetime.fromisoformat(str(ts))
                        except Exception:
                            return None

                # Build swipes_by_person map
                swipes_by_person = {}
                for iso_d in dates_iso:
                    for s in swipes_by_date.get(iso_d, []):
                        pids = []
                        if s.get("PersonGUID"):
                            pids.append(str(s.get("PersonGUID")))
                        if s.get("EmployeeID") is not None:
                            pids.append(str(s.get("EmployeeID")))
                        if s.get("CardNumber") is not None:
                            pids.append(str(s.get("CardNumber")))
                        for pid in pids:
                            swipes_by_person.setdefault(pid, []).append(s)

                # sort each person's swipes by timestamp
                for pid, arr in list(swipes_by_person.items()):
                    arr_ts = []
                    for s in arr:
                        ts = _parse_swipe_ts(s)
                        if ts is not None:
                            arr_ts.append((ts, s))
                    arr_ts.sort(key=lambda x: x[0])
                    swipes_by_person[pid] = [s for _, s in arr_ts]

                def _needs_shift_fix(emp):
                    for v in (emp.get("durations_seconds") or {}).values():
                        if v is None:
                            continue
                        if v < SHIFT_MIN_FIX_SECONDS or v > SHIFT_MAX_FIX_SECONDS:
                            return True
                    return False

                for emp in emp_list:
                    try:
                        # assemble all swipes for this person from swipes_by_person using multiple identifiers
                        person_keys = []
                        if emp.get("person_uid"):
                            person_keys.append(str(emp["person_uid"]))
                        if emp.get("EmployeeID") is not None:
                            person_keys.append(str(emp["EmployeeID"]))
                        if emp.get("CardNumber") is not None:
                            person_keys.append(str(emp["CardNumber"]))

                        all_swipes = []
                        for k in person_keys:
                            lst = swipes_by_person.get(k) or []
                            for s in lst:
                                ts = _parse_swipe_ts(s)
                                if ts is not None:
                                    all_swipes.append((ts, s))
                        if not all_swipes:
                            continue
                        all_swipes.sort(key=lambda x: x[0])
                        swipe_times = [ts for ts, _ in all_swipes]

                        if not _needs_shift_fix(emp):
                            continue

                        sessions = []
                        cur_start = swipe_times[0]
                        cur_last = swipe_times[0]
                        for ts in swipe_times[1:]:
                            gap = (ts - cur_last).total_seconds()
                            if gap > SHIFT_GAP_SECONDS:
                                sessions.append((cur_start, cur_last))
                                cur_start = ts
                                cur_last = ts
                            else:
                                cur_last = ts
                        sessions.append((cur_start, cur_last))

                        new_durations_seconds = {d: None for d in dates_iso}
                        for s_start, s_end in sessions:
                            session_secs = max(0, int((s_end - s_start).total_seconds()))
                            session_date_iso = s_start.date().isoformat()
                            if session_date_iso not in new_durations_seconds:
                                continue
                            prev = new_durations_seconds.get(session_date_iso)
                            if prev is None:
                                new_durations_seconds[session_date_iso] = session_secs
                            else:
                                new_durations_seconds[session_date_iso] = prev + session_secs

                        # preserve previous per-day values for untouched days
                        for d in dates_iso:
                            if new_durations_seconds.get(d) is None and emp.get("durations_seconds", {}).get(d) is not None:
                                new_durations_seconds[d] = emp["durations_seconds"][d]

                        new_durations_str = {}
                        for d, secs in new_durations_seconds.items():
                            if secs is None:
                                new_durations_str[d] = None
                            else:
                                try:
                                    new_durations_str[d] = str(timedelta(seconds=int(secs)))
                                except Exception:
                                    new_durations_str[d] = None

                        total = 0
                        for v in new_durations_seconds.values():
                            if v is not None:
                                total += int(v)

                        emp["durations_seconds"] = new_durations_seconds
                        emp["durations"] = new_durations_str
                        emp["total_seconds_present_in_range"] = total
                    except Exception:
                        logger.exception("shift-fix reconstruction failed for employee %s", emp.get("person_uid") or emp.get("EmployeeID"))

                # compute per-employee weekly compliance and categories
                for emp in emp_list:
                    try:
                        weeks_info = {}
                        weeks_met = 0
                        weeks_total = 0

                        cat_counts = {"0-30m": 0, "30m-2h": 0, "2h-6h": 0, "6h-8h": 0, "8h+": 0}
                        cat_dates = {k: [] for k in cat_counts.keys()}

                        for ws in week_starts:
                            week_start_iso = ws.isoformat()
                            week_dates = [(ws + timedelta(days=i)).isoformat() for i in range(7)]
                            relevant_dates = [d for d in week_dates if d in dates_iso]
                            if not relevant_dates:
                                continue

                            days_present = 0
                            days_ge8 = 0
                            per_date_durations = {}
                            per_date_compliance = {}

                            for d in relevant_dates:
                                secs = emp["durations_seconds"].get(d)
                                per_date_durations[d] = secs
                                if secs is not None and secs > 0:
                                    days_present += 1
                                is_ge8 = (secs is not None and secs >= 28800)
                                if is_ge8:
                                    days_ge8 += 1
                                per_date_compliance[d] = True if is_ge8 else False

                                if secs is not None and secs > 0:
                                    # use duration_report.categorize_seconds if available
                                    cat = "0-30m"
                                    try:
                                        if hasattr(duration_report, 'categorize_seconds'):
                                            cat = duration_report.categorize_seconds(secs)
                                        else:
                                            # fallback: simple bucket
                                            if secs <= 1800:
                                                cat = "0-30m"
                                            elif secs <= 7200:
                                                cat = "30m-2h"
                                            elif secs <= 21600:
                                                cat = "2h-6h"
                                            elif secs < 28800:
                                                cat = "6h-8h"
                                            else:
                                                cat = "8h+"
                                    except Exception:
                                        cat = "0-30m"
                                    if cat in cat_counts:
                                        cat_counts[cat] += 1
                                        cat_dates[cat].append(d)

                            ct = int(compliance_target or 3)
                            compliant = False
                            # Basic rule: weeks considered only if at least ct days present AND those present days are >=8h
                            if days_present >= ct:
                                if days_present == days_ge8:
                                    compliant = True
                                else:
                                    compliant = False

                            weeks_info[week_start_iso] = {
                                "week_start": week_start_iso,
                                "dates": per_date_durations,
                                "dates_compliance": per_date_compliance,
                                "days_present": days_present,
                                "days_ge8": days_ge8,
                                "compliant": compliant
                            }

                            weeks_total += 1
                            if compliant:
                                weeks_met += 1

                        dominant_category = None
                        max_count = -1
                        for k, v in cat_counts.items():
                            if v > max_count:
                                max_count = v
                                dominant_category = k

                        # cleanup
                        for _k in ("FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor"):
                            if _k in emp:
                                try:
                                    del emp[_k]
                                except Exception:
                                    pass

                        emp["compliance"] = {
                            "weeks": weeks_info,
                            "weeks_met": weeks_met,
                            "weeks_total": weeks_total,
                            "month_summary": f"{weeks_met}/{weeks_total}" if weeks_total > 0 else "0/0",
                            "compliance_target": int(compliance_target or 3)
                        }
                        emp["duration_categories"] = {
                            "counts": cat_counts,
                            "dominant_category": dominant_category,
                            "category_dates": cat_dates,
                            "red_flag": cat_counts.get("2h-6h", 0)
                        }
                    except Exception:
                        logger.exception("Failed computing compliance/categories for employee %s", emp.get("person_uid"))

                durations_sample = emp_list[:sample_rows] if sample_rows and sample_rows > 0 else []

                resp["regions"][r] = {
                    "dates": dates_iso,
                    "employees": emp_list,
                    "durations_sample": durations_sample,
                    "date_rows": date_rows,
                    "swipes_by_date": swipes_by_date
                }
            except Exception:
                logger.exception("Failed to aggregate range results for region %s", r)
                resp["regions"][r] = {"dates": [d.isoformat() for d in date_list], "employees": [], "durations_sample": [], "date_rows": {}, "swipes_by_date": {}}

        return JSONResponse(resp)
    except HTTPException:
        raise
    except Exception as exc:
        logger.exception("api_duration (range) failed")
        raise HTTPException(status_code=500, detail=f"duration api error: {exc}")









2)

# duration_report.py
"""
duration_report.py

Updated: supports querying multiple monthly ACVSUJournal databases (current + previous N).
Fixes: category counting only for present days (so dominant category is correct).
"""
import argparse
import logging
import os
import re
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd

# Optional: import pyodbc only when connecting (allows importing this module even without driver)
try:
    import pyodbc
except Exception:
    pyodbc = None

ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# (REGION_CONFIG unchanged - omitted here for brevity in inline view; keep your config)
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "partitions": [
            "APAC.Default", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur"
        ]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "database": "ACVSUJournal_00011028",
        "last_n_databases": 2,
        "partitions": [
            "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
            "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
        ]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "partitions": [
            "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
            "PA.Panama City", "PE.Lima"
        ]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 2,
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# GENERIC_SQL_TEMPLATE and DB helper functions remain unchanged.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
INNER JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
INNER JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
"""

# helper functions (split name, expand DB list, connect master, filter existing dbs)
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# DB connection helper (same as before)
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "CardNumber", "PersonnelTypeName", "EmployeeIdentity",
        "PartitionName2", "LocaleMessageTime", "MessageType", "Direction", "CompanyName", "PrimaryLocation"
    ]
    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = get_connection(region_key)
    try:
        df = pd.read_sql(sql, conn)
    finally:
        conn.close()

    for c in cols:
        if c not in df.columns:
            df[c] = None

    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    return df[cols]

def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    if df["LocaleMessageTime"].dtype == object:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    df = df.drop_duplicates(subset=dedupe_cols, keep="first")

    df["Date"] = df["LocaleMessageTime"].dt.date

    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    df["person_uid"] = df.apply(make_person_uid, axis=1)
    df = df[df["person_uid"].notna()].copy()

    def agg_for_group(g):
        g_sorted = g.sort_values("LocaleMessageTime")
        first = g_sorted.iloc[0]
        last = g_sorted.iloc[-1]
        first_dir = first.get("Direction")
        last_dir = last.get("Direction")

        return pd.Series({
            "person_uid": first["person_uid"],
            "EmployeeIdentity": first.get("EmployeeIdentity"),
            "EmployeeID": first.get("EmployeeID"),
            "EmployeeName": first.get("EmployeeName"),
            "CardNumber": first.get("CardNumber"),
            "Date": first["Date"],
            "FirstSwipe": first["LocaleMessageTime"],
            "LastSwipe": last["LocaleMessageTime"],
            "FirstDoor": first.get("Door"),
            "LastDoor": last.get("Door"),
            "CountSwipes": int(len(g_sorted)),
            "PersonnelTypeName": first.get("PersonnelTypeName"),
            "PartitionName2": first.get("PartitionName2"),
            "CompanyName": first.get("CompanyName"),
            "PrimaryLocation": first.get("PrimaryLocation"),
            "FirstDirection": first_dir,
            "LastDirection": last_dir
        })

    grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    return grouped[out_cols]

def categorize_seconds(s: Optional[int]) -> str:
    """
    Category labels (used when secs present > 0):
      - "0-30m"      -> 0 .. 1800 (inclusive)
      - "30m-2h"     -> 1801 .. 7200
      - "2h-6h"      -> 7201 .. 21600
      - "6h-8h"      -> 21601 .. 28800
      - "8h+"        -> >= 28800
    Note: this helper alone will still return "0-30m" for s None/0, but in counting code we
    only increment when s is present and >0.
    """
    try:
        if s is None or s <= 0:
            return "0-30m"
        s = int(s)
        if s <= 1800:
            return "0-30m"
        if s <= 7200:
            return "30m-2h"
        if s <= 21600:
            return "2h-6h"
        if s < 28800:
            return "6h-8h"
        return "8h+"
    except Exception:
        return "0-30m"

def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        try:
            swipes = fetch_swipes_for_region(r, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", r)
            swipes = pd.DataFrame()

        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                swipes = swipes[combined_mask].copy()
            else:
                logging.debug("City filter provided but no matching columns to filter on for region %s", r)

        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", r)
            durations = pd.DataFrame()

        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
        try:
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", r)
        try:
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", r)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
        results[r] = {"swipes": swipes, "durations": durations}

    return results

# CLI runner (unchanged)
def parse_args():
    p = argparse.ArgumentParser(description="Generate daily duration report from ACVSUJournal swipe logs.")
    p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
    p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
                   default="apac,emea,laca,namer")
    p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
    p.add_argument("--city", help="Optional city/location filter (e.g. Pune)", default=None)
    return p.parse_args()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    args = parse_args()

    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    else:
        tz = ZoneInfo("Asia/Kolkata")
        target_date = datetime.now(tz).date()

    regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
    outdir = args.outdir

    logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
    results = run_for_date(target_date, regions, outdir, city=args.city)

    for r, obj in results.items():
        dur = obj.get("durations")
        logging.info("Region %s: %d persons with computed durations", r, len(dur) if dur is not None else 0)
    logging.info("Completed. CSVs are in %s", Path(outdir).absolute())











3)

#for Active Employee and ACtive Contractor Comparision Report.

# C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service.py
"""
Compare CCURE active lists (employees & contractors) with uploaded Active sheets (from disk).
Provides compare_ccure_vs_sheets(mode='full', stats_detail='ActiveProfiles', limit_list=200, export=False)

When export=True, writes Excel report to OUTPUT_DIR and returns 'report_path'.
"""

from datetime import datetime
import os
import re
import uuid
import logging
import sys
from pathlib import Path
import pandas as pd

logger = logging.getLogger("data_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Settings fallback (matches app.py pattern)
try:
    from settings import OUTPUT_DIR as SETTINGS_OUTPUT_DIR, DATA_DIR as SETTINGS_DATA_DIR, RAW_UPLOAD_DIR as SETTINGS_RAW_UPLOAD_DIR
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    RAW_UPLOAD_DIR = Path(SETTINGS_RAW_UPLOAD_DIR)
except Exception:
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"
    DATA_DIR = Path(__file__).resolve().parent / "data"
    RAW_UPLOAD_DIR = DATA_DIR / "raw_uploads"

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# ccure client helper (optional)
try:
    import ccure_client
except Exception:
    ccure_client = None
    logger.warning("ccure_client not importable; CCURE calls will return None")

# ---------- small normalizers (kept local) ----------
def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

def _make_w_variant(s):
    if s is None:
        return None
    ss = str(s).strip()
    if ss.upper().startswith('W'):
        return ss
    return 'W' + ss

def _numeric_variants(s):
    out = set()
    if s is None:
        return out
    try:
        s = str(s)
        clean = re.sub(r'\D', '', s)
        if clean:
            out.add(clean)
            out.add(clean.lstrip('0') or clean)
            out.add('W' + clean)
    except Exception:
        pass
    return out

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

# ---------- CCURE fetch helpers ----------
def _fetch_ccure_list(detail_name):
    """
    Uses ccure_client.fetch_all_stats(detail_name) if available, otherwise tries per-page fetch.
    Returns list of dicts or [].
    """
    if ccure_client is None:
        logger.warning("ccure_client missing - cannot fetch CCURE lists")
        return []
    try:
        if hasattr(ccure_client, "fetch_all_stats"):
            res = ccure_client.fetch_all_stats(detail_name, limit=500)
            return res or []
    except Exception:
        logger.exception("fetch_all_stats failed")
    # fallback to paged fetch if available
    try:
        data = []
        page = 1
        limit = 500
        while True:
            if not hasattr(ccure_client, "fetch_stats_page"):
                break
            page_res = ccure_client.fetch_stats_page(detail_name, page=page, limit=limit)
            if not page_res:
                break
            part = page_res.get("data") or []
            if not part:
                break
            data.extend(part)
            total = int(page_res.get("total") or len(data) or 0)
            if len(data) >= total:
                break
            page += 1
            if page > 1000:
                break
        return data
    except Exception:
        logger.exception("per-page fetch failed for %s", detail_name)
        return []

# ---------- helpers to find/load latest sheet from disk ----------
def _find_latest_file_for_kind(kind: str):
    """
    kind == 'employee' or 'contractor'
    Prefer canonical file in DATA_DIR: active_<kind>.*,
    otherwise pick latest in RAW_UPLOAD_DIR that contains the kind token.
    """
    # 1) canonical in DATA_DIR
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_{kind}{ext}"
        if p.exists():
            return p

    # 2) fallback: find latest raw file with kind token in RAW_UPLOAD_DIR
    try:
        files = [p for p in RAW_UPLOAD_DIR.iterdir() if p.is_file() and kind in p.name.lower() and p.suffix.lower() in (".xlsx", ".xls", ".csv")]
        if not files:
            # last fallback: any active_{kind}.* in RAW_UPLOAD_DIR
            files = [p for p in RAW_UPLOAD_DIR.iterdir() if p.is_file() and f"active_{kind}" in p.name.lower()]
        if not files:
            return None
        files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        return files[0]
    except Exception:
        return None

def _read_table(path: Path):
    try:
        if path.suffix.lower() == ".csv":
            df = pd.read_csv(path, dtype=str)
        else:
            df = pd.read_excel(path, sheet_name=0, dtype=str)
        df.columns = [str(c).strip() for c in df.columns]
        return df.fillna("")
    except Exception:
        logger.exception("Failed to read file %s", path)
        return pd.DataFrame()

def _first_present_from_row(row, candidates):
    for c in candidates:
        if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
            return row[c]
    for k in row.index:
        for c in candidates:
            if k.strip().lower() == c.strip().lower():
                val = row[k]
                if pd.notna(val) and str(val).strip() != "":
                    return val
    return None

# ---------- disk-based loaders to replace DB loaders ----------
def _load_active_employees_disk():
    """
    Return (set of normalized employee_ids, dict mapping id -> row sample, total_rows)
    Reads the latest employee sheet from data/.
    """
    path = _find_latest_file_for_kind("employee")
    if not path:
        return set(), {}, 0
    df = _read_table(path)
    ids = set()
    mapping = {}
    id_cols = ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id','Employee_Id']
    name_cols = ['Full Name','FullName','EmpName','Name','First Name','FirstName','Last Name']
    location_cols = ['Location City','Location','Location Description','City']
    status_cols = ['Current Status','Status','Employee Status','Employee_Status','Status (Current)','CurrentStatus']

    for _, row in df.iterrows():
        raw_row = row.to_dict()
        emp_id = _first_present_from_row(row, id_cols)
        if emp_id is None or str(emp_id).strip() == "":
            continue
        emp_id = str(emp_id).strip()
        ids.add(emp_id)
        full_name = _first_present_from_row(row, name_cols) or ""
        mapping[emp_id] = {
            "employee_id": emp_id,
            "full_name": full_name,
            "location_city": _first_present_from_row(row, location_cols),
            "status": _first_present_from_row(row, status_cols),
            "raw": raw_row
        }
    return ids, mapping, len(df)

def _load_active_contractors_disk():
    """
    Return (set of candidate contractor ids, mapping, total_rows)
    Reads latest contractor sheet from data/.
    """
    path = _find_latest_file_for_kind("contractor")
    if not path:
        return set(), {}, 0
    df = _read_table(path)
    ids = set()
    mapping = {}
    wsid_cols = ['Worker System Id','Worker System ID','Worker ID','WorkerSystemId','WorkerId']
    ipass_cols = ['iPass ID','"W" iPass ID','IPassID','iPassID','Ipass ID','IpassID']
    name_cols = ['Full Name','FullName','Name']
    vendor_cols = ['Vendor Company Name','Vendor']

    for _, row in df.iterrows():
        raw_row = row.to_dict()
        wsid = _first_present_from_row(row, wsid_cols)
        ipass = _first_present_from_row(row, ipass_cols)
        full_name = _first_present_from_row(row, name_cols)
        if wsid:
            wsid = str(wsid).strip()
            ids.add(wsid)
            mapping[wsid] = {"worker_system_id": wsid, "full_name": full_name, "vendor": _first_present_from_row(row, vendor_cols), "raw": raw_row}
        if ipass:
            ipass = str(ipass).strip()
            ids.add(ipass)
            mapping[ipass] = {"ipass_id": ipass, "full_name": full_name, "vendor": _first_present_from_row(row, vendor_cols), "raw": raw_row}
            wvar = _make_w_variant(ipass)
            ids.add(wvar)
            mapping[wvar] = {"ipass_id": ipass, "w_ipass": wvar, "full_name": full_name, "raw": raw_row}
        for cand in (wsid, ipass):
            if cand:
                for v in _numeric_variants(cand):
                    ids.add(v)
                    if v not in mapping:
                        mapping[v] = {"derived_id": v, "full_name": full_name, "raw": raw_row}
    return ids, mapping, len(df)

# ---------- helpers to match ccure -> disk lists (same logic as before) ----------
def _employee_matches_disk(cid, emp_ids_disk, emp_map_disk, ccure_row):
    if cid in emp_ids_disk:
        return True
    for v in _numeric_variants(cid):
        if v in emp_ids_disk:
            return True
    try:
        ci = _safe_int(cid)
        if ci is not None:
            for dbk in emp_ids_disk:
                if _safe_int(dbk) == ci:
                    return True
    except Exception:
        pass
    card_cid = _normalize_card_like(cid)
    if card_cid:
        for dbk in emp_ids_disk:
            if _normalize_card_like(dbk) == card_cid:
                return True
    try:
        name = None
        if isinstance(ccure_row, dict):
            name = ccure_row.get("EmpName") or ccure_row.get("FullName") or ccure_row.get("Name")
        normname = _normalize_name(name)
        if normname:
            for dbk, dbv in emp_map_disk.items():
                if _normalize_name(dbv.get("full_name")) == normname:
                    return True
    except Exception:
        pass
    return False

def _contractor_matches_disk(cid, contr_ids_disk, contr_map_disk, ccure_row):
    if cid in contr_ids_disk:
        return True
    for v in _numeric_variants(cid):
        if v in contr_ids_disk:
            return True
    try:
        ci = _safe_int(cid)
        if ci is not None:
            for dbk in contr_ids_disk:
                if _safe_int(dbk) == ci:
                    return True
    except Exception:
        pass
    card_cid = _normalize_card_like(cid)
    if card_cid:
        for dbk in contr_ids_disk:
            if _normalize_card_like(dbk) == card_cid:
                return True
    try:
        name = None
        if isinstance(ccure_row, dict):
            name = ccure_row.get("EmpName") or ccure_row.get("FullName") or ccure_row.get("Name")
        normname = _normalize_name(name)
        if normname:
            for dbk, dbv in contr_map_disk.items():
                if _normalize_name(dbv.get("full_name")) == normname:
                    return True
    except Exception:
        pass
    return False

# ---------- core compare function ----------
def compare_ccure_vs_sheets(mode="full", stats_detail="ActiveProfiles", limit_list=200, export=False):
    """
    Main public function used by /ccure/compare.
    Reads latest uploaded sheets from disk instead of DB tables.
    """
    result = {
        "ccure_active_employees_count": None,
        "ccure_active_contractors_count": None,
        "active_sheet_employee_count": None,
        "active_sheet_contractor_count": None,
        "missing_employees_count": None,
        "missing_contractors_count": None,
        "missing_employees_sample": [],
        "missing_contractors_sample": [],
        "report_path": None
    }

    # 1) fetch CCURE lists
    ccure_emps = _fetch_ccure_list("ActiveEmployees")
    ccure_contrs = _fetch_ccure_list("ActiveContractors")

    result["ccure_active_employees_count"] = len(ccure_emps)
    result["ccure_active_contractors_count"] = len(ccure_contrs)

    # 2) load uploaded sheets from disk (preferred)  returns (ids_set, mapping, total_rows)
    emp_ids_disk, emp_map_disk, emp_total_rows = _load_active_employees_disk()
    contr_ids_disk, contr_map_disk, contr_total_rows = _load_active_contractors_disk()

    result["active_sheet_employee_count"] = int(emp_total_rows)
    result["active_sheet_contractor_count"] = int(contr_total_rows)

    # 3) build ccure id sets for employees
    ccure_emp_id_set = set()
    ccure_emp_rows_by_id = {}
    for row in ccure_emps:
        try:
            eid = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("EmpID") or row.get("Employee Id"))
            if not eid:
                eid = _normalize_card_like(row.get("CardNumber") or row.get("iPass ID") or row.get("IPassID") or row.get("Card"))
            if not eid:
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    eid = f"name::{fname}"
            if eid:
                ccure_emp_id_set.add(eid)
                ccure_emp_rows_by_id[eid] = row
        except Exception:
            continue

    # 4) employees missing = ccure_emp_id_set - emp_ids_disk (but consider numeric variants, int equality, card-like, name match)
    expanded_emp_disk_ids = set(emp_ids_disk)
    for v in list(emp_ids_disk):
        for nv in _numeric_variants(v):
            expanded_emp_disk_ids.add(nv)

    missing_emp_ids = []
    for cid in ccure_emp_id_set:
        try:
            if str(cid).startswith("name::"):
                name = cid.split("::", 1)[1]
                found = False
                for dbk, dbv in emp_map_disk.items():
                    n = _normalize_name(dbv.get("full_name"))
                    if n and n == name:
                        found = True
                        break
                if not found:
                    missing_emp_ids.append(cid)
                continue

            ccure_row = ccure_emp_rows_by_id.get(cid) or {}
            if _employee_matches_disk(cid, expanded_emp_disk_ids, emp_map_disk, ccure_row):
                continue

            found = False
            for v in _numeric_variants(cid):
                if v in expanded_emp_disk_ids:
                    found = True
                    break
            if not found:
                missing_emp_ids.append(cid)
        except Exception:
            missing_emp_ids.append(cid)

    result["missing_employees_count"] = len(missing_emp_ids)
    samp_emp = []
    for mid in missing_emp_ids[:limit_list]:
        r = ccure_emp_rows_by_id.get(mid) or {}

        # extract manager/profile/status from raw if present
        manager_name = r.get("Manager_Name") or r.get("ManagerName") or r.get("Manager") or r.get("Manager_WU_ID")
        profile_disabled = r.get("Profile_Disabled") if "Profile_Disabled" in r else r.get("profile_disabled") if "profile_disabled" in r else r.get("ProfileDisabled") if "ProfileDisabled" in r else None
        employee_status = r.get("Employee_Status") or r.get("Employee Status") or r.get("Status") or r.get("employee_status")

        # ensure string conversion for boolean-like values
        if isinstance(profile_disabled, bool):
            profile_disabled = str(profile_disabled)

        # vendorCompany doesn't apply to employees  keep blank
        vendor_company = r.get("Vendor Company Name") or r.get("Vendor") or r.get("vendor") or ""

        # build sample row with both old and new key names for compatibility
        samp_emp.append({
            "ccure_key": mid,
            # old keys (kept for compatibility)
            "EmployeeID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "PersonnelType": r.get("PersonnelType"),
            # "VendorCompany": vendor_company,
            "Manager_Name": manager_name,
            "Profile_Disabled": profile_disabled,
            "Employee_Status": employee_status,
            # new/canonical lowerCamel keys requested
            "employee_Id": r.get("EmployeeID") or r.get("employee_id") or r.get("Employee Id"),
            "empName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "personnelType": r.get("PersonnelType"),
            "vendorCompany": vendor_company,
            "managerName": manager_name,
            "profileDisabled": profile_disabled,
            "employeeStatus": employee_status,
            "raw": r
        })
    result["missing_employees_sample"] = samp_emp

    # 5) contractors
    ccure_contr_id_set = set()
    ccure_contr_rows_by_id = {}
    for row in ccure_contrs:
        try:
            cand_ids = []
            e1 = _normalize_employee_key(row.get("EmployeeID") or row.get("employee_id") or row.get("Employee Id"))
            if e1:
                cand_ids.append(e1)
            ip = _normalize_employee_key(row.get("IPassID") or row.get("iPass ID") or row.get("iPass") or row.get("IPASSID"))
            if ip:
                cand_ids.append(ip)
                cand_ids.append(_make_w_variant(ip))
            cardlike = _normalize_card_like(row.get("CardNumber") or row.get("card_number") or row.get("Badge") or row.get("BadgeNo"))
            if cardlike:
                cand_ids.append(cardlike)
                cand_ids.extend(list(_numeric_variants(cardlike)))
            if not cand_ids:
                fname = _normalize_name(row.get("EmpName") or row.get("FullName") or row.get("Name"))
                if fname:
                    cand_ids.append(f"name::{fname}")
            for cid in cand_ids:
                if cid:
                    ccure_contr_id_set.add(cid)
                    ccure_contr_rows_by_id[cid] = row
            if not cand_ids:
                key = f"unknown::{uuid.uuid4().hex[:8]}"
                ccure_contr_id_set.add(key)
                ccure_contr_rows_by_id[key] = row
        except Exception:
            continue

    expanded_contr_disk_ids = set(contr_ids_disk)
    for v in list(contr_ids_disk):
        for nv in _numeric_variants(v):
            expanded_contr_disk_ids.add(nv)

    missing_contr_ids = []
    for cid in ccure_contr_id_set:
        try:
            if str(cid).startswith("name::"):
                name = cid.split("::", 1)[1]
                found = False
                for dbk, dbv in contr_map_disk.items():
                    n = _normalize_name(dbv.get("full_name"))
                    if n and n == name:
                        found = True
                        break
                if not found:
                    missing_contr_ids.append(cid)
                continue

            ccure_row = ccure_contr_rows_by_id.get(cid) or {}
            if _contractor_matches_disk(cid, expanded_contr_disk_ids, contr_map_disk, ccure_row):
                continue

            found = False
            for v in _numeric_variants(cid):
                if v in expanded_contr_disk_ids:
                    found = True
                    break
            if not found:
                missing_contr_ids.append(cid)
        except Exception:
            missing_contr_ids.append(cid)

    result["missing_contractors_count"] = len(missing_contr_ids)
    samp_contr = []
    for mid in missing_contr_ids[:limit_list]:
        r = ccure_contr_rows_by_id.get(mid) or {}

        # vendor/company
        vendor_company = r.get("Vendor Company Name") or r.get("Vendor") or r.get("vendor") or ""

        # personnel, manager, profile, status extraction
        personnel_type = r.get("PersonnelType") or r.get("Personnel_Type") or r.get("Personnel Type") or None
        manager_name = r.get("Manager_Name") or r.get("ManagerName") or r.get("Manager") or r.get("Manager_WU_ID")
        profile_disabled = r.get("Profile_Disabled") if "Profile_Disabled" in r else r.get("profile_disabled") if "profile_disabled" in r else None
        employee_status = r.get("Employee_Status") or r.get("Employee Status") or r.get("Status") or r.get("employee_status")

        if isinstance(profile_disabled, bool):
            profile_disabled = str(profile_disabled)

        samp_contr.append({
            "ccure_key": mid,
            # old keys kept
            "Employee_ID": r.get("EmployeeID"),
            "EmpName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "VendorCompany": vendor_company,
            "PersonnelType": personnel_type,
            "Manager_Name": manager_name,
            "Profile_Disabled": profile_disabled,
            "Employee_Status": employee_status,
            # new requested lowerCamel keys
            "employeeId": r.get("EmployeeID") or r.get("employee_id") or r.get("Employee Id"),
            "empName": r.get("EmpName") or r.get("FullName") or r.get("Name"),
            "vendorCompany": vendor_company,
            "personnelType": personnel_type,
            "managerName": manager_name,
            "profileDisabled": profile_disabled,
            "employeeStatus": employee_status,
            "raw": r
        })
    result["missing_contractors_sample"] = samp_contr

    # 6) optionally export report
    if export:
        try:
            OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
            ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            fname = f"missing_vs_ccure_{ts}.xlsx"
            fullpath = OUTPUT_DIR / fname

            # Canonical column set requested (lowerCamel)
            columns = ["ccure_key", "employee_Id", "empName", "personnelType", "managerName", "profileDisabled", "employeeStatus"]

            # Build DataFrames and ensure columns order (fill missing with empty string)
            if samp_emp:
                df_emp = pd.DataFrame(samp_emp)
                # reindex to requested columns; missing columns will be added with NaN
                df_emp = df_emp.reindex(columns=columns).fillna("")
            else:
                df_emp = pd.DataFrame(columns=columns)

            if samp_contr:
                df_con = pd.DataFrame(samp_contr)
                df_con = df_con.reindex(columns=columns).fillna("")
            else:
                df_con = pd.DataFrame(columns=columns)

            try:
                with pd.ExcelWriter(fullpath, engine="openpyxl") as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
            except Exception:
                # fallback to default engine
                with pd.ExcelWriter(fullpath) as writer:
                    df_emp.to_excel(writer, sheet_name="MissingEmployees", index=False)
                    df_con.to_excel(writer, sheet_name="MissingContractors", index=False)
                result["report_path"] = fname
        except Exception:
            logger.exception("Failed to export report")
            result["report_path"] = None

    return result






#export 

def export_uploaded_sheets():
    """
    Create a single xlsx workbook with:
      - Sheet "Employee" -> contents of latest canonical employee sheet (if present)
      - Sheet "Contractor" -> contents of latest canonical contractor sheet (if present)
    Writes file into OUTPUT_DIR and returns the filename (not full path).
    """
    try:
        emp_path = _find_latest_file_for_kind("employee")
        contr_path = _find_latest_file_for_kind("contractor")

        # Ensure OUTPUT_DIR exists
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
        fname = f"uploaded_sheets_{ts}.xlsx"
        fullpath = OUTPUT_DIR / fname

        # Read tables (if present) or create empty dataframe placeholders
        if emp_path and emp_path.exists():
            df_emp = _read_table(emp_path)
        else:
            df_emp = pd.DataFrame()

        if contr_path and contr_path.exists():
            df_con = _read_table(contr_path)
        else:
            df_con = pd.DataFrame()

        # Write to one workbook (try openpyxl if available)
        try:
            with pd.ExcelWriter(fullpath, engine="openpyxl") as writer:
                df_emp.to_excel(writer, sheet_name="Employee", index=False)
                df_con.to_excel(writer, sheet_name="Contractor", index=False)
        except Exception:
            # fallback to default engine
            with pd.ExcelWriter(fullpath) as writer:
                df_emp.to_excel(writer, sheet_name="Employee", index=False)
                df_con.to_excel(writer, sheet_name="Contractor", index=False)

        return fname
    except Exception:
        logger.exception("export_uploaded_sheets failed")
        return None


# Expose public function name expected by app.py
__all__ =  ["compare_ccure_vs_sheets", "compare_ccure_vs_sheets", "export_uploaded_sheets"]  # keep backwards compatibility
# End of data_compare_service.py







4)

# data_compare_service_v2.py
"""
Comparison service (v2) with broadened matching heuristics, safer prefetch/cache,
and explicit Sheet vs AttendanceSummary comparison diagnostics.

Drop-in replacement for your existing data_compare_service_v2.py
"""

import sys
import re
import uuid
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
from typing import Optional, Dict, Any, List, Tuple

import pandas as pd

# HTTP client for region histories
try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except Exception:
    requests = None

# DB imports (same as your project)
from db import SessionLocal
from models import AttendanceSummary

# Settings / defaults
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR, REGION_HISTORY_URLS as SETTINGS_REGION_HISTORY_URLS
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
    REGION_HISTORY_URLS = SETTINGS_REGION_HISTORY_URLS
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"
    REGION_HISTORY_URLS = [
        "http://10.199.22.57:3008/api/occupancy/history",  # APAC
        "http://10.199.22.57:3006/api/occupancy/history",  # NAMER
        "http://10.199.22.57:3007/api/occupancy/history",  # EMEA
        "http://10.199.22.57:4000/api/occupancy/history"   # LACA
    ]

DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger("data_compare_service_v2")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# In-memory cache for prefetched region history entries
REGION_HISTORY_CACHE = None
REGION_HISTORY_CACHE_FETCHED_AT = None
REGION_HISTORY_CACHE_TTL_SECONDS = 300  # 5 minutes

# Matching config
ID_FIELD_CANDIDATES = [
    "EmployeeID","employeeId","Employee Id","EmpID","Emp Id","EmpNo","EmployeeNo","Employee_Number",
    "PersonID","PersonId","personId","person_id","employee_id","id","Id","employeeNumber","EmployeeNumber",
    "worker_system_id","wsid","WorkerID","Worker System Id","workerId","WorkerSystemId"
]
CARD_FIELD_CANDIDATES = [
    "CardNumber","Card","cardNumber","card_number","BadgeNumber","BadgeNo","Badge","badgeNumber","badge_no",
    "iPassID","IPassID","iPass","i_pass_id","CardNo","card_no","card","IPASSID","IPass"
]
NAME_FIELD_CANDIDATES = [
    "FullName","Full Name","EmpName","Name","full_name","displayName","personName","PersonName"
]

# ----------------------------
# Utilities
# ----------------------------
def _find_active_employee_file():
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_employee{ext}"
        if p.exists():
            return p
    for p in DATA_DIR.iterdir():
        if p.is_file() and "active_employee" in p.name.lower():
            return p
    return None

def _normalize_key(k):
    if k is None:
        return None
    try:
        s = str(k).strip()
        return s if s != "" else None
    except Exception:
        return None

def _digits_only(s):
    if s is None:
        return ""
    return re.sub(r'\D+', '', str(s))

def _safe_int(v, default=0):
    try:
        if v is None or v == "":
            return default
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return default

def _week_monday_and_friday(ref_date: Optional[date] = None):
    if ref_date is None:
        ref_date = date.today()
    iso = ref_date.isoweekday()
    monday = ref_date - timedelta(days=(iso - 1))
    friday = monday + timedelta(days=4)
    return monday, friday

def _maybe_mark_on_leave(status_str: Optional[str]) -> bool:
    if not status_str:
        return False
    s = str(status_str).strip().lower()
    for tok in ("leave", "vacation", "on leave", "holiday", "sabbatical", "furlough", "loa"):
        if tok in s:
            return True
    return False

# ----------------------------
# Load active sheet
# ----------------------------
def load_active_employees_dataframe() -> pd.DataFrame:
    src = _find_active_employee_file()
    if not src:
        raise FileNotFoundError(f"Active employee canonical file not found in {DATA_DIR}")
    ext = src.suffix.lower()
    if ext in (".xls", ".xlsx"):
        df = pd.read_excel(src, sheet_name=0, dtype=str)
    else:
        df = pd.read_csv(src, dtype=str)
    df.columns = [c.strip() for c in df.columns]

    def _first_present(row, candidates):
        for c in candidates:
            if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
                return row[c]
        return None

    rows = []
    for _, row in df.iterrows():
        emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id','EmployeeNo','Employee No'])
        if emp_id is None:
            continue
        full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or (
            f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
        )
        location_city = _first_present(row, ['Location City','Location','City'])
        location_desc = _first_present(row, ['Location Description','LocationDescription'])
        location_state = _first_present(row, ['Location State / Province','Location State','State','Province'])
        region_code = _first_present(row, ['Region Code','Region','RegionCode'])
        current_status = _first_present(row, ['Current Status','Status','Employee Status'])
        employee_type = _first_present(row, ['Employee Type','Type','Time Type'])
        rows.append({
            "employee_id": _normalize_key(emp_id),
            "full_name": _normalize_key(full_name),
            "location_city": _normalize_key(location_city),
            "location_desc": _normalize_key(location_desc),
            "location_state": _normalize_key(location_state),
            "region_code": (str(region_code).strip() if region_code is not None else None),
            "current_status": _normalize_key(current_status),
            "employee_type": _normalize_key(employee_type),
            "raw_row": row.to_dict()
        })
    ndf = pd.DataFrame(rows)
    for col in ("employee_id","full_name","location_city","location_desc","location_state","region_code","current_status","employee_type","raw_row"):
        if col not in ndf:
            ndf[col] = None
    ndf = ndf[ndf["employee_id"].notna() & (ndf["employee_id"].str.strip() != "")]
    ndf.reset_index(drop=True, inplace=True)
    return ndf

# ----------------------------
# HTTP Session factory (retries/backoff)
# ----------------------------
def _build_requests_session():
    if requests is None:
        return None
    s = requests.Session()
    allowed = frozenset(['GET', 'HEAD'])
    try:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=allowed
        )
    except TypeError:
        retry = Retry(
            total=3,
            backoff_factor=0.6,
            status_forcelist=(429, 500, 502, 503, 504),
            method_whitelist=allowed
        )
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

# ----------------------------
# Prefetch / cache helpers
# ----------------------------
def prefetch_region_history(timeout: int = 10, force: bool = False):
    """
    Fetch region history entries (cached). Returns list of raw entries.
    """
    global REGION_HISTORY_CACHE, REGION_HISTORY_CACHE_FETCHED_AT
    try:
        now = datetime.utcnow()
        if not force and REGION_HISTORY_CACHE is not None and REGION_HISTORY_CACHE_FETCHED_AT:
            elapsed = (now - REGION_HISTORY_CACHE_FETCHED_AT).total_seconds()
            if elapsed < REGION_HISTORY_CACHE_TTL_SECONDS:
                logger.info("[region_cache] Using cached region history (age %.1fs)", elapsed)
                return REGION_HISTORY_CACHE

        entries = []
        # Prefer region_clients when available
        try:
            import region_clients
            logger.info("[region_cache] fetching region history via region_clients.fetch_all_history()")
            try:
                got = region_clients.fetch_all_history(timeout=timeout)
            except TypeError:
                got = region_clients.fetch_all_history()
            entries = got or []
        except Exception:
            entries = []

        # If empty, try direct requests to configured URLs
        if not entries and requests is not None:
            logger.info("[region_cache] fetching region history directly from endpoints")
            session = _build_requests_session() or requests
            for url in REGION_HISTORY_URLS:
                if not url:
                    continue
                try:
                    resp = session.get(url, timeout=(3, max(5, timeout)))
                    if not resp or resp.status_code != 200:
                        continue
                    try:
                        payload = resp.json()
                    except Exception:
                        continue
                    # flatten possible lists
                    if isinstance(payload, list):
                        for p in payload:
                            if isinstance(p, dict):
                                p['_source_url'] = url
                                entries.append(p)
                    elif isinstance(payload, dict):
                        found_list = False
                        for k in ("results","summaryByDate","details","data","entries","list","people","items"):
                            if k in payload and isinstance(payload[k], list):
                                for p in payload[k]:
                                    if isinstance(p, dict):
                                        p['_source_url'] = url
                                        entries.append(p)
                                found_list = True
                                break
                        if not found_list:
                            payload['_source_url'] = url
                            entries.append(payload)
                except requests.exceptions.RequestException as e:
                    logger.warning("[region_cache] fetch failed for %s: %s", url, str(e))
                    continue

        REGION_HISTORY_CACHE = entries or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        logger.info("[region_cache] prefetched %d region history entries", len(REGION_HISTORY_CACHE))
        return REGION_HISTORY_CACHE
    except Exception:
        logger.exception("[region_cache] prefetch failed")
        REGION_HISTORY_CACHE = REGION_HISTORY_CACHE or []
        REGION_HISTORY_CACHE_FETCHED_AT = datetime.utcnow()
        return REGION_HISTORY_CACHE

# ----------------------------
# Payload helpers (deep search)
# ----------------------------
def _iter_scalars_in_obj(obj, parent_key=""):
    """
    Yield all scalar key->value pairs inside nested dict/list structures as (key_path, value).
    """
    if isinstance(obj, dict):
        for k, v in obj.items():
            new_key = f"{parent_key}.{k}" if parent_key else str(k)
            if isinstance(v, (dict, list)):
                yield from _iter_scalars_in_obj(v, parent_key=new_key)
            else:
                yield new_key, v
    elif isinstance(obj, list):
        for idx, it in enumerate(obj):
            new_key = f"{parent_key}[{idx}]" if parent_key else f"[{idx}]"
            if isinstance(it, (dict, list)):
                yield from _iter_scalars_in_obj(it, parent_key=new_key)
            else:
                yield new_key, it

def _extract_details_from_payload(payload):
    """
    Normalize to list of dict rows that look like detail records.
    """
    if payload is None:
        return []
    if isinstance(payload, list):
        return [p for p in payload if isinstance(p, dict)]
    if isinstance(payload, dict):
        for k in ("details","results","data","entries","list","people","items"):
            v = payload.get(k)
            if isinstance(v, list):
                return [x for x in v if isinstance(x, dict)]
        # single-record-like payload (date-summary) -> return as single element to allow higher-level scanner to inspect fields
        if any(k in payload for k in ("date","Employee","Contractor","total","day")) and len(payload.keys()) <= 40:
            return [payload]
    return []

# ----------------------------
# ID matching helpers
# ----------------------------
def _match_candidate_to_employees(candidate_raw, orig_ids_set, orig_ids_list, digits_map):
    """
    Try many heuristics to map candidate_raw to one of the orig_ids_list.
    Returns matched_orig_id or None.
    """
    if candidate_raw is None:
        return None
    cand_str = str(candidate_raw).strip()
    if not cand_str:
        return None

    # direct exact match
    if cand_str in orig_ids_set:
        return cand_str

    # direct case-insensitive match
    for o in orig_ids_list:
        if isinstance(o, str) and cand_str.lower() == o.lower():
            return o

    # numeric transformations
    cand_digits = _digits_only(cand_str)
    if cand_digits:
        cand_nolead = cand_digits.lstrip('0') or cand_digits
        # direct digits match to original ids
        for o in orig_ids_list:
            if not isinstance(o, str):
                continue
            if o == cand_digits or o == cand_nolead:
                return o
            od = _digits_only(o)
            if od == cand_digits or od == cand_nolead:
                return o
        # numeric equality by int
        try:
            ci = int(cand_nolead)
            for o in orig_ids_list:
                try:
                    od = _digits_only(o)
                    if not od:
                        continue
                    oi = int(od.lstrip('0') or od)
                    if oi == ci:
                        return o
                except Exception:
                    continue
        except Exception:
            pass

        # last-n digits heuristics (conservative)
        if len(cand_digits) >= 3:
            for n in (6, 4):
                suf = cand_digits[-n:]
                if not suf:
                    continue
                for o in orig_ids_list:
                    od = _digits_only(o)
                    if od and od.endswith(suf):
                        return o

    # strip common prefixes and retry
    up = cand_str.upper()
    for pref in ("W", "IPASS", "IPASSID", "IPass"):
        if up.startswith(pref):
            stripped = cand_str[len(pref):]
            m = _match_candidate_to_employees(stripped, orig_ids_set, orig_ids_list, digits_map)
            if m:
                return m

    # fallback: find numeric substrings inside string and try mapping
    for match in re.finditer(r'(\d{3,})', cand_str):
        ssub = match.group(1)
        m = _match_candidate_to_employees(ssub, orig_ids_set, orig_ids_list, digits_map)
        if m:
            return m

    return None

# ----------------------------
# Region history scanning -> build presence map
# ----------------------------
def _fetch_presence_from_region_histories(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None, preloaded_entries: Optional[List[dict]] = None):
    """
    Scans preloaded_entries (or global cache) and returns presence mapping: {employee_id -> {date: 0/1}}
    """
    presence = {eid: {} for eid in employee_ids}
    if not employee_ids:
        return presence

    if preloaded_entries is None:
        global REGION_HISTORY_CACHE
        preloaded_entries = REGION_HISTORY_CACHE or []

    if not preloaded_entries:
        logger.info("[region_history] no preloaded region history entries to scan")
    else:
        orig_ids_list = [str(e).strip() for e in employee_ids]
        orig_ids_set = set(orig_ids_list)
        digits_map = {e: _digits_only(e) for e in orig_ids_list}

        scanned = 0
        matched = 0
        examples_matched = []

        for entry in preloaded_entries:
            detail_rows = []
            if isinstance(entry, dict):
                # prefer explicit lists
                for key in ("details","people","items","list","results","entries","data"):
                    if key in entry and isinstance(entry.get(key), list):
                        detail_rows = [r for r in entry.get(key) if isinstance(r, dict)]
                        break
                if not detail_rows:
                    # treat entry itself as a candidate detail row if it has plausible fields (timestamp or id-like)
                    candidate_keys = set(ID_FIELD_CANDIDATES + CARD_FIELD_CANDIDATES + ["date","timestamp","time","SwipeDate","LocaleMessageTime","day"])
                    if any(k in entry for k in candidate_keys):
                        detail_rows = [entry]
                    else:
                        # try extract via helper (covers nested payload shapes)
                        detail_rows = _extract_details_from_payload(entry) or []
            else:
                continue

            scanned += len(detail_rows)
            for d in detail_rows:
                try:
                    # find a timestamp / date for row
                    ts = None
                    # common keys first
                    for tkey in ("LocaleMessageTime","LocaleMessageDateTime","SwipeDate","SwipeTime","timestamp","time","DateTime","date","swipeDate","day"):
                        if tkey in d and d.get(tkey):
                            ts = d.get(tkey)
                            break

                    # fallback: scan scalar values in row for iso-like date
                    if ts is None:
                        for k, v in _iter_scalars_in_obj(d):
                            if v is None:
                                continue
                            try:
                                s = str(v)
                            except Exception:
                                continue
                            # quick heuristic
                            if re.search(r'\d{4}-\d{2}-\d{2}', s) or re.search(r'\d{2}/\d{2}/\d{4}', s):
                                ts = s
                                break

                    if ts is None:
                        # skip row (no date info)
                        continue

                    # parse timestamp into date (robust)
                    t = None
                    if isinstance(ts, (int, float)):
                        try:
                            t = datetime.fromtimestamp(int(ts))
                        except Exception:
                            try:
                                t = datetime.utcfromtimestamp(int(ts) / 1000.0)
                            except Exception:
                                t = None
                    elif isinstance(ts, str):
                        s = ts.strip()
                        if s == "":
                            t = None
                        else:
                            parsed = None
                            # try dateutil if available (best)
                            try:
                                from dateutil import parser as _parser
                                parsed = _parser.parse(s)
                            except Exception:
                                parsed = None
                            if parsed:
                                try:
                                    if parsed.tzinfo is not None:
                                        parsed = parsed.astimezone(tz=None).replace(tzinfo=None)
                                except Exception:
                                    pass
                                t = parsed
                            else:
                                # try common formats
                                fmts = [
                                    "%Y-%m-%dT%H:%M:%S.%fZ",
                                    "%Y-%m-%dT%H:%M:%S.%f",
                                    "%Y-%m-%dT%H:%M:%S",
                                    "%Y-%m-%d %H:%M:%S",
                                    "%Y-%m-%d",
                                    "%d/%m/%Y %H:%M:%S",
                                    "%d/%m/%Y"
                                ]
                                for f in fmts:
                                    try:
                                        t = datetime.strptime(s, f)
                                        break
                                    except Exception:
                                        t = None
                    if t is None:
                        continue

                    dt = t.date()
                    if dt < start_date or dt > end_date:
                        continue

                    # partition filter (if provided)
                    if partition_filter:
                        part_values = []
                        for k in ("PartitionNameFriendly","PartitionName","PrimaryLocation","partition","location","Partition","Location","Site","PartitionName1","PartitionName2"):
                            v = d.get(k)
                            if v:
                                part_values.append(str(v))
                        if part_values:
                            ok = any(part_filter_match(part, partition_filter) for part in part_values)
                            if not ok:
                                continue
                        else:
                            # no partition info -> skip when filter exists
                            continue

                    # attempt to match explicit id keys first
                    matched_key = None
                    for k in ID_FIELD_CANDIDATES:
                        if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                            m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                            if m:
                                matched_key = m
                                break

                    # try card fields
                    if not matched_key:
                        for k in CARD_FIELD_CANDIDATES:
                            if k in d and d.get(k) is not None and str(d.get(k)).strip() != "":
                                m = _match_candidate_to_employees(d.get(k), orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break

                    # deep-scan scalars for numeric substrings and name fields
                    if not matched_key:
                        for key_path, val in _iter_scalars_in_obj(d):
                            if val is None:
                                continue
                            sval = str(val)
                            # numeric substring preference
                            if re.search(r'\d{3,}', sval):
                                m = _match_candidate_to_employees(sval, orig_ids_set, orig_ids_list, digits_map)
                                if m:
                                    matched_key = m
                                    break
                        # check name fields (disabled by default to avoid false positives)

                    if matched_key:
                        matched += 1
                        # coerce matched_key to exact string from orig list
                        matched_key = next((o for o in orig_ids_list if str(o).strip() == str(matched_key).strip()), str(matched_key).strip())
                        presence.setdefault(matched_key, {})
                        presence[matched_key][dt] = 1
                        if len(examples_matched) < 10:
                            examples_matched.append({"matched": matched_key, "date": dt.isoformat(), "sample_row_keys": list(d.keys())[:8]})
                except Exception:
                    continue

        logger.info("[region_history] scanned %d detail rows from preloaded entries; matched %d presence entries", scanned, matched)
        if examples_matched:
            logger.debug("[region_history] example matches (up to 10): %s", examples_matched)

    # fill zeros for any missing date
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            presence.setdefault(eid, {})
            if cur not in presence[eid]:
                presence[eid][cur] = 0
        cur = cur + timedelta(days=1)

    return presence

def part_filter_match(src_val, partition_filter):
    try:
        if not src_val:
            return False
        return partition_filter.strip().lower() in str(src_val).strip().lower()
    except Exception:
        return False

# ----------------------------
# DB / combined fetch
# ----------------------------
def _fetch_presence_for_employees(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None):
    """
    1) chunked DB IN queries (AttendanceSummary)
    2) fallback broad DB query
    3) fallback region history cache scan (prefetch_region_history)
    Returns mapping {employee_id: {date: 0/1}}
    """
    if not employee_ids:
        return {}

    orig_ids = [str(e).strip() for e in employee_ids]
    norm_id_set = set([s for s in orig_ids if s])
    result = {eid: {} for eid in orig_ids}

    # 1) chunked DB fetch
    rows = []
    chunk_size = 500
    try:
        with SessionLocal() as db:
            for i in range(0, len(orig_ids), chunk_size):
                chunk = orig_ids[i:i+chunk_size]
                try:
                    q = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date,
                        AttendanceSummary.employee_id.in_(chunk)
                    )
                    rows_chunk = q.all()
                    if rows_chunk:
                        rows.extend(rows_chunk)
                except Exception:
                    logger.exception("chunked query failed (continuing)")
                    continue

            # fallback broad query if none found
            if not rows:
                try:
                    rows = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date
                    ).all()
                    logger.info("[presence_fetch] fallback broad DB query returned %d rows for %s -> %s", len(rows), start_date, end_date)
                except Exception:
                    logger.exception("fallback broad DB query failed")
                    rows = []
    except Exception:
        logger.exception("DB session error in _fetch_presence_for_employees")
        rows = []

    # map DB rows to provided employee ids
    for r in rows:
        try:
            raw = r.employee_id
            if raw is None:
                continue
            db_key = str(raw).strip()
            match_key = None
            if db_key in norm_id_set:
                match_key = db_key
            else:
                digits = _digits_only(db_key)
                if digits:
                    cand = digits.lstrip('0') or digits
                    if cand in norm_id_set:
                        match_key = cand
                if match_key is None:
                    for o in orig_ids:
                        if o == db_key or o.lstrip('0') == db_key or db_key.lstrip('0') == o:
                            match_key = o
                            break
            if not match_key:
                continue
            d = r.date
            present = 0
            try:
                present = int(r.presence_count or 0)
            except Exception:
                present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
            result.setdefault(match_key, {})
            prev = result[match_key].get(d, 0)
            result[match_key][d] = 1 if (prev == 1 or present > 0) else 0
        except Exception:
            continue

    # fill zeros
    cur = start_date
    while cur <= end_date:
        for eid in orig_ids:
            result.setdefault(eid, {})
            if cur not in result[eid]:
                result[eid][cur] = 0
        cur = cur + timedelta(days=1)

    db_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] DB-derived presence found for %d/%d employees", db_positive, len(orig_ids))

    # fallback to region details/history if needed
    if db_positive == 0 or db_positive < max(10, int(0.1 * len(orig_ids))):
        try:
            logger.info("[presence_fetch] DB coverage low (%d/%d) - trying region occupancy detail/history fallback", db_positive, len(orig_ids))

            # 1) Try region_clients.fetch_all_details() first (per-person rows are most useful)
            try:
                import region_clients
                details = []
                if hasattr(region_clients, "fetch_all_details"):
                    try:
                        details = region_clients.fetch_all_details(timeout=10)
                    except TypeError:
                        details = region_clients.fetch_all_details()
                details = details or []
                if details:
                    logger.info("[presence_fetch] fetched %d detail rows from region_clients.fetch_all_details()", len(details))
                    # attempt to match details to employee ids and fill presence
                    for d in details:
                        try:
                            # extract candidate id fields (fast checks)
                            cand_fields = []
                            for k in ("EmployeeID","employee_id","EmpID","CardNumber","Card","CardNo","IPassID","iPassID","PersonGUID","PersonId"):
                                v = d.get(k) if isinstance(d, dict) else None
                                if v and str(v).strip():
                                    cand_fields.append(v)
                            # if no explicit candidate id, attempt to deep-scan for numeric substring
                            if not cand_fields:
                                for key_path, val in _iter_scalars_in_obj(d):
                                    if val is None:
                                        continue
                                    sval = str(val)
                                    if re.search(r'\d{3,}', sval):
                                        cand_fields.append(sval)
                            # determine date for this row (fast heuristics)
                            ts = None
                            for tkey in ("LocaleMessageTime","SwipeDate","SwipeTime","timestamp","time","DateTime","date"):
                                if tkey in d and d.get(tkey):
                                    ts = d.get(tkey)
                                    break
                            if ts is None:
                                # quick scalar scan for iso date fragment
                                for key_path, val in _iter_scalars_in_obj(d):
                                    if val is None:
                                        continue
                                    s = str(val)
                                    if re.search(r'\d{4}-\d{2}-\d{2}', s) or re.search(r'\d{2}/\d{2}/\d{4}', s):
                                        ts = s
                                        break
                            if ts is None:
                                continue
                            # parse to date - handle common string formats without heavy dateutil
                            parsed_dt = None
                            if isinstance(ts, (int, float)):
                                try:
                                    parsed_dt = datetime.fromtimestamp(int(ts))
                                except Exception:
                                    try:
                                        parsed_dt = datetime.utcfromtimestamp(int(ts) / 1000.0)
                                    except Exception:
                                        parsed_dt = None
                            elif isinstance(ts, str):
                                s = ts.strip()
                                # try ISO-like fast parse
                                m = re.search(r'(\d{4}-\d{2}-\d{2})', s)
                                if m:
                                    try:
                                        parsed_dt = datetime.fromisoformat(m.group(1))
                                    except Exception:
                                        try:
                                            parsed_dt = datetime.strptime(m.group(1), "%Y-%m-%d")
                                        except Exception:
                                            parsed_dt = None
                                else:
                                    # fallback: try a couple common formats
                                    for f in ("%Y-%m-%dT%H:%M:%S.%fZ","%Y-%m-%dT%H:%M:%S","%d/%m/%Y %H:%M:%S","%d/%m/%Y"):
                                        try:
                                            parsed_dt = datetime.strptime(s, f)
                                            break
                                        except Exception:
                                            parsed_dt = None
                            if parsed_dt is None:
                                continue
                            dt = parsed_dt.date()
                            if dt < start_date or dt > end_date:
                                continue

                            # attempt to map candidate fields to orig_ids using the same matching helper
                            for cand in cand_fields:
                                try:
                                    m = _match_candidate_to_employees(cand, set(orig_ids), orig_ids, {})  # re-use existing matcher
                                    if m:
                                        # coerce matched to orig string
                                        matched_key = next((o for o in orig_ids if str(o).strip() == str(m).strip()), str(m).strip())
                                        result.setdefault(matched_key, {})
                                        prev = result[matched_key].get(dt, 0)
                                        result[matched_key][dt] = 1 if (prev == 1 or 1) else prev
                                        break
                                except Exception:
                                    continue
                        except Exception:
                            continue
                else:
                    logger.info("[presence_fetch] region_clients.fetch_all_details returned no details")
            except Exception:
                logger.exception("region_clients.fetch_all_details fallback failed (continuing)")

            # 2) ensure region history cache is populated and try scanning preloaded history (less precise)
            try:
                prefetch_region_history()
                region_presence = _fetch_presence_from_region_histories(orig_ids, start_date, end_date, partition_filter=partition_filter, preloaded_entries=REGION_HISTORY_CACHE)
                for eid in orig_ids:
                    rp = region_presence.get(eid, {})
                    for d, v in rp.items():
                        if v and result.setdefault(eid, {}).get(d, 0) == 0:
                            result[eid][d] = 1
            except Exception:
                logger.exception("region_history fallback failed")
        except Exception:
            logger.exception("detail/history fallback collapsed")

    final_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] final presence coverage: %d/%d employees have at least one positive day", final_positive, len(orig_ids))
    return result

# ----------------------------
# Sheet vs AttendanceSummary comparison
# ----------------------------
def _compare_sheet_vs_db_summary(sel_df: pd.DataFrame, start_date: date, end_date: date) -> Dict[str, Any]:
    """
    For each employee in sel_df compute DB-derived weekly presence totals and produce mismatch diagnostics.
    Returns dict with:
      - per_employee_summary (employee_id -> {sheet:..., db_total:..., db_by_date: {...}})
      - missing_in_summary (list of employee rows with zero DB presence)
      - mismatches (list where sheet vs DB total differ)
      - extra_summary_ids (IDs present in DB during week but not in sheet selection)
    """
    out = {
        "per_employee_summary": {},
        "missing_in_summary": [],
        "mismatches": [],
        "extra_summary_ids": []
    }
    employee_ids = sel_df["employee_id"].astype(str).str.strip().tolist()
    employee_set = set(employee_ids)

    # fetch attendance rows for the range and for relevant employee ids (DB query)
    try:
        with SessionLocal() as db:
            rows = db.query(AttendanceSummary).filter(
                AttendanceSummary.date >= start_date,
                AttendanceSummary.date <= end_date
            ).all()
    except Exception:
        logger.exception("Failed to query AttendanceSummary for sheet-vs-db comparison; will attempt per-id chunked queries")
        rows = []

    # Map DB rows by normalized employee id
    db_map = {}
    for r in rows:
        try:
            rid = r.employee_id
            if rid is None:
                continue
            rk = str(rid).strip()
            # prefer exact string match to sheet ids
            candidates = [rk]
            digits = _digits_only(rk)
            if digits:
                candidates.append(digits.lstrip('0') or digits)
            # find best matching sheet id
            matched = None
            for c in candidates:
                if c in employee_set:
                    matched = c
                    break
            if not matched:
                # try more expensive matching
                for s in employee_ids:
                    if s == rk or rk.lstrip('0') == s or s.lstrip('0') == rk:
                        matched = s
                        break
            if not matched:
                # add as extra (db-only) under its raw key
                db_map.setdefault(rk, {})
                db_map[rk].setdefault(r.date, 0)
                try:
                    present = int(r.presence_count or 0)
                except Exception:
                    present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
                db_map[rk][r.date] = 1 if (db_map[rk].get(r.date, 0) == 1 or present > 0) else db_map[rk].get(r.date, 0)
            else:
                db_map.setdefault(matched, {})
                db_map[matched].setdefault(r.date, 0)
                try:
                    present = int(r.presence_count or 0)
                except Exception:
                    present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0
                db_map[matched][r.date] = 1 if (db_map[matched].get(r.date, 0) == 1 or present > 0) else db_map[matched].get(r.date, 0)
        except Exception:
            continue

    # ensure all sheet employees have entries per day (0 default)
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            db_map.setdefault(eid, {})
            if cur not in db_map[eid]:
                db_map[eid][cur] = 0
        cur = cur + timedelta(days=1)

    # compute per-employee summary and mismatches
    for eid in employee_ids:
        try:
            db_by_date = {d.isoformat(): int(v) for d, v in db_map.get(eid, {}).items()}
            db_total = sum(int(v) for v in db_map.get(eid, {}).values())
            # sheet-side: we don't have presence per-day on the sheet; so sheet's weekly presence is unknown.
            # But we can at least mark employees missing from DB or with low coverage.
            entry = {
                "employee_id": eid,
                "full_name": sel_df[sel_df["employee_id"] == eid]["full_name"].iloc[0] if not sel_df[sel_df["employee_id"] == eid].empty else None,
                "db_total_week_presence": db_total,
                "db_by_date": db_by_date
            }
            out["per_employee_summary"][eid] = entry
            if db_total == 0:
                out["missing_in_summary"].append(entry)
            # If you had a target expected presence from sheet metadata you could compare here; for now, put thresholds:
            # We'll treat employees with db_total < 3 (for regulars) as defaulters -> included in mismatches so they are visible
            out["mismatches"].append(entry) if db_total < 3 else None
        except Exception:
            continue

    # compute extras: DB IDs with presence >0 that are not in sheet employee_ids
    extras = []
    for dbid, bydates in db_map.items():
        try:
            if dbid not in employee_set:
                total = sum(int(v) for v in bydates.values())
                if total > 0:
                    extras.append({"employee_id": dbid, "db_total_week_presence": total, "db_by_date": {d.isoformat(): v for d, v in bydates.items()}})
        except Exception:
            continue
    out["extra_summary_ids"] = extras

    return out

# ----------------------------
# Main compare function (public)
# ----------------------------
def compare_ccure_vs_sheets(
    mode: str = "full",
    stats_detail: str = "ActiveProfiles",
    limit_list: int = 200,
    export: bool = False,
    region_filter: Optional[str] = None,
    location_city: Optional[str] = None,
    location_state: Optional[str] = None,
    location_description: Optional[str] = None,
    week_ref_date: Optional[str] = None
):
    # compute week window
    if week_ref_date:
        monday, friday = _week_monday_and_friday(date.fromisoformat(week_ref_date))
    else:
        monday, friday = _week_monday_and_friday(date.today())

    try:
        df = load_active_employees_dataframe()
    except Exception as e:
        logger.exception("Failed to load active employees sheet")
        return {"error": f"active sheet load failed: {e}"}

    rf = region_filter.strip().lower() if region_filter else None
    lc = location_city.strip().lower() if location_city else None
    ls = location_state.strip().lower() if location_state else None
    ld = location_description.strip().lower() if location_description else None

    sel = df.copy()
    if rf:
        sel = sel[sel["region_code"].fillna("").str.strip().str.lower() == rf]
    if lc:
        sel = sel[sel["location_city"].fillna("").str.strip().str.lower() == lc]
    if ls:
        sel = sel[sel["location_state"].fillna("").str.strip().str.lower() == ls]
    if ld:
        sel = sel[sel["location_desc"].fillna("").str.strip().str.lower() == ld]

    total_active = len(sel)
    employee_ids = sel["employee_id"].astype(str).str.strip().tolist()

    # Ensure region history cache is primed (should be done by caller/app)
    try:
        prefetch_region_history(timeout=10)
    except Exception:
        logger.exception("prefetch_region_history failed (continuing)")

    # presence_map: best-effort using DB + region history fallback
    presence_map = _fetch_presence_for_employees(employee_ids, monday, friday, partition_filter=location_city)

    # compute today count (today inside week may differ; we compute today's presence using presence_map and DB fallback)
    today = date.today()
    today_count = 0
    for eid in employee_ids:
        pm = presence_map.get(eid, {})
        if pm.get(today, 0) > 0:
            today_count += 1
        else:
            # fallback DB check
            try:
                with SessionLocal() as db:
                    row = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == eid, AttendanceSummary.date == today).first()
                    if row and getattr(row, "presence_count", 0) > 0:
                        today_count += 1
                        continue
                    digits = _digits_only(eid)
                    if digits:
                        cand = digits.lstrip('0') or digits
                        row2 = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == cand, AttendanceSummary.date == today).first()
                        if row2 and getattr(row2, "presence_count", 0) > 0:
                            today_count += 1
            except Exception:
                continue

    today_pct = round((today_count / float(total_active)) * 100.0, 2) if total_active > 0 else None

    sel["on_leave"] = sel["current_status"].apply(lambda x: _maybe_mark_on_leave(x))
    leave_count = int(sel["on_leave"].sum())
    sel["employee_type_norm"] = sel["employee_type"].fillna("").str.strip().str.lower()
    type_counts = sel["employee_type_norm"].value_counts().to_dict()

    regular_df = sel[sel["employee_type_norm"].str.contains("regular", na=False)]
    regular_ids = regular_df["employee_id"].astype(str).str.strip().tolist()

    regular_presence = {}
    for eid in regular_ids:
        week_map = presence_map.get(eid, {})
        days_present = sum(1 for d, v in week_map.items() if v and (monday <= d <= friday))
        days_present = int(days_present)
        regular_presence[eid] = {
            "days_present": days_present,
            "on_leave": bool(sel[sel["employee_id"] == eid]["on_leave"].any()),
            "full_name": sel[sel["employee_id"] == eid]["full_name"].iloc[0] if not sel[sel["employee_id"] == eid].empty else None
        }

    present_5_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 5)
    present_3_or_more_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 3)
    present_less_than_3_count = sum(1 for v in regular_presence.values() if v["days_present"] < 3)
    present_only_1_count = sum(1 for v in regular_presence.values() if v["days_present"] == 1)

    present_5_list = []
    present_3_list = []
    defaulters_list = []

    for eid, info in regular_presence.items():
        entry = {
            "employee_id": eid,
            "full_name": info["full_name"],
            "days_present": info["days_present"],
            "on_leave": info["on_leave"]
        }
        if info["days_present"] >= 5:
            present_5_list.append(entry)
        if info["days_present"] >= 3:
            present_3_list.append(entry)
        if info["days_present"] < 3:
            defaulters_list.append(entry)

    present_5_list = sorted(present_5_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    present_3_list = sorted(present_3_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    defaulters_list = sorted(defaulters_list, key=lambda x: (x["days_present"], x["on_leave"], x["full_name"] or ""))[:limit_list]

    # NEW: explicit sheet vs DB summary comparison diagnostics
    try:
        sheet_vs_summary = _compare_sheet_vs_db_summary(sel, monday, friday)
    except Exception:
        logger.exception("sheet vs summary comparison failed")
        sheet_vs_summary = {"error": "comparison failure"}

    summary = {
        "filters": {
            "region": region_filter,
            "location_city": location_city,
            "location_state": location_state,
            "location_description": location_description,
            "week_monday": monday.isoformat(),
            "week_friday": friday.isoformat()
        },
        "counts": {
            "total_active_in_sheet": total_active,
            "today_headcount_from_summary": today_count,
            "today_headcount_pct_vs_sheet": today_pct,
            "on_leave_count_in_sheet": leave_count,
            "employee_type_counts": type_counts
        },
        "regular_attendance_summary": {
            "regular_total": len(regular_ids),
            "present_5_day_count": present_5_count,
            "present_3_or_more_count": present_3_or_more_count,
            "present_less_than_3_count": present_less_than_3_count,
            "present_only_1_day_count": present_only_1_count
        },
        # attach diagnostics
        "sheet_vs_summary": sheet_vs_summary
    }

    details = {
        "present_5_days": present_5_list,
        "present_3_or_more_days": present_3_list,
        "defaulters_less_than_3_days": defaulters_list,
        "sheet_vs_summary_rows": sheet_vs_summary.get("per_employee_summary") if isinstance(sheet_vs_summary, dict) else {}
    }

    report_path = None
    if export:
        try:
            report_name = f"attendance_compare_{(region_filter or 'all')}_{(location_city or 'all')}_{uuid.uuid4().hex[:8]}.xlsx"
            report_file = OUTPUT_DIR / report_name
            with pd.ExcelWriter(report_file, engine="openpyxl") as writer:
                sel_df_for_export = sel.copy()
                sel_df_for_export["raw_row_str"] = sel_df_for_export["raw_row"].apply(lambda r: str(r) if r is not None else "")
                sel_df_for_export.to_excel(writer, sheet_name="active_sheet_selection", index=False)
                pd.DataFrame([summary["counts"]]).to_excel(writer, sheet_name="summary_counts", index=False)
                pd.DataFrame([summary["regular_attendance_summary"]]).to_excel(writer, sheet_name="regular_summary", index=False)
                pd.DataFrame(details["present_5_days"]).to_excel(writer, sheet_name="present_5_days", index=False)
                pd.DataFrame(details["present_3_or_more_days"]).to_excel(writer, sheet_name="present_3_plus", index=False)
                pd.DataFrame(details["defaulters_less_than_3_days"]).to_excel(writer, sheet_name="defaulters_lt3", index=False)
                # sheet vs summary diagnostics
                try:
                    # per_employee_summary -> long table
                    per_emp = sheet_vs_summary.get("per_employee_summary") or {}
                    per_emp_rows = []
                    for k, v in per_emp.items():
                        r = {"employee_id": k, "full_name": v.get("full_name"), "db_total_week_presence": v.get("db_total_week_presence")}
                        # flatten db_by_date
                        for d, val in sorted((v.get("db_by_date") or {}).items()):
                            r[f"day_{d}"] = val
                        per_emp_rows.append(r)
                    if per_emp_rows:
                        pd.DataFrame(per_emp_rows).to_excel(writer, sheet_name="sheet_vs_db_per_employee", index=False)
                    if sheet_vs_summary.get("missing_in_summary"):
                        pd.DataFrame(sheet_vs_summary.get("missing_in_summary")).to_excel(writer, sheet_name="missing_in_summary", index=False)
                    if sheet_vs_summary.get("extra_summary_ids"):
                        pd.DataFrame(sheet_vs_summary.get("extra_summary_ids")).to_excel(writer, sheet_name="extra_summary_ids", index=False)
                except Exception:
                    logger.exception("Failed writing sheet_vs_summary sections to export")
            report_path = str(report_file.name)
        except Exception:
            logger.exception("Failed to write export report")
            report_path = None

    out = {
        "mode": mode,
        "stats_detail": stats_detail,
        "summary": summary,
        "details": details
    }
    if report_path:
        out["report_path"] = report_path
    return out


if __name__ == "__main__":
    res = compare_ccure_vs_sheets(region_filter="APAC", location_city="Pune", export=False, limit_list=20)
    import json as _json
    print(_json.dumps(res, indent=2, default=str))









5)# C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\compare_service.py
import pandas as pd
import numpy as np
from datetime import datetime, date, timezone
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary
import re
from dateutil import parser as dateutil_parser
import traceback
import logging

logger = logging.getLogger("compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# --- Helpers -----------------------------------------------------------------
def _to_native(value):
    if value is None:
        return None
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    if isinstance(value, (np.integer,)):
        return int(value)
    if isinstance(value, (np.floating,)):
        return float(value)
    if isinstance(value, (np.bool_, bool)):
        return bool(value)
    try:
        import datetime as _dt
        if isinstance(value, _dt.datetime):
            try:
                if value.tzinfo is not None:
                    utc = value.astimezone(timezone.utc)
                    return utc.replace(tzinfo=None).isoformat() + "Z"
                else:
                    return value.isoformat()
            except Exception:
                return str(value)
        if hasattr(value, 'isoformat'):
            try:
                return value.isoformat()
            except Exception:
                return str(value)
    except Exception:
        pass
    return value

def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

# timestamp parsing helpers (unchanged)
def _parse_timestamp_from_value(val):
    if val is None:
        return None
    import datetime as _dt
    if isinstance(val, _dt.datetime):
        dt = val
        try:
            if dt.tzinfo is not None:
                return dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            return dt
    try:
        import numpy as _np
        if isinstance(val, (int, float, _np.integer, _np.floating)):
            v = int(val)
            if v > 1e12:
                return _dt.fromtimestamp(v / 1000.0, tz=timezone.utc).replace(tzinfo=None)
            if v > 1e9:
                return _dt.fromtimestamp(v, tz=timezone.utc).replace(tzinfo=None)
    except Exception:
        pass
    if isinstance(val, str):
        s = val.strip()
        if s == "":
            return None
        try:
            dt = dateutil_parser.parse(s)
            if dt.tzinfo is not None:
                dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            fmts = ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M:%S.%f",
                    "%d/%m/%Y %H:%M:%S", "%d-%m-%Y %H:%M:%S",
                    "%Y-%m-%dT%H:%M:%S")
            for fmt in fmts:
                try:
                    return _dt.strptime(s, fmt)
                except Exception:
                    pass
    return None

def _extract_timestamp_from_detail(detail):
    fields = [
        "LocaleMessageDateTime", "LocalMessageDateTime", "LocaleMessageTime", "LocalMessageTime",
        "LocaleMessageDate", "Timestamp", "timestamp", "Time", "LocaleTime", "LocalTime",
        "time", "date", "LocaleMessageDateTimeUtc", "LocalMessageDateTimeUtc",
        "Swipe_Time", "SwipeTime", "SwipeTimeLocal", "SwipeTimestamp", "SwipeDateTime"
    ]
    if isinstance(detail, dict):
        for k in fields:
            if k in detail:
                dt = _parse_timestamp_from_value(detail.get(k))
                if dt is not None:
                    return dt
        for v in detail.values():
            dt = _parse_timestamp_from_value(v)
            if dt is not None:
                return dt
    else:
        return _parse_timestamp_from_value(detail)
    return None



# --- Main functions ----------------------------------------------------------

def ingest_live_details_list(details_list):
    """Persist details_list into LiveSwipe. returns counts."""
    from db import SessionLocal as _SessionLocal
    inserted = 0
    skipped = 0
    with _SessionLocal() as db:
        for d in details_list:
            try:
                ts_parsed = _extract_timestamp_from_detail(d)
            except Exception:
                ts_parsed = None
            if ts_parsed is None:
                # skip rows without parseable timestamp
                skipped += 1
                continue

            # robust extraction of employee id and card fields (many alias names)
            emp = None
            for k in ("EmployeeID", "employee_id", "employeeId", "Employee Id", "EmpID", "Emp Id"):
                if isinstance(d, dict) and k in d:
                    emp = d.get(k)
                    break
            emp = _normalize_employee_key(emp)

            card = None
            for k in ("CardNumber", "card_number", "Card", "Card No", "CardNo", "Badge", "BadgeNo", "badge_number", "IPassID", "iPass ID"):
                if isinstance(d, dict) and k in d:
                    card = d.get(k)
                    break
            card = _normalize_card_like(card)

            full_name = None
            for k in ("ObjectName1", "FullName", "full_name", "EmpName", "Name"):
                if isinstance(d, dict) and k in d:
                    full_name = d.get(k)
                    break

            partition = None
            for k in ("PartitionName2", "PartitionName1", "Partition", "PartitionName", "Region"):
                if isinstance(d, dict) and k in d:
                    partition = d.get(k)
                    break

            floor = d.get("Floor") if isinstance(d, dict) else None
            door = None
            for k in ("Door", "DoorName", "door"):
                if isinstance(d, dict) and k in d:
                    door = d.get(k)
                    break

            region = d.get("__region") if isinstance(d, dict) and "__region" in d else d.get("Region") if isinstance(d, dict) else None

            try:
                rec = LiveSwipe(
                    timestamp=ts_parsed,
                    employee_id=emp,
                    card_number=card,
                    full_name=full_name,
                    partition=partition,
                    floor=floor,
                    door=door,
                    region=region,
                    raw=d
                )
                db.add(rec)
                inserted += 1
            except Exception:
                # skip insertion errors but continue
                db.rollback()
                skipped += 1
                continue
        db.commit()
    print(f"[ingest_live_details_list] inserted={inserted} skipped={skipped}")
    return {"inserted": inserted, "skipped_invalid_timestamp": skipped}


def compute_daily_attendance(target_date: date):
    """Build AttendanceSummary rows for target_date (upserts)."""
    with SessionLocal() as db:
        start = datetime.combine(target_date, datetime.min.time())
        end = datetime.combine(target_date, datetime.max.time())
        swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
        if not swipes:
            print(f"[compute_daily_attendance] no swipes for {target_date}")
            return []

        rows = []
        for s in swipes:
            rows.append({
                "id": s.id,
                "timestamp": s.timestamp,
                "employee_id": _normalize_employee_key(s.employee_id),
                "card_number": _normalize_card_like(s.card_number),
                "full_name": s.full_name,
                "partition": s.partition,
                "floor": s.floor,
                "door": s.door
            })
        df = pd.DataFrame(rows)
        if df.empty:
            print(f"[compute_daily_attendance] dataframe empty after rows -> {target_date}")
            return []

        # create grouping key: prefer employee_id, otherwise card_number
        df['key'] = df['employee_id'].fillna(df['card_number'])
        df = df[df['key'].notna()]
        if df.empty:
            print("[compute_daily_attendance] no usable keys after filling employee_id/card")
            return []

        grouped = df.groupby('key', dropna=False).agg(
            presence_count=('id', 'count'),
            first_seen=('timestamp', 'min'),
            last_seen=('timestamp', 'max'),
            full_name=('full_name', 'first'),
            partition=('partition', 'first'),
            card_number=('card_number', 'first')
        ).reset_index().rename(columns={'key': 'employee_id'})

        # upsert AttendanceSummary rows (merge)
        for _, row in grouped.iterrows():
            try:
                derived_obj = {
                    "partition": (row.get('partition') or None),
                    "full_name": (row.get('full_name') or None),
                    "card_number": (row.get('card_number') or None)
                }
                rec = AttendanceSummary(
                    employee_id=str(row['employee_id']) if pd.notna(row['employee_id']) else None,
                    date=target_date,
                    presence_count=int(row['presence_count']),
                    first_seen=row['first_seen'],
                    last_seen=row['last_seen'],
                    derived=derived_obj
                )
                db.merge(rec)
            except Exception as e:
                print("[compute_daily_attendance] upsert error:", e)
                continue
        db.commit()
        print(f"[compute_daily_attendance] built {len(grouped)} attendance keys for {target_date}")
        return grouped.to_dict(orient='records')


def compare_with_active(target_date: date):
    """Compare AttendanceSummary for date with ActiveEmployee & ActiveContractor and return json-safe dict."""
    # NOTE: we intentionally do NOT import get_global_stats_or_none from ccure_client;
    # a local helper wrapper below will call ccure_client.get_global_stats() safely.
    with SessionLocal() as db:
        att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == target_date).all()
        if not att_rows:
            att_df = pd.DataFrame(columns=["employee_id", "presence_count", "first_seen", "last_seen", "card_number", "partition", "full_name"])
        else:
            att_df = pd.DataFrame([{
                "employee_id": _normalize_employee_key(a.employee_id),
                "presence_count": a.presence_count,
                "first_seen": a.first_seen,
                "last_seen": a.last_seen,
                "card_number": _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None),
                "partition": (a.derived.get('partition') if (a.derived and isinstance(a.derived, dict)) else None),
                "full_name": (a.derived.get('full_name') if (a.derived and isinstance(a.derived, dict)) else None)
            } for a in att_rows])

        act_rows = db.query(ActiveEmployee).all()
        contractor_rows = db.query(ActiveContractor).all()

        # Build maps & active list
        act_list = []
        card_to_emp = {}
        name_to_emp = {}

        # Employees
        for e in act_rows:
            emp_id_norm = _normalize_employee_key(e.employee_id)
            # extract card-like from raw if present
            card_from_raw = None
            try:
                rr = e.raw_row or {}
                if isinstance(rr, dict):
                    ck_list = [
                        "CardNumber","card_number","Card","Card No","CardNo","IPassID","IpassID","iPass ID","IPASSID",
                        "Badge Number","BadgeNo","Badge"
                    ]
                    for ck in ck_list:
                        v = rr.get(ck)
                        if v:
                            ckey = _normalize_card_like(v)
                            if ckey:
                                card_from_raw = ckey
                                break
                    # fallback: scan all values for numeric candidate
                    if not card_from_raw:
                        for v in rr.values():
                            try:
                                tmp = _normalize_card_like(v)
                                if tmp and 3 <= len(tmp) <= 12:
                                    card_from_raw = tmp
                                    break
                            except Exception:
                                pass
            except Exception:
                card_from_raw = None

            act_list.append({
                "employee_id": emp_id_norm,
                "full_name": e.full_name,
                "location_city": e.location_city,
                "status": e.current_status,
                "card_number": card_from_raw
            })
            if emp_id_norm:
                card_to_emp[emp_id_norm] = emp_id_norm
            if card_from_raw:
                card_to_emp[card_from_raw] = emp_id_norm
            n = _normalize_name(e.full_name)
            if n:
                name_to_emp[n] = emp_id_norm

        # Contractors
        for c in contractor_rows:
            worker_id = _normalize_employee_key(c.worker_system_id)
            ipass = _normalize_employee_key(c.ipass_id)
            w_ipass = ("W" + ipass) if ipass and not str(ipass).startswith("W") else ipass
            primary_id = worker_id or ipass or None
            act_list.append({
                "employee_id": primary_id,
                "full_name": c.full_name,
                "location_city": c.location,
                "status": c.status,
                "card_number": None
            })
            if primary_id:
                card_to_emp[primary_id] = primary_id
            if ipass:
                card_to_emp[ipass] = primary_id
            if w_ipass:
                card_to_emp[w_ipass] = primary_id
            try:
                rr = c.raw_row or {}
                if isinstance(rr, dict):
                    for ck in ("Worker System Id","Worker System ID","iPass ID","IPASSID","CardNumber","card_number"):
                        if ck in rr and rr.get(ck):
                            key = _normalize_card_like(rr.get(ck))
                            if key:
                                card_to_emp[key] = primary_id
            except Exception:
                pass
            n = _normalize_name(c.full_name)
            if n:
                name_to_emp[n] = primary_id

        act_df = pd.DataFrame(act_list)

        # If no active rows, return attendance-only view
        if act_df.empty:
            if att_df.empty:
                return {"by_location": [], "merged": [], "ccure": get_global_stats_or_none()}
            att_df['partition'] = att_df.get('partition').fillna('Unknown')
            att_df['presence_count'] = att_df['presence_count'].fillna(0)
            att_df['present_today'] = att_df['presence_count'].apply(lambda x: bool(x and x != 0))
            loc_group = att_df.groupby('partition', dropna=False).agg(
                total_n=('employee_id', 'count'),
                present_n=('present_today', 'sum')
            ).reset_index().rename(columns={'partition':'location_city'})
            loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
            by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]
            merged_list = []
            for r in att_df.to_dict(orient='records'):
                merged_list.append({
                    "employee_id": _to_native(r.get('employee_id')),
                    "presence_count": _to_native(r.get('presence_count')),
                    "first_seen": _to_native(r.get('first_seen')),
                    "last_seen": _to_native(r.get('last_seen')),
                    "full_name": _to_native(r.get('full_name')),
                    "location_city": _to_native(r.get('partition')),
                    "present_today": _to_native(r.get('present_today'))
                })
            return {"by_location": by_location, "merged": merged_list, "ccure": get_global_stats_or_none()}

        # normalize columns
        act_df['employee_id'] = act_df['employee_id'].astype(object).apply(_normalize_employee_key)
        att_df['employee_id'] = att_df['employee_id'].astype(object).apply(_normalize_employee_key)
        act_df['card_number'] = act_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in act_df.columns else pd.Series([pd.NA]*len(act_df))
        att_df['card_number'] = att_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in att_df.columns else pd.Series([pd.NA]*len(att_df))

        # ensure card_to_emp includes act_df card_numbers
        for r in act_df.to_dict(orient='records'):
            c = r.get('card_number')
            eid = r.get('employee_id')
            if c and eid:
                card_to_emp[c] = eid
            if eid:
                # also map numeric-only forms of eid
                n = re.sub(r'\D','', str(eid))
                if n:
                    card_to_emp[n.lstrip('0') or n] = eid

        # mapping function tries multiple strategies
        emp_set = set([x for x in act_df['employee_id'].dropna().astype(str)])

        def numeric_variants(s):
            s = str(s)
            clean = re.sub(r'\D','', s)
            variants = set()
            if clean:
                variants.add(clean)
                variants.add(clean.lstrip('0') or clean)
                if not s.startswith('W'):
                    variants.add('W' + clean)
            return list(variants)

        def remap_att_key(row):
            primary = row.get('employee_id') or None
            card = row.get('card_number') or None

            primary_norm = _normalize_employee_key(primary)
            card_norm = _normalize_card_like(card)

            # 1) exact employee id exists in active list
            if primary_norm and primary_norm in emp_set:
                return primary_norm

            # 2) numeric-variants of primary may map to card_to_emp
            if primary_norm:
                for v in numeric_variants(primary_norm):
                    if v in card_to_emp:
                        return card_to_emp[v]
                if primary_norm in card_to_emp:
                    return card_to_emp[primary_norm]

            # 3) direct card mapping
            if card_norm:
                if card_norm in card_to_emp:
                    return card_to_emp[card_norm]
                if (card_norm.lstrip('0') or card_norm) in card_to_emp:
                    return card_to_emp[card_norm.lstrip('0') or card_norm]
                if ('W' + card_norm) in card_to_emp:
                    return card_to_emp['W' + card_norm]

            # 4) name matching fallback
            fname = _normalize_name(row.get('full_name') or row.get('full_name_att') or None)
            if fname and fname in name_to_emp:
                return name_to_emp[fname]

            # 5) last resort - return primary_norm (maybe non-mapped) so it still shows up
            return primary_norm or card_norm or None

        att_df['mapped_employee_id'] = att_df.apply(remap_att_key, axis=1)

        # drop original employee_id column to avoid duplicate label conflict
        att_merge_df = att_df.drop(columns=['employee_id'], errors='ignore').copy()

        # merge left: act_df left_on employee_id, right_on mapped_employee_id
        merged = pd.merge(
            act_df,
            att_merge_df,
            left_on='employee_id',
            right_on='mapped_employee_id',
            how='left',
            suffixes=('', '_att')
        )

        # fill and finalize
        merged['presence_count'] = merged.get('presence_count', pd.Series([0]*len(merged))).fillna(0)
        # ensure ints when possible
        def safe_int(v):
            try:
                if pd.isna(v):
                    return 0
                iv = int(float(v))
                return iv
            except Exception:
                return v
        merged['presence_count'] = merged['presence_count'].apply(safe_int)
        merged['present_today'] = merged['presence_count'].apply(lambda x: bool(x and x != 0))
        merged['location_city'] = merged.get('location_city').fillna('Unknown')

        # by_location
        loc_group = merged.groupby('location_city', dropna=False).agg(
            total_n=('employee_id', 'count'),
            present_n=('present_today', 'sum')
        ).reset_index()
        loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
        by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]

        merged_list = []
        for r in merged.to_dict(orient='records'):
            clean = {k:_to_native(v) for k,v in r.items()}
            # unify keys for clarity in API response
            clean['mapped_employee_id'] = clean.get('mapped_employee_id')
            clean['card_number_att'] = clean.get('card_number') or clean.get('card_number_att') or None
            # include status if present
            if 'status' not in clean:
                clean['status'] = None
            # ensure employee_id key exists
            if 'employee_id' not in clean:
                clean['employee_id'] = None
            merged_list.append(clean)

        # CCURE stats fetch (best-effort)
        ccure_stats = get_global_stats_or_none()

        # compare counts summary between CCure and Active sheets
        try:
            ccure_summary = ccure_stats or {}
            cc_total_profiles = ccure_summary.get('TotalProfiles')
            cc_active_profiles = ccure_summary.get('ActiveProfiles')
            cc_active_emps = ccure_summary.get('ActiveEmployees')
            cc_active_contractors = ccure_summary.get('ActiveContractors')
        except Exception:
            cc_total_profiles = cc_active_profiles = cc_active_emps = cc_active_contractors = None

        # local sheet counts
        active_emp_count = len(act_rows)
        active_contract_count = len(contractor_rows)

        diff = {
            "active_sheet_employee_count": active_emp_count,
            "active_sheet_contractor_count": active_contract_count,
            "ccure_active_employees": cc_active_emps,
            "ccure_active_contractors": cc_active_contractors,
            "delta_employees": (cc_active_emps - active_emp_count) if (isinstance(cc_active_emps, int) and isinstance(active_emp_count, int)) else None,
            "delta_contractors": (cc_active_contractors - active_contract_count) if (isinstance(cc_active_contractors, int) and isinstance(active_contract_count, int)) else None
        }

        result = {
            "by_location": by_location,
            "merged": merged_list,
            "ccure": ccure_stats,
            "count_comparison": diff
        }
        return result

# Helper wrapper
def get_global_stats_or_none():
    try:
        from ccure_client import get_global_stats
        return get_global_stats()
    except Exception:
        logger.debug("ccure_client.get_global_stats not available", exc_info=True)
        return None





6)
# ccure_client.py
"""
Lightweight CCURE client wrappers used by compare service.
This file is defensive: missing 'requests' or network failures return None instead of raising.
"""

import math
import logging
from requests.exceptions import RequestException

logger = logging.getLogger("ccure_client")
logger.setLevel(logging.INFO)
if not logger.handlers:
    import sys
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Base URL for CCURE API - adjust if necessary
BASE = "http://10.199.22.57:5001"
DEFAULT_TIMEOUT = 10

HEADERS = {
    "Accept": "application/json"
}

# Defensive import of requests
try:
    import requests
except Exception:
    requests = None
    logger.warning("requests module not available; ccure_client will return None for HTTP calls")

def _safe_get(path, params=None, timeout=DEFAULT_TIMEOUT):
    """
    Safe GET wrapper. Returns parsed JSON on success or None on failure.
    path may include leading slash or not; we join safely.
    """
    if requests is None:
        logger.debug("_safe_get: requests not available")
        return None
    # ensure path begins with '/'
    if not path.startswith("/"):
        path = "/" + path
    url = BASE.rstrip("/") + path
    try:
        r = requests.get(url, params=params, headers=HEADERS, timeout=timeout)
        r.raise_for_status()
        return r.json()
    except RequestException as e:
        logger.warning(f"[ccure_client] request failed {url} params={params} -> {e}")
        return None
    except ValueError:
        logger.warning(f"[ccure_client] response JSON decode error for {url}")
        return None

def fetch_all_employees_full():
    """Try to fetch a full dump from /api/employees (may return list or None)."""
    return _safe_get("/api/employees")

def fetch_stats_page(detail, page=1, limit=500):
    """
    One page of /api/stats?details=detail&page=page&limit=limit
    Returns page dict or None.
    """
    params = {"details": detail, "page": page, "limit": limit}
    return _safe_get("/api/stats", params=params)

def fetch_all_stats(detail, limit=1000):
    """
    Iterate pages for /api/stats detail and return combined data list.
    Returns list or None.
    """
    first = fetch_stats_page(detail, page=1, limit=limit)
    if not first:
        return None
    data = first.get("data") or []
    total = int(first.get("total") or len(data) or 0)
    if total <= len(data):
        return data
    pages = int(math.ceil(total / float(limit)))
    for p in range(2, pages + 1):
        page_res = fetch_stats_page(detail, page=p, limit=limit)
        if not page_res:
            # stop early on error
            break
        data.extend(page_res.get("data") or [])
    return data

def get_global_stats():
    """
    Best-effort summary using /api/stats (preferred) or /api/employees (fallback).
    Returns dict or None.
    """
    # First: try to call /api/stats endpoints for canonical totals (preferred).
    details = ["TotalProfiles", "ActiveProfiles", "ActiveEmployees", "ActiveContractors",
               "TerminatedProfiles", "TerminatedEmployees", "TerminatedContractors"]
    out = {}

    # Try a single call to /api/stats with no detail (some CCURE deployments return a summary dict)
    try:
        summary = _safe_get("/api/stats")
        if isinstance(summary, dict) and any(k in summary for k in details):
            # normalize keys to expected names
            for k in details:
                # attempt case-insensitive lookup
                for key in summary.keys():
                    if key.lower() == k.lower():
                        out[k] = summary.get(key)
                        break
            if out:
                # convert numeric-like to int where possible
                safe_out = {}
                for k, v in out.items():
                    try:
                        safe_out[k] = int(v) if v is not None and str(v).strip() != "" else None
                    except Exception:
                        safe_out[k] = v
                return safe_out
    except Exception:
        pass

    # If that didn't work, try per-detail endpoints (some setups expose /api/stats?details=...)
    try:
        any_found = False
        for d in details:
            resp = fetch_stats_page(d, page=1, limit=1)
            if isinstance(resp, dict):
                # common patterns:
                # - { "total": 123, "data": [...] }
                # - { "TotalProfiles": 123, ... } (summary response)
                if 'total' in resp and isinstance(resp['total'], (int, float, str)):
                    out[d] = int(resp['total'])
                    any_found = True
                elif d in resp:
                    out[d] = resp.get(d)
                    any_found = True
                else:
                    # try case-insensitive key match
                    for key in resp.keys():
                        if key.lower() == d.lower() and isinstance(resp.get(key), (int, float, str)):
                            try:
                                out[d] = int(resp.get(key))
                                any_found = True
                            except Exception:
                                out[d] = resp.get(key)
                                any_found = True
                            break
        if any_found:
            return {k: (int(v) if (v is not None and str(v).strip() != "") else None) for k, v in out.items()}
    except Exception:
        logger.exception("fetch per-detail stats failed")

    # Fallback: try /api/employees full dump and compute counts locally.
    try:
        full = fetch_all_employees_full()
        if isinstance(full, list):
            total = len(full)
            active_profiles = sum(1 for r in full if (r.get("Employee_Status") or "").lower() == "active")
            active_emps = sum(1 for r in full if (r.get("PersonnelType") or "").lower().startswith("employee") and (r.get("Employee_Status") or "").lower() == "active")
            active_contractors = sum(1 for r in full if (r.get("PersonnelType") or "").lower().startswith("contractor") and (r.get("Employee_Status") or "").lower() == "active")
            terminated = sum(1 for r in full if (r.get("Employee_Status") or "").lower() in ("deactive", "deactivated", "inactive", "terminated"))
            return {
                "TotalProfiles": total,
                "ActiveProfiles": active_profiles,
                "ActiveEmployees": active_emps,
                "ActiveContractors": active_contractors,
                "TerminatedProfiles": terminated
            }
    except Exception:
        logger.exception("Error calculating global stats from full dump fallback")

    # nothing available
    return None





7)


# region_clients.py
"""
HTTP helpers for region occupancy endpoints.
Returns:
 - fetch_all_regions() -> list of {"region": <name>, "count": <int>}
 - fetch_all_details(timeout=...) -> list of person-detail dicts (where available)
 - fetch_all_history(timeout=...) -> list of date-or-detail dicts (history endpoints)

Drop-in replacement for your region_clients.py.
"""

import requests
from requests.exceptions import RequestException
from datetime import datetime
import logging
import time
import sys

logger = logging.getLogger("region_clients")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# endpoints (edit if your hosts differ)
endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/live-summary",
    "emea":  "http://10.199.22.57:3007/api/occupancy/live-summary",
    "laca":  "http://10.199.22.57:4000/api/occupancy/live-summary",
    "apac":  "http://10.199.22.57:3008/api/occupancy/live-summary"
}

history_endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/history",
    "emea":  "http://10.199.22.57:3007/api/occupancy/history",
    "apac":  "http://10.199.22.57:3008/api/occupancy/history",
    "laca":  "http://10.199.22.57:4000/api/occupancy/history"
}

DEFAULT_ATTEMPTS = 3
DEFAULT_BACKOFF = 0.6

def _do_get_with_retries(url, timeout, attempts=DEFAULT_ATTEMPTS, backoff=DEFAULT_BACKOFF):
    last_err = None
    for attempt in range(1, attempts + 1):
        try:
            # use connect,read timeouts tuple for clarity
            # r = requests.get(url, timeout=(3, max(5, timeout)))

            r = requests.get(url, timeout=(3, max(10, timeout)))

            r.raise_for_status()
            try:
                return r.json()
            except ValueError:
                try:
                    return {"_raw_text": r.text}
                except Exception:
                    return None
        except RequestException as e:
            last_err = e
            logger.warning(f"[region_clients] attempt {attempt}/{attempts} failed for {url}: {e}")
            if attempt < attempts:
                time.sleep(backoff * attempt)
            continue
    logger.warning(f"[region_clients] all {attempts} attempts failed for {url}: {last_err}")
    return None

def fetch_all_regions(timeout=6):
    results = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            realtime = {}
            if isinstance(data, dict):
                realtime = data.get("realtime", {}) or {}
            total = 0
            for site in realtime.values():
                try:
                    total += int(site.get("total", 0))
                except Exception:
                    pass
            if total == 0 and isinstance(data, dict):
                for k, v in data.items():
                    if isinstance(v, dict) and "total" in v:
                        try:
                            total += int(v.get("total", 0))
                        except Exception:
                            pass
            results.append({"region": region, "count": total})
        except Exception as e:
            logger.exception(f"[region_clients] unexpected error fetching live-summary for {region}: {e}")
            results.append({"region": region, "count": None})
    return results

def _extract_list_candidates(data):
    """
    Helper to find lists that may contain detail rows inside a payload.
    """
    if isinstance(data, list):
        return [x for x in data if isinstance(x, dict)]
    if isinstance(data, dict):
        for k in ("details","people","list","items","results","data","entries"):
            if k in data and isinstance(data[k], list):
                return [x for x in data[k] if isinstance(x, dict)]
        # top-level dict might be single row; return empty here (caller will inspect)
        return []
    return []

def fetch_all_details(timeout=6):
    """
    Attempt to fetch person-level detail records from live-summary endpoints.
    Returns a list of dicts (flattened detail rows). Regions that don't provide details will be skipped.
    """
    all_details = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            details = _extract_list_candidates(data)
            # also attempt to find nested partitions that contain details
            if not details and isinstance(data, dict):
                for v in data.values():
                    if isinstance(v, dict):
                        candidates = _extract_list_candidates(v)
                        if candidates:
                            details.extend(candidates)
            for d in details:
                try:
                    d2 = dict(d)
                    d2["_region"] = region
                    d2["_source_url"] = url
                    all_details.append(d2)
                except Exception:
                    continue
        except Exception as e:
            logger.warning(f"[region_clients] warning: cannot fetch details from {region}@{url}: {e}")
            continue

    # try history endpoints as second chance (they sometimes include per-person rows)
    if not all_details:
        for region, url in history_endpoints.items():
            try:
                data = _do_get_with_retries(url, timeout=timeout) or {}
                details = _extract_list_candidates(data)
                for d in details:
                    try:
                        d2 = dict(d)
                        d2["_region"] = region
                        d2["_source_url"] = url
                        all_details.append(d2)
                    except Exception:
                        continue
            except Exception:
                logger.debug(f"[region_clients] history details fetch for {region} failed", exc_info=True)
                continue

    logger.info("[region_clients] fetched %d detail rows across endpoints", len(all_details))
    return all_details

def fetch_history_for_region(region, timeout=6):
    url = history_endpoints.get(region)
    if not url:
        logger.debug(f"[fetch_history_for_region] no history endpoint for {region}")
        return []
    try:
        data = _do_get_with_retries(url, timeout=timeout) or {}
        summary = []
        if isinstance(data, dict):
            # try common key names
            for key in ("summaryByDate","summary","data","entries","results","details","items"):
                if key in data and isinstance(data.get(key), list):
                    for s in data.get(key):
                        if isinstance(s, dict):
                            s2 = dict(s)
                            s2["_region"] = region
                            s2["_source_url"] = url
                            summary.append(s2)
                    break
            else:
                # maybe single dict date-summary
                if "date" in data or "day" in data:
                    s2 = dict(data)
                    s2["_region"] = region
                    s2["_source_url"] = url
                    summary.append(s2)
        elif isinstance(data, list):
            for s in data:
                if isinstance(s, dict):
                    s2 = dict(s)
                    s2["_region"] = region
                    s2["_source_url"] = url
                    summary.append(s2)
        return summary
    except Exception as e:
        logger.warning(f"[region_clients] cannot fetch history for {region}@{url}: {e}")
        return []

def fetch_all_history(timeout=6):
    all_entries = []
    for region in history_endpoints.keys():
        try:
            entries = fetch_history_for_region(region, timeout=timeout)
            if entries:
                all_entries.extend(entries)
        except Exception:
            logger.exception(f"[region_clients] error aggregating history for {region}")
            continue
    logger.info("[region_clients] fetched %d history entries", len(all_entries))
    return all_entries








