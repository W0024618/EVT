    try:
        if not combined_df.empty and 'IsFlagged' in combined_df.columns:
            flagged_df = combined_df[combined_df['IsFlagged'] == True].copy()
        else:
            flagged_df = pd.DataFrame()
    except Exception:
        flagged_df = pd.DataFrame()

    # ... existing computations for flagged_count, etc. ...

    try:
        # If we have flagged rows, return ALL flagged rows (strict)
        if flagged_df is not None and not flagged_df.empty:
            sample_source = flagged_df
            # return exactly flagged_count rows (no hidden head(10) truncation)
            samples = _clean_sample_df(sample_source, max_rows=int(len(flagged_df)))
        else:
            # old behaviour: show a small sample of combined_df
            sample_source = combined_df
            samples = _clean_sample_df(sample_source.head(10), max_rows=10) if sample_source is not None and not sample_source.empty else []
    except Exception:
        samples = []

















def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    out: Dict[str, Any] = {}
    logging.info("get_personnel_info: lookup called with candidate_identifier=%s", candidate_identifier)
    if candidate_identifier is None:
        logging.debug("get_personnel_info: no candidate provided")
        return out
    conn = _get_acvscore_conn()
    if conn is None:
        logging.info("get_personnel_info: ACVSCore connection unavailable (skipping DB lookup)")
        return out
    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        # avoid passing non-GUID strings to the GUID parameter (prevents SQL Server conversion errors)
        cand_guid = cand if _looks_like_guid(cand) else None
        params = (cand, cand_guid, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()
        if row:
            # columns: ObjectID, GUID, Name, EmailAddress, ManagerEmail
            try:
                out['ObjectID'] = row[0]
                out['GUID'] = row[1]
                out['Name'] = row[2]
                # canonical email fields (provide aliases for downstream code)
                email_val = row[3] if len(row) > 3 else None
                out['EmailAddress'] = email_val or None
                out['EmployeeEmail'] = email_val or None
                out['Email'] = email_val or None
                out['ManagerEmail'] = row[4] if len(row) > 4 else None
            except Exception:
                out = {
                    'ObjectID': row[0] if len(row) > 0 else None,
                    'GUID': row[1] if len(row) > 1 else None,
                    'Name': row[2] if len(row) > 2 else None,
                    'EmailAddress': row[3] if len(row) > 3 else None,
                    'EmployeeEmail': row[3] if len(row) > 3 else None,
                    'Email': row[3] if len(row) > 3 else None,
                    'ManagerEmail': row[4] if len(row) > 4 else None
                }
            logging.info("get_personnel_info: found personnel row for candidate=%s -> ObjectID=%s Email=%s",
                         candidate_identifier, out.get('ObjectID'), out.get('EmailAddress'))
        else:
            logging.debug("get_personnel_info: no personnel row found for candidate=%s", candidate_identifier)
    except Exception:
        logging.exception("Failed personnel lookup for candidate: %s", candidate_identifier)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out








    try:
        if not combined_df.empty and 'IsFlagged' in combined_df.columns:
            flagged_df = combined_df[combined_df['IsFlagged'] == True].copy()
        else:
            flagged_df = pd.DataFrame()
    except Exception:
        flagged_df = pd.DataFrame()

    # ... existing computations for flagged_count, etc. ...

    try:
        # If we have flagged rows, return ALL flagged rows (strict)
        if flagged_df is not None and not flagged_df.empty:
            sample_source = flagged_df
            # return exactly flagged_count rows (no hidden head(10) truncation)
            samples = _clean_sample_df(sample_source, max_rows=int(len(flagged_df)))
        else:
            # old behaviour: show a small sample of combined_df
            sample_source = combined_df
            samples = _clean_sample_df(sample_source.head(10), max_rows=10) if sample_source is not None and not sample_source.empty else []
    except Exception:
        samples = []



















Onece Check Why this issue happen check both file carefully and fix below issue carefully ...
there are Multiple issue i found 1 is 
Flagged rows display 15
Flagged rows

this Count is Diffrent for each region is Correct but below 
Showing 1 / 10 rows
Showing only 10 rows WHich need to to fix..


if Flagged rows are 25 then display 25 rows Strickly....



Available Evidence Files
swipes_pune_20251114_shifted.csv

Download
swipes_pune_20251114.csv

Download
swipes_pune_20251113.csv

Download
swipes_pune_20251112_shifted.csv

Download
swipes_pune_20251112.csv

Download
swipes_pune_20251111.csv

Download
swipes_pune_20251110.csv

Download
swipes_pune_20251109.csv

Download
swipes_pune_20251108.csv

Download
swipes_pune_20251107_shifted.csv

Download
swipes_pune_20251107.csv

Download
swipes_pune_20251106.csv

Download
swipes_pune_20251105.csv

Download
swipes_pune_20251104.csv

Download
swipes_pune_20251103_shifted.csv

Download
swipes_pune_20251103.csv

Download
swipes_pune_20251102.csv



Avilable Evidanec File is same for all Location when i check Pune same above file 
when check for Costa Rica we got same file 

In Evidanec file make only for That specific EMployee detail Swipe timeLine ..
in Exceel File display only 
Employee Name	Employee ID	Card	Date	Time	SwipeGap	Door	Direction	Zone
this column 


alos in Swipe timeLine I have found one issue alos ...


Check below Swipe Time Line here Emmployee Name is Blank we need to fix this issue 
check here Employee Name is Blank

-	317745	234546	2025-11-13	12:23:21	00:00:35	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	Yellow Zone	-
-	317745	234546	2025-11-13	12:24:01	00:00:40	APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)	OutDirection	Yellow Zone	-
-	317745	234546	2025-11-13	12:25:57	00:01:56	APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)	InDirection	Yellow Zone	-
-	317745	234546	2025-11-13	12:26:14	00:00:17	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	OutDirection	Out of office	-
-	317745	234546	2025-11-13	12:46:03	00:19:49	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
-	317745	234546	2025-11-13	12:51:18	00:05:15	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	Reception Area	-
-	317745	234546	2025-11-13	12:52:03	00:00:45	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4 -OUT DOOR	OutDirection	Out of office	-
-	317745	234546	2025-11-13	16:03:15	03:11:12	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 3-DOOR	InDirection	Reception Area	-
-	317745	234546	2025-11-13	16:03:50	00:00:35	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	Yellow Zone	-
-	317745	234546	2025-11-13	16:04:10	00:00:20	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	OutDirection	Out of office	-
-	317745	234546	2025-11-13	16:15:29	00:11:19	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
-	317745	234546	2025-11-13	16:26:18	00:10:49	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	Reception Area	-
-	317745	234546	2025-11-13	16:26:56	00:00:38	APAC_IN_PUN-PODIUM_P-1 TURNSTILE 3 -OUT DOOR	OutDirection	Out of office	-
-	317745	234546	2025-11-14	07:56:10	00:00:00	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2-DOOR	InDirection	Reception Area	-
-	317745	234546	2025-11-14	07:56:49	00:00:39	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	Yellow Zone	-
-	317745	234546	2025-11-14	07:59:25	00:02:36	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	OutDirection	Out of office	-
-	317745	234546	2025-11-14	08:09:49	00:10:24	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
-	317745	234546	2025-11-14	08:51:12	00:41:23	APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)	OutDirection	Yellow Zone	-
-	317745	234546	2025-11-14	08:53:32	00:02:20	APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)	InDirection	Yellow Zone	-
-	317745	234546	2025-11-14	08:53:49	00:00:17	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	OutDirection	Out of office	-
-	317745	234546	2025-11-14	09:22:43	00:28:54	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
-	317745	234546	2025-11-14	09:23:06	00:00:23	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
Yadav, Vivek	317745	234546	2025-11-14	09:56:10	00:00:00	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2-DOOR	InDirection	Reception Area	-
Yadav, Vivek	317745	234546	2025-11-14	09:56:49	00:00:39	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	Yellow Zone	-
Yadav, Vivek	317745	234546	2025-11-14	09:59:25	00:02:36	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	OutDirection	Out of office	-
-	317745	234546	2025-11-14	10:00:45	00:37:39	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	Reception Area	-
-	317745	234546	2025-11-14	10:01:25	00:00:40	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2 -OUT DOOR	OutDirection	Out of office	-
Yadav, Vivek	317745	234546	2025-11-14	10:09:49	00:10:24	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
Yadav, Vivek	317745	234546	2025-11-14	10:51:12	00:41:23	APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)	OutDirection	Yellow Zone	-
Yadav, Vivek	317745	234546	2025-11-14	10:53:32	00:02:20	APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)	InDirection	Yellow Zone	-
Yadav, Vivek	317745	234546	2025-11-14	10:53:49	00:00:17	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	OutDirection	Out of office	-
Yadav, Vivek	317745	234546	2025-11-14	11:22:43	00:28:54	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
Yadav, Vivek	317745	234546	2025-11-14	11:23:06	00:00:23	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
Yadav, Vivek	317745	234546	2025-11-14	12:00:45	00:37:39	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	Reception Area	-
Yadav, Vivek	317745	234546	2025-11-14	12:01:25	00:00:40	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2 -OUT DOOR	OutDirection	Out of office	-
-	317745	234546	2025-11-14	16:07:35	06:06:10	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 3-DOOR	InDirection	Reception Area	-
-	317745	234546	2025-11-14	16:08:13	00:00:38	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	Yellow Zone	-
-	317745	234546	2025-11-14	16:08:32	00:00:19	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	OutDirection	Out of office	-
-	317745	234546	2025-11-14	16:18:19	00:09:47	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
-	317745	234546	2025-11-14	16:50:26	00:32:07	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	OutDirection	Out of office	-
-	317745	234546	2025-11-14	17:24:40	00:34:14	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
-	317745	234546	2025-11-14	17:26:27	00:01:47	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	Reception Area	-
-	317745	234546	2025-11-14	17:27:06	00:00:39	APAC_IN_PUN-PODIUM_P-1 TURNSTILE 3 -OUT DOOR	OutDirection	Out of office	-
Yadav, Vivek	317745	234546	2025-11-14	18:07:35	06:06:10	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 3-DOOR	InDirection	Reception Area	-
Yadav, Vivek	317745	234546	2025-11-14	18:08:13	00:00:38	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	InDirection	Yellow Zone	-
Yadav, Vivek	317745	234546	2025-11-14	18:08:32	00:00:19	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	OutDirection	Out of office	-
Yadav, Vivek	317745	234546	2025-11-14	18:18:19	00:09:47	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
Yadav, Vivek	317745	234546	2025-11-14	18:50:26	00:32:07	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	OutDirection	Out of office	-
Yadav, Vivek	317745	234546	2025-11-14	19:24:40	00:34:14	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	InDirection	Yellow Zone	-
Yadav, Vivek	317745	234546	2025-11-14	19:26:27	00:01:47	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	OutDirection	Reception Area	-
Yadav, Vivek	317745	234546	2025-11-14	19:27:06	00:00:39	APAC_IN_PUN-PODIUM_P-1 TURNSTILE 3 -OUT DOOR	OutDirection	Out of office	-

(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python app.py
>>
 * Serving Flask app 'app'
 * Debug mode: on
INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8002
 * Running on http://10.199.47.235:8002
INFO:werkzeug:Press CTRL+C to quit
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 134-209-644
INFO:root:Fetching swipes for region apac on 2025-11-14
INFO:root:Databases present for server SRVWUPNQ0986V: ['ACVSUJournal_00010030', 'ACVSUJournal_00010029', 'ACVSUJournal_00010028', 'ACVSUJournal_00010027', 'ACVSUJournal_00010026', 'ACVSUJournal_00010025']
INFO:root:Built SQL for region apac, date 2025-11-14
INFO:root:City filter 'pune' applied for region apac: rows before=10874 after=5758
INFO:root:Wrote duration CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_duration_20251114.csv (rows=278)
INFO:root:Wrote swipes CSV for apac to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\apac_swipes_20251114.csv (rows=5758)
INFO:root:PersonnelTypeName values example: ['Employee']
INFO:root:PersonnelTypeName filter applied: before=5758 after=5758
INFO:root:PersonnelTypeName values example: ['Employee']
INFO:root:PersonnelTypeName filter applied: before=5758 after=5758
INFO:root:Saved raw swipes to C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\swipes_pune_20251114.csv
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2153: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2228: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.        
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:2228: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.        
  merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1671: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`        
  for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py:1737: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`        
  df['PresentToday'] = df['CountSwipes'].fillna(0).astype(int) > 0
INFO:root:run_trend_for_date: wrote C:\Users\W0024618\Desktop\Trend Analysis\backend\outputs\trend_pune_20251114.csv (rows=278)
INFO:root:get_personnel_info: lookup called with candidate_identifier=321712
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
ERROR:root:Failed personnel lookup for candidate: 321712
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 2740, in get_personnel_info
    cur.execute(sql, params)
    ~~~~~~~~~~~^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting from a character string to uniqueidentifier. (8169) (SQLExecDirectW)')
INFO:root:get_personnel_info: lookup called with candidate_identifier=329189
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
ERROR:root:Failed personnel lookup for candidate: 329189
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 2740, in get_personnel_info
    cur.execute(sql, params)
    ~~~~~~~~~~~^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting from a character string to uniqueidentifier. (8169) (SQLExecDirectW)')
INFO:root:get_personnel_info: lookup called with candidate_identifier=324581
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
ERROR:root:Failed personnel lookup for candidate: 324581
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 2740, in get_personnel_info
    cur.execute(sql, params)
    ~~~~~~~~~~~^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting from a character string to uniqueidentifier. (8169) (SQLExecDirectW)')
INFO:root:get_personnel_info: lookup called with candidate_identifier=327004
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
ERROR:root:Failed personnel lookup for candidate: 327004
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 2740, in get_personnel_info
    cur.execute(sql, params)
    ~~~~~~~~~~~^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting from a character string to uniqueidentifier. (8169) (SQLExecDirectW)')
INFO:root:get_personnel_info: lookup called with candidate_identifier=326430
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
ERROR:root:Failed personnel lookup for candidate: 326430
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 2740, in get_personnel_info
    cur.execute(sql, params)
    ~~~~~~~~~~~^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting from a character string to uniqueidentifier. (8169) (SQLExecDirectW)')
INFO:root:get_personnel_info: lookup called with candidate_identifier=317745
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
ERROR:root:Failed personnel lookup for candidate: 317745
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 2740, in get_personnel_info
    cur.execute(sql, params)
    ~~~~~~~~~~~^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting from a character string to uniqueidentifier. (8169) (SQLExecDirectW)')
INFO:root:get_personnel_info: lookup called with candidate_identifier=323220
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
ERROR:root:Failed personnel lookup for candidate: 323220
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 2740, in get_personnel_info
    cur.execute(sql, params)
    ~~~~~~~~~~~^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting from a character string to uniqueidentifier. (8169) (SQLExecDirectW)')
INFO:root:get_personnel_info: lookup called with candidate_identifier=324619
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
ERROR:root:Failed personnel lookup for candidate: 324619
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 2740, in get_personnel_info
    cur.execute(sql, params)
    ~~~~~~~~~~~^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting from a character string to uniqueidentifier. (8169) (SQLExecDirectW)')
INFO:root:get_personnel_info: lookup called with candidate_identifier=325872
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
ERROR:root:Failed personnel lookup for candidate: 325872
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 2740, in get_personnel_info
    cur.execute(sql, params)
    ~~~~~~~~~~~^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting from a character string to uniqueidentifier. (8169) (SQLExecDirectW)')
INFO:root:get_personnel_info: lookup called with candidate_identifier=319936
INFO:root:Connected to ACVSCore on server SRVWUPNQ0986V using REGION_CONFIG[apac] (sql auth).
ERROR:root:Failed personnel lookup for candidate: 319936
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 2740, in get_personnel_info
    cur.execute(sql, params)
    ~~~~~~~~~~~^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Conversion failed when converting from a character string to uniqueidentifier. (8169) (SQLExecDirectW)')
INFO:werkzeug:127.0.0.1 - - [15/Nov/2025 11:38:25] "GET /run?start=2025-11-14&end=2025-11-14&full=true&region=apac&city=Pune HTTP/1.1" 200 -
INFO:werkzeug:127.0.0.1 - - [15/Nov/2025 11:38:55] "GET /record?employee_id=317745 HTTP/1.1" 200 -






# backend/app.py
from flask import Flask, jsonify, request, send_from_directory, jsonify, send_file
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re
import io
import base64
import os
import difflib
from io import BytesIO
from flask import send_file  # add if not present
from pathlib import Path
from typing import Optional, List, Dict, Any
from duration_report import REGION_CONFIG
from datetime import date, timedelta, datetime
from flask import jsonify, request
import logging
logging.basicConfig(level=logging.INFO)
#from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR
from trend_runner import run_trend_for_date, build_monthly_training, _enrich_with_personnel_info
from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE


# app = Flask(__name__)



def _safe_read_csv(fp):
    try:
        return pd.read_csv(fp, parse_dates=['LocaleMessageTime'], low_memory=False)
    except Exception:
        try:
            return pd.read_csv(fp, low_memory=False)
        except Exception:
            return pd.DataFrame()


# ---------- Ensure outputs directory exists early (so OVERRIDES_FILE can be defined safely) ----------

BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)

OUTDIR = DEFAULT_OUTDIR

OVERRIDES_FILE = DEFAULT_OUTDIR / "overrides.csv"


def _load_overrides():
    if not OVERRIDES_FILE.exists():
        return {}
    try:
        df = pd.read_csv(OVERRIDES_FILE, dtype=str)
        out = {}
        for _, r in df.iterrows():
            emp = str(r.get('EmployeeID') or r.get('person_uid') or '').strip()
            if not emp:
                continue
            out[emp] = {
                'level': str(r.get('OverrideLevel') or '').strip(),
                'reason': str(r.get('Reason') or '').strip(),
                'ts': str(r.get('Timestamp') or '').strip()
            }
        return out
    except Exception:
        logging.exception("Failed reading overrides file")
        return {}

def _save_override(employee_key, level, reason):
    now = datetime.now().isoformat()
    row = {'EmployeeID': employee_key, 'OverrideLevel': level, 'Reason': reason or '', 'Timestamp': now}
    try:
        if OVERRIDES_FILE.exists():
            df = pd.read_csv(OVERRIDES_FILE, dtype=str)
            # pandas.DataFrame.append is deprecated -> use concat
            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        else:
            df = pd.DataFrame([row])
        df.to_csv(OVERRIDES_FILE, index=False)
        return True
    except Exception:
        logging.exception("Failed to save override")
        return False

def _slug_city(s):
    """
    Convert a city/site string into a safe slug: lowercase, alphanumeric+hyphen
    """
    if not s:
        return ''
    # Remove special chars, spaces to hyphens, lower
    slug = re.sub(r'[^\w\s-]', '', str(s)).strip().lower()
    slug = re.sub(r'[\s_]+', '-', slug)
    return slug


# --- Use REGION_CONFIG servers to talk to ACVSCore (no separate ACVSCORE_DB_CONFIG) ---
# ODBC driver variable is already defined later: ODBC_DRIVER (safe to reference only at runtime)

_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20

def _get_acvscore_conn():
    """
    Try to connect to ACVSCore by reusing credentials from REGION_CONFIG.
    Loops through REGION_CONFIG entries and attempts:
      1) SQL auth (UID/PWD) to database "ACVSCore" using region server + credentials
      2) If SQL auth fails on that server, try Trusted_Connection (Windows auth) as a fallback
    If that fails, optionally attempt ACVSCORE_DB_CONFIG if defined (safe: checked via globals()).
    Returns first successful pyodbc connection or None.
    Implements a short backoff after recent failure to reduce log noise.
    """
    try:
        import pyodbc
    except Exception:
        logging.exception("pyodbc not installed; ACVSCore lookups unavailable.")
        return None

    # basic backoff: skip attempts if we just failed recently
    from datetime import datetime
    now = datetime.now().timestamp()
    last = _acvscore_backoff.get("ts")
    if last and _acvscore_backoff.get("failed") and (now - last) < _ACVSCORE_BACKOFF_SECONDS:
        logging.debug("Skipping ACVSCore connection attempt (backoff active).")
        return None

    # iterate region servers (use the same credentials defined in REGION_CONFIG)
    tried = []
    for region_key, rc in (REGION_CONFIG or {}).items():
        server = rc.get("server")
        user = rc.get("user")
        pwd = rc.get("password")
        if not server:
            continue

        # Try SQL auth first if credentials present
        if user and pwd:
            tried.append(f"{region_key}@{server}(sql)")
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={server};DATABASE=ACVSCore;UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;"
            )
            try:
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (sql auth).", server, region_key)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("SQL auth to %s failed: %s", server, e)

        # Try Trusted Connection fallback on same server
        tried.append(f"{region_key}@{server}(trusted)")
        conn_str_trusted = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={server};DATABASE=ACVSCore;Trusted_Connection=yes;"
            "TrustServerCertificate=Yes;"
        )
        try:
            conn = pyodbc.connect(conn_str_trusted, autocommit=True, timeout=5)
            logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (trusted connection).", server, region_key)
            _acvscore_backoff["ts"] = None
            _acvscore_backoff["failed"] = False
            return conn
        except Exception as e:
            logging.debug("Trusted connection to %s failed: %s", server, e)
            continue

    # Fallback: if a global ACVSCORE_DB_CONFIG exists, try it (safe check)
    try:
        if 'ACVSCORE_DB_CONFIG' in globals() and isinstance(globals().get('ACVSCORE_DB_CONFIG'), dict):
            cfg = globals().get('ACVSCORE_DB_CONFIG')
            server = cfg.get('server')
            user = cfg.get('user')
            pwd = cfg.get('password')
            database = cfg.get('database', 'ACVSCore')
            tried.append(f"ACVSCORE_DB_CONFIG@{server}")
            try:
                conn_str = (
                    f"DRIVER={{{ODBC_DRIVER}}};"
                    f"SERVER={server};DATABASE={database};UID={user};PWD={pwd};"
                    "TrustServerCertificate=Yes;"
                )
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore using ACVSCORE_DB_CONFIG (%s).", server)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("ACVSCORE_DB_CONFIG connection failed: %s", e)
    except Exception:
        # defensive: do not propagate any errors from fallback logic
        logging.debug("ACVSCORE_DB_CONFIG fallback not available or failed.")

    # record failure to backoff
    _acvscore_backoff["ts"] = now
    _acvscore_backoff["failed"] = True
    logging.error("Failed to connect to ACVSCore using REGION_CONFIG servers. Tried: %s", tried)
    return None


# ODBC driver (keep existing env-based driver)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data



def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    out: Dict[str, Any] = {}
    logging.info("get_personnel_info: lookup called with candidate_identifier=%s", candidate_identifier)
    if candidate_identifier is None:
        logging.debug("get_personnel_info: no candidate provided")
        return out
    conn = _get_acvscore_conn()
    if conn is None:
        logging.info("get_personnel_info: ACVSCore connection unavailable (skipping DB lookup)")
        return out
    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        params = (cand, cand, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()
        if row:
            # columns: ObjectID, GUID, Name, EmailAddress, ManagerEmail
            try:
                out['ObjectID'] = row[0]
                out['GUID'] = row[1]
                out['Name'] = row[2]
                # canonical email fields (provide aliases for downstream code)
                email_val = row[3] if len(row) > 3 else None
                out['EmailAddress'] = email_val or None
                out['EmployeeEmail'] = email_val or None
                out['Email'] = email_val or None
                out['ManagerEmail'] = row[4] if len(row) > 4 else None
            except Exception:
                out = {
                    'ObjectID': row[0] if len(row) > 0 else None,
                    'GUID': row[1] if len(row) > 1 else None,
                    'Name': row[2] if len(row) > 2 else None,
                    'EmailAddress': row[3] if len(row) > 3 else None,
                    'EmployeeEmail': row[3] if len(row) > 3 else None,
                    'Email': row[3] if len(row) > 3 else None,
                    'ManagerEmail': row[4] if len(row) > 4 else None
                }
            logging.info("get_personnel_info: found personnel row for candidate=%s -> ObjectID=%s Email=%s",
                         candidate_identifier, out.get('ObjectID'), out.get('EmailAddress'))
        else:
            logging.debug("get_personnel_info: no personnel row found for candidate=%s", candidate_identifier)
    except Exception:
        logging.exception("Failed personnel lookup for candidate: %s", candidate_identifier)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out




def get_person_image_bytes(parent_id) -> Optional[bytes]:
    logging.info("get_person_image_bytes: lookup for ParentId=%s", parent_id)
    # 1) try DB (as before) — keep behaviour if available
    try:
        conn = _get_acvscore_conn()
        if conn is not None:
            try:
                cur = conn.cursor()
                sql = """
                    SELECT TOP 1 AI.Image
                    FROM ACVSCore.Access.Images AI
                    WHERE AI.ParentId = ?
                      AND DATALENGTH(AI.Image) > 0
                    ORDER BY AI.ObjectID DESC
                """
                cur.execute(sql, (str(parent_id),))
                row = cur.fetchone()
                if row and row[0] is not None:
                    logging.info("get_person_image_bytes: image found in DB for ParentId=%s (len=%d)", parent_id, len(row[0]) if row[0] else 0)
                    try:
                        b = bytes(row[0])
                        return b
                    except Exception:
                        return row[0]
            except Exception:
                logging.exception("Failed to fetch image for ParentId=%s via DB", parent_id)
            finally:
                try:
                    cur.close()
                except Exception:
                    pass
                try:
                    conn.close()
                except Exception:
                    pass
    except Exception:
        logging.debug("ACVSCore DB unavailable for image lookup; will try filesystem fallbacks for ParentId=%s", parent_id)

    # 2) Try filesystem fallbacks under DEFAULT_OUTDIR (use typical file extensions)
    try:
        # ensure DEFAULT_OUTDIR exists and convert parent_id to safe filename
        cand_ids = []
        if parent_id is None:
            return None
        pid_raw = str(parent_id).strip()
        # add raw and numeric-only variants
        cand_ids.append(pid_raw)
        try:
            # if numeric-like, add int form
            if '.' in pid_raw:
                f = float(pid_raw)
                if f.is_integer():
                    cand_ids.append(str(int(f)))
        except Exception:
            pass
        # also try stripped non-alphanumeric variants
        cand_ids = list(dict.fromkeys(cand_ids))

        for c in cand_ids:
            for folder in (Path(DEFAULT_OUTDIR) / "images", Path(DEFAULT_OUTDIR), Path(".")):
                if not folder.exists():
                    continue
                for ext in (".jpg", ".jpeg", ".png", ".bmp", ".gif", ".webp"):
                    fp = folder / (f"{c}{ext}")
                    logging.debug("get_person_image_bytes: checking path %s", fp)
                    if fp.exists() and fp.is_file():
                        logging.info("get_person_image_bytes: loaded image file %s", fp)
                        try:
                            return fp.read_bytes()
                        except Exception:
                            continue
                # also try files where parent_id might be part of filename
                for fp in folder.glob(f"*{c}*"):
                    logging.debug("get_person_image_bytes: checking glob match %s", fp)
                    if fp.is_file():
                        try:
                            b = fp.read_bytes()
                            if b:
                                logging.info("get_person_image_bytes: loaded image via glob %s", fp)
                                return b
                        except Exception:
                            continue
    except Exception:
        logging.exception("Filesystem image lookup failed for ParentId=%s", parent_id)

    # nothing found
    return None


# ---------- New route to serve employee image ----------
# We'll import send_file later where used; define route after app created.

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

# send_file is needed for Excel responses
from flask import send_file
try:
    # optional import; used for styling
    from openpyxl import load_workbook
    from openpyxl.styles import Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        # convert floats NaN handled above
        return x
    try:
        # fallback to string
        return str(x)
    except Exception:
        return None


_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False


# Helper: format seconds to HH:MM:SS
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return None
        # guard against floats and NaN
        s = int(float(seconds))
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return None


# Placeholder tokens (keep consistent with trend_runner expectations)
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


def _resolve_field_from_record(record: dict, candidate_tokens: list):
    """
    Search a single row `record` (dict) for likely columns listed in candidate_tokens.
    Return first non-placeholder value found (converted to Python scalar), else None.
    """
    if record is None:
        return None

    # 1) exact key matches (case-sensitive & common casing)
    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)

    # 2) case-insensitive contains match
    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)

    # 3) xml / value parsing fallback for CardNumber
    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue

    # 4) final fallback: first non-placeholder value
    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)

    return None


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Clean a dataframe for JSON output (convert NaN -> None, pandas types -> native, format datetimes).
    """
    if df is None or df.empty:
        return []
    df = df.copy()

    # remove duplicate suffix columns
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    # Date normalization
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    # Datetime columns to ISO strings
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # Replace NaN/inf -> None
    df = df.where(pd.notnull(df), None)

    # Convert records to safe Python types
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        # Typical fields
        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        # ----- Schema-aware fallback resolution -----
        if not emp_id:
            emp_tokens = ['Int1', 'Text12', 'EmployeeID', 'empid', 'id']
            resolved_emp = _resolve_field_from_record(r, emp_tokens)
            if resolved_emp is not None:
                try:
                    s = str(resolved_emp).strip()
                    # remove trailing .0 for floats
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    if _looks_like_guid(s):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = s
                        emp_id = s
                except Exception:
                    if _looks_like_guid(resolved_emp):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = resolved_emp
                        emp_id = resolved_emp

        # Prefer Credential.CardNumber / CHUID / Card as CardNumber when missing — reject GUIDs/placeholders
        if out.get('CardNumber') in (None, '', 'nan'):
            card_tokens = ['CardNumber', 'CHUID', 'Card', 'card_no', 'cardnum']
            resolved_card = _resolve_field_from_record(r, card_tokens)
            if resolved_card is not None:
                try:
                    cs = str(resolved_card).strip()
                    if _looks_like_guid(cs) or _is_placeholder_str(cs):
                        out['CardNumber'] = None
                    else:
                        out['CardNumber'] = cs
                except Exception:
                    out['CardNumber'] = None

        # final safety: ensure EmployeeID/CardNumber are not GUID-like tokens
        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None
        if 'CardNumber' in out and isinstance(out['CardNumber'], str) and _looks_like_guid(out['CardNumber']):
            out['CardNumber'] = None

        # If EmployeeName empty or looks like a GUID, prefer EmployeeID (human id) over GUIDs
        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            if emp_id not in (None, '', 'nan') and not _looks_like_guid(emp_id):
                out['EmployeeName'] = str(emp_id)
            else:
                out['EmployeeName'] = None

        cleaned.append(out)
    return cleaned

# ----- Helpers added to match commented (Pune) file functionality but multi-city-aware -----

def _replace_placeholder_strings(obj):
    """
    If obj is a DataFrame, replace known placeholder strings with None (NaN).
    If obj is a scalar/string, return None for placeholder strings else return obj.
    """
    if obj is None:
        return obj
    try:
        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
            for col in df.columns:
                try:
                    # Replace placeholder strings (case-insensitive)
                    df[col] = df[col].apply(lambda x: None if _is_placeholder_str(x) else x)
                except Exception:
                    continue
            return df
        else:
            # scalar
            return None if _is_placeholder_str(obj) else obj
    except Exception:
        return obj

def _normalize_id_local(v):
    """
    Normalize an identifier for robust matching/counting:
    - treat NaN/None/empty as None
    - strip and convert float-like integers to integer strings
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == '' or s.lower() == 'nan':
        return None
    try:
        if '.' in s:
            fv = float(s)
            if math.isfinite(fv) and fv.is_integer():
                s = str(int(fv))
    except Exception:
        pass
    return s





def _find_swipe_files(outdir: Path, date_obj: Optional[date] = None, city_slug: Optional[str] = None):
    """
    Robust swipe-file discovery.
    - Supports filenames like:
        - swipes_YYYYMMDD.csv
        - swipes_<city>_YYYYMMDD.csv
        - <region>_swipes_YYYYMMDD.csv
        - <region>_swipes_*.csv
        - any file containing '_swipes_' or starting with 'swipes'
        - fallback: any file that ends with _YYYYMMDD.csv
    - If date_obj is None, returns recent files that look like swipe files.
    - Returns list of Path objects sorted by mtime (newest first).
    """
    p = Path(outdir)
    files_set = set()
    try:
        # Normalize city slug for matching
        city_slug_l = (city_slug or "").lower().strip()

        def add_glob(pattern):
            try:
                for fp in p.glob(pattern):
                    if fp.is_file():
                        files_set.add(fp)
            except Exception:
                pass

        if date_obj is None:
            # recent swipe-like files
            add_glob("*_swipes_*.csv")        # region_swipes_YYYY or region_swipes_any.csv
            add_glob("swipes_*.csv")         # swipes_YYYY or swipes_city_YYYY
            add_glob("*swipes*.csv")         # permissive
            add_glob("*_swipes.csv")
            # also include any file that includes 'swipe' (some exporters use 'swipe' singular)
            add_glob("*swipe*.csv")
            # city-specific guesses
            if city_slug_l:
                add_glob(f"*{city_slug_l}*_swipes_*.csv")
                add_glob(f"*{city_slug_l}*swipes*.csv")
                add_glob(f"*{city_slug_l}*.csv")
        else:
            target = date_obj.strftime("%Y%m%d")
            # common patterns observed in pipeline
            patterns = [
                f"*_{target}.csv",                 # anything ending _YYYYMMDD.csv
                f"*_swipes_{target}.csv",         # region_swipes_YYYYMMDD.csv or swipes_YYYYMMDD
                f"swipes*_{target}.csv",
                f"swipes_{target}.csv",
                f"*swipes*_{target}.csv",
                f"*{city_slug_l}*_{target}.csv",
                f"*{city_slug_l}*swipes*_{target}.csv",
                f"*{city_slug_l}_{target}.csv"
            ]
            for pat in patterns:
                add_glob(pat)

        # final fallback: any CSV in folder that contains '_swipe' (case-insensitive)
        try:
            for fp in p.iterdir():
                if not fp.is_file():
                    continue
                name = fp.name.lower()
                if ('_swipe' in name) or ('swipe' in name and name.endswith('.csv')):
                    files_set.add(fp)
        except Exception:
            pass

    except Exception:
        logging.exception("Error while searching for swipe files in %s", outdir)

    # sort by modification time (most recent first)
    files = sorted(list(files_set), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
    return files

# -----------------------
# Routes
# -----------------------




@app.route('/')
def root():
    return "Trend Analysis API — Multi-city"

@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.get_json(force=True) or {}
        else:
            try:
                params = request.form.to_dict() or {}
            except Exception:
                params = {}

    date_str = (params.get('date') or params.get('Date') or '').strip() or None
    start_str = (params.get('start') or params.get('Start') or '').strip() or None
    end_str = (params.get('end') or params.get('End') or '').strip() or None

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    regions_param = params.get('regions') or params.get('region') or ''
    if regions_param:
        regions = [r.strip().lower() for r in re.split(r'[;,|]', str(regions_param)) if r.strip()]
    else:
        try:
            regions = [k.lower() for k in list(REGION_CONFIG.keys())]
        except Exception:
            regions = ['apac']

    valid_regions = []
    for r in regions:
        if r in (REGION_CONFIG or {}):
            valid_regions.append(r)
        else:
            logging.debug("Requested region '%s' not in REGION_CONFIG - skipping", r)
    if not valid_regions:
        valid_regions = [k.lower() for k in REGION_CONFIG.keys()] if REGION_CONFIG else ['apac']
    params['_regions_to_run'] = valid_regions

    city_param = params.get('city') or params.get('site') or params.get('site_name') or None
    city_slug = _slug_city(city_param) if city_param else None
    params['_city'] = city_slug

    combined_rows = []
    files = []

    # ---------------------------
    # Run trend for each requested date
    # ---------------------------
    for d in dates:
        try:
            if run_trend_for_date is None:
                raise RuntimeError("run_trend_for_date helper not available in trend_runner")
            try:
                df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug)
            except TypeError:
                try:
                    df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
                except Exception:
                    # Last-resort: try duration_report fallback if available
                    try:
                        from duration_report import run_for_date as _dr_run_for_date
                        region_results = _dr_run_for_date(d, valid_regions, str(DEFAULT_OUTDIR), city_param)
                        combined_list = []
                        for rkey, res in (region_results or {}).items():
                            try:
                                df_dur = res.get('durations')
                                if df_dur is not None and not df_dur.empty:
                                    combined_list.append(df_dur)
                            except Exception:
                                continue
                        df = pd.concat(combined_list, ignore_index=True) if combined_list else pd.DataFrame()
                    except Exception:
                        raise
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_{city_slug}_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        if df is None or (hasattr(df, 'empty') and df.empty):
            continue

        try:
            df = _replace_placeholder_strings(df)
        except Exception:
            pass

        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        combined_rows.append(df)

    # *** Important: combine after loop to avoid UnboundLocalError and extra repeated concat inside loop ***
    try:
        combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()
    except Exception:
        combined_df = pd.DataFrame()

    try:
        if not combined_df.empty:
            if 'person_uid' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['person_uid'].dropna().astype(str).nunique())
            elif 'EmployeeID' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['EmployeeID'].dropna().astype(str).nunique())
            else:
                raw_unique_person_uids = int(len(combined_df))
        else:
            raw_unique_person_uids = 0
    except Exception:
        raw_unique_person_uids = int(len(combined_df)) if combined_df is not None else 0

    try:
        if not combined_df.empty and 'IsFlagged' in combined_df.columns:
            flagged_df = combined_df[combined_df['IsFlagged'] == True].copy()
        else:
            flagged_df = pd.DataFrame()
    except Exception:
        flagged_df = pd.DataFrame()

    try:
        analysis_count = int(raw_unique_person_uids)
    except Exception:
        analysis_count = int(len(combined_df)) if combined_df is not None else 0

    try:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = float((flagged_count / analysis_count * 100.0) if analysis_count and analysis_count > 0 else 0.0)
    except Exception:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = 0.0

    try:
        sample_source = flagged_df if not flagged_df.empty else combined_df
        samples = _clean_sample_df(sample_source.head(10), max_rows=10) if sample_source is not None and not sample_source.empty else []
    except Exception:
        samples = []

    # -----------------------------
    # NEW: Enrich the sample rows with EmployeeEmail and imageUrl using trend_runner helper
    # -----------------------------
    try:
        if isinstance(samples, list) and samples:
            try:
                base = (request.url_root or request.host_url).rstrip('/')
            except Exception:
                base = ''
            try:
                # build a small dataframe and call trend_runner enrichment helper
                tmp_df = pd.DataFrame(samples)
                # pass a fully-qualified image endpoint template so frontend img src works directly
                template = "/employee/{}/image"
                if base:
                    # result will be like: http://host:port/employee/<id>/image
                    template = f"{base}/employee/{{}}/image"
                enriched = _enrich_with_personnel_info(tmp_df, image_endpoint_template=template)
                # ensure Python native types and safe values via cleaning function
                enriched_clean = _clean_sample_df(enriched, max_rows=len(enriched))
                samples = enriched_clean
            except Exception:
                logging.exception("Failed to enrich /run sample with personnel info (non-fatal).")
    except Exception:
        logging.exception("Sample enrichment failed (non-fatal).")

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "aggregated_rows_total_raw": int(len(combined_df)),
        "aggregated_unique_persons": int(analysis_count),
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "flagged_rate_percent": float(flagged_rate_pct),
        "files": files,
        "sample": (samples[:10] if isinstance(samples, list) else samples),
        "reasons_count": {},
        "risk_counts": {},
        "flagged_persons": (samples if samples else []),
        "_raw_unique_person_uids": int(raw_unique_person_uids),
        "regions_run": params.get('_regions_to_run', []),
        "city_used": city_slug
    }

    return jsonify(resp)



@app.route('/latest', methods=['GET'])
def latest_results():
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]

    start_date_iso = None
    end_date_iso = None
    try:
        m = re.search(r'(\d{8})', latest.name)
        if m:
            ymd = m.group(1)
            dt = datetime.strptime(ymd, "%Y%m%d").date()
            start_date_iso = dt.isoformat()
            end_date_iso = dt.isoformat()
    except Exception:
        start_date_iso = None
        end_date_iso = None

    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)

    df = _replace_placeholder_strings(df)

    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in df.columns), None)

    def _norm_val_for_latest(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                fv = float(s)
                if math.isfinite(fv) and fv.is_integer():
                    s = str(int(fv))
        except Exception:
            pass
        return s

    if id_col is None:
        unique_persons = int(len(df))
    else:
        ids_series = df[id_col].apply(_norm_val_for_latest) if id_col in df.columns else pd.Series([None]*len(df))
        if id_col != 'person_uid' and 'person_uid' in df.columns:
            ids_series = ids_series.fillna(df['person_uid'].astype(str).replace('nan','').replace('None',''))
        unique_persons = int(len(set([x for x in ids_series.unique() if x])))

    # build initial sample (list of dicts)
    sample = _clean_sample_df(df, max_rows=5)  # returns list




    # --- Enrich sample rows with EmployeeEmail and imageUrl (best-effort) ---
    try:
        if isinstance(sample, list) and sample:
            # compute base once
            try:
                base = (request.url_root or request.host_url).rstrip('/')
            except Exception:
                base = ''

            for s in sample:
                # ensure keys exist (consistent shape)
                s.setdefault('EmployeeEmail', None)
                s.setdefault('imageUrl', None)
                s.setdefault('HasImage', False)

                # pick a lookup token (EmployeeID / person_uid / EmployeeName / EmployeeIdentity)
                lookup_token = (
                    s.get('EmployeeID')
                    or s.get('person_uid')
                    or s.get('EmployeeName')
                    or s.get('EmployeeIdentity')
                )

                pi = {}
                if lookup_token:
                    try:
                        pi = get_personnel_info(lookup_token) or {}
                    except Exception:
                        pi = {}

                # Prefer personnel DB email if available
                if pi:
                    email = pi.get('EmailAddress') or pi.get('EmployeeEmail') or pi.get('Email') or None
                    if email:
                        s['EmployeeEmail'] = email

                    # prefer ObjectID / GUID for images
                    objid = pi.get('ObjectID') or pi.get('GUID') or None
                    if objid:
                        # provide relative image path (frontend resolves with API_BASE)
                        s['imageUrl'] = f"/employee/{objid}/image"
                        try:
                            b = get_person_image_bytes(objid)
                            s['HasImage'] = True if b else False
                        except Exception:
                            s['HasImage'] = False

                # fallback: look up email from the CSV rows read (df) if still missing
                if not s.get('EmployeeEmail') and isinstance(df, pd.DataFrame):
                    try:
                        match_mask = pd.Series(False, index=df.index)
                        if s.get('person_uid') and 'person_uid' in df.columns:
                            match_mask |= df['person_uid'].astype(str).str.strip() == str(s.get('person_uid')).strip()
                        if s.get('EmployeeID') and 'EmployeeID' in df.columns:
                            match_mask |= df['EmployeeID'].astype(str).str.strip() == str(s.get('EmployeeID')).strip()

                        if match_mask.any():
                            idx = df[match_mask].index[0]
                            for col in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                                if col in df.columns:
                                    val = df.at[idx, col]
                                    if val not in (None, '', 'nan'):
                                        s['EmployeeEmail'] = val
                                        break
                    except Exception:
                        pass

                # Ensure we at least provide a consistent image route (use EmployeeID/person_uid if no ObjectID)
                if not s.get('imageUrl'):
                    empid = s.get('EmployeeID') or s.get('person_uid')
                    if empid:
                        s['imageUrl'] = f"/employee/{empid}/image"
                        # don't try to check bytes here (avoid extra DB hit) — HasImage remains False if unknown

    except Exception:
        # if enrichment fails, continue (sample remains as-is)
        pass


    resp = {
        
        "file": latest.name,
        "rows_raw": int(len(df)),
        "rows": unique_persons,
        "sample": sample,
        "start_date": start_date_iso,
        "end_date": end_date_iso,
        "city": city_slug
    }
    return jsonify(resp)



@app.route("/record")
def record_endpoint():

    employee_id = request.args.get('employee_id', '').strip()
    if not employee_id:
        return jsonify({'error': 'employee_id required'}), 400

    # 1) find the latest trend CSV that contains this employee
    trend_files = sorted(OUTDIR.glob("trend_*.csv"), reverse=True)
    aggregated_rows = []
    found_date_iso = None
    for tf in trend_files:
        try:
            df = pd.read_csv(tf, dtype=str)
            # normalise columns that might not exist
            cols = df.columns.str.lower()
            # try matching against common id columns
            mask = pd.Series(False, index=df.index)
            for col in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber'):
                if col in df.columns:
                    mask = mask | (df[col].astype(str).fillna('') == employee_id)
            if mask.any():
                # convert matched rows to proper dicts
                matches = df[mask].to_dict(orient='records')
                aggregated_rows = matches
                # pick date from the first match (if available)
                try:
                    d = matches[0].get('Date') or matches[0].get('DisplayDate') or None
                    found_date_iso = d
                except Exception:
                    found_date_iso = None
                break
        except Exception:
            continue

    # 2) list raw swipe files (any swipes_*.csv in outputs)
    raw_swipe_files = [p.name for p in sorted(OUTDIR.glob("swipes_*_*.csv"), reverse=True)]

    # 3) build raw_swipes filtered for this person & date if possible
    raw_swipes = []
    try:
        # read each swipes CSV and filter rows
        for swf in OUTDIR.glob("swipes_*_*.csv"):
            sdf = _safe_read_csv(swf)
            if sdf.empty:
                continue
            # ensure LocaleMessageTime exists as datetime
            if 'LocaleMessageTime' in sdf.columns:
                sdf['LocaleMessageTime'] = pd.to_datetime(sdf['LocaleMessageTime'], errors='coerce')
            else:
                # try fallback candidates
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp','Date'):
                    if cand in sdf.columns:
                        sdf['LocaleMessageTime'] = pd.to_datetime(sdf[cand], errors='coerce')
                        break
            # filter rows matching employee by either EmployeeID, person_uid, CardNumber or EmployeeIdentity
            sel_mask = pd.Series(False, index=sdf.index)
            for c in ('EmployeeID','person_uid','CardNumber','EmployeeIdentity'):
                if c in sdf.columns:
                    sel_mask = sel_mask | (sdf[c].astype(str).fillna('') == employee_id)
            filtered = sdf[sel_mask]
            if filtered.empty:
                continue
            # compute Date/Time and SwipeGapSeconds for the filtered rows (sorted)
            filtered = filtered.sort_values('LocaleMessageTime')
            filtered['Date'] = filtered['LocaleMessageTime'].dt.date.astype(str)
            filtered['Time'] = filtered['LocaleMessageTime'].dt.time.astype(str)
            filtered['SwipeGapSeconds'] = filtered['LocaleMessageTime'].diff().dt.total_seconds().fillna(0).astype(int)
            filtered['SwipeGap'] = filtered['SwipeGapSeconds'].apply(lambda s: f"{int(s//3600):02d}:{int((s%3600)//60):02d}:{int(s%60):02d}" if s is not None else "-")
            # ensure Zone (if not) by mapping Door/Direction using your map_door_to_zone in trend_runner (import it or reimplement)
            if 'Zone' not in filtered.columns and 'Door' in filtered.columns:
                try:
                    from trend_runner import map_door_to_zone
                    filtered['Zone'] = filtered.apply(lambda r: map_door_to_zone(r.get('Door'), r.get('Direction')), axis=1)
                except Exception:
                    filtered['Zone'] = filtered.get('Zone', None)
            # project the columns the frontend expects
            want = []
            for c in ('EmployeeName','EmployeeID','CardNumber','Date','Time','SwipeGap','SwipeGapSeconds','Door','Direction','Zone','Note','_source'):
                if c in filtered.columns:
                    want.append(c)
            if not want:
                # fallback: return basic fields
                want = list(filtered.columns[:12])
            rows = filtered[want].to_dict(orient='records')
            raw_swipes.extend(rows)
    except Exception:
        # non fatal - return what we have
        pass

    # 4) Enrich aggregated_rows by trying to add Email + imageUrl using trend_runner helper if available
    try:
        from trend_runner import _enrich_with_personnel_info
        if aggregated_rows:
            agg_df = pd.DataFrame(aggregated_rows)
            agg_df = _enrich_with_personnel_info(agg_df, image_endpoint_template="/employee/{}/image")
            aggregated_rows = agg_df.to_dict(orient='records')
    except Exception:
        pass

    return jsonify({
        'aggregated_rows': aggregated_rows,
        'raw_swipe_files': raw_swipe_files,
        'raw_swipes': raw_swipes
    })



#Above Working 


@app.route('/record', methods=['GET'])
def get_record():
    """
    Single unified /record handler:
    - If no employee id passed, return sample of aggregated rows.
    - If employee_id/person_uid passed, return enriched aggregated_rows + raw_swipes + raw_swipe_files.
    Uses DEFAULT_OUTDIR consistently for reading trend and swipe CSVs.
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    # Use DEFAULT_OUTDIR consistently (this module-level variable is created earlier)
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        # no trend files -> return empty result set gracefully
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    # build a single DataFrame with all matching trend files
    df_list = []
    for fp in csvs:
        try:
            tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
        except Exception:
            try:
                tmp = pd.read_csv(fp, dtype=str)
                if 'Date' in tmp.columns:
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
            except Exception:
                continue
        df_list.append(tmp)
    if not df_list:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200
    df = pd.concat(df_list, ignore_index=True)
    df = _replace_placeholder_strings(df)

    # if no query param, return cleaned sample (head)
    if q is None:
        cleaned = _clean_sample_df(df, max_rows=10)
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    # Normalize query
    q_str = str(q).strip()
    def normalize_series(s):
        if s is None:
            return pd.Series([''] * len(df))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    # find matching rows
    found_mask = pd.Series(False, index=df.index)
    if 'EmployeeID' in df.columns:
        emp_series = normalize_series(df['EmployeeID'])
        found_mask = found_mask | (emp_series == q_str)
    if 'person_uid' in df.columns:
        uid_series = normalize_series(df['person_uid'])
        found_mask = found_mask | (uid_series == q_str)
    if 'Int1' in df.columns and not found_mask.any():
        int1_series = normalize_series(df['Int1'])
        found_mask = found_mask | (int1_series == q_str)

    if not found_mask.any():
        # try numeric match
        try:
            q_numeric = float(q_str)
            if 'EmployeeID' in df.columns:
                emp_numeric = pd.to_numeric(df['EmployeeID'], errors='coerce')
                found_mask = found_mask | (emp_numeric == q_numeric)
            if 'Int1' in df.columns and not found_mask.any():
                int_numeric = pd.to_numeric(df['Int1'], errors='coerce')
                found_mask = found_mask | (int_numeric == q_numeric)
        except Exception:
            pass

    matched = df[found_mask].copy()
    if matched.empty:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    # Clean matched rows for JSON and enrichment
    cleaned_matched = _clean_sample_df(matched, max_rows=len(matched))

    # Enrich matched rows: email / image / explanations
    # Try to find personnel info using multiple candidate tokens (but avoid DB hits for obvious missing tokens)
    try:
        # lazy import helpers from trend_runner if available (safe)
        from trend_runner import compute_violation_days_map, _strip_uid_prefix as _strip_uid_prefix_tr
        violation_map = compute_violation_days_map(str(DEFAULT_OUTDIR), 90, datetime.now().date())
    except Exception:
        violation_map = {}
        _strip_uid_prefix_tr = (lambda x: x)

    matched_indexed = matched.reset_index(drop=True)

    # For each cleaned row build candidate and enrich
    for idx_c, cleaned in enumerate(cleaned_matched):
        candidate_row = None
        try:
            # 1) try to match by person_uid/EmployeeID/EmployeeName
            if cleaned.get('person_uid') and 'person_uid' in matched_indexed.columns:
                mr = matched_indexed[matched_indexed['person_uid'].astype(str).str.strip() == str(cleaned['person_uid']).strip()]
                if not mr.empty:
                    candidate_row = mr.iloc[0].to_dict()

            if candidate_row is None and cleaned.get('EmployeeID') and 'EmployeeID' in matched_indexed.columns:
                mr = matched_indexed[matched_indexed['EmployeeID'].astype(str).str.strip() == str(cleaned['EmployeeID']).strip()]
                if not mr.empty:
                    candidate_row = mr.iloc[0].to_dict()

            if candidate_row is None and cleaned.get('EmployeeName') and 'EmployeeName' in matched_indexed.columns:
                mr = matched_indexed[matched_indexed['EmployeeName'].astype(str).str.strip().str.lower() == str(cleaned['EmployeeName']).strip().lower()]
                if not mr.empty:
                    candidate_row = mr.iloc[0].to_dict()

            # 2) fallback to other columns
            if candidate_row is None and not matched_indexed.empty:
                for ccol in ('CardNumber', 'EmployeeIdentity', 'Int1', 'Text12', 'ObjectIdentity1', 'ObjectID', 'GUID'):
                    if ccol in matched_indexed.columns and cleaned.get(ccol) not in (None, '', 'nan'):
                        mr = matched_indexed[matched_indexed[ccol].astype(str).str.strip() == str(cleaned.get(ccol)).strip()]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                            break

            # 3) last resort — pick first
            if candidate_row is None and not matched_indexed.empty:
                candidate_row = matched_indexed.iloc[0].to_dict()
        except Exception:
            candidate_row = None

        # Build explanation text if present
        violation_expl = None
        try:
            if candidate_row:
                violation_expl = candidate_row.get('Explanation') or candidate_row.get('explanation') or None

            if not violation_expl:
                reasons = cleaned.get('Reasons') or (candidate_row.get('Reasons') if candidate_row else None)
                if reasons:
                    parts = [p.strip() for p in re.split(r'[;,\|]', str(reasons)) if p.strip()]
                    mapped = []
                    for p in parts:
                        try:
                            if 'SCENARIO_EXPLANATIONS' in globals() and p in SCENARIO_EXPLANATIONS:
                                mapped.append(SCENARIO_EXPLANATIONS[p](candidate_row or {}))
                            else:
                                mapped.append(p.replace("_", " ").replace(">=", "≥"))
                        except Exception:
                            mapped.append(p)
                    violation_expl = " ".join(mapped) if mapped else None

            # remove GUIDs inside explanation if possible
            if violation_expl:
                try:
                    emp_name_for_expl = None
                    if candidate_row:
                        emp_name_for_expl = candidate_row.get('EmployeeName') or candidate_row.get('employee_name') or candidate_row.get('ObjectName1')
                    if not emp_name_for_expl:
                        emp_name_for_expl = cleaned.get('EmployeeName')
                    if emp_name_for_expl:
                        violation_expl = GUID_IN_TEXT_RE.sub(str(emp_name_for_expl), str(violation_expl))
                except Exception:
                    pass
        except Exception:
            violation_expl = None

        # violation days lookup (if available)
        try:
            candidates = []
            for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
                if cleaned.get(k) not in (None, '', 'nan'):
                    candidates.append(cleaned.get(k))
            if candidate_row:
                for k in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
                    if candidate_row.get(k) not in (None,'','nan'):
                        candidates.append(candidate_row.get(k))
            vdays = 0
            if violation_map:
                for c in candidates:
                    if c is None:
                        continue
                    n = _normalize_id_local(c)
                    if n and n in violation_map:
                        vdays = int(violation_map.get(n, 0))
                        break
                    try:
                        stripped = _strip_uid_prefix_tr(str(n))
                        if stripped != n and stripped in violation_map:
                            vdays = int(violation_map.get(stripped, 0))
                            break
                    except Exception:
                        pass
        except Exception:
            vdays = 0

        # Personnel lookup: attempt DB lookup only if we have useful tokens (avoid unnecessary DB hits)
        personnel_info = {}
        try:
            lookup_candidates = []
            if candidate_row:
                for k in ('EmployeeObjID','EmployeeObjId','EmployeeIdentity','ObjectID','GUID','EmployeeID','Int1','Text12','EmployeeName'):
                    if candidate_row.get(k) not in (None,'','nan'):
                        lookup_candidates.append(candidate_row.get(k))
            for k in ('EmployeeID','person_uid','EmployeeName'):
                if cleaned.get(k) not in (None,'','nan'):
                    lookup_candidates.append(cleaned.get(k))

            for cand in lookup_candidates:
                if cand is None:
                    continue
                try:
                    info = get_personnel_info(cand)
                    if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None or info.get('ManagerEmail') is not None):
                        personnel_info = info
                        break
                except Exception:
                    continue

            # Extra fallback: try by EmployeeID explicitly
            if not personnel_info:
                try:
                    cand = cleaned.get('EmployeeID') or cleaned.get('EmployeeIdentity')
                    if cand:
                        info = get_personnel_info(cand)
                        if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None):
                            personnel_info = info
                except Exception:
                    pass
        except Exception:
            personnel_info = {}

        # Attach computed fields (robust)
        try:
            cleaned['ViolationDaysLast90'] = int(vdays or 0)
            cleaned['ViolationExplanation'] = violation_expl
            cleaned['Explanation'] = violation_expl or cleaned.get('ViolationExplanation') or None

            # Ensure EmployeeName
            try:
                if (not cleaned.get('EmployeeName')) or _looks_like_guid(cleaned.get('EmployeeName')):
                    if candidate_row and candidate_row.get('EmployeeName') and not _looks_like_guid(candidate_row.get('EmployeeName')):
                        cleaned['EmployeeName'] = candidate_row.get('EmployeeName')
                    elif personnel_info and personnel_info.get('Name'):
                        cleaned['EmployeeName'] = personnel_info.get('Name')
            except Exception:
                pass

            # Populate EmployeeEmail robustly: prefer personnel_info, then candidate_row, then matched_indexed source row
            email_val = None
            if personnel_info:
                email_val = personnel_info.get('EmailAddress') or personnel_info.get('Email')
            if not email_val and candidate_row:
                for fk in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                    if candidate_row.get(fk) not in (None, '', 'nan'):
                        email_val = candidate_row.get(fk)
                        break
            if not email_val:
                # try the matched_indexed first row
                for col in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                    if col in matched_indexed.columns:
                        try:
                            val = matched_indexed.iloc[0].get(col)
                            if val not in (None, '', 'nan'):
                                email_val = val
                                break
                        except Exception:
                            continue

            # Final fallback: sanitize value
            cleaned['EmployeeEmail'] = (email_val if email_val not in (None, '', 'nan') else None) or cleaned.get('EmployeeEmail') or None

            # IMAGE resolution: prefer personnel_info.ObjectID then candidate_row fields, otherwise EmployeeID/person_uid
            try:
                img_obj = None
                if personnel_info and personnel_info.get('ObjectID') is not None:
                    img_obj = personnel_info.get('ObjectID')
                elif candidate_row:
                    for k in ('EmployeeObjID', 'EmployeeObjId', 'ObjectID', 'ObjectIdentity1'):
                        if candidate_row.get(k) not in (None, '', 'nan'):
                            img_obj = candidate_row.get(k)
                            break

                try:
                    base = (request.url_root or request.host_url).rstrip('/')
                except Exception:
                    base = ''

                if img_obj:
                    if base:
                        cleaned['imageUrl'] = f"{base}/employee/{img_obj}/image"
                    else:
                        cleaned['imageUrl'] = f"/employee/{img_obj}/image"
                    try:
                        b = get_person_image_bytes(img_obj)
                        cleaned['HasImage'] = True if b else False
                    except Exception:
                        cleaned['HasImage'] = False
                else:
                    emp_for_img = cleaned.get('EmployeeID') or cleaned.get('person_uid') or None
                    if emp_for_img:
                        if base:
                            cleaned['imageUrl'] = f"{base}/employee/{emp_for_img}/image"
                        else:
                            cleaned['imageUrl'] = f"/employee/{emp_for_img}/image"
                        try:
                            b = get_person_image_bytes(emp_for_img)
                            cleaned['HasImage'] = True if b else False
                        except Exception:
                            cleaned['HasImage'] = False
                    else:
                        cleaned['imageUrl'] = None
                        cleaned['HasImage'] = False

            except Exception:
                cleaned['imageUrl'] = cleaned.get('imageUrl') or None
                cleaned['HasImage'] = cleaned.get('HasImage') or False

            # ensure EmployeeID surfaced if absent
            if not cleaned.get('EmployeeID') and candidate_row:
                for k in ('EmployeeID','Int1','Text12','EmployeeIdentity'):
                    if candidate_row.get(k) not in (None, '', 'nan'):
                        cleaned['EmployeeID'] = candidate_row.get(k)
                        break

        except Exception:
            # safe defaults if enrichment fails
            cleaned.setdefault('EmployeeEmail', None)
            cleaned.setdefault('imageUrl', None)
            cleaned.setdefault('HasImage', False)

    # Build raw_swipes timeline (scan swipe files using DEFAULT_OUTDIR)
    raw_files = set()
    raw_swipes_out = []
    seen_swipe_keys = set()
    def _append_swipe(out_row, source_name):
        key = (
            out_row.get('Date') or '',
            out_row.get('Time') or '',
            (out_row.get('Door') or '').strip(),
            (out_row.get('Direction') or '').strip(),
            (out_row.get('CardNumber') or out_row.get('Card') or '').strip()
        )
        if key in seen_swipe_keys:
            return
        seen_swipe_keys.add(key)
        out_row['_source'] = source_name
        raw_swipes_out.append(out_row)

    # dates discovered from matched rows
    dates_to_scan = set()
    for _, agg_row in matched.iterrows():
        try:
            if 'Date' in agg_row and pd.notna(agg_row['Date']):
                try:
                    d = pd.to_datetime(agg_row['Date']).date()
                    dates_to_scan.add(d)
                except Exception:
                    pass
            for col in ('FirstSwipe','LastSwipe'):
                if col in agg_row and pd.notna(agg_row[col]):
                    try:
                        d = pd.to_datetime(agg_row[col]).date()
                        dates_to_scan.add(d)
                    except Exception:
                        pass
        except Exception:
            continue
    if not dates_to_scan:
        dates_to_scan = {None}

    for d in dates_to_scan:
        candidates = _find_swipe_files(DEFAULT_OUTDIR, date_obj=d, city_slug=city_slug)
        if not candidates:
            candidates = _find_swipe_files(DEFAULT_OUTDIR, date_obj=d, city_slug=None)

        for fp in candidates:
            raw_files.add(fp.name)
            try:
                try:
                    raw_df = pd.read_csv(fp, parse_dates=['LocaleMessageTime'])
                except Exception:
                    raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

            raw_df = _replace_placeholder_strings(raw_df)
            # lower->orig mapping
            cols_lower = {c.lower(): c for c in raw_df.columns}
            tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
            emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
            name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
            card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
            door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
            dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
            note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
            person_uid_col = cols_lower.get('person_uid')

            mask = pd.Series(False, index=raw_df.index)
            if person_uid_col and person_uid_col in raw_df.columns:
                mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
            if emp_col and emp_col in raw_df.columns:
                mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
            if not mask.any() and emp_col and emp_col in raw_df.columns:
                try:
                    q_numeric = float(q)
                    emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                    mask = mask | (emp_numeric == q_numeric)
                except Exception:
                    pass
            if not mask.any() and name_col and name_col in raw_df.columns:
                try:
                    mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())
                except Exception:
                    pass

            if not mask.any():
                continue

            filtered = raw_df[mask].copy()
            if filtered.empty:
                continue

            if tcol and tcol in filtered.columns:
                try:
                    filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                except Exception:
                    pass

            if tcol and tcol in filtered.columns:
                filtered = filtered.sort_values(by=tcol)
                filtered['_prev_ts'] = filtered[tcol].shift(1)
                try:
                    filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                except Exception:
                    filtered['_swipe_gap_seconds'] = 0.0
                try:
                    cur_dates = filtered[tcol].dt.date
                    prev_dates = cur_dates.shift(1)
                    day_start_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
                    filtered.loc[day_start_mask, '_swipe_gap_seconds'] = 0.0
                except Exception:
                    pass
            else:
                filtered['_swipe_gap_seconds'] = 0.0

            try:
                if door_col and door_col in filtered.columns:
                    if dir_col and dir_col in filtered.columns:
                        filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                    else:
                        filtered['_zone'] = filtered.get(door_col, None)
            except Exception:
                filtered['_zone'] = None

            for _, r in filtered.iterrows():
                out = {}
                out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in filtered.columns else _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else q)
                # employee id
                emp_val = None
                if emp_col and emp_col in filtered.columns:
                    emp_val = _to_python_scalar(r.get(emp_col))
                else:
                    for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                        if cand.lower() in cols_lower:
                            emp_val = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                            if emp_val not in (None,'','nan'):
                                break
                    if emp_val in (None,'','nan'):
                        emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
                if emp_val is not None:
                    try:
                        s = str(emp_val).strip()
                        if '.' in s:
                            f = float(s)
                            if math.isfinite(f) and f.is_integer():
                                s = str(int(f))
                        if _looks_like_guid(s) or _is_placeholder_str(s):
                            emp_val = None
                        else:
                            emp_val = s
                    except Exception:
                        if _looks_like_guid(emp_val):
                            emp_val = None
                out['EmployeeID'] = emp_val

                # Card number
                card_val = None
                if card_col and card_col in filtered.columns:
                    card_val = _to_python_scalar(r.get(card_col))
                else:
                    for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                        if cand.lower() in cols_lower:
                            card_val = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                            if card_val not in (None,'','nan'):
                                break
                    if card_val in (None,'','nan'):
                        card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                if card_val is not None:
                    try:
                        cs = str(card_val).strip()
                        if _looks_like_guid(cs) or _is_placeholder_str(cs):
                            card_val = None
                        else:
                            card_val = cs
                    except Exception:
                        card_val = None
                out['CardNumber'] = card_val
                out['Card'] = card_val

                # timestamps
                if tcol and tcol in filtered.columns:
                    ts = r.get(tcol)
                    try:
                        ts_py = pd.to_datetime(ts)
                        out['Date'] = ts_py.date().isoformat()
                        out['Time'] = ts_py.time().isoformat()
                        out['LocaleMessageTime'] = ts_py.isoformat()
                    except Exception:
                        txt = str(r.get(tcol))
                        out['Date'] = txt[:10]
                        out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                        out['LocaleMessageTime'] = txt
                else:
                    out['Date'] = None
                    out['Time'] = None
                    out['LocaleMessageTime'] = None

                out['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
                out['SwipeGap'] = format_seconds_to_hms(out['SwipeGapSeconds'])
                out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
                out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in r else None
                out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
                try:
                    out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else map_door_to_zone(out['Door'], out['Direction'])
                except Exception:
                    out['Zone'] = None
                out['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
                out['_source_file'] = fp.name
                _append_swipe(out, fp.name)

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": sorted(list(raw_files)),
        "raw_swipes": raw_swipes_out
    }), 200



@app.route('/record/export', methods=['GET'])
def export_record_excel():
    q = request.args.get('employee_id') or request.args.get('person_uid')
    date_str = request.args.get('date')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    if not q:
        return jsonify({"error":"employee_id or person_uid is required"}), 400

    p = Path(DEFAULT_OUTDIR)
    files_to_scan = []
    if date_str:
        try:
            dd = pd.to_datetime(date_str).date()
            files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=dd, city_slug=city_slug)
        except Exception:
            return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400
    else:
        files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=city_slug)
    if not files_to_scan:
        # show any available swipe-style files to help frontend debugging
        avail = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=None)
        avail_names = [f.name for f in avail] if avail else []
        logging.info(
            "export_record_excel: no files matched for date=%s city=%s; available swipe files=%s",
            date_str, city_slug, avail_names
        )
        return jsonify({
            "error": "no raw swipe files found for requested date / outputs",
            "available_swipe_files": avail_names
        }), 404

    all_rows = []
    for fp in files_to_scan:
        try:
            raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
        except Exception:
            try:
                raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

        raw_df = _replace_placeholder_strings(raw_df)
        cols_lower = {c.lower(): c for c in raw_df.columns}
        tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
        emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
        name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
        card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
        door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
        dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
        note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
        person_uid_col = cols_lower.get('person_uid')

        mask = pd.Series(False, index=raw_df.index)
        if person_uid_col and person_uid_col in raw_df.columns:
            mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
        if emp_col and emp_col in raw_df.columns:
            mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
        if not mask.any() and emp_col and emp_col in raw_df.columns:
            try:
                q_numeric = float(q)
                emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                mask = mask | (emp_numeric == q_numeric)
            except Exception:
                pass
        if not mask.any() and name_col and name_col in raw_df.columns:
            mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())

        if not mask.any():
            continue

        filtered = raw_df[mask].copy()
        if filtered.empty:
            continue

        if tcol and tcol in filtered.columns:
            try:
                filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
            except Exception:
                pass

        if tcol and tcol in filtered.columns:
            filtered = filtered.sort_values(by=tcol)
            filtered['_prev_ts'] = filtered[tcol].shift(1)
            try:
                filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
            except Exception:
                filtered['_swipe_gap_seconds'] = 0.0
        else:
            filtered['_swipe_gap_seconds'] = 0.0

        try:
            if door_col and door_col in filtered.columns:
                if dir_col and dir_col in filtered.columns:
                    filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                else:
                    filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
            else:
                filtered['_zone'] = filtered.get('PartitionName2', None)
        except Exception:
            filtered['_zone'] = None

        for _, r in filtered.iterrows():
            row = {}
            row['EmployeeName'] = _to_python_scalar(r.get(name_col)) if (name_col and name_col in filtered.columns) else None
            emp_val = None
            if emp_col and emp_col in filtered.columns:
                emp_val = _to_python_scalar(r.get(emp_col))
            else:
                for cand in ('int1','text12','employeeid','employee_identity','employeeidentity'):
                    if cand in cols_lower and cols_lower[cand] in filtered.columns:
                        emp_val = _to_python_scalar(r.get(cols_lower[cand]))
                        if emp_val:
                            break
            row['EmployeeID'] = emp_val
            row['Card'] = _to_python_scalar(r.get(card_col)) if (card_col and card_col in filtered.columns) else None

            if tcol and tcol in filtered.columns:
                ts = r.get(tcol)
                try:
                    ts_py = pd.to_datetime(ts)
                    row['Date'] = ts_py.date().isoformat()
                    row['Time'] = ts_py.time().isoformat()
                    row['LocaleMessageTime'] = ts_py.isoformat()
                except Exception:
                    txt = str(r.get(tcol))
                    row['Date'] = txt[:10]
                    row['Time'] = txt[11:19] if len(txt) >= 19 else None
                    row['LocaleMessageTime'] = txt
            else:
                row['Date'] = None
                row['Time'] = None
                row['LocaleMessageTime'] = None

            row['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
            row['SwipeGap'] = format_seconds_to_hms(row['SwipeGapSeconds'])
            row['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
            row['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else None
            row['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
            try:
                zone_val = r.get('_zone') if '_zone' in r else None
                if zone_val is None:
                    zone_val = map_door_to_zone(row['Door'], row['Direction'])
                row['Zone'] = _to_python_scalar(zone_val)
            except Exception:
                row['Zone'] = None
            row['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
            row['_source_file'] = fp.name
            all_rows.append(row)

    if not all_rows:
        return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

    df_out = pd.DataFrame(all_rows)
    details_cols = ['EmployeeName','EmployeeID','Door','Direction','Zone','Date','LocaleMessageTime','SwipeGapSeconds','PartitionName2','_source_file']
    timeline_cols = ['EmployeeName','EmployeeID','Card','Date','Time','SwipeGapSeconds','Door','Direction','Zone','Note','_source_file']

    details_df = df_out[[c for c in details_cols if c in df_out.columns]].copy()
    timeline_df = df_out[[c for c in timeline_cols if c in df_out.columns]].copy()

    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            details_df.to_excel(writer, sheet_name='Details — Evidence', index=False)
            timeline_df.to_excel(writer, sheet_name='Swipe timeline', index=False)
            writer.save()
            output.seek(0)
    except Exception as e:
        logging.exception("Failed to create Excel: %s", e)
        return jsonify({"error":"failed to create excel"}), 500

    if OPENPYXL_AVAILABLE:
        try:
            wb = load_workbook(output)
            thin = Side(border_style="thin", color="000000")
            thick = Side(border_style="medium", color="000000")
            for ws in wb.worksheets:
                header = ws[1]
                for cell in header:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                        cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
                for col in ws.columns:
                    max_len = 0
                    col_letter = col[0].column_letter
                    for cell in col:
                        try:
                            v = str(cell.value) if cell.value is not None else ""
                        except Exception:
                            v = ""
                        if len(v) > max_len:
                            max_len = len(v)
                    width = min(max(10, max_len + 2), 50)
                    ws.column_dimensions[col_letter].width = width
            out2 = io.BytesIO()
            wb.save(out2)
            out2.seek(0)
            return send_file(out2, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        except Exception:
            logging.exception("Excel styling failed, returning raw file")
            output.seek(0)
            return send_file(output, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    else:
        output.seek(0)
        return send_file(output, as_attachment=True,
                         download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                         mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)

@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        if build_monthly_training is None:
            raise RuntimeError("build_monthly_training not available")
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500


# @app.route("/employee/<pid>/image")
# def employee_image(pid):
#     """
#     Return image bytes for a person id used by frontend imageUrl template.
#     If you already have get_person_image_bytes(pid) helper, this will call it.
#     """
#     try:
#         # try to call app-level helper if present
#         import importlib
#         appmod = importlib.import_module('app')  # adjust module name if helper is in another module
#         gpib = getattr(appmod, 'get_person_image_bytes', None)
#         if gpib:
#             data = gpib(pid)  # should return bytes or None
#             if data:
#                 return send_file(BytesIO(data), mimetype='image/jpeg')
#     except Exception:
#         logging.exception("employee_image route failed")
#     # fallback: 404 — frontend shows placeholder if not found
#     return ('', 404)



# chatbot helpers (kept mostly as-is)
try:
    from trend_runner import _read_past_trend_csvs, _normalize_id_val, SCENARIO_EXPLANATIONS
except Exception:
    _read_past_trend_csvs = None
    _normalize_id_val = None
    SCENARIO_EXPLANATIONS = {}

def _load_latest_trend_df(outdir: Path, city: str = "pune"):
    city_slug = _slug_city(city)
    csvs = sorted(outdir.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(outdir.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return None, None
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    df = _replace_placeholder_strings(df)
    return df, latest.name

def _find_person_rows(identifier: str, days: int = 90, outdir: Path = DEFAULT_OUTDIR):
    if _normalize_id_val:
        norm = _normalize_id_val(identifier)
    else:
        norm = str(identifier).strip()
        if '.' in norm:
            try:
                f = float(norm)
                if f.is_integer():
                    norm = str(int(f))
            except Exception:
                pass
    today = datetime.now().date()
    try:
        if _read_past_trend_csvs:
            past = _read_past_trend_csvs(str(outdir), days, today)
        else:
            files = sorted(Path(outdir).glob("trend_pune_*.csv"), reverse=True)
            dfs = []
            cutoff = today - timedelta(days=days)
            for fp in files:
                try:
                    tmp = pd.read_csv(fp, parse_dates=['Date'])
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                    tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                    dfs.append(tmp)
                except Exception:
                    try:
                        tmp = pd.read_csv(fp, dtype=str)
                        if 'Date' in tmp.columns:
                            tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                            tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                            dfs.append(tmp)
                    except Exception:
                        continue
            past = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
    except Exception:
        past = pd.DataFrame()

    if past is None or past.empty:
        return pd.DataFrame()

    past = _replace_placeholder_strings(past)
    match_mask = pd.Series(False, index=past.index)
    for col in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
        if col in past.columns:
            try:
                match_mask = match_mask | (past[col].astype(str).fillna('').str.strip() == str(norm).strip())
            except Exception:
                continue

    if not match_mask.any():
        try:
            qnum = float(norm)
            for col in ('EmployeeID','Int1'):
                if col in past.columns:
                    try:
                        numcol = pd.to_numeric(past[col], errors='coerce')
                        match_mask = match_mask | (numcol == qnum)
                    except Exception:
                        continue
        except Exception:
            pass

    if not match_mask.any() and 'EmployeeName' in past.columns:
        names = past['EmployeeName'].dropna().astype(str).unique().tolist()
        close = difflib.get_close_matches(str(identifier), names, n=5, cutoff=0.7)
        if close:
            match_mask = match_mask | past['EmployeeName'].astype(str).isin(close)

    return past[match_mask].copy()

def _explain_scenario_code(code):
    if not code:
        return None
    code = str(code).strip()
    if code in SCENARIO_EXPLANATIONS:
        try:
            fn = SCENARIO_EXPLANATIONS.get(code)
            try:
                txt = fn({})
                return txt
            except Exception:
                return code.replace("_", " ").replace(">= ", "≥ ")
        except Exception:
            return code.replace("_", " ").replace(">= ", "≥ ")
    return code.replace("_", " ").replace(">=", "≥")

def _map_score_to_label_fallback(score: float):
    try:
        s = float(score)
    except Exception:
        return (0.0, "Low")
    if s >= 0.75:
        return (s, "High")
    if s >= 0.4:
        return (s, "Medium")
    return (s, "Low")

@app.route('/chatbot/query', methods=['POST'])
def chatbot_query():
    payload = request.get_json(force=True)
    q = (payload.get('q') or '').strip()
    if not q:
        return jsonify({"error":"query text 'q' required"}), 400
    lang = payload.get('lang')
    q_l = q.lower().strip()

    if re.search(r"\bwho is (high|low) risk\b", q_l) or re.search(r"\b(high|low) risk (people|persons|people) (today)?\b", q_l):
        want = 'high' if 'high' in q_l else 'low' if 'low' in q_l else None
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer": "No trend data available.", "evidence": []})
        if 'RiskLevel' not in df.columns:
            if 'RiskScore' in df.columns:
                def _map_rs(s):
                    try:
                        if pd.isna(s):
                            return 'Low'
                    except Exception:
                        pass
                    try:
                        if 'map_score_to_label' in globals() and callable(globals().get('map_score_to_label')):
                            try:
                                return globals().get('map_score_to_label')(float(s))[1]
                            except Exception:
                                pass
                        return _map_score_to_label_fallback(float(s))[1]
                    except Exception:
                        return 'Low'
                df['RiskLevel'] = df['RiskScore'].apply(lambda s: _map_rs(s))
            else:
                df['RiskLevel'] = df.get('RiskLevel', 'Low')
        if want == 'high':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'high']
        elif want == 'low':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'low']
        else:
            sel = df
        names = sel['EmployeeName'].dropna().astype(str).unique().tolist()
        if not names:
            ans = f"No {want} risk persons found in the latest data." if want else "No persons found."
            return jsonify({"answer": ans, "evidence": []})
        else:
            ans = f"{want.capitalize()} risk persons today: " + ", ".join(names[:40])
            sample = _clean_sample_df(sel.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bexplain\s+([A-Za-z0-9_\-]+)\b.*", q_l)
    if m:
        code = m.group(1).strip()
        explanation = _explain_scenario_code(code)
        ans = f"Explanation for '{code}': {explanation}"
        return jsonify({"answer": ans, "evidence": []})

    if 'trend details' in q_l or 'top reasons' in q_l or 'trend details for today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'Reasons' in df.columns:
            reasons = {}
            for v in df['Reasons'].dropna().astype(str):
                for part in re.split(r'[;,\|]', v):
                    key = part.strip()
                    if key and not _is_placeholder_str(key):
                        reasons[key] = reasons.get(key, 0) + 1
            top = sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:10]
            if not top:
                return jsonify({"answer":"No reason counts available today.","evidence":[]})
            ans = "Top reasons today: " + ", ".join([f"{k} ({c})" for k,c in top])
            sample = []
            try:
                top_reasons = [k for k,_ in top]
                mask = df['Reasons'].astype(str).apply(lambda s: any(tr in s for tr in top_reasons))
                sample_df = df[mask].head(10)
                sample = _clean_sample_df(sample_df, max_rows=10)
            except Exception:
                sample = []
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bshow (?:me )?([A-Za-z0-9\-\:\s]+?) (?:for )?(?:last )?(\d+)\s*days\b", q_l)
    if not m:
        m = re.match(r".*\b(show|display)\s+(?:me\s+)?([A-Za-z0-9\-\:\s]+?)\s+last\s+(\d+)\s*days\b", q_l)
    if m:
        if len(m.groups()) == 2:
            identifier, days = m.group(1).strip(), int(m.group(2))
        else:
            identifier = m.group(1).strip()
            days = int(m.group(2))
        rows = _find_person_rows(identifier, days=days, outdir=DEFAULT_OUTDIR)
        if rows is None or rows.empty:
            return jsonify({"answer": f"No records found for '{identifier}' in last {days} days.", "evidence": []})
        flagged = rows[rows.get('IsFlagged', False) == True] if 'IsFlagged' in rows.columns else pd.DataFrame()
        flagged_count = int(len(flagged))
        total_days = int(len(rows))
        latest_row = rows.sort_values('Date', ascending=False).iloc[0].to_dict()
        name = latest_row.get('EmployeeName') or latest_row.get('person_uid') or latest_row.get('EmployeeID')
        ans = f"Found {total_days} day(s) for {name} in the last {days} days. Flagged days: {flagged_count}."
        sample = _clean_sample_df(rows.sort_values('Date', ascending=False).head(10), max_rows=10)
        return jsonify({"answer": ans, "evidence": sample})

    if 'present today' in q_l or 'who is present today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'PresentToday' in df.columns:
            present = df[df['PresentToday'] == True]
            names = present['EmployeeName'].dropna().unique().tolist()
            ans = f"Present today: {', '.join(names[:40]) if names else 'None'}"
            sample = _clean_sample_df(present.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})
        else:
            return jsonify({"answer":"PresentToday field not available in latest trends.","evidence":[]})

    hint = "I can answer: 'Who is high risk today', 'Who is low risk today', 'Show me <EmployeeID|Name> last 90 days', 'Explain <scenario_code>', 'Trend details for today — top reasons'."
    return jsonify({"answer": f"I can help with trend & risk questions. I recognized: {q}. Try: {hint}", "evidence":[]})

@app.route('/employee/<empid>/image', methods=['GET'])
def serve_employee_image(empid):
    if empid is None:
        return jsonify({"error": "employee id required"}), 400
    try:
        img_bytes = get_person_image_bytes(empid)
        # If no image found, return a small inline SVG placeholder so <img> can still render
        if not img_bytes:
            svg = (
                '<svg xmlns="http://www.w3.org/2000/svg" width="160" height="160">'
                '<rect fill="#eef2f7" width="100%" height="100%"/>'
                '<text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle" fill="#64748b" font-size="18">'
                'No image</text></svg>'
            )
            bio = io.BytesIO(svg.encode('utf-8'))
            bio.seek(0)
            resp = send_file(bio, mimetype='image/svg+xml')
            resp.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            return resp

        # detect content type heuristically (jpeg/png/bmp)
        header = img_bytes[:8] if isinstance(img_bytes, (bytes, bytearray)) else bytes(img_bytes)[:8]
        content_type = 'application/octet-stream'
        try:
            if len(header) >= 2 and header[0] == 0xFF and header[1] == 0xD8:
                content_type = 'image/jpeg'
            elif header.startswith(b'\x89PNG\r\n\x1a\n'):
                content_type = 'image/png'
            elif header.startswith(b'BM'):
                content_type = 'image/bmp'
            else:
                content_type = 'application/octet-stream'
        except Exception:
            content_type = 'application/octet-stream'

        bio = io.BytesIO(img_bytes if isinstance(img_bytes, (bytes, bytearray)) else bytes(img_bytes))
        bio.seek(0)
        resp = send_file(bio, mimetype=content_type)
        resp.headers['Cache-Control'] = 'private, max-age=300'
        return resp
    except Exception:
        logging.exception("Error serving image for employee %s", empid)
        return jsonify({"error": "failed to serve image"}), 500

# run
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)















# backend/trend_runner.py
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re
import os
import calendar
import json
from collections import defaultdict
from datetime import datetime as _datetime
from datetime import timedelta as _timedelta
from typing import Optional, List



# ------------------ personnel enrichment (lazy) ------------------
def _get_personnel_funcs_lazy():
    """
    Try to import personnel helpers from app.py at call time (avoids circular imports).
    Returns tuple (get_personnel_info_fn_or_None, get_person_image_bytes_fn_or_None).
    """
    try:
        import importlib
        appmod = importlib.import_module('app')  # adjust if your app module name differs
        gpi = getattr(appmod, 'get_personnel_info', None)
        gpib = getattr(appmod, 'get_person_image_bytes', None)
        return gpi, gpib
    except Exception:
        return None, None

def _normalize_for_lookup(val):
    if val is None:
        return None
    s = str(val).strip()
    if not s:
        return None
    if len(s) > 36 and '-' in s:
        return s
    return s

def _enrich_with_personnel_info(df, image_endpoint_template="/employee/{}/image"):
    if df is None or df.empty:
        return df
    get_personnel_info, get_person_image_bytes = _get_personnel_funcs_lazy()
    emails = []
    image_urls = []
    for _, row in df.iterrows():
        email = None
        image_url = None
        cand_empid = None
        for tok in ('EmployeeID', 'EmployeeID_feat', 'EmployeeID_dur', 'Int1', 'Text12'):
            if tok in row and row.get(tok) not in (None, '', float('nan')):
                cand_empid = _normalize_for_lookup(row.get(tok))
                break
        cand_uid = row.get('EmployeeIdentity') or row.get('person_uid') or None
        if get_personnel_info:
            try:
                lookup = cand_empid or cand_uid
                pi = get_personnel_info(lookup) if lookup else None
                if pi and isinstance(pi, dict):
                    email = pi.get('EmployeeEmail') or pi.get('EmailAddress') or None
                    parent = pi.get('ObjectID') or pi.get('GUID') or cand_empid or cand_uid
                    if parent:
                        image_url = image_endpoint_template.format(str(parent))
            except Exception:
                pass
        if email is None:
            for fld in ('EmployeeEmail', 'Email', 'EmailAddress', 'ManagerEmail'):
                if fld in row and row.get(fld) not in (None, '', float('nan')):
                    email = row.get(fld)
                    break
        if image_url is None:
            if cand_empid:
                image_url = image_endpoint_template.format(cand_empid)
            elif cand_uid:
                image_url = image_endpoint_template.format(cand_uid)
            else:
                image_url = None
        emails.append(email)
        image_urls.append(image_url)
    out = df.copy()
    out['EmployeeEmail'] = emails
    out['imageUrl'] = image_urls
    return out

# ---------------------------------------------------------------------------

# ------------------ duration_report imports (robust) ------------------
try:
    from duration_report import run_for_date, compute_daily_durations, REGION_CONFIG
except Exception:
    try:
        from duration_report import run_for_date, compute_daily_durations
        REGION_CONFIG = {}
    except Exception:
        try:
            from duration_report import run_for_date
            compute_daily_durations = None
            REGION_CONFIG = {}
        except Exception:
            run_for_date = None
            compute_daily_durations = None
            REGION_CONFIG = {}

# ------------------ optional config door_zone mapping ------------------
try:
    from config.door_zone import map_door_to_zone as config_map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    config_map_door_to_zone = None
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

# History profile (optional)
CANDIDATE_HISTORY = [
    Path(__file__).parent / "config" / "current_analysis.csv",
    Path(__file__).parent.parent / "config" / "current_analysis.csv",
    Path.cwd() / "current_analysis.csv",
    Path(__file__).parent / "current_analysis.csv"
]
HIST_PATH = None
for p in CANDIDATE_HISTORY:
    if p.exists():
        HIST_PATH = p
        break

if HIST_PATH:
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception:
        logging.warning("Failed to load historical profile from %s", HIST_PATH)
        HIST_DF = pd.DataFrame()
else:
    HIST_DF = pd.DataFrame()

# Outdir
OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)

# Small helpers
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')


# ---------------------------------------------------------------------------
# small time formatting helper used for raw_swipes_all
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return "-"
        s = int(seconds)
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return "-"


def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _normalize_id_val(v):
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s

def _looks_like_guid(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        if _looks_like_guid(st):
            return False
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None

# Short card xml extractor
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None

# door -> zone mapping fallback
try:
    _BREAK_ZONES = BREAK_ZONES
    _OUT_OF_OFFICE_ZONE = OUT_OF_OFFICE_ZONE
except Exception:
    _BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    _OUT_OF_OFFICE_ZONE = "Out of office"

def map_door_to_zone(door: object, direction: object = None) -> str:
    try:
        if config_map_door_to_zone is not None:
            return config_map_door_to_zone(door, direction)
    except Exception:
        pass
    try:
        if door is None:
            return None
        s = str(door).strip()
        if not s:
            return None
        s_l = s.lower()
        if direction and isinstance(direction, str):
            d = direction.strip().lower()
            if "out" in d:
                return _OUT_OF_OFFICE_ZONE
            if "in" in d:
                return "Reception Area"
        if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
            return _OUT_OF_OFFICE_ZONE
        return "Working Area"
    except Exception:
        return None

# ----- Config and scenarios -----
VIOLATION_WINDOW_DAYS = 90
RISK_THRESHOLDS = [
    (0.5, "Low"),
    (1.5, "Low Medium"),
    (2.5, "Medium"),
    (4.0, "Medium High"),
    (float("inf"), "High"),
]

def map_score_to_label(score: float) -> (int, str):
    try:
        if score is None:
            score = 0.0
        s = float(score)
    except Exception:
        s = 0.0
    bucket = 1
    label = "Low"
    for i, (threshold, lbl) in enumerate(RISK_THRESHOLDS, start=1):
        if s <= threshold:
            bucket = i
            label = lbl
            break
    return bucket, label

# scenario functions (kept from your improved version)
def scenario_long_gap(row):
    try:
        gap = int(row.get('MaxSwipeGapSeconds') or 0)
        return gap >= int(4.5 * 3600)
    except Exception:
        return False

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = int(row.get('CountSwipes') or 0)
    dur = float(row.get('DurationMinutes') or 0.0)
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = rec.get('TotalSwipes_median', np.nan)
            if pd.notna(median) and median > 0:
                return (cur > 3 * float(median)) and (dur < 60)
    except Exception:
        pass
    try:
        if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
            global_med = HIST_DF['TotalSwipes_median'].median()
            if pd.notna(global_med) and global_med > 0:
                return (cur > 3 * float(global_med)) and (dur < 60)
    except Exception:
        pass
    return (cur > 50) and (dur < 60)

def scenario_high_swipes_benign(row):
    cur = int(row.get('CountSwipes') or 0)
    dur = float(row.get('DurationMinutes') or 0.0)
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = rec.get('TotalSwipes_median', np.nan)
            if pd.notna(median) and median > 0:
                return (cur > 3 * float(median)) and (dur >= 60)
    except Exception:
        pass
    try:
        if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
            global_med = HIST_DF['TotalSwipes_median'].median()
            if pd.notna(global_med) and global_med > 0:
                return (cur > 3 * float(global_med)) and (dur >= 60)
    except Exception:
        pass
    return (cur > 50) and (dur >= 60)

def scenario_behaviour_shift(row, hist_df=None, minutes_threshold=180):
    try:
        if pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None:
            return False
        first_ts = pd.to_datetime(row.get('FirstSwipe'))
        today_minutes = first_ts.hour * 60 + first_ts.minute
        empid = row.get('EmployeeID')
        hist = hist_df if hist_df is not None else (HIST_DF if (HIST_DF is not None and not HIST_DF.empty) else None)
        if hist is None or hist.empty or empid is None:
            return False
        try:
            rec = hist[hist['EmployeeID'] == empid]
            if rec.empty:
                return False
            if 'FirstSwipeMinutes_median' in rec.columns:
                median_min = rec.iloc[0].get('FirstSwipeMinutes_median')
            else:
                median_min = rec.iloc[0].get('AvgFirstSwipeMins_median', None)
            if pd.isna(median_min) or median_min is None:
                return False
            diff = abs(today_minutes - float(median_min))
            return diff >= int(minutes_threshold)
        except Exception:
            return False
    except Exception:
        return False

def scenario_repeated_short_breaks(row):
    try:
        break_count = int(row.get('BreakCount') or 0)
        total_break_mins = float(row.get('TotalBreakMinutes') or 0.0)
        long_break_count = int(row.get('LongBreakCount') or 0)
        short_gap_count = int(row.get('ShortGapCount') or 0)
        if break_count >= 2:
            return True
        if short_gap_count >= 5:
            return True
        if total_break_mins >= 180 and short_gap_count >= 2:
            return True
        return False
    except Exception:
        return False

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map

def scenario_shortstay_longout_repeat(row):
    return bool(row.get('PatternShortLongRepeat', False))

SCENARIOS = [
    ("long_gap_>=4.5h", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap),
    ("high_swipes_benign", scenario_high_swipes_benign),
    ("behaviour_shift", scenario_behaviour_shift),
    ("shortstay_longout_repeat", scenario_shortstay_longout_repeat)
]

# --- improved human-readable scenario explanations (use hours for duration/gaps) ---
def _hrs_from_minutes(mins):
    try:
        m = float(mins or 0.0)
        return round(m / 60.0, 1)
    except Exception:
        return None

def _hrs_from_seconds(sec):
    try:
        s = float(sec or 0.0)
        return round(s / 3600.0, 1)
    except Exception:
        return None

SCENARIO_EXPLANATIONS = {
    "long_gap_>=4.5h": lambda r: (
        (lambda h: f"Long gap between swipes (~{h} h)." if h is not None else "Long gap between swipes.")
        (_hrs_from_seconds(r.get('MaxSwipeGapSeconds')))
    ),
    "short_duration_<4h": lambda r: (
        # if duration is zero but we only saw only_in/only_out, be explicit
        "Only 'IN' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyIn', 0)) == 1 else
        "Only 'OUT' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyOut', 0)) == 1 else
        (lambda h: f"Short total presence (~{h} h)." if h is not None else "Short total presence.")(_hrs_from_minutes(r.get('DurationMinutes')))
    ),
    "coffee_badging": lambda r: "Multiple quick swipes in short time.",
    "low_swipe_count_<=2": lambda r: "Very few swipes on day.",
    "single_door": lambda r: "Only a single door used during the day.",
    "only_in": lambda r: "Only 'IN' events recorded.",
    "only_out": lambda r: "Only 'OUT' events recorded.",
    "overtime_>=10h": lambda r: "Overtime detected (>=10 hours).",
    "very_long_duration_>=16h": lambda r: "Very long presence (>=16 hours).",
    "zero_swipes": lambda r: "No swipes recorded on this day.",
    "unusually_high_swipes": lambda r: "Unusually high number of swipes compared to peers/history.",
    "repeated_short_breaks": lambda r: "Many short gaps between swipes.",
    "multiple_location_same_day": lambda r: "Multiple locations/partitions used in same day.",
    "weekend_activity": lambda r: "Activity recorded on weekend day.",
    "repeated_rejection_count": lambda r: "Multiple rejection events recorded.",
    "badge_sharing_suspected": lambda r: "Same card used by multiple users on same day — possible badge sharing.",
    "early_arrival_before_06": lambda r: "First swipe earlier than 06:00.",
    "late_exit_after_22": lambda r: "Last swipe after 22:00.",
    "shift_inconsistency": lambda r: "Duration deviates from historical shift patterns.",
    "trending_decline": lambda r: "Employee shows trending decline in presence.",
    "consecutive_absent_days": lambda r: "Consecutive absent days observed historically.",
    "high_variance_duration": lambda r: "High variance in daily durations historically.",
    "short_duration_on_high_presence_days": lambda r: "Short duration despite normally high presence days.",
    "swipe_overlap": lambda r: "Overlap in swipe times with other persons on same door.",
    "behaviour_shift": lambda r: "Significant change in arrival time compared to historical baseline.",
    "shortstay_longout_repeat": lambda r: "Repeated pattern: short in → long out → short return."
}


def _explain_scenarios_detected(row, detected_list):
    pieces = []
    # derive a human display label consisting of Name and EmployeeID where possible
    try:
        empid = None
        # prefer explicit EmployeeID (non-GUID) from various tokens
        for tok in ('EmployeeID', 'EmployeeID_feat', 'EmployeeID_dur', 'Int1', 'Text12'):
            if tok in row and row.get(tok) not in (None, '', 'nan'):
                val = _normalize_id_val(row.get(tok))
                if val and not _looks_like_guid(val):
                    empid = str(val)
                    break
        # if empid still None, allow non-GUID EmployeeIdentity
        if not empid and row.get('EmployeeIdentity') not in (None, '', 'nan'):
            tmp = _normalize_id_val(row.get('EmployeeIdentity'))
            if tmp and not _looks_like_guid(tmp):
                empid = str(tmp)

        name = None
        try:
            nm = row.get('EmployeeName')
            if nm and _looks_like_name(nm) and not _is_placeholder_str(nm):
                name = str(nm).strip()
            else:
                # fallback: pick first non-guid textual name from common tokens
                for cand in ('EmployeeName', 'EmployeeName_feat', 'EmployeeName_dur', 'ObjectName1'):
                    if cand in row and row.get(cand) not in (None, '', 'nan'):
                        v = row.get(cand)
                        if v and not _looks_like_guid(v) and _looks_like_name(v):
                            name = str(v).strip()
                            break
                if not name:
                    # try to strip person_uid prefixes if present
                    pu = row.get('person_uid')
                    if isinstance(pu, str) and (pu.startswith('emp:') or pu.startswith('uid:') or pu.startswith('name:')):
                        try:
                            stripped = _strip_uid_prefix(pu)
                            if stripped and not _looks_like_guid(stripped):
                                name = str(stripped)
                        except Exception:
                            pass
        except Exception:
            name = None

        # build prefix
        prefix = ""
        if name and empid:
            prefix = f"{name} ({empid}) - "
        elif name:
            prefix = f"{name} - "
        elif empid:
            prefix = f"{empid} - "
        else:
            prefix = ""
    except Exception:
        prefix = ""

    for sc in detected_list:
        sc = sc.strip()
        fn = SCENARIO_EXPLANATIONS.get(sc)
        try:
            if fn:
                pieces.append(fn(row))
            else:
                pieces.append(sc.replace("_", " ").replace(">=", "≥"))
        except Exception:
            pieces.append(sc)
    if not pieces:
        return None
    explanation = " ".join([p if p.endswith('.') else p + '.' for p in pieces])

    # Replace any GUID that accidentally remained inside explanation with the chosen human identifier (without duplicating parentheses)
    try:
        GUID_IN_TEXT_RE = re.compile(r'[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}')
        if prefix and isinstance(explanation, str) and GUID_IN_TEXT_RE.search(explanation):
            # replace GUIDs inside with the prefix label (strip trailing ' - ' from prefix)
            label = prefix.rstrip(' - ')
            explanation = GUID_IN_TEXT_RE.sub(str(label), explanation)
    except Exception:
        pass

    return prefix + explanation


# ---------------- compute_features (robust merged version) ----------------
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:

    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)

    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
    else:
        if 'Date' in sw.columns:
            sw['LocaleMessageTime'] = None
            try:
                sw['LocaleMessageTime'] = pd.to_datetime(sw['Date'], errors='coerce')
            except Exception:
                sw['LocaleMessageTime'] = None

    # By default Date comes from LocaleMessageTime (local, human timestamps).
    # However if an AdjustedMessageTime column exists (the Pune 2AM boundary) prefer that
    # for date assignment so trend grouping matches compute_daily_durations().
    if 'AdjustedMessageTime' in sw.columns and sw['AdjustedMessageTime'].notna().any():
        try:
            sw['AdjustedMessageTime'] = pd.to_datetime(sw['AdjustedMessageTime'], errors='coerce')
            # Prefer adjusted date for rows where it exists (this mirrors duration_report logic).
            mask_adj = sw['AdjustedMessageTime'].notna()
            # Ensure LocaleMessageTime parsed for those not adjusted
            sw.loc[~mask_adj, 'Date'] = pd.to_datetime(sw.loc[~mask_adj, 'LocaleMessageTime'], errors='coerce').dt.date
            sw.loc[mask_adj, 'Date']  = sw.loc[mask_adj,  'AdjustedMessageTime'].dt.date
        except Exception:
            # fallback to LocaleMessageTime date
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
    else:
        # normal path
        sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date

    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    try:
        if dir_col and dir_col in sw.columns:
            sw['Direction'] = sw[dir_col]
        if door_col and door_col in sw.columns:
            sw['Door'] = sw[door_col]
        if empid_col and empid_col in sw.columns:
            sw['EmployeeID'] = sw[empid_col]
        if name_col and name_col in sw.columns:
            sw['EmployeeName'] = sw[name_col]
        if card_col and card_col in sw.columns:
            sw['CardNumber'] = sw[card_col]
        if 'LocaleMessageTime' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
        elif 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
    except Exception:
        logging.exception("Normalization of swipe columns failed.")

    # PersonnelType filtering (tolerant) - avoid dropping if column absent
    if 'PersonnelTypeName' in sw.columns:
        sw['PersonnelTypeName'] = sw['PersonnelTypeName'].astype(str).str.strip()
        mask = sw['PersonnelTypeName'].str.lower().str.contains(r'employee|terminated', na=False)
        logging.info("PersonnelTypeName values example: %s", list(sw['PersonnelTypeName'].dropna().unique()[:6]))
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelTypeName filter applied: before=%d after=%d", before, len(sw))
    elif 'PersonnelType' in sw.columns:
        sw['PersonnelType'] = sw['PersonnelType'].astype(str).str.strip()
        mask = sw['PersonnelType'].str.lower().str.contains(r'employee|terminated', na=False)
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelType filter applied: before=%d after=%d", before, len(sw))

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # person_uid canonical
    if 'person_uid' not in sw.columns:
        def make_person_uid_local(r):
            empid_val = None
            if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
                empid_val = r.get(empid_col)
            elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
                empid_val = r.get('EmployeeID')
            empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
            name_val = None
            if name_col and name_col in r:
                name_val = r.get(name_col)
            elif 'EmployeeName' in r:
                name_val = r.get('EmployeeName')
            elif 'ObjectName1' in r:
                name_val = r.get('ObjectName1')
            return _canonical_person_uid({
                'EmployeeID': empid_val,
                'EmployeeIdentity': empident_val,
                'EmployeeName': name_val
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
                    'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
                    'EmployeeIdentity'])
    if name_col: sel_cols.add(name_col)
    if empid_col: sel_cols.add(empid_col)
    if card_col: sel_cols.add(card_col)
    if door_col: sel_cols.add(door_col)
    if dir_col: sel_cols.add(dir_col)
    sel_cols = [c for c in sel_cols if c in sw.columns]

    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0

        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

        # card extraction
        card_numbers = []
        if card_col and card_col in g.columns:
            card_numbers = list(pd.unique(g[card_col].dropna()))
        if not card_numbers and 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna()))
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
                    try:
                        vals = list(pd.unique(g[c].dropna()))
                        if vals:
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'xml' in cl:
                    for raw in g[c].dropna().astype(str):
                        extracted = _extract_card_from_xml(raw)
                        if extracted:
                            card_numbers.append(extracted)
        card_numbers = list(dict.fromkeys(card_numbers))
        card_number = None
        for c in card_numbers:
            n = _normalize_id_val(c)
            if n and not _looks_like_guid(n):
                card_number = n
                break

        # stable id/name
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None

        if empid_col and empid_col in g.columns:
            vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        elif 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized

        if (not employee_id) and 'PersonnelType' in g.columns:
            try:
                pvals = g['PersonnelType'].dropna().astype(str)
                if not pvals.empty:
                    p0 = pvals.iloc[0]
                    if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
                        for c in g.columns:
                            if c.lower() == 'text12':
                                vals = g[c].dropna().astype(str).map(lambda x: x.strip())
                                employee_id = _pick_first_non_guid_value(vals)
                                if employee_id:
                                    break
            except Exception:
                pass

        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]

        candidate_name_vals = None
        if name_col and name_col in g.columns:
            candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
        elif 'EmployeeName' in g.columns:
            candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
        elif 'ObjectName1' in g.columns:
            candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

        if candidate_name_vals is not None and not candidate_name_vals.empty:
            employee_name = _pick_first_non_guid_value(candidate_name_vals)
            if employee_name is None:
                for v in candidate_name_vals:
                    if _looks_like_name(v) and not _is_placeholder_str(v):
                        employee_name = str(v).strip()
                        break

        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        first_swipe = None
        last_swipe = None
        if times:
            first_swipe = times[0]
            last_swipe = times[-1]

        # timeline & segments with mapping to zones
        timeline = []
        for _, row in g.sort_values('LocaleMessageTime').iterrows():
            t = row.get('LocaleMessageTime')
            dname = None
            if door_col and door_col in row and pd.notna(row.get(door_col)):
                dname = row.get(door_col)
            elif 'Door' in row and pd.notna(row.get('Door')):
                dname = row.get('Door')
            direction = None
            if dir_col and dir_col in row and pd.notna(row.get(dir_col)):
                direction = row.get(dir_col)
            elif 'Direction' in row and pd.notna(row.get('Direction')):
                direction = row.get('Direction')
            zone = map_door_to_zone(dname, direction)
            timeline.append((t, dname, direction, zone))

        segments = []
        if timeline:
            cur_zone = None
            seg_start = timeline[0][0]
            seg_label = None
            for (t, dname, direction, zone) in timeline:
                if zone in _BREAK_ZONES:
                    lbl = 'break'
                elif zone == _OUT_OF_OFFICE_ZONE:
                    lbl = 'out_of_office'
                else:
                    lbl = 'work'
                if cur_zone is None:
                    cur_zone = zone
                    seg_label = lbl
                    seg_start = t
                else:
                    if lbl != seg_label:
                        segments.append({
                            'label': seg_label,
                            'start': seg_start,
                            'end': t,
                            'start_zone': cur_zone
                        })
                        seg_start = t
                        seg_label = lbl
                        cur_zone = zone
                    else:
                        cur_zone = cur_zone or zone
            if seg_label is not None:
                segments.append({
                    'label': seg_label,
                    'start': seg_start,
                    'end': timeline[-1][0],
                    'start_zone': cur_zone
                })

        break_count = 0
        long_break_count = 0
        total_break_minutes = 0.0

        BREAK_MINUTES_THRESHOLD = 60
        OUT_OFFICE_COUNT_MINUTES = 180
        LONG_BREAK_FLAG_MINUTES = 120

        for i, s in enumerate(segments):
            lbl = s.get('label')
            start = s.get('start')
            end = s.get('end')
            dur_mins = ((end - start).total_seconds() / 60.0) if (start and end) else 0.0
            if lbl == 'break':
                if dur_mins >= BREAK_MINUTES_THRESHOLD:
                    break_count += 1
                    total_break_minutes += dur_mins
                    if dur_mins >= LONG_BREAK_FLAG_MINUTES:
                        long_break_count += 1
            elif lbl == 'out_of_office':
                prev_lbl = segments[i-1]['label'] if i > 0 else None
                next_lbl = segments[i+1]['label'] if i < len(segments)-1 else None
                if prev_lbl == 'work' and next_lbl == 'work' and dur_mins >= OUT_OFFICE_COUNT_MINUTES:
                    break_count += 1
                    total_break_minutes += dur_mins
                    if dur_mins >= LONG_BREAK_FLAG_MINUTES:
                        long_break_count += 1

        pattern_flag = False
        pattern_sequence_readable = None
        try:
            seq = []
            for s in segments:
                dur_mins = (s['end'] - s['start']).total_seconds() / 60.0 if (s['end'] and s['start']) else 0
                seq.append((s['label'], int(round(dur_mins))))
            for i in range(len(seq)-2):
                a = seq[i]
                b = seq[i+1]
                c = seq[i+2]
                if (a[0] == 'work' and a[1] < 60) and \
                   (b[0] in ('out_of_office','break') and b[1] >= LONG_BREAK_FLAG_MINUTES) and \
                   (c[0] == 'work' and c[1] < 60):
                    pattern_flag = True
                    seq_fragment = [a, b, c]
                    pattern_sequence_readable = " -> ".join([f"{lbl} ({mins}m)" for lbl, mins in seq_fragment])
                    break
        except Exception:
            pattern_flag = False
            pattern_sequence_readable = None

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type,
            'FirstSwipe': first_swipe,
            'LastSwipe': last_swipe,
            'BreakCount': int(break_count),
            'LongBreakCount': int(long_break_count),
            'TotalBreakMinutes': float(round(total_break_minutes,1)),
            'PatternShortLongRepeat': bool(pattern_flag),
            'PatternSequenceReadable': pattern_sequence_readable,
            'PatternSequence': None
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])[sel_cols]
    grouped = grouped.apply(agg_swipe_group).reset_index()

    # POST-PROCESS: merge early-morning fragments into previous day (heuristic)
    try:
        grouped['FirstSwipe_dt'] = pd.to_datetime(grouped['FirstSwipe'], errors='coerce')
        grouped['LastSwipe_dt']  = pd.to_datetime(grouped['LastSwipe'],  errors='coerce')
        rows_to_drop = set()
        MERGE_GAP_SECONDS = int(4 * 3600)
        for pid, sub in grouped.sort_values(['person_uid','Date']).groupby('person_uid'):
            prev_idx = None
            for idx, r in sub.reset_index().iterrows():
                real_idx = int(r['index']) if 'index' in r else r.name
                cur_first = pd.to_datetime(grouped.at[real_idx, 'FirstSwipe_dt'])
                if prev_idx is not None:
                    prev_last = pd.to_datetime(grouped.at[prev_idx, 'LastSwipe_dt'])
                    if (not pd.isna(cur_first)) and (not pd.isna(prev_last)):
                        gap = (cur_first - prev_last).total_seconds()
                        if 0 <= gap <= MERGE_GAP_SECONDS and cur_first.time().hour < 2:
                            try:
                                grouped.at[prev_idx, 'CountSwipes'] = int(grouped.at[prev_idx, 'CountSwipes']) + int(grouped.at[real_idx, 'CountSwipes'])
                                grouped.at[prev_idx, 'MaxSwipeGapSeconds'] = max(int(grouped.at[prev_idx, 'MaxSwipeGapSeconds'] or 0), int(grouped.at[real_idx, 'MaxSwipeGapSeconds'] or 0), int(gap))
                                if not pd.isna(grouped.at[real_idx, 'LastSwipe_dt']):
                                    if pd.isna(grouped.at[prev_idx, 'LastSwipe_dt']) or grouped.at[real_idx, 'LastSwipe_dt'] > grouped.at[prev_idx, 'LastSwipe_dt']:
                                        grouped.at[prev_idx, 'LastSwipe_dt'] = grouped.at[real_idx, 'LastSwipe_dt']
                                        grouped.at[prev_idx, 'LastSwipe'] = grouped.at[real_idx, 'LastSwipe']
                                if not grouped.at[prev_idx, 'CardNumber']:
                                    grouped.at[prev_idx, 'CardNumber'] = grouped.at[real_idx, 'CardNumber']
                                grouped.at[prev_idx, 'UniqueDoors'] = int(max(int(grouped.at[prev_idx].get('UniqueDoors') or 0), int(grouped.at[real_idx].get('UniqueDoors') or 0)))
                                grouped.at[prev_idx, 'UniqueLocations'] = int(max(int(grouped.at[prev_idx].get('UniqueLocations') or 0), int(grouped.at[real_idx].get('UniqueLocations') or 0)))
                                rows_to_drop.add(real_idx)
                                continue
                            except Exception:
                                pass
                prev_idx = real_idx
        if rows_to_drop:
            grouped = grouped.drop(index=list(rows_to_drop)).reset_index(drop=True)
    except Exception:
        logging.exception("Failed merge-early-morning fragments (non-fatal).")

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # coalesce duplicated columns (_x/_y) produced by merge
    def _coalesce_merge_columns(df, bases):
        for base in bases:
            x = base + "_x"
            y = base + "_y"
            try:
                has_base = base in df.columns
                base_all_null = False
                if has_base:
                    base_all_null = df[base].isnull().all()
            except Exception:
                has_base = base in df.columns
                base_all_null = True
            if (not has_base) or base_all_null:
                if x in df.columns and y in df.columns:
                    try:
                        df[base] = df[x].combine_first(df[y])
                    except Exception:
                        try:
                            df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
                        except Exception:
                            if x in df.columns:
                                df[base] = df[x]
                            elif y in df.columns:
                                df[base] = df[y]
                elif x in df.columns:
                    df[base] = df[x]
                elif y in df.columns:
                    df[base] = df[y]
    _coalesce_merge_columns(merged, [
        "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
    ])
    drop_cols = [c for c in merged.columns if c.endswith("_x") or c.endswith("_y")]
    if drop_cols:
        try:
            merged.drop(columns=drop_cols, inplace=True)
        except Exception:
            for c in drop_cols:
                if c in merged.columns:
                    try:
                        merged.drop(columns=[c], inplace=True)
                    except Exception:
                        pass

    # ensure columns exist and normalized
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)
    ensure_col(merged, 'BreakCount', 0)
    ensure_col(merged, 'LongBreakCount', 0)
    ensure_col(merged, 'TotalBreakMinutes', 0.0)
    ensure_col(merged, 'PatternShortLongRepeat', False)
    ensure_col(merged, 'PatternSequenceReadable', None)
    ensure_col(merged, 'PatternSequence', None)

    if 'EmployeeName' in merged.columns:
        def choose_best_name(row):
            gname = row.get('EmployeeName')
            dname = None
            for cand in ('EmployeeName', 'employee_name', 'objectname1', 'ObjectName1'):
                if cand in row and row.get(cand) is not None:
                    dname = row.get(cand)
                    break
            if _looks_like_name(gname):
                return str(gname).strip()
            if _looks_like_name(dname):
                return str(dname).strip()
            if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
                return str(gname).strip()
            if dname and not _is_placeholder_str(dname):
                return str(dname).strip()
            return None
        merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
    else:
        if not dur.empty:
            def fill_name_from_dur(row):
                gname = row.get('EmployeeName')
                if _looks_like_name(gname) and not _is_placeholder_str(gname):
                    return gname
                for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
                    if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
                        return row[cand]
                return None
            merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

    def normalize_empid(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            try:
                if '.' in s:
                    f = float(s)
                    if math.isfinite(f) and f.is_integer():
                        return str(int(f))
            except Exception:
                pass
            return s
        except Exception:
            return None

    merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

    def normalize_card(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            return s
        except Exception:
            return None

    merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

    if 'DurationSeconds' not in merged.columns or merged['DurationSeconds'].isnull().all():
        try:
            merged['DurationSeconds'] = (pd.to_datetime(merged['LastSwipe']) - pd.to_datetime(merged['FirstSwipe'])).dt.total_seconds().clip(lower=0).fillna(0)
        except Exception:
            merged['DurationSeconds'] = merged.get('DurationSeconds', 0)

    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)
    merged['BreakCount'] = merged['BreakCount'].fillna(0).astype(int)
    merged['LongBreakCount'] = merged['LongBreakCount'].fillna(0).astype(int)
    merged['TotalBreakMinutes'] = merged['TotalBreakMinutes'].fillna(0.0).astype(float)
    merged['PatternShortLongRepeat'] = merged['PatternShortLongRepeat'].fillna(False).astype(bool)

    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
        if c in merged.columns:
            def _clean_str_val(v):
                if v is None:
                    return None
                try:
                    s = str(v).strip()
                    if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                        return None
                    return s
                except Exception:
                    return None
            merged[c] = merged[c].apply(_clean_str_val)

    if 'EmployeeName' in merged.columns:
        merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

    return merged

# ---------------- SCENARIO WEIGHTS ----------------
WEIGHTS = {
    "long_gap_>=4.5h": 0.3,
    "short_duration_<4h": 1.0,
    "coffee_badging": 1.0,
    "low_swipe_count_<=2": 0.5,
    "single_door": 0.25,
    "only_in": 0.8,
    "only_out": 0.8,
    "overtime_>=10h": 0.2,
    "very_long_duration_>=16h": 1.5,
    "zero_swipes": 0.4,
    "unusually_high_swipes": 1.5,
    "repeated_short_breaks": 0.5,
    "multiple_location_same_day": 0.6,
    "weekend_activity": 0.6,
    "repeated_rejection_count": 0.8,
    "badge_sharing_suspected": 2.0,
    "early_arrival_before_06": 0.4,
    "late_exit_after_22": 0.4,
    "shift_inconsistency": 1.2,
    "trending_decline": 0.7,
    "consecutive_absent_days": 1.2,
    "high_variance_duration": 0.8,
    "short_duration_on_high_presence_days": 1.1,
    "swipe_overlap": 2.0,
    "high_swipes_benign": 0.1,
    "shortstay_longout_repeat": 2.0
}
ANOMALY_THRESHOLD = 1.5

def _read_past_trend_csvs(outdir: str, window_days: int, target_date: date):
    p = Path(outdir)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return pd.DataFrame()
    dfs = []
    cutoff = target_date - timedelta(days=window_days)
    for fp in csvs:
        try:
            df = pd.read_csv(fp, parse_dates=['Date'])
            if 'Date' in df.columns:
                try:
                    df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                except Exception:
                    pass
                # include target_date in the window (cutoff <= Date <= target_date)
                def _date_in_window(d):
                    try:
                        return d is not None and (d >= cutoff and d <= target_date)
                    except Exception:
                        return False
                df = df[df['Date'].apply(_date_in_window)]
            dfs.append(df)
        except Exception:
            try:
                df = pd.read_csv(fp, dtype=str)
                if 'Date' in df.columns:
                    try:
                        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                        # include target_date in the window (cutoff <= Date <= target_date)
                        def _date_in_window(d):
                            try:
                                return d is not None and (d >= cutoff and d <= target_date)
                            except Exception:
                                return False
                        df = df[df['Date'].apply(_date_in_window)]
                    except Exception:
                        pass
                dfs.append(df)
            except Exception:
                continue
    if not dfs:
        return pd.DataFrame()
    try:
        out = pd.concat(dfs, ignore_index=True)
        return out
    except Exception:
        return pd.DataFrame()

def _read_scenario_counts_by_person(outdir: str, window_days: int, target_date: date, scenario_col: str):
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty or scenario_col not in df.columns:
        return {}
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass
    id_cols = [c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in df.columns]
    out = defaultdict(int)
    q = df[df[scenario_col] == True] if df[scenario_col].dtype == bool else df[df[scenario_col].astype(str).str.lower() == 'true']
    for _, r in q.iterrows():
        for col in id_cols:
            try:
                raw = r.get(col)
                if raw in (None, '', float('nan')):
                    continue
                norm = _normalize_id_val(raw)
                if norm:
                    out[str(norm)] += 1
                    stripped = _strip_uid_prefix(str(norm))
                    if stripped != str(norm):
                        out[str(stripped)] += 1
            except Exception:
                continue
        for fallback in ('Int1', 'Text12'):
            if fallback in r and r.get(fallback) not in (None, '', 'nan'):
                try:
                    norm = _normalize_id_val(r.get(fallback))
                    if norm:
                        out[str(norm)] += 1
                except Exception:
                    continue
    return dict(out)

def _compute_weeks_with_threshold(past_df: pd.DataFrame,
                                  person_col: str = 'person_uid',
                                  date_col: str = 'Date',
                                  scenario_col: str = 'short_duration_<4h',
                                  threshold_days: int = 3) -> dict:
    if past_df is None or past_df.empty:
        return {}
    df = past_df.copy()
    if date_col not in df.columns:
        return {}
    try:
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.date
    except Exception:
        pass
    if scenario_col not in df.columns:
        return {}
    try:
        if df[scenario_col].dtype == bool:
            df['__scenario_flag__'] = df[scenario_col].astype(bool)
        else:
            df['__scenario_flag__'] = df[scenario_col].astype(str).str.strip().str.lower().isin({'true', '1', 'yes', 'y', 't'})
    except Exception:
        df['__scenario_flag__'] = df[scenario_col].apply(lambda v: str(v).strip().lower() in ('true','1','yes','y','t') if v is not None else False)
    df = df[df['__scenario_flag__'] == True].copy()
    if df.empty:
        return {}
    if person_col not in df.columns:
        fallback = next((c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in past_df.columns), None)
        if fallback is None:
            return {}
        person_col = fallback
    def _week_monday(d):
        try:
            if d is None or (isinstance(d, float) and np.isnan(d)):
                return None
            iso = d.isocalendar()
            return date.fromisocalendar(iso[0], iso[1], 1)
        except Exception:
            return None
    df['__week_monday__'] = df[date_col].apply(_week_monday)
    df = df.dropna(subset=['__week_monday__', person_col])
    if df.empty:
        return {}
    week_counts = (df.groupby([person_col, '__week_monday__'])
                     .size()
                     .reset_index(name='days_flagged'))
    valid_weeks = week_counts[week_counts['days_flagged'] >= int(threshold_days)].copy()
    if valid_weeks.empty:
        return {}
    person_weeks = {}
    for person, grp in valid_weeks.groupby(person_col):
        wlist = sorted(pd.to_datetime(grp['__week_monday__']).dt.date.unique(), reverse=True)
        person_weeks[str(person)] = wlist
    def _consecutive_week_count(week_dates_desc):
        if not week_dates_desc:
            return 0
        count = 1
        prev = week_dates_desc[0]
        for cur in week_dates_desc[1:]:
            try:
                if (prev - cur).days == 7:
                    count += 1
                    prev = cur
                else:
                    break
            except Exception:
                break
        return count
    out = {}
    for pid, weeks in person_weeks.items():
        c = _consecutive_week_count(weeks)
        out[str(pid)] = int(c)
        try:
            stripped = _strip_uid_prefix(str(pid))
            if stripped and stripped != str(pid):
                out[str(stripped)] = int(c)
        except Exception:
            pass
    return out

def _strip_uid_prefix(s):
    try:
        if s is None:
            return s
        st = str(s)
        for p in ('emp:', 'uid:', 'name:'):
            if st.startswith(p):
                return st[len(p):]
        return st
    except Exception:
        return s

def compute_violation_days_map(outdir: str, window_days: int, target_date: date):
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty:
        return {}
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass
    id_cols = []
    for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber'):
        if c in df.columns:
            id_cols.append(c)
    if 'IsFlagged' not in df.columns:
        if 'AnomalyScore' in df.columns:
            df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: float(s) >= ANOMALY_THRESHOLD if not pd.isna(s) else False)
        else:
            df['IsFlagged'] = False
    ident_dates = defaultdict(set)
    try:
        flagged = df[df['IsFlagged'] == True]
        for _, r in flagged.iterrows():
            d = r.get('Date')
            if d is None:
                continue
            for col in id_cols:
                try:
                    raw = r.get(col)
                    if raw is None:
                        continue
                    norm = _normalize_id_val(raw)
                    if norm:
                        ident_dates[str(norm)].add(d)
                        stripped = _strip_uid_prefix(str(norm))
                        if stripped != str(norm):
                            ident_dates[str(stripped)].add(d)
                except Exception:
                    continue
            for fallback in ('Int1', 'Text12'):
                if fallback in r and r.get(fallback) not in (None, '', 'nan'):
                    try:
                        norm = _normalize_id_val(r.get(fallback))
                        if norm:
                            ident_dates[str(norm)].add(d)
                            stripped = _strip_uid_prefix(str(norm))
                            if stripped != str(norm):
                                ident_dates[str(stripped)].add(d)
                    except Exception:
                        continue
    except Exception:
        logging.exception("Error building violation days map from history.")
    out = {k: int(len(v)) for k, v in ident_dates.items()}
    return out



def score_trends_from_durations(combined_df: pd.DataFrame, swipes_df: Optional[pd.DataFrame] = None, outdir: Optional[str] = None, target_date: Optional[date] = None) -> pd.DataFrame:
    """
    Take combined durations DataFrame and optional swipes DataFrame and compute:
      - scenario boolean columns
      - Reasons (semicolon-separated scenario keys)
      - ViolationExplanation (human text)
      - AnomalyScore (weighted sum)
      - IsFlagged (AnomalyScore >= ANOMALY_THRESHOLD)
      - ViolationDaysLast90 (from history)
      - historical bumps, weekly bump, MonitorFlag, etc.
    """
    if combined_df is None or combined_df.empty:
        return pd.DataFrame()

    df = combined_df.copy()
    # Ensure person_uid exists
    if 'person_uid' not in df.columns:
        df['person_uid'] = df.apply(lambda r: _canonical_person_uid(r), axis=1)

    # Ensure key columns exist
    for c in ['FirstSwipe','LastSwipe','CountSwipes','DurationMinutes','MaxSwipeGapSeconds','EmployeeID','CardNumber','person_uid','Date']:
        if c not in df.columns:
            df[c] = None

    # reconcile zero CountSwipes with raw swipes (if provided)
    if swipes_df is not None and not swipes_df.empty and 'person_uid' in swipes_df.columns:
        tsw = swipes_df.copy()
        # ensure LocaleMessageTime parsed
        if 'LocaleMessageTime' in tsw.columns:
            tsw['LocaleMessageTime'] = pd.to_datetime(tsw['LocaleMessageTime'], errors='coerce')
        else:
            for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                if cand in tsw.columns:
                    tsw['LocaleMessageTime'] = pd.to_datetime(tsw[cand], errors='coerce')
                    break
        if 'Date' not in tsw.columns:
            if 'LocaleMessageTime' in tsw.columns:
                tsw['Date'] = tsw['LocaleMessageTime'].dt.date
            else:
                for cand in ('date','Date'):
                    if cand in tsw.columns:
                        try:
                            tsw['Date'] = pd.to_datetime(tsw[cand], errors='coerce').dt.date
                        except Exception:
                            tsw['Date'] = None
                        break

        try:
            grp = tsw.dropna(subset=['person_uid', 'Date']).groupby(['person_uid', 'Date'])
            counts = grp.size().to_dict()
            firsts = grp['LocaleMessageTime'].min().to_dict()
            lasts = grp['LocaleMessageTime'].max().to_dict()
        except Exception:
            counts = {}
            firsts = {}
            lasts = {}

        def _fix_row_by_raw(idx, row):
            key = (row.get('person_uid'), row.get('Date'))
            if key in counts and (int(row.get('CountSwipes') or 0) == 0 or pd.isna(row.get('CountSwipes'))):
                try:
                    c = int(counts.get(key, 0))
                    df.at[idx, 'CountSwipes'] = c
                    f = firsts.get(key)
                    l = lasts.get(key)
                    if pd.notna(f) and (pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None):
                        df.at[idx, 'FirstSwipe'] = pd.to_datetime(f)
                    if pd.notna(l) and (pd.isna(row.get('LastSwipe')) or row.get('LastSwipe') is None):
                        df.at[idx, 'LastSwipe'] = pd.to_datetime(l)
                    try:
                        fs = df.at[idx, 'FirstSwipe']
                        ls = df.at[idx, 'LastSwipe']
                        if pd.notna(fs) and pd.notna(ls):
                            dursec = (pd.to_datetime(ls) - pd.to_datetime(fs)).total_seconds()
                            dursec = max(0, dursec)
                            df.at[idx, 'DurationSeconds'] = float(dursec)
                            df.at[idx, 'DurationMinutes'] = float(dursec / 60.0)
                    except Exception:
                        pass
                except Exception:
                    pass

        for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
            try:
                _fix_row_by_raw(ix, r)
            except Exception:
                logging.debug("Failed to reconcile row %s with raw swipes", ix)

    # Build badge map and swipe overlap maps for higher-severity scenarios
    badge_map = {}
    swipe_overlap_map = {}
    if swipes_df is not None and not swipes_df.empty:
        try:
            tmp = swipes_df[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
            if not tmp.empty:
                grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
                badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}
        except Exception:
            badge_map = {}

        overlap_window_seconds = 2
        if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes_df.columns):
            try:
                tmp2 = swipes_df[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
                if not tmp2.empty:
                    tmp2 = tmp2.sort_values(['Door', 'LocaleMessageTime'])
                    for (d, door), g in tmp2.groupby(['Date', 'Door']):
                        items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                        n = len(items)
                        for i in range(n):
                            t_i, uid_i = items[i]
                            j = i+1
                            while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                                uid_j = items[j][1]
                                if uid_i != uid_j:
                                    swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                                    swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                                j += 1
            except Exception:
                swipe_overlap_map = {}

    # Evaluate scenarios (use weighting to compute anomaly score)
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            df[name] = df.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
        elif name == "swipe_overlap":
            df[name] = df.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map=swipe_overlap_map), axis=1)
        else:
            df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)

    def compute_score(r):
        score = 0.0
        detected = []
        for name, _ in SCENARIOS:
            val = bool(r.get(name))
            w = WEIGHTS.get(name, 0.0)
            if val and w > 0:
                score += float(w)
                detected.append(name)
        return score, detected

    scores = df.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
    df['AnomalyScore'] = scores['AnomalyScore'].astype(float)
    df['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
    df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

    # PresentToday flag, ViolationDays from history, and weekly adjustments
    try:
        df['PresentToday'] = df['CountSwipes'].fillna(0).astype(int) > 0

        # historical scenario counts (for escalation)
        hist_pattern_counts = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today(), 'shortstay_longout_repeat')
        hist_rep_breaks = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today(), 'repeated_short_breaks')
        hist_short_duration = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today(), 'short_duration_<4h')

        def get_hist_count_for_row(row, hist_map):
            for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
                if k in row and row.get(k) not in (None, '', float('nan')):
                    try:
                        norm = _normalize_id_val(row.get(k))
                        if norm and str(norm) in hist_map:
                            return int(hist_map.get(str(norm), 0))
                        stripped = _strip_uid_prefix(str(norm)) if norm else None
                        if stripped and str(stripped) in hist_map:
                            return int(hist_map.get(str(stripped), 0))
                    except Exception:
                        continue
            return 0

        df['HistPatternShortLongCount90'] = df.apply(lambda r: get_hist_count_for_row(r, hist_pattern_counts), axis=1)
        df['HistRepeatedShortBreakCount90'] = df.apply(lambda r: get_hist_count_for_row(r, hist_rep_breaks), axis=1)
        df['HistShortDurationCount90'] = df.apply(lambda r: get_hist_count_for_row(r, hist_short_duration), axis=1)

        pat_mask = df['HistPatternShortLongCount90'].fillna(0).astype(int) >= 3
        if pat_mask.any():
            df.loc[pat_mask, 'AnomalyScore'] = df.loc[pat_mask, 'AnomalyScore'].astype(float)  # keep value but escalate risk below
            df.loc[pat_mask, 'RiskScore'] = 5
            df.loc[pat_mask, 'RiskLevel'] = 'High'
            df.loc[pat_mask, 'IsFlagged'] = True

        rep_mask = df['HistRepeatedShortBreakCount90'].fillna(0).astype(int) >= 5
        if rep_mask.any():
            df.loc[rep_mask, 'AnomalyScore'] = df.loc[rep_mask, 'AnomalyScore'].astype(float)
            df.loc[rep_mask, 'RiskScore'] = 5
            df.loc[rep_mask, 'RiskLevel'] = 'High'
            df.loc[rep_mask, 'IsFlagged'] = True

        # ViolationDaysLast90
        vmap = compute_violation_days_map(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today())
        def lookup_violation_days(row):
            try:
                candidates = []
                for k in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
                    v = row.get(k)
                    if v not in (None, '', float('nan')):
                        candidates.append(_normalize_id_val(v))
                for c in candidates:
                    if c is None:
                        continue
                    if c in vmap:
                        return int(vmap.get(c, 0))
                    stripped = _strip_uid_prefix(c)
                    if stripped != c and stripped in vmap:
                        return int(vmap.get(stripped, 0))
                return 0
            except Exception:
                return 0
        df['ViolationDaysLast90'] = df.apply(lookup_violation_days, axis=1)

        # Append monitoring note for persons who have past violations and are present today.
        def _append_monitor_note(idx, row):
            try:
                vd = int(row.get('ViolationDaysLast90') or 0)
            except Exception:
                vd = 0
            if vd <= 0:
                return row.get('ViolationExplanation') or row.get('Explanation')
            if not row.get('PresentToday', False):
                return row.get('ViolationExplanation') or row.get('Explanation')
            note = f"Note: Previously flagged {vd} time{'s' if vd!=1 else ''} in the last {VIOLATION_WINDOW_DAYS} days — monitor when present today."
            ex = row.get('ViolationExplanation') or row.get('Explanation') or ''
            if ex and not ex.strip().endswith('.'):
                ex = ex.strip() + '.'
            if note in ex:
                return ex
            return (ex + ' ' + note).strip()
        df['ViolationExplanation'] = df.apply(lambda r: _append_monitor_note(r.name, r), axis=1)

        df['MonitorFlag'] = df.apply(lambda r: (int(r.get('ViolationDaysLast90') or 0) > 0) and bool(r.get('PresentToday')), axis=1)

        # Now compute consecutive-week short-duration runs (post scoring)
        past_df = _read_past_trend_csvs(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today())
        week_runs = _compute_weeks_with_threshold(past_df, person_col='person_uid', date_col='Date', scenario_col='short_duration_<4h', threshold_days=3)

        def _get_week_run_for_row(r):
            for k in ('person_uid', 'EmployeeID'):
                if k in r and r.get(k):
                    key = str(r.get(k))
                    if key in week_runs:
                        return int(week_runs[key])
                    stripped = _strip_uid_prefix(key)
                    if stripped in week_runs:
                        return int(week_runs[stripped])
            return 0

        df['ConsecWeeksShort4hrs'] = df.apply(_get_week_run_for_row, axis=1)

        # Apply anomaly score bumps now that AnomalyScore exists
        df['AnomalyScore'] = df['AnomalyScore'].astype(float).fillna(0.0)

        mask1 = df['ConsecWeeksShort4hrs'].fillna(0).astype(int) >= 1
        mask2 = df['ConsecWeeksShort4hrs'].fillna(0).astype(int) >= 2

        if mask1.any():
            df.loc[mask1, 'AnomalyScore'] = df.loc[mask1, 'AnomalyScore'].astype(float) + 0.5
        if mask2.any():
            df.loc[mask2, 'AnomalyScore'] = df.loc[mask2, 'AnomalyScore'].astype(float) + 1.0

        # Recompute IsFlagged and RiskLevel after bumping AnomalyScore
        df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

        def _map_risk_after_bump(r):
            score = r.get('AnomalyScore') or 0.0
            bucket, label = map_score_to_label(score)
            return int(bucket), label
        rs2 = df.apply(lambda r: pd.Series(_map_risk_after_bump(r), index=['RiskScore', 'RiskLevel']), axis=1)
        df['RiskScore'] = rs2['RiskScore']
        df['RiskLevel'] = rs2['RiskLevel']

        # OVERRIDE: force High risk when ViolationDaysLast90 >= 4
        try:
            high_violation_mask = df['ViolationDaysLast90'] >= 4
            if high_violation_mask.any():
                df.loc[high_violation_mask, 'RiskScore'] = 5
                df.loc[high_violation_mask, 'RiskLevel'] = 'High'
        except Exception:
            pass

    except Exception:
        logging.exception("Failed post-scoring weekly-run / monitoring augmentation.")

    # Build textual Reasons and Explanation (if not already)
    def reasons_for_row(r):
        if not bool(r.get('IsFlagged')):
            return None, None
        ds_raw = r.get('DetectedScenarios')
        if ds_raw:
            ds = [s.strip() for s in ds_raw.split(";") if s and s.strip()]
            explanation = _explain_scenarios_detected(r, ds)
            reasons_codes = "; ".join(ds) if ds else None
            return reasons_codes, explanation
        return None, None

    reason_tuples = df.apply(lambda r: pd.Series(reasons_for_row(r), index=['Reasons', 'ViolationExplanation']), axis=1)
    def _sanitize_reason_val(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == "" or _is_placeholder_str(s):
                return None
            return s
        except Exception:
            return None

    df['Reasons'] = reason_tuples['Reasons'].apply(_sanitize_reason_val)
    df['ViolationExplanation'] = reason_tuples['ViolationExplanation'].apply(lambda v: None if _is_placeholder_str(v) else (str(v).strip() if v is not None else None))

    # If flagged but no Reasons, ensure fallback
    def _ensure_reason_for_flagged(row):
        if bool(row.get('IsFlagged')) and (row.get('Reasons') is None or row.get('Reasons') == ''):
            ds = row.get('DetectedScenarios')
            if ds and not _is_placeholder_str(ds):
                parts = [p.strip() for p in re.split(r'[;,\|]', str(ds)) if p and not _is_placeholder_str(p)]
                if parts:
                    return "; ".join(parts)
            if int(row.get('ConsecWeeksShort4hrs') or 0) >= 1:
                return "consecutive_short_weeks"
            if int(row.get('ViolationDaysLast90') or 0) > 0:
                return "historical_monitoring"
            return None
        return row.get('Reasons')

    if 'IsFlagged' in df.columns:
        df['Reasons'] = df.apply(_ensure_reason_for_flagged, axis=1)
    else:
        df['Reasons'] = df['Reasons'].apply(_sanitize_reason_val)

    if 'OverlapWith' not in df.columns:
        def overlap_with_fn(r):
            d = r.get('Date')
            uid = r.get('person_uid')
            if (d, uid) in swipe_overlap_map:
                return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
            return None
        df['OverlapWith'] = df.apply(overlap_with_fn, axis=1)

    # ensure booleans native
    for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
        if col in df.columns:
            df[col] = df[col].astype(bool)

    # return DataFrame
    return df


# ---------------- run_trend_for_date ----------------
def _slug_city(city: str) -> str:
    if not city:
        return "pune"
    return str(city).strip().lower().replace(" ", "_")


def run_trend_for_date(target_date: date,
                       regions: Optional[List[str]] = None,
                       outdir: str = None,
                       city: str = "pune",
                       as_dict: bool = False) -> pd.DataFrame:
    city_slug = _slug_city(city)
    if regions is None:
        try:
            regions = list(REGION_CONFIG.keys()) if isinstance(REGION_CONFIG, dict) and REGION_CONFIG else []
        except Exception:
            regions = []
    regions = [r.lower() for r in regions if r]
    outdir_path = Path(outdir) if outdir else OUTDIR
    if run_for_date is None:
        raise RuntimeError("duration_report.run_for_date is not available in this environment.")
    # call run_for_date defensively
    try:
        results = run_for_date(target_date, regions, str(outdir_path), city)
    except TypeError:
        try:
            results = run_for_date(target_date, regions, str(outdir_path))
        except Exception:
            try:
                results = run_for_date(target_date)
            except Exception as e:
                logging.exception("run_for_date failed entirely.")
                raise
    # combine durations and swipes across regions
    dur_list = []
    swipe_list = []
    try:
        for rkey, rr in (results or {}).items():
            try:
                dfdur = rr.get('durations')
                if dfdur is not None and not dfdur.empty:
                    dfdur = dfdur.copy()
                    dfdur['region'] = rkey
                    dur_list.append(dfdur)
            except Exception:
                pass
            try:
                dfsw = rr.get('swipes')
                if dfsw is not None and not dfsw.empty:
                    dfcopy = dfsw.copy()
                    dfcopy['region'] = rkey
                    swipe_list.append(dfcopy)
            except Exception:
                pass
    except Exception:
        logging.exception("Failed to iterate results returned by run_for_date.")
    combined = pd.concat(dur_list, ignore_index=True) if dur_list else pd.DataFrame()
    sw_combined = pd.concat(swipe_list, ignore_index=True) if swipe_list else pd.DataFrame()

    # Decide Pune 2AM boundary
    use_pune_2am_boundary = False
    try:
        if city and isinstance(city, str) and 'pun' in city.strip().lower():
            use_pune_2am_boundary = True
        else:
            if os.getenv("PUNE_2AM_BOUNDARY", "0") == "1":
                use_pune_2am_boundary = True
    except Exception:
        use_pune_2am_boundary = False


 # Prepare for features; possibly shift times for Pune 02:00 grouping
    sw_for_features = sw_combined.copy() if sw_combined is not None else pd.DataFrame()
    durations_for_features = combined.copy() if combined is not None else pd.DataFrame()

    if use_pune_2am_boundary and (sw_for_features is not None) and (not sw_for_features.empty):
        try:
            if 'LocaleMessageTime' in sw_for_features.columns:
                sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features['LocaleMessageTime'], errors='coerce')
            else:
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                    if cand in sw_for_features.columns:
                        sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features[cand], errors='coerce')
                        break
            sw_for_features['OriginalLocaleMessageTime'] = sw_for_features['LocaleMessageTime']
            sw_for_features['LocaleMessageTime'] = sw_for_features['LocaleMessageTime'] - pd.Timedelta(hours=2)
            # recompute durations if compute_daily_durations is available
            if callable(compute_daily_durations):
                try:
                    durations_for_features = compute_daily_durations(sw_for_features)
                except Exception:
                    logging.exception("compute_daily_durations failed for shifted swipes; falling back to original durations.")
                    durations_for_features = combined.copy()
            # save shifted raw optionally
            try:
                sw_shifted_out = Path(outdir_path) / f"swipes_{city_slug}_{target_date.strftime('%Y%m%d')}_shifted.csv"
                cols_keep = [c for c in sw_for_features.columns if c in ('person_uid','EmployeeID','CardNumber','LocaleMessageTime','OriginalLocaleMessageTime','Door','Direction','PartitionName2')]
                sw_for_features[cols_keep].to_csv(sw_shifted_out, index=False)
            except Exception:
                logging.debug("Could not write shifted swipes file.")
        except Exception:
            logging.exception("Failed to prepare shifted swipes for Pune 2AM logic.")
            sw_for_features = sw_combined.copy()
            durations_for_features = combined.copy()


    # compute features using possibly-shifted data (so grouping uses 02:00 boundary for Pune)
    features = compute_features(sw_for_features, durations_for_features)
    if features is None:
        features = pd.DataFrame()
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        if as_dict:
            return {'rows': 0, 'flagged_rows': 0, 'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': 0}
        return pd.DataFrame()


    # compute features
    features = compute_features(sw_for_features, durations_for_features)
    if features is None:
        features = pd.DataFrame()
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        if as_dict:
            return {'rows': 0, 'flagged_rows': 0, 'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': 0}
        return pd.DataFrame()

    # restore FirstSwipe/LastSwipe to original timeline if shifted
    if use_pune_2am_boundary:
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in features.columns:
                try:
                    features[dtcol] = pd.to_datetime(features[dtcol], errors='coerce') + pd.Timedelta(hours=2)
                except Exception:
                    pass

    # Save raw swipes for evidence
    try:
        if sw_combined is not None and not sw_combined.empty:
            sw_out = Path(outdir_path) / f"swipes_{city_slug}_{target_date.strftime('%Y%m%d')}.csv"
            sw_combined.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception:
        logging.warning("Failed to save raw swipes")

    # Recompute per-row metrics from raw swipes and merge into features
    try:
        if sw_combined is not None and not sw_combined.empty:
            if 'LocaleMessageTime' not in sw_combined.columns:
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                    if cand in sw_combined.columns:
                        sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined[cand], errors='coerce')
                        break
            else:
                sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined['LocaleMessageTime'], errors='coerce')
            if use_pune_2am_boundary:
                sw_combined['DisplayDateKey'] = (sw_combined['LocaleMessageTime'] - pd.Timedelta(hours=2)).dt.date
            else:
                sw_combined['DisplayDateKey'] = sw_combined['LocaleMessageTime'].dt.date

            def _agg_metrics(g):
                times_sorted = sorted(list(pd.to_datetime(g['LocaleMessageTime'].dropna())))
                count_swipes = len(times_sorted)
                max_gap = 0
                short_gap_count = 0
                if len(times_sorted) >= 2:
                    gaps = []
                    for i in range(1, len(times_sorted)):
                        s = (times_sorted[i] - times_sorted[i-1]).total_seconds()
                        gaps.append(s)
                        if s <= 5*60:
                            short_gap_count += 1
                    max_gap = int(max(gaps)) if gaps else 0
                first_ts = times_sorted[0] if times_sorted else None
                last_ts = times_sorted[-1] if times_sorted else None
                unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
                unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
                def _pick_non_guid(colname):
                    if colname in g.columns:
                        for v in pd.unique(g[colname].dropna().astype(str).map(lambda x: x.strip())):
                            if v and (not _GUID_RE.match(v)) and v.lower() not in _PLACEHOLDER_STRS:
                                return v
                    return None
                card = _pick_non_guid('CardNumber')
                empid = _pick_non_guid('EmployeeID') or _pick_non_guid('Int1') or _pick_non_guid('Text12')
                empname = _pick_non_guid('EmployeeName') or _pick_non_guid('ObjectName1')
                duration_sec = 0.0
                if first_ts is not None and last_ts is not None:
                    try:
                        duration_sec = float((pd.to_datetime(last_ts) - pd.to_datetime(first_ts)).total_seconds())
                        if duration_sec < 0:
                            duration_sec = 0.0
                    except Exception:
                        duration_sec = 0.0
                return pd.Series({
                    'FirstSwipe_raw': first_ts,
                    'LastSwipe_raw': last_ts,
                    'CountSwipes_raw': int(count_swipes),
                    'DurationSeconds_raw': float(duration_sec),
                    'DurationMinutes_raw': float(duration_sec/60.0),
                    'MaxSwipeGapSeconds_raw': int(max_gap),
                    'ShortGapCount_raw': int(short_gap_count),
                    'UniqueDoors_raw': int(unique_doors),
                    'UniqueLocations_raw': int(unique_locations),
                    'CardNumber_raw': card,
                    'EmployeeID_raw': empid,
                    'EmployeeName_raw': empname
                })

            grouped_raw = sw_combined.dropna(subset=['person_uid', 'DisplayDateKey'], how='any').groupby(['person_uid', 'DisplayDateKey'])
            if not grouped_raw.ngroups:
                raw_metrics_df = pd.DataFrame(columns=[
                    'person_uid','DisplayDate','FirstSwipe_raw','LastSwipe_raw','CountSwipes_raw','DurationSeconds_raw',
                    'DurationMinutes_raw','MaxSwipeGapSeconds_raw','ShortGapCount_raw','UniqueDoors_raw','UniqueLocations_raw',
                    'CardNumber_raw','EmployeeID_raw','EmployeeName_raw'
                ])
            else:
                raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
                raw_metrics_df.rename(columns={'DisplayDateKey':'DisplayDate'}, inplace=True)

            # --- robust creation of merge keys for DisplayDate ---
            # We used to assume 'DisplayDate' exists; sometimes it doesn't which caused KeyError/AttributeError.
            # Create two helper columns that are safe for joining: a normalized Timestamp and a safe string.
            try:
                if 'DisplayDate' in features.columns:
                    try:
                        features['_DisplayDate_for_merge'] = pd.to_datetime(features['DisplayDate'], errors='coerce').dt.normalize()
                    except Exception:
                        features['_DisplayDate_for_merge'] = pd.NaT
                    try:
                        features['_DisplayDate_for_merge_str'] = pd.to_datetime(features['DisplayDate'], errors='coerce').astype(str).fillna('')
                    except Exception:
                        # fallback to stringification of the original series
                        try:
                            features['_DisplayDate_for_merge_str'] = features['DisplayDate'].astype(str).fillna('')
                        except Exception:
                            features['_DisplayDate_for_merge_str'] = ''
                else:
                    features['_DisplayDate_for_merge'] = pd.NaT
                    features['_DisplayDate_for_merge_str'] = ''
            except Exception:
                logging.exception("Failed to build feature merge keys for DisplayDate; proceeding without them")
                features['_DisplayDate_for_merge'] = pd.NaT
                features['_DisplayDate_for_merge_str'] = ''

            try:
                if 'DisplayDate' in raw_metrics_df.columns:
                    raw_metrics_df['_DisplayDate_for_merge'] = pd.to_datetime(raw_metrics_df['DisplayDate'], errors='coerce').dt.normalize()
                    raw_metrics_df['_DisplayDate_for_merge_str'] = pd.to_datetime(raw_metrics_df['DisplayDate'], errors='coerce').astype(str).fillna('')
                else:
                    raw_metrics_df['_DisplayDate_for_merge'] = pd.NaT
                    raw_metrics_df['_DisplayDate_for_merge_str'] = ''
            except Exception:
                logging.exception("Failed to build raw_metrics merge keys; falling back to string keys")
                raw_metrics_df['_DisplayDate_for_merge'] = pd.NaT
                raw_metrics_df['_DisplayDate_for_merge_str'] = ''

            # Prefer the datetime normalized join if available, else fall back to string join
            merged_metrics = None
            try:
                merged_metrics = pd.merge(features, raw_metrics_df, how='left',
                                          left_on=['person_uid', '_DisplayDate_for_merge'],
                                          right_on=['person_uid', '_DisplayDate_for_merge'],
                                          suffixes=('','_rawagg'))
            except Exception:
                try:
                    merged_metrics = pd.merge(features, raw_metrics_df, how='left',
                                              left_on=['person_uid', '_DisplayDate_for_merge_str'],
                                              right_on=['person_uid', '_DisplayDate_for_merge_str'],
                                              suffixes=('','_rawagg'))
                except Exception:
                    logging.exception("Both merge attempts failed; continuing without raw-agg merge")
                    merged_metrics = features.copy()

            # coalesce raw columns back into features (if present)
            try:
                for base_col, raw_col in [
                    ('FirstSwipe','FirstSwipe_raw'),
                    ('LastSwipe','LastSwipe_raw'),
                    ('CountSwipes','CountSwipes_raw'),
                    ('DurationSeconds','DurationSeconds_raw'),
                    ('DurationMinutes','DurationMinutes_raw'),
                    ('MaxSwipeGapSeconds','MaxSwipeGapSeconds_raw'),
                    ('ShortGapCount','ShortGapCount_raw'),
                    ('UniqueDoors','UniqueDoors_raw'),
                    ('UniqueLocations','UniqueLocations_raw'),
                    ('CardNumber','CardNumber_raw'),
                    ('EmployeeID','EmployeeID_raw'),
                    ('EmployeeName','EmployeeName_raw')
                ]:
                    if raw_col in merged_metrics.columns:
                        try:
                            merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
                        except Exception:
                            # best-effort: if combine_first fails, keep original
                            pass
                feature_cols = list(features.columns)
                if all(c in merged_metrics.columns for c in feature_cols):
                    features = merged_metrics[feature_cols].copy()
                else:
                    features = merged_metrics.copy()
                for helper_col in ['_DisplayDate_for_merge', '_DisplayDate_for_merge_str']:
                    if helper_col in features.columns:
                        try:
                            features.drop(columns=[helper_col], inplace=True)
                        except Exception:
                            pass
            except Exception:
                logging.exception("Post-merge coalescing failed; leaving features as-is.")



    except Exception:
        logging.exception("Failed recomputing raw metrics (non-fatal)")









    # If we used shifted timeline restore displayed times (safety)
    if use_pune_2am_boundary:
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in features.columns:
                try:
                    features[dtcol] = pd.to_datetime(features[dtcol], errors='coerce') + pd.Timedelta(hours=2)
                except Exception:
                    pass

    # Merge features with durations (prefer features)
    try:
        merged = pd.merge(features, combined, how='left', on=['person_uid', 'Date'], suffixes=('_feat', '_dur'))
    except Exception:
        merged = features

    trend_df = score_trends_from_durations(merged, swipes_df=sw_combined, outdir=str(outdir_path), target_date=target_date)

    # write csv
    try:
        write_df = trend_df.copy()
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        write_df = write_df.where(pd.notnull(write_df), None)
        out_csv = Path(outdir_path) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception:
        logging.exception("Failed to write trend CSV")

    # Format DisplayDate
    try:
        if 'DisplayDate' in trend_df.columns:
            trend_df['DisplayDate'] = pd.to_datetime(trend_df['DisplayDate'], errors='coerce').dt.date
            trend_df['DisplayDate'] = trend_df['DisplayDate'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
    except Exception:
        pass

    if as_dict:
        # ------------------ Ensure sample/aggregated rows contain enrichment (email/image) ------------------
        try:
            # make a copy we will return
            rec_df = trend_df.copy()

            # add friendly string times for First/Last for JSON output
            for dtcol in ('FirstSwipe', 'LastSwipe'):
                if dtcol in rec_df.columns:
                    rec_df[dtcol] = pd.to_datetime(rec_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')

            # add Date ISO strings
            if 'Date' in rec_df.columns:
                try:
                    rec_df['Date'] = pd.to_datetime(rec_df['Date'], errors='coerce').dt.date
                    rec_df['Date'] = rec_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
                except Exception:
                    pass

            # Enrich with personnel info (email, imageUrl) using the helper already defined
            try:
                # use endpoint template that the frontend expects (it will call /employee/<id>/image)
                rec_df = _enrich_with_personnel_info(rec_df, image_endpoint_template="/employee/{}/image")
            except Exception:
                # non-fatal - if enrichment fails, continue without email/image
                logging.exception("Personnel enrichment failed (non-fatal).")

            # Build files list (raw swipe files written earlier)
            files_list = []
            try:
                # looks for swipes file for this city/date naming conventions saved earlier
                # collect any swipes_*.csv in OUTDIR for this run date
                globp = list(Path(outdir_path).glob("swipes_*_*.csv"))
                files_list = [p.name for p in globp]
            except Exception:
                files_list = []

            # Optionally build a raw_swipes map from sw_combined (if large, this can be trimmed later)
            raw_swipes_all = []
            try:
                if sw_combined is not None and not sw_combined.empty:
                    # ensure LocaleMessageTime parsed
                    if 'LocaleMessageTime' in sw_combined.columns:
                        sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined['LocaleMessageTime'], errors='coerce')
                    else:
                        # try common candidates
                        for cand in ('MessageUTC','MessageTime','Timestamp','timestamp','Date'):
                            if cand in sw_combined.columns:
                                sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined[cand], errors='coerce')
                                break
                    # minimal projection fields used by frontend screenshot timeline
                    proj_cols = []
                    for c in ('EmployeeName','EmployeeID','person_uid','CardNumber','Door','Direction','Zone','PartitionName2'):
                        if c in sw_combined.columns:
                            proj_cols.append(c)
                    # add a safe time/date/time-string, and compute gap per door/person grouping if possible
                    tmp = sw_combined.copy()
                    tmp['Date'] = tmp['LocaleMessageTime'].dt.date.astype(str)
                    tmp['Time'] = tmp['LocaleMessageTime'].dt.time.astype(str)
                    # sort so gap calc is consistent
                    tmp = tmp.sort_values(['person_uid','LocaleMessageTime'])
                    tmp['SwipeGapSeconds'] = tmp.groupby(['person_uid','Date'])['LocaleMessageTime'].diff().dt.total_seconds().fillna(0).astype(int)
                    tmp['SwipeGap'] = tmp['SwipeGapSeconds'].apply(lambda s: format_seconds_to_hms(s) if s is not None else "-")
                    # include Zone column if map_door_to_zone produced it earlier - otherwise attempt mapping by door/direction
                    if 'Zone' not in tmp.columns and 'Door' in tmp.columns:
                        tmp['Zone'] = tmp.apply(lambda r: map_door_to_zone(r.get('Door'), r.get('Direction')), axis=1)
                    raw_swipes_all = tmp.to_dict(orient='records')
            except Exception:
                logging.exception("Building raw_swipes list failed (non-fatal).")
                raw_swipes_all = []

            # Build reason counts and risk counts (existing logic)
            reasons_count = {}
            risk_counts = {}
            try:
                if 'Reasons' in rec_df.columns:
                    for v in rec_df['Reasons'].dropna().astype(str):
                        for part in re.split(r'[;,\|]', v):
                            key = part.strip()
                            if key:
                                reasons_count[key] = reasons_count.get(key, 0) + 1
                if 'RiskLevel' in rec_df.columns:
                    for v in rec_df['RiskLevel'].fillna('').astype(str):
                        if v:
                            risk_counts[v] = risk_counts.get(v, 0) + 1
            except Exception:
                pass

            # sample records (top 20)
            sample_records = rec_df.head(20).to_dict(orient='records') if not rec_df.empty else []

            return {
                'rows': int(len(rec_df)),
                'flagged_rows': int(rec_df['IsFlagged'].sum()) if 'IsFlagged' in rec_df.columns else 0,
                'aggregated_unique_persons': int(len(rec_df)),
                'sample': sample_records,
                'reasons_count': reasons_count,
                'risk_counts': risk_counts,
                'files': files_list,
                # convenience additions used by the frontend record endpoint for quick lookup
                'raw_swipes_all': raw_swipes_all
            }
        except Exception:
            logging.exception("Failed to build as_dict output for run_trend_for_date")
            # fallback minimal structure
            return {'rows': len(trend_df), 'flagged_rows': int(trend_df['IsFlagged'].sum() if 'IsFlagged' in trend_df.columns else 0),
                    'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': len(trend_df), 'files': []}

    return trend_df




# ---------------- helper wrappers ----------------
from datetime import datetime as _datetime
from datetime import timedelta as _timedelta
def _ensure_date_obj(d):
    if d is None:
        return None
    if isinstance(d, date):
        return d
    if isinstance(d, _datetime):
        return d.date()
    if isinstance(d, str):
        try:
            return _datetime.strptime(d, "%Y-%m-%d").date()
        except Exception:
            try:
                return _datetime.fromisoformat(d).date()
            except Exception:
                raise ValueError(f"Unsupported date string: {d}")
    raise ValueError(f"Unsupported date type: {type(d)}")

def build_monthly_training(start_date=None, end_date=None, outdir: str = None, city: str = 'Pune', as_dict: bool = False):
    od = Path(outdir) if outdir else OUTDIR
    od.mkdir(parents=True, exist_ok=True)
    if start_date is None and end_date is None:
        today = date.today()
        first = date(today.year, today.month, 1)
        last = date(today.year, today.month, calendar.monthrange(today.year, today.month)[1])
    else:
        if start_date is None:
            raise ValueError("start_date must be provided when end_date is provided")
        first = _ensure_date_obj(start_date)
        if end_date is None:
            last = date(first.year, first.month, calendar.monthrange(first.year, first.month)[1])
        else:
            last = _ensure_date_obj(end_date)
    if last < first:
        raise ValueError("end_date must be >= start_date")
    cur = first
    ran = []
    errors = {}
    total_flagged = 0
    total_rows = 0
    while cur <= last:
        try:
            logging.info("build_monthly_training: running for %s (city=%s)", cur.isoformat(), city)
            res = run_trend_for_date(cur, outdir=str(od), city=city, as_dict=as_dict)
            ran.append({'date': cur.isoformat(), 'result': res})
            if isinstance(res, dict):
                total_flagged += int(res.get('flagged_rows', 0) or 0)
                total_rows += int(res.get('rows', 0) or 0)
        except KeyboardInterrupt:
            raise
        except Exception as e:
            logging.exception("build_monthly_training: failed for %s", cur)
            errors[cur.isoformat()] = str(e)
        cur = cur + _timedelta(days=1)
    summary = {
        'start_date': first.isoformat(),
        'end_date': last.isoformat(),
        'dates_attempted': (last - first).days + 1,
        'dates_succeeded': len([r for r in ran if r.get('result') is not None]),
        'dates_failed': len(errors),
        'errors': errors,
        'total_rows': total_rows,
        'total_flagged': total_flagged
    }
    if as_dict:
        return summary
    return ran

def read_90day_cache(outdir: str = None):
    od = Path(outdir) if outdir else OUTDIR
    fp = od / "90day_cache.json"
    if not fp.exists():
        return {}
    try:
        with fp.open("r", encoding="utf8") as fh:
            return json.load(fh)
    except Exception:
        logging.exception("read_90day_cache: failed to read %s", str(fp))
        return {}

if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today, as_dict=False)
    print("Completed; rows:", len(df) if df is not None else 0)


