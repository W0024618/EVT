When i upadte this Function we got all in High risk 
Coud you distribute in Violation day wise 

Low (0)
Low Medium (0)
Medium (0)
Medium High (0)
High (7)

Khamparia, Divyanshu	328827	620059	02/12/2025, 05:30:00	9:46; 8:53; 10:07	3	long_gap_>=4.5h	Evidence
Mohanty, Udaya Kumar	328731	620049	03/12/2025, 05:30:00	8:05; 8:14; 8:04	1	long_gap_>=4.5hsingle_door	Evidence
Narang, Sachin	328464	619980	03/12/2025, 05:30:00	8:32; 8:07; 8:39	1	long_gap_>=4.5hlow_swipe_count_<=5single_door	Evidence
Panwar, Adhiraj Singh	320761	604989	03/12/2025, 05:30:00	5:29; 23:12; 0:00; 22:47	1	early_arrival_before_06late_exit_after_23overtime_>=14hvery_long_duration_>=16h	Evidence
Rathore, Pooja	322571	605060	01/12/2025, 05:30:00	8:15; 8:25; 7:51	1	long_gap_>=4.5hlow_swipe_count_<=5	Evidence
Shaikh, Rubeena Bano A. Wahed	314273	615497	01/12/2025, 05:30:00	7:59; 8:04; 8:01	1	long_gap_>=4.5hlow_swipe_count_<=5	Evidence
Vishwakarma, Anilkumar Ramlal	327576	615765	03/12/2025, 05:30:00	7:45; 19:24; 8:07	1	early_arrival_before_06overtime_>=14hvery_long_duration_>=16h	Evidence


segrigate Employee Viovation day wise


def _consolidate_trend_rows(df: pd.DataFrame, combine_dates: bool = False) -> pd.DataFrame:
    """
    Consolidate multiple rows per person (and optionally per-date) into a single row.
    If combine_dates == False: behave as before (group by person_key + Date).
    If combine_dates == True: group by person_key only and aggregate Dates, Durations and Reasons
    into single columns (semi-colon separated), while preserving the best representative row's
    fields (using your existing priority rules).
    """
    if df is None or df.empty:
        return df

    df2 = df.copy()

    # Normalize Date -> date objects
    if 'Date' in df2.columns:
        try:
            df2['Date'] = pd.to_datetime(df2['Date'], errors='coerce').dt.date
        except Exception:
            pass
    else:
        for c in ('FirstSwipe', 'LastSwipe'):
            if c in df2.columns:
                try:
                    df2['Date'] = pd.to_datetime(df2[c], errors='coerce').dt.date
                    break
                except Exception:
                    pass
        if 'Date' not in df2.columns:
            df2['Date'] = None

    # Build stable consolidation key: prefer person_uid, then EmployeeID, EmployeeIdentity, CardNumber, EmployeeName
    id_cols = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'EmployeeName']
    def _pick_key(row):
        for c in id_cols:
            try:
                if c in row and row.get(c) not in (None, '', float('nan')):
                    v = row.get(c)
                    s = str(v).strip()
                    if s:
                        return s
            except Exception:
                continue
        return None

    df2['_trend_key'] = df2.apply(_pick_key, axis=1)

    # helper to format duration (use existing format_seconds_to_hms if available)
    def _fmt_duration_from_row(r):
        try:
            # prefer explicit 'Duration' string if present
            if 'Duration' in r and r.get('Duration') not in (None, '', float('nan')):
                return str(r.get('Duration')).strip()
            if 'DurationSeconds' in r and r.get('DurationSeconds') not in (None, '', float('nan')):
                try:
                    return format_seconds_to_hms(float(r.get('DurationSeconds')))
                except Exception:
                    pass
            if 'DurationMinutes' in r and r.get('DurationMinutes') not in (None, '', float('nan')):
                try:
                    mins = float(r.get('DurationMinutes'))
                    secs = int(round(mins * 60.0))
                    return format_seconds_to_hms(secs)
                except Exception:
                    pass
            # fallback to any duration-like column
            for c in ('DurSeconds','Dur','Duration_str'):
                if c in r and r.get(c) not in (None, '', float('nan')):
                    return str(r.get(c))
        except Exception:
            pass
        return None

    # grouping keys: if combine_dates -> group by key only; else group by key+Date
    group_cols = ['_trend_key'] if combine_dates else ['_trend_key', 'Date']

    out_rows = []
    try:
        grouped = df2.groupby(group_cols, sort=False, dropna=False)
    except Exception:
        grouped = [(None, df2)]

    for gkeys, group in grouped:
        g = group.copy()  # unified group dataframe

        # If there is only one row in this group -> keep it (still normalize Date type)
        if len(g) == 1:
            out_rows.append(g.iloc[0])
            continue

        # pick a representative row using priority rules
        picked = None
        try:
            if 'IsFlagged' in g.columns:
                flagged = g[g['IsFlagged'] == True]
                if not flagged.empty:
                    if 'AnomalyScore' in flagged.columns:
                        try:
                            idx = pd.to_numeric(flagged['AnomalyScore'], errors='coerce').fillna(-1).astype(float).idxmax()
                            picked = flagged.loc[idx].copy()
                        except Exception:
                            picked = flagged.iloc[0].copy()
                    else:
                        picked = flagged.iloc[0].copy()
        except Exception:
            picked = None

        if picked is None:
            if 'CountSwipes' in g.columns:
                try:
                    idx = pd.to_numeric(g['CountSwipes'], errors='coerce').fillna(-1).astype(float).idxmax()
                    picked = g.loc[idx].copy()
                except Exception:
                    picked = None

        if picked is None:
            if 'DurationSeconds' in g.columns:
                try:
                    idx = pd.to_numeric(g['DurationSeconds'], errors='coerce').fillna(-1).astype(float).idxmax()
                    picked = g.loc[idx].copy()
                except Exception:
                    picked = None

        if picked is None:
            if 'LastSwipe' in g.columns:
                try:
                    g['__ls__'] = pd.to_datetime(g['LastSwipe'], errors='coerce')
                    idx = g['__ls__'].idxmax()
                    picked = g.loc[idx].copy()
                except Exception:
                    picked = None
                finally:
                    try:
                        if '__ls__' in g.columns:
                            g = g.drop(columns=['__ls__'])
                    except Exception:
                        pass

        if picked is None:
            picked = g.iloc[0].copy()

        # If not combining across dates: keep representative row
        if not combine_dates:
            out_rows.append(picked)
            continue

        # --- combine multiple dates into single row (combine_dates == True) ---
        try:
            # sort group (best-effort)
            g_sorted = g.copy()
            try:
                g_sorted = g_sorted.sort_values(by='Date')
            except Exception:
                pass

            dates_list = []
            durations_list = []
            reasons_set = set()
            anomaly_scores = []
            flagged_any = False
            countswipes_list = []

            # detect ViolationDaysLast* column (prefer highest numeric suffix)
            def _detect_violation_col(cols):
                cand = [c for c in cols if isinstance(c, str) and c.startswith('ViolationDaysLast')]
                if not cand:
                    return None
                def _suffix_num(name):
                    m = re.search(r'ViolationDaysLast(\d+)', name)
                    if m:
                        try:
                            return int(m.group(1))
                        except Exception:
                            return 0
                    return 0
                cand_sorted = sorted(cand, key=_suffix_num, reverse=True)
                return cand_sorted[0]

            violation_col = _detect_violation_col(g_sorted.columns)

            # collect per-row values (NO per-row summing of violation_col here)
            for _, rr in g_sorted.iterrows():
                # date string
                try:
                    dval = rr.get('Date')
                    if pd.notna(dval):
                        ds = pd.to_datetime(dval, errors='coerce')
                        if pd.notna(ds):
                            dates_list.append(ds.date().strftime("%d/%m/%Y"))
                        else:
                            dates_list.append(str(dval))
                    else:
                        dates_list.append('')
                except Exception:
                    dates_list.append(str(rr.get('Date')))

                # duration
                dstr = _fmt_duration_from_row(rr)
                durations_list.append(dstr if dstr else "")

                # reasons
                try:
                    rs = rr.get('Reasons') or rr.get('DetectedScenarios') or None
                    if rs:
                        for part in re.split(r'[;,\|]', str(rs)):
                            p = part.strip()
                            if p and p.lower() not in ('nan','none',''):
                                reasons_set.add(p)
                except Exception:
                    pass

                # anomaly score
                try:
                    if 'AnomalyScore' in rr and rr.get('AnomalyScore') not in (None, '', float('nan')):
                        anomaly_scores.append(float(rr.get('AnomalyScore')))
                except Exception:
                    pass

                # flagged
                try:
                    if 'IsFlagged' in rr and bool(rr.get('IsFlagged')):
                        flagged_any = True
                except Exception:
                    pass

                # CountSwipes
                try:
                    if 'CountSwipes' in rr and rr.get('CountSwipes') not in (None, '', float('nan')):
                        countswipes_list.append(int(float(rr.get('CountSwipes'))))
                except Exception:
                    pass

            # ---- compute ViolationDays properly: number of distinct Dates where IsFlagged == True ----
            violation_days_total = 0
            try:
                if 'IsFlagged' in g_sorted.columns and 'Date' in g_sorted.columns:
                    flagged_rows = g_sorted[g_sorted['IsFlagged'] == True]
                    if not flagged_rows.empty:
                        norm_set = set()
                        for _, fr in flagged_rows.iterrows():
                            dd = fr.get('Date')
                            if pd.isna(dd) or dd in (None, '', float('nan')):
                                continue
                            try:
                                dnorm = pd.to_datetime(dd, errors='coerce').date()
                                if pd.notna(dnorm):
                                    norm_set.add(dnorm)
                                else:
                                    norm_set.add(str(dd))
                            except Exception:
                                norm_set.add(str(dd))
                        violation_days_total = int(len(norm_set))
                elif violation_col:
                    vt = 0
                    for _, rr in g_sorted.iterrows():
                        try:
                            v = rr.get(violation_col)
                            if v not in (None, '', float('nan')):
                                vt += int(float(v))
                        except Exception:
                            pass
                    violation_days_total = int(vt)
                else:
                    violation_days_total = 0
            except Exception:
                violation_days_total = 0

            # attach aggregated fields to picked row
            pairs = []
            for d, du in zip(dates_list, durations_list):
                if d and du:
                    pairs.append(f"{d} {du}")
                elif d and not du:
                    pairs.append(f"{d}")
                elif du and not d:
                    pairs.append(f"{du}")

            picked['Dates'] = "; ".join([p for p in dates_list if p])
            picked['Duration'] = "; ".join([p for p in durations_list if p])
            picked['DurationByDate'] = "; ".join(pairs) if pairs else None

            if reasons_set:
                picked['Reasons'] = "; ".join(sorted(reasons_set))
            else:
                if 'Reasons' not in picked or picked.get('Reasons') in (None, '', float('nan')):
                    picked['Reasons'] = None

            # --- NEW: aggregate AnomalyScore & DetectedScenarios across dates (sum, union) ---
            try:
                # aggregated score = sum of per-day AnomalyScore values (float)
                summed_score = float(sum([float(x) for x in anomaly_scores]) if anomaly_scores else 0.0)

                # union of detected scenarios / reasons (we already built reasons_set earlier)
                detected_set = set()
                # also try to collect from 'DetectedScenarios' fields present in group rows
                try:
                    for _, rr2 in g_sorted.iterrows():
                        ds = rr2.get('DetectedScenarios') or rr2.get('DetectedScenario') or None
                        if ds:
                            for part in re.split(r'[;,\|]', str(ds)):
                                p = part.strip()
                                if p and p.lower() not in ('nan','none',''):
                                    detected_set.add(p)
                except Exception:
                    pass
                # merge with the reasons_set we already collected from per-row Reasons
                detected_set.update(reasons_set)

                # write combined fields onto picked row
                picked['AnomalyScore'] = float(round(summed_score, 3))
                picked['DetectedScenarios'] = "; ".join(sorted(detected_set)) if detected_set else None

                # keep IsFlagged true if any day was flagged
                picked['IsFlagged'] = bool(flagged_any) or bool(picked.get('IsFlagged'))

                # ViolationDays already computed above (violation_days_total)
                picked['ViolationDays'] = int(violation_days_total)
                try:
                    picked[f'ViolationDaysLast{int(DEFAULT_VIOLATION_DAYS)}'] = int(violation_days_total)
                except Exception:
                    pass

                # Derive RiskScore / RiskLevel from aggregated AnomalyScore (use map_score_to_label if available)
                try:
                    if 'map_score_to_label' in globals() and callable(globals().get('map_score_to_label')):
                        bucket, label = globals().get('map_score_to_label')(float(picked['AnomalyScore']))
                    else:
                        # fallback defined in this module (use existing function if present)
                        if '_map_score_to_label_fallback' in globals():
                            bucket, label = _map_score_to_label_fallback(float(picked['AnomalyScore']))
                        else:
                            # safe default mapping
                            bucket, label = (1, "Low")
                    picked['RiskScore'] = int(bucket) if bucket is not None else None
                    picked['RiskLevel'] = label
                except Exception:
                    # best-effort fallback
                    try:
                        _, label = _map_score_to_label_fallback(float(picked.get('AnomalyScore') or 0.0))
                        picked['RiskLevel'] = label
                        picked['RiskScore'] = None
                    except Exception:
                        picked['RiskLevel'] = None
                        picked['RiskScore'] = None

                # optional: slight escalation based on number of distinct violation days
                # keep existing override: if ViolationDays >= 4 -> High
                try:
                    if int(picked.get('ViolationDays') or 0) >= 4:
                        picked['RiskScore'] = 5
                        picked['RiskLevel'] = 'High'
                except Exception:
                    pass

            except Exception:
                logging.exception("Failed to aggregate anomaly & detected scenarios for key=%s", str(gkeys))

            try:
                picked['CountSwipes'] = int(max(countswipes_list)) if countswipes_list else picked.get('CountSwipes', 0)
            except Exception:
                pass

        except Exception:
            logging.exception("Failed to aggregate multi-date group for key=%s", str(gkeys))
            # fallback: use representative row as-is (picked already assigned)
        out_rows.append(picked)

    try:
        out_df = pd.DataFrame(out_rows).reset_index(drop=True)
    except Exception:
        return df

    if '_trend_key' in out_df.columns:
        try:
            out_df = out_df.drop(columns=['_trend_key'])
        except Exception:
            pass

    return out_df

