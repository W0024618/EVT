# ---- keep a single, clean _strip_person_uid_prefix implementation ----
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _strip_person_uid_prefix(token: object) -> Optional[str]:
    """
    Canonical behaviour: if token like 'emp:123' or 'uid:GUID' or 'name:xxxxx' return the suffix;
    otherwise return plain stripped string. Returns None for empty/placeholder tokens.
    """
    if token is None:
        return None
    try:
        s = str(token).strip()
        if not s:
            return None
        if ':' in s:
            prefix, rest = s.split(':', 1)
            if prefix.lower() in ('emp', 'uid', 'name'):
                rest = rest.strip()
                if rest:
                    return rest
        return s
    except Exception:
        return None

# Alias for modules that expect `_strip_uid_prefix`
def _strip_uid_prefix(token: object) -> Optional[str]:
    return _strip_person_uid_prefix(token)


# ---- clean, single normalize_apac_partition implementation ----
def normalize_apac_partition(row):
    """
    Robust / minimal mapping for APAC PartitionName2 from Door/PartitionName2 fields.
    Returns canonical PartitionName2 (or empty string if unknown).
    """
    try:
        door = str(row.get("Door") or "") or ""
        part = str(row.get("PartitionName2") or "") or ""
        d = door.upper()
        p = part.upper()

        # Strict regex-based mappings first
        if re.search(r'\bAPAC[_\-]?PI\b', d) or re.search(r'\bAPAC[_\-]?PI[_\-]', d):
            return "Taguig City"
        if re.search(r'\bAPAC[_\-]?PH\b', d) or re.search(r'\bAPAC[_\-]?PH[_\-]', d):
            return "Quezon City"
        # Kuala Lumpur robust pattern: require APAC + MY + KL/KUALA
        if re.search(r'APAC[^A-Z0-9]*MY[^A-Z0-9]*(?:KL\b|KUALA\b|KUALA[^A-Z0-9]*LUMPUR)', d):
            return "MY.Kuala Lumpur"

        # minimal canonical token map (keeps previously-known tokens)
        token_map = {
            "APAC_IN_PUN": "Pune",
            "APAC_PUN": "Pune",
            "VIS_PUN": "Pune",
            "VIS_PUN_177": "Pune",
            "PUN": "Pune",
            "APAC_IN_HYD": "IN.HYD",
            "APAC_HYD": "IN.HYD",
            "HYD": "IN.HYD",
            "IN.HYD": "IN.HYD",
            "SG": "SG.Singapore",
            "SINGAPORE": "SG.Singapore",
        }

        # Prefer explicit tokens already present in PartitionName2
        for key, canonical in token_map.items():
            if key in p:
                return canonical

        # Tokenize door and partition into alphanumeric tokens and match exact tokens
        toks = [t for t in re.split(r'[^A-Z0-9]+', d + " " + p) if t]
        for t in toks:
            if t in token_map:
                return token_map[t]

        # Substring heuristics for common cities if not matched above
        if re.search(r'\bTAGUIG\b', d) or re.search(r'\bTAGUIG\b', p):
            return "Taguig City"
        if re.search(r'\bQUEZON\b', d) or re.search(r'\bQUEZON\b', p) or re.search(r'\bMANILA\b', d) or re.search(r'\bMANILA\b', p):
            return "Quezon City"
        if re.search(r'\bPUN(E)?\b', d) or re.search(r'\bPUN(E)?\b', p):
            return "Pune"
        if re.search(r'\bHYD\b', d) or re.search(r'\bHYD\b', p):
            return "IN.HYD"
        if re.search(r'\bSINGAPORE\b', d) or re.search(r'\bSG\b', d) or re.search(r'\bSINGAPORE\b', p):
            return "SG.Singapore"
        if re.search(r'\bKUALA\b', d) or re.search(r'\bKUALA\b', p) or re.search(r'\bMY\b', d) or re.search(r'\bMY\b', p):
            return "MY.Kuala Lumpur"

        # If PartitionName2 already contains a meaningful value, return it
        if p and p.strip():
            return part

        # Unknown -> return empty string (caller may filter strictly)
        return ""
    except Exception:
        return ""









# at top of app.py - add these
from io import BytesIO
import imghdr



def _guess_image_kind(b: bytes) -> Optional[str]:
    """
    Best-effort lightweight image kind detection. Uses imghdr.what on bytes.
    Returns string like 'jpeg', 'png', 'gif', etc. or None.
    """
    try:
        if not b:
            return None
        kind = imghdr.what(None, h=b)
        if kind == 'jpeg':
            return 'jpeg'
        return kind
    except Exception:
        return None










# inside export_record_excel (place near function start), add this small helper:
def _resolve_swipe_files(base_outdir, date_obj=None, city_slug=None, include_shifted=True):
    """
    Use global _find_swipe_files if present; otherwise fallback to glob-based search
    in base_outdir. Returns list of Path objects (sorted newest-first).
    """
    try:
        # Try to use existing helper if available
        if '_find_swipe_files' in globals() and callable(globals().get('_find_swipe_files')):
            try:
                cand = _find_swipe_files(base_outdir, date_obj=date_obj, city_slug=city_slug, include_shifted=include_shifted)
                if cand:
                    return cand
            except Exception:
                # fall through to glob fallback
                logging.exception("_find_swipe_files helper failed inside export_record_excel; falling back to glob")
                pass

        p = Path(base_outdir)
        files = []
        if date_obj:
            ymd = date_obj.strftime('%Y-%m-%d')
            ymd2 = date_obj.strftime('%Y%m%d')
            cand1 = [q for q in p.glob("swipes_*_*.csv") if (ymd in q.name or ymd2 in q.name)]
            cand2 = [q for q in p.glob("swipes_*.csv") if (ymd in q.name or ymd2 in q.name)]
            files = cand1 + cand2
        else:
            files = list(p.glob("swipes_*_*.csv")) + list(p.glob("swipes_*.csv")) + list(p.glob("*swipe*.csv"))

        files = sorted(list({pp for pp in files if pp.exists()}), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
        if not include_shifted:
            files = [f for f in files if 'shift' not in f.name.lower()]
        return files
    except Exception:
        logging.exception("export_record_excel: swipe file discovery failed")
        return []






if target_date_obj:
    files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=target_date_obj, city_slug=city_slug, include_shifted=False if city_slug == 'pune' else True)
else:
    files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=city_slug)




if target_date_obj:
    files_to_scan = _resolve_swipe_files(DEFAULT_OUTDIR, date_obj=target_date_obj, city_slug=city_slug, include_shifted=(False if city_slug == 'pune' else True))
else:
    files_to_scan = _resolve_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=city_slug)






avail = _resolve_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=None, include_shifted=False)













Check below backend 
details carefully 
from app.py 

routes 



# -----------------------
# Routes
# -----------------------


@app.route('/')
def root():
    return "Trend Analysis API — Multi-city"

@app.route('/employee/<path:pid>/image', methods=['GET'])
@app.route('/api/employees/<path:pid>/image', methods=['GET'])
def serve_employee_image(pid):
    """
    Try to return image bytes for pid using employeeimage.get_person_image_bytes.
    Fallbacks:
      - try stripped prefix (emp:, uid:, name:)
      - try resolving pid -> Personnel.ObjectID / GUID using get_personnel_info()
    If none found -> 404 JSON.
    """
    try:
        b = None
        # attempt with raw pid and stripped prefix
        logging.info("serve_employee_image: requested pid=%s", pid)
        b = get_person_image_bytes(pid)
        if not b:
            # try stripping common prefixes if present
            try:
                if ':' in pid:
                    _, rest = pid.split(':', 1)
                    logging.debug("serve_employee_image: trying stripped pid=%s", rest.strip())
                    b = get_person_image_bytes(rest.strip())
            except Exception:
                pass

        # NEW: if still not found, try to resolve pid via personnel lookup (EmployeeID -> ObjectID)
        if not b:
            try:
                logging.debug("serve_employee_image: attempt personnel resolution for pid=%s", pid)
                pinfo = get_personnel_info(pid) or {}
                # prefer ObjectID, then GUID
                obj = pinfo.get("ObjectID")
                guid = pinfo.get("GUID")
                tried = []
                if obj is not None:
                    obj_s = str(obj).strip()
                    tried.append(obj_s)
                    logging.debug("serve_employee_image: trying image with resolved ObjectID=%s", obj_s)
                    b = get_person_image_bytes(obj_s)
                if not b and guid:
                    guid_s = str(guid).strip()
                    tried.append(guid_s)
                    logging.debug("serve_employee_image: trying image with resolved GUID=%s", guid_s)
                    b = get_person_image_bytes(guid_s)
                if b:
                    logging.info("serve_employee_image: resolved pid=%s -> used parent id(s)=%s to fetch image", pid, tried)
            except Exception:
                logging.exception("serve_employee_image: personnel resolution attempt failed for pid=%s", pid)

        if not b:
            logging.warning("serve_employee_image: no image found for pid=%s", pid)
            return jsonify({"error": "image not found"}), 404

        try:
            kind = _guess_image_kind(b)
            if not kind:
                kind = 'jpeg'
            mime = 'image/' + ('jpeg' if kind == 'jpg' else kind)
        except Exception:
            mime = 'image/jpeg'

        bio = BytesIO(b)
        bio.seek(0)
        response = send_file(bio, mimetype=mime, as_attachment=False)
        try:
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            response.headers['Pragma'] = 'no-cache'
            response.headers['Expires'] = '0'
        except Exception:
            pass
        return response

    except Exception as e:
        logging.exception("serve_employee_image failed for %s", pid)
        return jsonify({"error": "internal server error", "details": str(e)}), 500


@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.get_json(force=True) or {}
        else:
            try:
                params = request.form.to_dict() or {}
            except Exception:
                params = {}

    date_str = (params.get('date') or params.get('Date') or '').strip() or None
    start_str = (params.get('start') or params.get('Start') or '').strip() or None
    end_str = (params.get('end') or params.get('End') or '').strip() or None

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    regions_param = params.get('regions') or params.get('region') or ''
    if regions_param:
        regions = [r.strip().lower() for r in re.split(r'[;,|]', str(regions_param)) if r.strip()]
    else:
        try:
            regions = [k.lower() for k in list(REGION_CONFIG.keys())]
        except Exception:
            regions = ['apac']

    valid_regions = []
    for r in regions:
        if r in (REGION_CONFIG or {}):
            valid_regions.append(r)
        else:
            logging.debug("Requested region '%s' not in REGION_CONFIG - skipping", r)
    if not valid_regions:
        valid_regions = [k.lower() for k in REGION_CONFIG.keys()] if REGION_CONFIG else ['apac']
    params['_regions_to_run'] = valid_regions



    city_param = params.get('city') or params.get('site') or params.get('site_name') or None
    city_slug = _slug_city(city_param) if city_param else None
    params['_city'] = city_slug
    force = str(params.get('force', '')).lower() in ('1', 'true', 'yes')
    city_slug_safe = city_slug or 'pune'


    combined_rows = []
    files = []

    
    # ---------------------------
    # Run trend for each requested date (updated: skip if CSV exists unless force=True)
    # ---------------------------
    for d in dates:
        try:
            csv_path = DEFAULT_OUTDIR / f"trend_{city_slug_safe}_{d.strftime('%Y%m%d')}.csv"

            # If CSV already exists and caller did not request force -> reuse it (cache)
            if csv_path.exists() and not force:
                logging.info("Skipping generation for %s (city=%s) because %s exists (use force to override)", d.isoformat(), city_slug_safe, csv_path.name)
                try:
                    # attempt to read CSV produced earlier
                    try:
                        df = pd.read_csv(csv_path, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
                    except Exception:
                        # fallback if parse_dates causes trouble
                        df = pd.read_csv(csv_path, dtype=str)
                except Exception:
                    logging.exception("Failed to read existing CSV %s; will attempt regeneration", csv_path)
                    df = None
                files.append(csv_path.name)
            else:
                # Need to generate (either file missing or force=True)
                if run_trend_for_date is None:
                    raise RuntimeError("run_trend_for_date helper not available in trend_runner")

                # try calling with the richer signature first (regions, outdir, city)
                df = None
                try:
                    # attempt most-common signature
                    df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug_safe)
                except TypeError:
                    # fallback attempts for older signatures
                    try:
                        df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR), city=city_slug_safe)
                    except TypeError:
                        try:
                            df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
                        except TypeError:
                            try:
                                df = run_trend_for_date(d)
                            except Exception as e_inner:
                                logging.exception("All call attempts to run_trend_for_date failed for %s: %s", d, e_inner)
                                raise

                # If run_trend_for_date returned a DataFrame, ensure we record csv name if file created
                try:
                    # if generator wrote csv to DEFAULT_OUTDIR, append it
                    if csv_path.exists():
                        files.append(csv_path.name)
                except Exception:
                    pass

        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        # If we got here and df is None or empty -> continue without adding rows
        if df is None or (hasattr(df, 'empty') and df.empty):
            continue

        # Replace placeholder strings etc (same as before)
        try:
            df = _replace_placeholder_strings(df)
        except Exception:
            pass

        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        # still support hybrid store (existing behavior)
        try:
            if str(params.get('hybrid', '')).lower() in ('1','true','yes'):
                try:
                    per_city_dir = DEFAULT_OUTDIR / (city_slug_safe or 'unknown_city')
                    per_city_dir.mkdir(parents=True, exist_ok=True)
                    per_city_path = per_city_dir / f"trend_{city_slug_safe}_{d.strftime('%Y%m%d')}.csv"
                    # write CSV copy if df exists and not empty
                    if df is not None and not (hasattr(df, 'empty') and df.empty):
                        try:
                            df.to_csv(per_city_path, index=False)
                            logging.info("Hybrid store: wrote per-city file %s", per_city_path)
                        except Exception:
                            logging.exception("Failed writing per-city hybrid CSV %s", per_city_path)
                except Exception:
                    logging.exception("Hybrid-per-city write block failed")
        except Exception:
            pass

        combined_rows.append(df)
    

    # *** Important: combine after loop to avoid UnboundLocalError and extra repeated concat inside loop ***
    try:
        combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()
    except Exception:
        combined_df = pd.DataFrame()

    # ---------- NEW: employee filtering support ----------
    try:
        employees_tokens = _parse_employees_param(params)
    except Exception:
        employees_tokens = []

    if employees_tokens:
        try:
            before_count = int(len(combined_df))
            # if combined_df empty just fast-fail
            if combined_df is None or combined_df.empty:
                logging.info("Employee filter requested but no combined_df rows present.")
                return jsonify({"message": "No scenario met", "rows": 0}), 200

            # apply row filter
            try:
                mask = combined_df.apply(lambda r: _row_matches_tokens(r, employees_tokens), axis=1)
                combined_df = combined_df[mask].copy()
            except Exception:
                # per-row apply can fail on exotic frames, fallback to naive string contains across key cols
                logging.exception("Per-row employee filter failed; trying fallback contains filter.")
                cols = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'EmployeeName']
                mask2 = pd.Series(False, index=combined_df.index)
                for c in cols:
                    if c in combined_df.columns:
                        try:
                            mask2 = mask2 | combined_df[c].astype(str).fillna('').str.strip().isin(employees_tokens)
                        except Exception:
                            try:
                                for t in employees_tokens:
                                    mask2 = mask2 | combined_df[c].astype(str).str.contains(str(t), na=False, case=False)
                            except Exception:
                                continue
                combined_df = combined_df[mask2].copy()

            after_count = int(len(combined_df))
            logging.info("Employee filter tokens=%s applied: rows_before=%d rows_after=%d", employees_tokens, before_count, after_count)

            # If nothing matched, return friendly message for frontend
            if combined_df.empty:
                return jsonify({"message": "No scenario met", "rows": 0}), 200

        except Exception:
            logging.exception("Failed applying employee filter; continuing without employee filter.")
    # ---------- END employee filtering ----------


    # --- CONSOLIDATION PATCH APPLIED HERE ---
    try:
        # Keep raw count of aggregated rows before consolidation
        aggregated_rows_total_raw = int(len(combined_df)) if combined_df is not None else 0

        try:
            combined_agg_df = _consolidate_trend_rows(combined_df, combine_dates=True)
        except Exception:
            logging.exception("run_trend: consolidation failed; falling back to raw combined_df")
            combined_agg_df = combined_df.copy() if combined_df is not None else pd.DataFrame()

        # Compute unique persons from consolidated dataframe
        if combined_agg_df is not None and not combined_agg_df.empty:
            if 'person_uid' in combined_agg_df.columns:
                raw_unique_person_uids = int(combined_agg_df['person_uid'].dropna().astype(str).nunique())
            elif 'EmployeeID' in combined_agg_df.columns:
                raw_unique_person_uids = int(combined_agg_df['EmployeeID'].dropna().astype(str).nunique())
            else:
                raw_unique_person_uids = int(len(combined_agg_df))
        else:
            raw_unique_person_uids = 0
    except Exception:
        # conservative fallback
        aggregated_rows_total_raw = int(len(combined_df)) if combined_df is not None else 0
        raw_unique_person_uids = int(len(combined_df)) if combined_df is not None else 0
        combined_agg_df = combined_df.copy() if combined_df is not None else pd.DataFrame()
    # --- END CONSOLIDATION PATCH ---


    try:
        if not combined_agg_df.empty and 'IsFlagged' in combined_agg_df.columns:
            flagged_df = combined_agg_df[combined_agg_df['IsFlagged'] == True].copy()
        else:
            flagged_df = pd.DataFrame()
    except Exception:
        flagged_df = pd.DataFrame()

    try:
        analysis_count = int(raw_unique_person_uids)
    except Exception:
        analysis_count = int(len(combined_agg_df)) if combined_agg_df is not None else 0

    try:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = float((flagged_count / analysis_count * 100.0) if analysis_count and analysis_count > 0 else 0.0)
    except Exception:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = 0.0

    try:
        # If we have flagged rows, return ALL flagged rows (strict)
        if flagged_df is not None and not flagged_df.empty:
            sample_source = flagged_df
            # return exactly flagged_count rows (no hidden head(10) truncation)
            samples = _clean_sample_df(sample_source, max_rows=int(len(flagged_df)))
        else:
            # new behaviour: prefer sample from consolidated aggregated dataframe
            sample_source = combined_agg_df
            samples = _clean_sample_df(sample_source.head(10), max_rows=10) if sample_source is not None and not sample_source.empty else []
    except Exception:
        samples = []

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "aggregated_rows_total_raw": aggregated_rows_total_raw,
        "aggregated_unique_persons": int(raw_unique_person_uids),
        "rows": int(raw_unique_person_uids),
        "flagged_rows": int(flagged_count),
        "flagged_rate_percent": float(flagged_rate_pct),
        "files": files,
         "sample": (samples if isinstance(samples, list) else samples),
   
        "reasons_count": {},
        "risk_counts": {},
      
         "flagged_persons": (samples if samples else []),
        "_raw_unique_person_uids": int(raw_unique_person_uids),
        "regions_run": params.get('_regions_to_run', []),
        "city_used": city_slug
    }

    return jsonify(resp)



@app.route('/latest', methods=['GET'])
def latest_results():
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]

    start_date_iso = None
    end_date_iso = None
    try:
        m = re.search(r'(\d{8})', latest.name)
        if m:
            ymd = m.group(1)
            dt = datetime.strptime(ymd, "%Y%m%d").date()
            start_date_iso = dt.isoformat()
            end_date_iso = dt.isoformat()
    except Exception:
        start_date_iso = None
        end_date_iso = None

    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)

    df = _replace_placeholder_strings(df)

    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in df.columns), None)

    def _norm_val_for_latest(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                fv = float(s)
                if math.isfinite(fv) and fv.is_integer():
                    s = str(int(fv))
        except Exception:
            pass
        return s

    if id_col is None:
        unique_persons = int(len(df))
    else:
        ids_series = df[id_col].apply(_norm_val_for_latest) if id_col in df.columns else pd.Series([None]*len(df))
        if id_col != 'person_uid' and 'person_uid' in df.columns:
            ids_series = ids_series.fillna(df['person_uid'].astype(str).replace('nan','').replace('None',''))
        unique_persons = int(len(set([x for x in ids_series.unique() if x])))

    # build initial sample (list of dicts)
    sample = _clean_sample_df(df, max_rows=5)  # returns list



    resp = {
        
        "file": latest.name,
        "rows_raw": int(len(df)),
        "rows": unique_persons,
        "sample": sample,
        "start_date": start_date_iso,
        "end_date": end_date_iso,
        "city": city_slug
    }
    return jsonify(resp)


@app.route('/record', methods=['GET'])
def record():
    try:
        # --- BEGIN existing record() logic ---
        from pathlib import Path
        import pandas as pd
        import math
        import re
        from datetime import datetime, date
        try:
            q = request.args.get('employee_id') or request.args.get('person_uid')
        except Exception:
            q = None
        include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
        city_param = request.args.get('city') or request.args.get('site') or 'pune'

        # pick outdir consistently
        try:
            base_out = Path(DEFAULT_OUTDIR)
        except Exception:
            try:
                base_out = Path(OUTDIR)
            except Exception:
                base_out = Path.cwd()


            
        # helper safe wrappers (use existing ones if present)
        def _safe_read(fp, **kwargs):
            try:
                if '_safe_read_csv' in globals():
                    return _safe_read_csv(fp)
                return pd.read_csv(fp, **kwargs)
            except Exception:
                try:
                    return pd.read_csv(fp, dtype=str, **{k: v for k, v in kwargs.items() if k != 'parse_dates'})
                except Exception:
                    return pd.DataFrame()

        def _to_python_scalar(v):
            if pd.isna(v):
                return None
            try:
                return v.item() if hasattr(v, 'item') else v
            except Exception:
                return v

        # 1) find trend CSVs (city-specific first)
        def _slug(s):
            return re.sub(r'[^a-z0-9]+', '_', str(s or '').strip().lower()).strip('_')

        city_slug = _slug(city_param)
        trend_glob = list(base_out.glob(f"trend_{city_slug}_*.csv"))
        if not trend_glob:
            trend_glob = list(base_out.glob("trend_*.csv"))
        trend_glob = sorted(trend_glob, reverse=True)

        df_list = []
        for fp in trend_glob:
            try:
                tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
            except Exception:
                try:
                    tmp = pd.read_csv(fp, dtype=str)
                    if 'Date' in tmp.columns:
                        tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                except Exception:
                    continue
            df_list.append(tmp)
        if df_list:
            trends_df = pd.concat(df_list, ignore_index=True)
            try:
                trends_df = _replace_placeholder_strings(trends_df)
            except Exception:
                pass
        else:
            trends_df = pd.DataFrame()

            


        # ---------- ADDED: enrichment helper ----------
        def _enrich_with_contact_info(rows):
            """Given list-of-dict rows, best-effort populate EmployeeEmail / Email if missing."""
            try:
                if not rows:
                    return rows
                out = []
                for r in rows:
                    # do not mutate original in-place (defensive)
                    rr = dict(r)
                    if not rr.get('EmployeeEmail') and not rr.get('Email'):
                        candidate = rr.get('EmployeeID') or rr.get('person_uid') or rr.get('ObjectID') or rr.get('GUID')
                        try:
                            if candidate:
                                pinfo = {}
                                try:
                                    pinfo = get_personnel_info(candidate) or {}
                                except Exception:
                                    pinfo = {}
                                # populate from keys commonly provided by get_personnel_info
                                for key in ('EmployeeEmail','EmailAddress','Email','WorkEmail'):
                                    if not rr.get('EmployeeEmail') and pinfo.get(key):
                                        rr['EmployeeEmail'] = pinfo.get(key)
                                        rr['Email'] = pinfo.get(key)
                                        break
                                # fallback: ManagerEmail if nothing else
                                if (not rr.get('EmployeeEmail')) and pinfo.get('ManagerEmail'):
                                    rr['ManagerEmail'] = pinfo.get('ManagerEmail')
                            # else no candidate => nothing we can do
                        except Exception:
                            pass
                    out.append(rr)
                return out
            except Exception:
                return rows
        # ---------- END enrichment helper ----------


        # if no query param, return a small sample of trend rows (if any)
        if q is None:
            try:
                if not trends_df.empty and '_clean_sample_df' in globals():
                    cleaned = _clean_sample_df(trends_df, max_rows=10)
                elif not trends_df.empty:
                    cleaned = trends_df.head(10).to_dict(orient='records')
                else:
                    cleaned = []
            except Exception:
                cleaned = []
            # ENRICH CONTACT INFO
            try:
                cleaned = _enrich_with_contact_info(cleaned)
            except Exception:
                pass
            return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

        q_str = str(q).strip()

        # helper to normalise series values to comparable strings/numerics
        def normalize_series(s):
            if s is None:
                return pd.Series([''] * (len(trends_df) if not trends_df.empty else 0))
            s = s.fillna('').astype(str).str.strip()
            def _norm_val(v):
                if not v:
                    return ''
                try:
                    if '.' in v:
                        fv = float(v)
                        if math.isfinite(fv) and fv.is_integer():
                            return str(int(fv))
                except Exception:
                    pass
                return v
            return s.map(_norm_val)





        # find matching rows in trends_df
        found_mask = pd.Series(False, index=trends_df.index) if not trends_df.empty else pd.Series(dtype=bool)
        candidates_cols = ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12', 'EmployeeName')
        for c in candidates_cols:
            if c in trends_df.columns:
                try:
                    ser = normalize_series(trends_df[c])
                    found_mask = found_mask | (ser == q_str)
                except Exception:
                    pass

        # numeric fallback
        if (found_mask is None) or (not found_mask.any() if len(found_mask) else True):
            try:
                q_numeric = float(q_str)
                for c in ('EmployeeID', 'Int1'):
                    if c in trends_df.columns:
                        try:
                            numser = pd.to_numeric(trends_df[c], errors='coerce')
                            found_mask = found_mask | (numser == q_numeric)
                        except Exception:
                            pass
            except Exception:
                pass

        matched = trends_df[found_mask].copy() if not trends_df.empty else pd.DataFrame()
        if matched.empty:
            cleaned_matched = []
        else:
            try:
                cleaned_matched = _clean_sample_df(matched, max_rows=len(matched)) if '_clean_sample_df' in globals() else matched.to_dict(orient='records')
            except Exception:
                cleaned_matched = matched.to_dict(orient='records')

        # ENRICH CONTACT INFO for matched aggregated rows
        try:
            cleaned_matched = _enrich_with_contact_info(cleaned_matched)
        except Exception:
            pass


        # build list of dates to scan for swipe files (from matched rows)
        dates_to_scan = set()
        try:
            for _, r in (matched.iterrows() if not matched.empty else []):
                try:
                    if 'Date' in r and pd.notna(r['Date']):
                        try:
                            d = pd.to_datetime(r['Date']).date()
                            dates_to_scan.add(d)
                        except Exception:
                            pass
                    for col in ('FirstSwipe','LastSwipe'):
                        if col in r and pd.notna(r[col]):
                            try:
                                d = pd.to_datetime(r[col]).date()
                                dates_to_scan.add(d)
                            except Exception:
                                pass
                except Exception:
                    continue
        except Exception:
            pass
        if not dates_to_scan:
            dates_to_scan = {None}  # indicates scan all swipes files

        # ---------- helper: find swipe files for a date (robust) ----------
        def _find_swipes_for_date(date_obj=None):
            try:
                include_shifted = True
                try:
                    if city_slug and str(city_slug).strip().lower() == 'pune':
                        include_shifted = False
                except Exception:
                    include_shifted = True

                if '_find_swipe_files' in globals() and callable(globals().get('_find_swipe_files')):
                    try:
                        cand = _find_swipe_files(str(base_out), date_obj=date_obj, city_slug=city_slug if city_slug else None, include_shifted=include_shifted)
                        if cand:
                            return cand
                    except Exception:
                        logging.exception("_find_swipe_files helper failed; falling back to glob search.")

                files = []
                if date_obj is None:
                    files = list(base_out.glob("swipes_*_*.csv")) + list(base_out.glob("swipes_*.csv")) + list(base_out.glob("*swipe*.csv"))
                else:
                    ymd = date_obj.strftime('%Y-%m-%d')
                    ymd2 = date_obj.strftime('%Y%m%d')
                    cand1 = [p for p in base_out.glob("swipes_*_*.csv") if (ymd in p.name or ymd2 in p.name)]
                    cand2 = [p for p in base_out.glob("swipes_*.csv") if (ymd in p.name or ymd2 in p.name)]
                    files = cand1 + cand2
                files = sorted(list({p for p in files if p.exists()}), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
                if not include_shifted:
                    files = [f for f in files if 'shift' not in f.name.lower()]
                return files
            except Exception:
                logging.exception("Error while searching for swipe files for date=%s city=%s", date_obj, city_slug)
                return []


        # ---------- scan swipe files for the target person (dates_to_scan computed earlier) ----------
        raw_files_set = set()
        raw_swipes_out = []
        seen_keys = set()

        def _append_row_for_evidence(out_row, source_name):
            # avoid exact duplicate rows from same file
            key = (
                str(out_row.get('LocaleMessageTime') or ''),
                str(out_row.get('DateOnly') or ''),
                str(out_row.get('Swipe_Time') or ''),
                str(out_row.get('Door') or '').strip(),
                str(out_row.get('Direction') or '').strip(),
                str(out_row.get('CardNumber') or '').strip()
            )
            if key in seen_keys:
                return False
            seen_keys.add(key)
            out_row['_source'] = source_name
            raw_swipes_out.append(out_row)
            return True

        # helper to format datetime to requested display formats
        def _format_time_fields(ts):
            # ts is a pandas Timestamp or datetime or None
            if ts is None or (isinstance(ts, float) and math.isnan(ts)):
                return (None, None, None)
            try:
                dt = pd.to_datetime(ts)
            except Exception:
                return (None, None, None)
            try:
                locale_iso = dt.isoformat()
            except Exception:
                locale_iso = str(dt)
            try:
                date_only = dt.strftime("%d-%b-%y")  # e.g. 17-Nov-25
            except Exception:
                try:
                    date_only = dt.date().isoformat()
                except Exception:
                    date_only = None
            try:
                # 12-hour time with AM/PM, strip leading zero
                swipe_time = dt.strftime("%I:%M:%S %p").lstrip("0")
            except Exception:
                swipe_time = None
            return (locale_iso, date_only, swipe_time)

        # loop over each date we want to scan (these are violation dates if matched rows existed)
        for d in dates_to_scan:
            swipe_candidates = _find_swipes_for_date(d)
            if d is not None and not swipe_candidates:
                # fallback to scanning all swipe files if none found for the exact date pattern
                swipe_candidates = _find_swipes_for_date(None)

            for fp in swipe_candidates:
                try:
                    sdf = _safe_read(fp, parse_dates=['LocaleMessageTime'])
                except Exception:
                    try:
                        sdf = _safe_read(fp)
                    except Exception:
                        continue
                if sdf is None or sdf.empty:
                    continue

                # minimal column-normalization for detection (case-insensitive)
                cols_lower = {c.lower(): c for c in sdf.columns}
                tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or \
                       cols_lower.get('timestamp') or cols_lower.get('time') or None
                emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or \
                          cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or \
                           cols_lower.get('employee_name') or None
                card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or \
                           cols_lower.get('chuid') or cols_lower.get('value') or None
                door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
                dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
                admit_cols = [c for c in ('admitcode','admit','admit_code','admit_type','admitstatus') if c in cols_lower]
                admit_col = cols_lower.get(admit_cols[0]) if admit_cols else None
                personnel_col = cols_lower.get('personneltype') or cols_lower.get('personneltypename') or None
                location_col = cols_lower.get('partitionname2') or cols_lower.get('location') or cols_lower.get('partitionname') or None
                person_uid_col = cols_lower.get('person_uid')

                # build boolean mask for rows that match the query identifier q_str
                try:
                    mask = pd.Series(False, index=sdf.index)
                except Exception:
                    mask = pd.Series([False] * len(sdf))

                try:
                    if person_uid_col and person_uid_col in sdf.columns:
                        mask = mask | (sdf[person_uid_col].astype(str).str.strip() == q_str)
                except Exception:
                    pass
                try:
                    if emp_col and emp_col in sdf.columns:
                        mask = mask | (sdf[emp_col].astype(str).str.strip() == q_str)
                except Exception:
                    pass
                # numeric fallback for employee id
                if (not mask.any()) and emp_col and emp_col in sdf.columns:
                    try:
                        q_numeric = float(q_str)
                        emp_numeric = pd.to_numeric(sdf[emp_col], errors='coerce')
                        mask = mask | (emp_numeric == q_numeric)
                    except Exception:
                        pass
                # name fallback
                if (not mask.any()) and name_col and name_col in sdf.columns:
                    try:
                        mask = mask | (sdf[name_col].astype(str).str.strip().str.lower() == q_str.lower())
                    except Exception:
                        pass

                if not mask.any():
                    # no matching rows in this file -> skip adding this file
                    continue

                filtered = sdf[mask].copy()
                if filtered.empty:
                    continue

                # parse/normalize timestamp column if available
                if tcol and tcol in filtered.columns:
                    try:
                        filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                    except Exception:
                        pass
                else:
                    # attempt common fallback column names to produce a timestamp
                    for cand in ('MessageUTC', 'MessageTime', 'Timestamp', 'timestamp', 'Date'):
                        if cand in filtered.columns:
                            try:
                                filtered['LocaleMessageTime'] = pd.to_datetime(filtered[cand], errors='coerce')
                                tcol = 'LocaleMessageTime'
                                break
                            except Exception:
                                pass

                # sort by timestamp for consistent timeline order
                if tcol and tcol in filtered.columns:
                    try:
                        filtered = filtered.sort_values(by=tcol)
                    except Exception:
                        pass

                    # --- compute swipe gaps (preserve previous logic) ---
                    try:
                        filtered['_prev_ts'] = filtered[tcol].shift(1)
                        filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                        # reset gap at day boundary or when previous is NaT
                        try:
                            cur_dates = filtered[tcol].dt.date
                            prev_dates = cur_dates.shift(1)
                            day_boundary_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
                            filtered.loc[day_boundary_mask, '_swipe_gap_seconds'] = 0.0
                        except Exception:
                            pass
                    except Exception:
                        filtered['_swipe_gap_seconds'] = 0.0
                else:
                    # no timestamp column -> defaults
                    filtered['_swipe_gap_seconds'] = 0.0

                # For each matching swipe row, build the slim evidence record expected by frontend
                added_any = False
                for _, r in filtered.iterrows():
                    # timestamp conversions
                    ts_val = None
                    if tcol and tcol in filtered.columns:
                        ts_val = r.get(tcol)
                    else:
                        # fallback: try Date column
                        if 'Date' in filtered.columns:
                            ts_val = r.get('Date')
                    locale_iso, date_only, swipe_time = _format_time_fields(ts_val)

                    # EmployeeID: prefer emp_col, then Int1/Text12, then fallback to matched trends row
                    emp_val = None
                    try:
                        if emp_col and emp_col in filtered.columns:
                            emp_val = _to_python_scalar(r.get(emp_col))
                        else:
                            for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                                cl = cols_lower.get(cand.lower())
                                if cl and cl in filtered.columns:
                                    emp_val = _to_python_scalar(r.get(cl))
                                    if emp_val:
                                        break
                            if emp_val in (None, '', 'nan'):
                                emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
                    except Exception:
                        emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)

                    # ObjectName1 / EmployeeName (human name)
                    obj_name = None
                    try:
                        if name_col and name_col in filtered.columns:
                            obj_name = _to_python_scalar(r.get(name_col))
                        elif 'ObjectName1' in filtered.columns:
                            obj_name = _to_python_scalar(r.get('ObjectName1'))
                        elif 'EmployeeName' in filtered.columns:
                            obj_name = _to_python_scalar(r.get('EmployeeName'))
                        else:
                            obj_name = _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else None)
                    except Exception:
                        obj_name = _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else None)

                    # PersonnelType
                    personnel_val = _to_python_scalar(r.get(personnel_col)) if (personnel_col and personnel_col in filtered.columns) else None
                    # Location / Partition
                    location_val = _to_python_scalar(r.get(location_col)) if (location_col and location_col in filtered.columns) else _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None

                    # CardNumber
                    card_val = None
                    try:
                        if card_col and card_col in filtered.columns:
                            card_val = _to_python_scalar(r.get(card_col))
                        else:
                            for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                                cl = cols_lower.get(cand.lower())
                                if cl and cl in filtered.columns:
                                    card_val = _to_python_scalar(r.get(cl))
                                    if card_val not in (None, '', 'nan'):
                                        break
                            if card_val in (None, '', 'nan'):
                                card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                    except Exception:
                        card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                    if card_val is not None:
                        try:
                            card_val = str(card_val).strip()
                        except Exception:
                            pass

                    # AdmitCode / Note
                    admit_val = _to_python_scalar(r.get(admit_col)) if (admit_col and admit_col in filtered.columns) else None
                    # some logs store admit/rejection text in 'Note' or 'Rejection_Type'
                    if not admit_val:
                        for cand in ('Admit','AdmitCode','Admit_Type','Rejection_Type','Note','NoteType','Source'):
                            cl = cols_lower.get(cand.lower())
                            if cl and cl in filtered.columns:
                                admit_val = _to_python_scalar(r.get(cl))
                                if admit_val:
                                    break

                    # Direction & Door
                    direction_val = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in filtered.columns else None
                    door_val = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else _to_python_scalar(r.get('Door')) if 'Door' in filtered.columns else None

                    # Zone: prefer precomputed _zone, else map using map_door_to_zone if available
                    zone_val = None
                    try:
                        if '_zone' in r and r.get('_zone') not in (None, '', 'nan'):
                            zone_val = _to_python_scalar(r.get('_zone'))
                        else:
                            if 'map_door_to_zone' in globals():
                                try:
                                    zone_val = map_door_to_zone(door_val, direction_val)
                                except Exception:
                                    zone_val = None
                    except Exception:
                        zone_val = None

                    # Swipe gap
                    try:
                        swipe_gap_seconds = float(r.get('_swipe_gap_seconds') or 0.0)
                    except Exception:
                        swipe_gap_seconds = 0.0
                    swipe_gap_str = format_seconds_to_hms(swipe_gap_seconds)

                    # build output row: include EmployeeName (frontend expects this), plus legacy keys
                    row_out = {
                        "EmployeeName": obj_name,
                        "ObjectName1": obj_name,
                        "EmployeeID": emp_val,
                        "CardNumber": card_val,
                        "Card": card_val,
                        "LocaleMessageTime": locale_iso,
                        "DateOnly": date_only,
                        "Date": date_only,
                        "Time": swipe_time,
                        "Swipe_Time": swipe_time,
                        "SwipeGapSeconds": swipe_gap_seconds,
                        "SwipeGap": swipe_gap_str,
                        "Door": door_val,
                        "Direction": direction_val,
                        "Zone": zone_val,
                        "Note": admit_val,
                        "PersonnelType": personnel_val,
                        "Location": location_val,
                        "PartitionName2": _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None,
                        "_source_file": fp.name
                    }

                    # Attempt to attach an email for the evidence row too
                    try:
                        if not row_out.get('EmployeeEmail') and row_out.get('EmployeeID'):
                            pinfo = {}
                            try:
                                pinfo = get_personnel_info(row_out.get('EmployeeID')) or {}
                            except Exception:
                                pinfo = {}
                            if pinfo:
                                if pinfo.get('EmployeeEmail') and not row_out.get('EmployeeEmail'):
                                    row_out['EmployeeEmail'] = pinfo.get('EmployeeEmail')
                                elif pinfo.get('EmailAddress') and not row_out.get('EmployeeEmail'):
                                    row_out['EmployeeEmail'] = pinfo.get('EmailAddress')
                                elif pinfo.get('Email') and not row_out.get('EmployeeEmail'):
                                    row_out['EmployeeEmail'] = pinfo.get('Email')
                    except Exception:
                        pass

                    added = _append_row_for_evidence(row_out, fp.name)
                    if added:
                        added_any = True

                # only add file name to available evidence list if we actually added rows from it
                if added_any:
                    raw_files_set.add(fp.name)

        raw_swipe_files = sorted(list(raw_files_set))

        # --- ENRICH: attach meta and image_url for frontend convenience ---
        meta = {}
        image_url = None
        try:
            # pick a candidate identifier for personnel lookup:
            # prefer explicit query token q_str, else fallback to first matched aggregated row
            candidate = None
            if q_str:
                candidate = q_str
            elif cleaned_matched and len(cleaned_matched) > 0:
                first = cleaned_matched[0]
                candidate = first.get('EmployeeID') or first.get('person_uid') or first.get('ObjectID') or first.get('GUID') or None

            if candidate:
                try:
                    pinfo = {}
                    if 'get_personnel_info' in globals() and callable(globals().get('get_personnel_info')):
                        pinfo = get_personnel_info(candidate) or {}
                    # populate meta keys commonly used by frontend
                    if pinfo:
                        meta['email'] = pinfo.get('EmployeeEmail') or pinfo.get('EmailAddress') or pinfo.get('Email') or None
                        meta['objectid'] = pinfo.get('ObjectID') or None
                        meta['guid'] = pinfo.get('GUID') or None
                    # convenient image_url: prefer ObjectID -> GUID -> candidate id fallback
                    if meta.get('objectid'):
                        image_url = f"/employee/{meta['objectid']}/image"
                    elif meta.get('guid'):
                        image_url = f"/employee/{meta['guid']}/image"
                    else:
                        # avoid setting GUID-like identifiers as EmployeeID image path
                        cand_s = str(candidate).strip()
                        if cand_s:
                            image_url = f"/employee/{cand_s}/image"
                except Exception:
                    # swallow personnel lookup errors (frontend will still try fallback)
                    logging.debug("record: personnel enrichment failed for candidate=%s", candidate)
        except Exception:
            pass

        # Prepare response with enrichment fields
        resp_payload = {
            "aggregated_rows": cleaned_matched,
            "raw_swipe_files": raw_swipe_files,
            "raw_swipes": raw_swipes_out,
            "meta": meta or None,
            "image_url": image_url
        }
        return jsonify(resp_payload), 200


    except Exception as e:
        # Close the try-block above with a proper except handler to avoid SyntaxError
        logging.exception("Unhandled exception in /record endpoint")
        return jsonify({"error": "internal server error in /record", "details": str(e)}), 500


@app.route('/record/export', methods=['GET'])
def export_record_excel():
    q = request.args.get('employee_id') or request.args.get('person_uid')
    date_str = request.args.get('date')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    if not q:
        return jsonify({"error":"employee_id or person_uid is required"}), 400

    q_str = str(q).strip()

    # Helper: load trend CSV(s) and build set of flagged (id, date) tuples
    def _load_flagged_map(target_date=None):
        flagged_pairs = set()
        try:
            p = Path(DEFAULT_OUTDIR)
            candidates = []
            if target_date:
                # try city/date specific trend file first
                ymd = target_date.strftime('%Y%m%d')
                f = p / f"trend_{city_slug}_{ymd}.csv"
                if f.exists():
                    candidates = [f]
            if not candidates:
                # fallback to any trend_*.csv in outputs
                candidates = sorted(p.glob("trend_*.csv"), reverse=True)
            for fp in candidates:
                try:
                    tdf = pd.read_csv(fp)
                except Exception:
                    try:
                        tdf = pd.read_csv(fp, dtype=str)
                    except Exception:
                        continue
                if tdf is None or tdf.empty:
                    continue
                tdf = _replace_placeholder_strings(tdf)
                # ensure Date is normalized to iso date string
                if 'Date' in tdf.columns:
                    try:
                        tdf['Date'] = pd.to_datetime(tdf['Date'], errors='coerce').dt.date
                        tdf['DateISO'] = tdf['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
                    except Exception:
                        tdf['DateISO'] = tdf['Date'].astype(str)
                else:
                    tdf['DateISO'] = None
                # find flagged rows (IsFlagged True or AnomalyScore >= threshold)
                if 'IsFlagged' in tdf.columns:
                    sel = tdf[tdf['IsFlagged'] == True]
                else:
                    # fallback: consider AnomalyScore >= ANOMALY_THRESHOLD as flagged
                    if 'AnomalyScore' in tdf.columns:
                        try:
                            sel = tdf[pd.to_numeric(tdf['AnomalyScore'], errors='coerce').fillna(0) >= ANOMALY_THRESHOLD]
                        except Exception:
                            sel = pd.DataFrame()
                    else:
                        sel = pd.DataFrame()

                if sel is None or sel.empty:
                    continue

                for _, rr in sel.iterrows():
                    date_iso = None
                    try:
                        date_iso = rr.get('DateISO') or (pd.to_datetime(rr.get('Date'), errors='coerce').date().isoformat() if rr.get('Date') is not None else None)
                    except Exception:
                        date_iso = None
                    # collect EmployeeID and person_uid if present
                    for idcol in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'Int1', 'Text12', 'CardNumber'):
                        try:
                            if idcol in rr and rr.get(idcol) not in (None, '', float('nan')):
                                val = str(rr.get(idcol)).strip()
                                if val:
                                    flagged_pairs.add((idcol, val, date_iso))
                                    # also add date-agnostic tuple for easy contains checks
                                    flagged_pairs.add((idcol, val, None))
                        except Exception:
                            continue
            return flagged_pairs
        except Exception:
            return set()

    # parse date param for targeted checking (optional)
    target_date_obj = None
    if date_str:
        try:
            target_date_obj = pd.to_datetime(date_str).date()
        except Exception:
            return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400

    flagged_map = _load_flagged_map(target_date_obj)

    # Quick check: ensure the requested employee/q is flagged for the requested date (or flagged at all)
    def _is_q_flagged(qtoken, date_iso=None):
        if not qtoken:
            return False
        # check across multiple id columns recorded in trend files
        for idcol in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'Int1', 'Text12', 'CardNumber'):
            if (idcol, qtoken, date_iso) in flagged_map or (idcol, qtoken, None) in flagged_map:
                return True
        return False

    # if date provided require flagged on that date; otherwise accept flagged any-date
    q_flagged = _is_q_flagged(q_str, target_date_obj.isoformat() if target_date_obj else None)
    if not q_flagged:
        # if not flagged with exact id, try numeric-normalized attempts (strip trailing .0 etc)
        try:
            if '.' in q_str:
                fq = None
                try:
                    f = float(q_str)
                    if math.isfinite(f) and f.is_integer():
                        fq = str(int(f))
                except Exception:
                    fq = None
                if fq and _is_q_flagged(fq, target_date_obj.isoformat() if target_date_obj else None):
                    q_flagged = True
        except Exception:
            pass

    if not q_flagged:
        return jsonify({"error": "employee not flagged (no evidence rows for requested employee/date)"}), 404

    # find swipe files to scan
    p = Path(DEFAULT_OUTDIR)
    files_to_scan = []
    if target_date_obj:
        files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=target_date_obj, city_slug=city_slug, include_shifted=False if city_slug == 'pune' else True)
    else:
        files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=city_slug)
    if not files_to_scan:
        avail = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=None, include_shifted=False)
        avail_names = [f.name for f in avail] if avail else []
        logging.info("export_record_excel: no files matched for date=%s city=%s; available swipe files=%s", date_str, city_slug, avail_names)
        return jsonify({"error": "no raw swipe files found for requested date / outputs", "available_swipe_files": avail_names}), 404

    all_rows = []
    for fp in files_to_scan:
        try:
            raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
        except Exception:
            try:
                raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

        raw_df = _replace_placeholder_strings(raw_df)
        cols_lower = {c.lower(): c for c in raw_df.columns}
        tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
        emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
        name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
        card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
        door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
        dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
        admit_cols = [c for c in ('admitcode','admit','admit_code','admit_type','admitstatus') if c in cols_lower]
        admit_col = cols_lower.get(admit_cols[0]) if admit_cols else None
        personnel_col = cols_lower.get('personneltype') or cols_lower.get('personneltypename') or None
        location_col = cols_lower.get('partitionname2') or cols_lower.get('location') or cols_lower.get('partitionname') or None
        person_uid_col = cols_lower.get('person_uid')

        mask = pd.Series(False, index=raw_df.index)
        if person_uid_col and person_uid_col in raw_df.columns:
            mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == q_str)
        if emp_col and emp_col in raw_df.columns:
            mask = mask | (raw_df[emp_col].astype(str).str.strip() == q_str)
        if not mask.any() and emp_col and emp_col in raw_df.columns:
            try:
                q_numeric = float(q_str)
                emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                mask = mask | (emp_numeric == q_numeric)
            except Exception:
                pass
        if not mask.any() and name_col and name_col in raw_df.columns:
            try:
                mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == q_str.lower())
            except Exception:
                pass

        if not mask.any():
            continue

        filtered = raw_df[mask].copy()
        if filtered.empty:
            continue

        if tcol and tcol in filtered.columns:
            try:
                filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
            except Exception:
                pass

        if tcol and tcol in filtered.columns:
            filtered = filtered.sort_values(by=tcol)
            filtered['_prev_ts'] = filtered[tcol].shift(1)
            try:
                filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
            except Exception:
                filtered['_swipe_gap_seconds'] = 0.0
        else:
            filtered['_swipe_gap_seconds'] = 0.0

        try:
            if door_col and door_col in filtered.columns:
                if dir_col and dir_col in filtered.columns:
                    filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                else:
                    filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
            else:
                filtered['_zone'] = filtered.get('PartitionName2', None)
        except Exception:
            filtered['_zone'] = None

        for _, r in filtered.iterrows():
            # compute timestamp variants
            ts_val = None
            if tcol and tcol in filtered.columns:
                ts_val = r.get(tcol)
            else:
                if 'Date' in filtered.columns:
                    ts_val = r.get('Date')
            try:
                dt = pd.to_datetime(ts_val)
                locale_iso = dt.isoformat() if pd.notna(dt) else None
            except Exception:
                locale_iso = str(ts_val) if ts_val is not None else None
            try:
                date_only = dt.strftime("%d-%b-%y") if pd.notna(dt) else None
            except Exception:
                date_only = None
            try:
                swipe_time = dt.strftime("%I:%M:%S %p").lstrip("0") if pd.notna(dt) else None
            except Exception:
                swipe_time = None

            # EmployeeID resolution
            emp_val = None
            try:
                if emp_col and emp_col in filtered.columns:
                    emp_val = _to_python_scalar(r.get(emp_col))
                else:
                    for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                        cl = cols_lower.get(cand.lower())
                        if cl and cl in filtered.columns:
                            emp_val = _to_python_scalar(r.get(cl))
                            if emp_val:
                                break
            except Exception:
                emp_val = None
            if emp_val is not None:
                try:
                    s = str(emp_val).strip()
                    if '.' in s:
                        try:
                            f = float(s)
                            if math.isfinite(f) and f.is_integer():
                                s = str(int(f))
                        except Exception:
                            pass
                    emp_val = s
                except Exception:
                    pass

            # ObjectName1 / EmployeeName
            obj_name = None
            try:
                if name_col and name_col in filtered.columns:
                    obj_name = _to_python_scalar(r.get(name_col))
                elif 'ObjectName1' in filtered.columns:
                    obj_name = _to_python_scalar(r.get('ObjectName1'))
                elif 'EmployeeName' in filtered.columns:
                    obj_name = _to_python_scalar(r.get('EmployeeName'))
            except Exception:
                obj_name = None

            personnel_val = _to_python_scalar(r.get(personnel_col)) if (personnel_col and personnel_col in filtered.columns) else None
            location_val = _to_python_scalar(r.get(location_col)) if (location_col and location_col in filtered.columns) else (_to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None)

            # Card Number
            card_val = None
            try:
                if card_col and card_col in filtered.columns:
                    card_val = _to_python_scalar(r.get(card_col))
                else:
                    for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                        cl = cols_lower.get(cand.lower())
                        if cl and cl in filtered.columns:
                            card_val = _to_python_scalar(r.get(cl))
                            if card_val not in (None, '', 'nan'):
                                break
                if card_val is not None:
                    card_val = str(card_val).strip()
            except Exception:
                card_val = None

            admit_val = _to_python_scalar(r.get(admit_col)) if (admit_col and admit_col in filtered.columns) else None
            if not admit_val:
                for cand in ('Admit','AdmitCode','Admit_Type','Rejection_Type','Note','NoteType','Source'):
                    cl = cols_lower.get(cand.lower())
                    if cl and cl in filtered.columns:
                        admit_val = _to_python_scalar(r.get(cl))
                        if admit_val:
                            break

            direction_val = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in filtered.columns else None
            door_val = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else _to_python_scalar(r.get('Door')) if 'Door' in filtered.columns else None

            row_out = {
                "LocaleMessageTime": locale_iso,
                "DateOnly": date_only,
                "Swipe_Time": swipe_time,
                "EmployeeID": emp_val,
                "ObjectName1": obj_name,
                "PersonnelType": personnel_val,
                "Location": location_val,
                "CardNumber": card_val,
                "AdmitCode": admit_val,
                "Direction": direction_val,
                "Door": door_val,
                "_source_file": fp.name
            }

            all_rows.append(row_out)

    if not all_rows:
        return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

    df_out = pd.DataFrame(all_rows)

    # Further restrict to only rows that match a flagged trend row on the same Date & EmployeeID/person_uid
    # Build flagged lookup using trend CSV(s) again but now keyed by EmployeeID/Date
    flagged_dates = set()
    try:
        p = Path(DEFAULT_OUTDIR)
        # load relevant trend csv(s)
        trend_candidates = []
        if target_date_obj:
            fp_try = p / f"trend_{city_slug}_{target_date_obj.strftime('%Y%m%d')}.csv"
            if fp_try.exists():
                trend_candidates = [fp_try]
        if not trend_candidates:
            trend_candidates = sorted(p.glob("trend_*.csv"), reverse=True)
        for tf in trend_candidates:
            try:
                tdf = pd.read_csv(tf)
            except Exception:
                try:
                    tdf = pd.read_csv(tf, dtype=str)
                except Exception:
                    continue
            if tdf is None or tdf.empty:
                continue
            tdf = _replace_placeholder_strings(tdf)
            if 'IsFlagged' in tdf.columns:
                tsel = tdf[tdf['IsFlagged'] == True]
            else:
                if 'AnomalyScore' in tdf.columns:
                    try:
                        tsel = tdf[pd.to_numeric(tdf['AnomalyScore'], errors='coerce').fillna(0) >= ANOMALY_THRESHOLD]
                    except Exception:
                        tsel = pd.DataFrame()
                else:
                    tsel = pd.DataFrame()
            if tsel is None or tsel.empty:
                continue
            for _, tt in tsel.iterrows():
                try:
                    dval = None
                    if 'Date' in tt and pd.notna(tt.get('Date')):
                        try:
                            dval = pd.to_datetime(tt.get('Date')).date().isoformat()
                        except Exception:
                            dval = str(tt.get('Date'))
                    # collect candidate ids
                    for idcol in ('EmployeeID','person_uid','EmployeeIdentity','Int1','Text12','CardNumber'):
                        if idcol in tt and tt.get(idcol) not in (None, '', float('nan')):
                            try:
                                ival = str(tt.get(idcol)).strip()
                                if ival:
                                    flagged_dates.add((idcol, ival, dval))
                                    flagged_dates.add((idcol, ival, None))
                            except Exception:
                                continue
                except Exception:
                    continue
    except Exception:
        flagged_dates = set()

    # Keep rows where employee/date is present in flagged_dates
    def _row_is_flagged(r):
        emp = r.get('EmployeeID') or ''
        date_only = r.get('DateOnly')
        # try iso date from DateOnly (it is in DD-MMM-YY), so translate to iso for matching if possible
        date_iso = None
        try:
            if date_only:
                # parse using day-month-year short form
                date_iso = pd.to_datetime(date_only, format="%d-%b-%y", errors='coerce')
                if pd.notna(date_iso):
                    date_iso = date_iso.date().isoformat()
                else:
                    date_iso = None
        except Exception:
            date_iso = None
        # match by EmployeeID & date or EmployeeID with any date
        if emp and ((('EmployeeID', emp, date_iso) in flagged_dates) or (('EmployeeID', emp, None) in flagged_dates)):
            return True
        # also try matching by person_uid if employee string includes emp:/uid: patterns
        for idcol in ('person_uid', 'EmployeeIdentity', 'Int1', 'Text12', 'CardNumber'):
            if ((idcol, q_str, date_iso) in flagged_dates) or ((idcol, q_str, None) in flagged_dates):
                return True
        # fallback: if q matched and we earlier accepted q_flagged, accept rows for q
        try:
            if str(q_str) and (str(q_str) == str(emp) or str(q_str) == str(r.get('CardNumber'))):
                return True
        except Exception:
            pass
        return False

    df_filtered = df_out[df_out.apply(_row_is_flagged, axis=1)].copy()
    if df_filtered.empty:
        return jsonify({"error":"no flagged swipe rows found for the requested employee/date"}), 404

    # final column ordering and ensure only the columns requested
    final_cols = ["LocaleMessageTime","DateOnly","Swipe_Time","EmployeeID","ObjectName1","PersonnelType","Location","CardNumber","AdmitCode","Direction","Door"]
    # ensure all final_cols exist in df_filtered (create missing as None)
    for c in final_cols:
        if c not in df_filtered.columns:
            df_filtered[c] = None
    df_final = df_filtered[final_cols].copy()

    # write excel with single sheet "Evidence" (strict columns only)
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df_final.to_excel(writer, sheet_name='Evidence', index=False)
            writer.save()
            output.seek(0)
    except Exception as e:
        logging.exception("Failed to create Excel: %s", e)
        return jsonify({"error":"failed to create excel"}), 500

    if OPENPYXL_AVAILABLE:
        try:
            wb = load_workbook(output)
            thin = Side(border_style="thin", color="000000")
            thick = Side(border_style="medium", color="000000")
            for ws in wb.worksheets:
                header = ws[1]
                for cell in header:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                        cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
                for col in ws.columns:
                    max_len = 0
                    col_letter = col[0].column_letter
                    for cell in col:
                        try:
                            v = str(cell.value) if cell.value is not None else ""
                        except Exception:
                            v = ""
                        if len(v) > max_len:
                            max_len = len(v)
                    width = min(max(10, max_len + 2), 50)
                    ws.column_dimensions[col_letter].width = width
            out2 = io.BytesIO()
            wb.save(out2)
            out2.seek(0)
            return send_file(out2, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        except Exception:
            logging.exception("Excel styling failed, returning raw file")
            output.seek(0)
            return send_file(output, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    else:
        output.seek(0)
        return send_file(output, as_attachment=True,
                         download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                         mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")



@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)







also check beow duration file 




# backend/duration_report.py
from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np
import hashlib

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name 
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
try:
    from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    # fallback — keep behaviour if config file unavailable
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

    def map_door_to_zone(door: object, direction: object = None) -> str:
        """
        Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
        """
        try:
            if door is None:
                return None
            s = str(door).strip()
            if not s:
                return None
            s_l = s.lower()
            if direction and isinstance(direction, str):
                d = direction.strip().lower()
                if "out" in d:
                    return OUT_OF_OFFICE_ZONE
                if "in" in d:
                    return "Reception Area"
            if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
                return OUT_OF_OFFICE_ZONE
            return "Working Area"
        except Exception:
            return None

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "SG.Singapore", "PH.Manila","IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023","ACVSUJournal_00011022","CVSUJournal_00011021"
        ],
        "partitions": ["LT.Vilnius","AUT.Vienna","MA.Casablanca","RU.Moscow","IT.Rome","UK.London","IE.DUblin",
                        "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition","PA.Panama City","PE.Lima", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["US.CO.OBS", "USA/Canada Default", "US.FL.Miami", "US.NYC"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# logic in this file no longer depends on it for date assignment.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND t3.[Name] = 'Employee'
  {date_condition}
  {region_filter}
"""

# Helpers
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

# GUID / placeholders helpers
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])



def _strip_person_uid_prefix(token: object) -> Optional[str]:
    """
    If token is like 'emp:123' or 'uid:GUID' or 'name:xxxxx' return the suffix;
    otherwise return plain stripped string.
    Returns None for empty/placeholder tokens.
    """
    if token is None:
        return None
    try:
        s = str(token).strip()
        if not s:
            return None
        # common canonical prefixes used by duration_report: emp:, uid:, name:
        if ':' in s:
            prefix, rest = s.split(':', 1)
            if prefix.lower() in ('emp', 'uid', 'name'):
                rest = rest.strip()
                if rest:
                    return rest
        return s
    except Exception:
        return None

# keep existing _strip_person_uid_prefix (unchanged) and expose a shorter alias
def _strip_person_uid_prefix(token: object) -> Optional[str]:
    """
    Existing behaviour: if token like 'emp:123' or 'uid:GUID' or 'name:xxxxx' return suffix;
    otherwise return plain stripped string. Returns None for empty/placeholder tokens.
    """
    if token is None:
        return None
    try:
        s = str(token).strip()
        if not s:
            return None
        # common canonical prefixes used by duration_report: emp:, uid:, name:
        if ':' in s:
            prefix, rest = s.split(':', 1)
            if prefix.lower() in ('emp', 'uid', 'name'):
                rest = rest.strip()
                if rest:
                    return rest
        return s
    except Exception:
        return None

# Alias for modules that expect `_strip_uid_prefix`
def _strip_uid_prefix(token: object) -> Optional[str]:
    """
    Backwards-compatible alias for _strip_person_uid_prefix.
    """
    return _strip_person_uid_prefix(token)



def _looks_like_guid(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
        except Exception:
            continue
        if not s:
            continue
        if _is_placeholder_str(s):
            continue
        if _looks_like_guid(s):
            continue
        return s
    return None

def _canonical_person_uid_from_row(row):
    """
    Produce canonical person_uid in the form:
      - 'emp:<employeeid>' (if a sensible non-GUID EmployeeID present),
      - 'uid:<EmployeeIdentity>' (if present),
      - 'name:<sha1 10chars>' fallback when name present,
      - otherwise None.
    """
    empid = None
    for cand in ('EmployeeID', 'Int1', 'Text12'):
        if cand in row and row.get(cand) not in (None, '', float('nan')):
            empid = row.get(cand)
            break
    empident = row.get("EmployeeIdentity", None)
    name = row.get("EmployeeName", None)

    # normalize empid numeric floats -> ints
    if empid is not None:
        try:
            s = str(empid).strip()
            if '.' in s:
                f = float(s)
                if f.is_integer():
                    s = str(int(f))
            if s and not _looks_like_guid(s) and not _is_placeholder_str(s):
                return f"emp:{s}"
        except Exception:
            pass

    if empident not in (None, '', float('nan')):
        try:
            si = str(empident).strip()
            if si:
                return f"uid:{si}"
        except Exception:
            pass

    if name not in (None, '', float('nan')):
        try:
            sn = str(name).strip()
            if sn and not _looks_like_guid(sn) and not _is_placeholder_str(sn):
                h = hashlib.sha1(sn.lower().encode('utf8')).hexdigest()[:10]
                return f"name:{h}"
        except Exception:
            pass

    return None

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        partitions = rc.get("partitions", [])
        if partitions:
            parts_sql = ", ".join(f"'{p}'" for p in partitions)
            region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
        else:
            likes = rc.get("logical_like", [])
            if likes:
                like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
                region_filter = f"AND ({like_sql})"
            else:
                region_filter = ""
    else:
        region_filter = ""

    # NOTE: AdjustedMessageTime / 2AM boundary logic removed from date selection.
    date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
        "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
        "Direction", "CompanyName", "PrimaryLocation"
    ]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    # ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Dates parsing
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # Keep AdjustedMessageTime if present for debugging but we do NOT use it for date boundaries anymore.
    try:
        if "AdjustedMessageTime" in df.columns:
            df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
        else:
            df["AdjustedMessageTime"] = pd.NaT
    except Exception:
        df["AdjustedMessageTime"] = pd.NaT

    # defensive: make text fields strings (avoid object type surprises)
    for tcol in ("Door", "PartitionName2", "PersonnelTypeName", "EmployeeName", "CompanyName", "PrimaryLocation"):
        if tcol in df.columns:
            df[tcol] = df[tcol].fillna("").astype(str)

    # Filter: only Employees (defensive; the SQL template already requests t3.Name = 'Employee')
    try:
        if "PersonnelTypeName" in df.columns:
            df = df[df["PersonnelTypeName"].str.strip().str.lower() == "employee"].copy()
    except Exception:
        logging.debug("Could not apply PersonnelTypeName filter for region %s", region_key)

    # canonical person_uid (consistent with trend_runner)
    def make_person_uid(row):
        try:
            return _canonical_person_uid_from_row(row)
        except Exception:
            # fallback: try simple concatenation if canonical fails
            try:
                eid = row.get("EmployeeIdentity")
                if pd.notna(eid) and str(eid).strip() != "":
                    return str(eid).strip()
            except Exception:
                pass
            parts = []
            for c in ("EmployeeID", "CardNumber", "EmployeeName"):
                try:
                    v = row.get(c)
                    if v not in (None, '', float('nan')):
                        parts.append(str(v).strip())
                except Exception:
                    continue
            return "|".join(parts) if parts else None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)

    # APAC partition normalization (more robust)
    if region_key == "apac" and not df.empty:
        def normalize_apac_partition(row):
            """
            Minimal / strict APAC partition mapping (only adjust Taguig / Quezon / KL cases).
            - APAC_PI_*  -> Taguig City
            - APAC_PH_*  -> Quezon City
            - APAC_MY_KL* or APAC_MY + KL -> MY.Kuala Lumpur
            If none match, fall back to previously-known tokens or keep original PartitionName2.
            """
            door = str(row.get("Door") or "") or ""
            part = str(row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()

            # 1) Strict: APAC_PI => Taguig City
            # Matches examples like: "APAC_PI_Manila_DR_MainEntrance"
            if re.search(r'\bAPAC[_\-]?PI\b', d) or re.search(r'\bAPAC[_\-]?PI[_\-]', d):
                return "Taguig City"

            # 2) Strict: APAC_PH => Quezon City
            # Matches examples like: "APAC_PH_Manila_7th Floor_Open Office Door 2-721"
            if re.search(r'\bAPAC[_\-]?PH\b', d) or re.search(r'\bAPAC[_\-]?PH[_\-]', d):
                return "Quezon City"

            # 3) Strict: Kuala Lumpur patterns
            # Accept variants like:
            #   "APAC_MY_KL_MAIN ENTRANCE DOOR", "APAC_MY_KL", "APAC_MY KL MAIN", "APAC_MY_KUALA-LUMPUR"
            # We require APAC + MY + (KL or KUALA) tokens in the door/partition string.
            # if ("APAC" in d) and ("MY" in d) and (
            #     "KL" in d or
            #     "KUALA" in d or
            #     re.search(r'KUALA[^A-Z0-9]*LUMPUR', d)
            # ):
            #     return "MY.Kuala Lumpur"


            # 3) Strict: Kuala Lumpur patterns (robust to underscores, dashes, spaces, extra text)
            # Matches: APAC_MY_KL_MAIN ENTRANCE DOOR, APAC-MY-KL, APAC MY KUALA-LUMPUR, etc.
            if re.search(r'APAC[^A-Z0-9]*MY[^A-Z0-9]*(?:KL\b|KUALA\b|KUALA[^A-Z0-9]*LUMPUR)', d):
                return "MY.Kuala Lumpur"



            # --- Minimal remaining token map (kept small & safe) ---
            token_map = {
                "APAC_IN_PUN": "Pune",
                "APAC_PUN": "Pune",
                "VIS_PUN": "Pune",
                "VIS_PUN_177": "Pune",
                "PUN": "Pune",
                "APAC_IN_HYD": "IN.HYD",
                "APAC_HYD": "IN.HYD",
                "HYD": "IN.HYD",
                "IN.HYD": "IN.HYD",
                "SG": "SG.Singapore",
                "SINGAPORE": "SG.Singapore",
                
            }

            # Prefer explicit tokens already present in PartitionName2
            for key, canonical in token_map.items():
                if key in p:
                    return canonical

            # Check door/partition tokens for any remaining known tokens
            toks = [t for t in re.split(r'[^A-Z0-9]+', d + " " + p) if t]
            for t in toks:
                if t in token_map:
                    return token_map[t]

            # If PartitionName2 already has a meaningful value, keep it
            if p and p.strip():
                return part

            # Unknown -> return empty string (caller applies strict masking)
            return ""

            # helper: split door/partition into alphanumeric tokens (keeps mixed tokens like VIS_PUN)
            def make_tokens(s: str):
                toks = [t for t in re.split(r'[^A-Z0-9\-]+', s or "") if t]
                return toks

            # 1) If PartitionName2 already contains a known canonical token, prefer it.
            for key, canonical in token_map.items():
                if key in p:
                    return canonical

            # 2) Tokenize Door and PartitionName2 and check tokens (exact token match)
            door_tokens = make_tokens(d)
            part_tokens = make_tokens(p)
            all_tokens = door_tokens + part_tokens
            for t in all_tokens:
                if t in token_map:
                    return token_map[t]

            # 3) Substring / regex fallbacks -- more permissive
            # Taguig-specific patterns
            if re.search(r'\bTAGUIG\b', d) or re.search(r'\bTAGUIG\b', p):
                return "Taguig City"
            # Quezon / Manila
            if re.search(r'\bQUEZON\b', d) or re.search(r'\bQUEZON\b', p) or re.search(r'\bMANILA\b', d) or re.search(r'\bMANILA\b', p):
                return "Quezon City"
            # Pune / PUN
            if re.search(r'\bPUN(E)?\b', d) or re.search(r'\bPUN(E)?\b', p):
                return "Pune"
            # Hyderabad
            if re.search(r'\bHYD\b', d) or re.search(r'\bHYD\b', p):
                return "IN.HYD"
            # Singapore
            if re.search(r'\bSINGAPORE\b', d) or re.search(r'\bSG\b', d) or re.search(r'\bSINGAPORE\b', p):
                return "SG.Singapore"
            # Kuala Lumpur / MY
            if re.search(r'\bKUALA\b', d) or re.search(r'\bKUALA\b', p) or re.search(r'\bMY\b', d) or re.search(r'\bMY\b', p) or re.search(r'KUALA.?LUMPUR', d):
                return "MY.Kuala Lumpur"

            # 4) If PartitionName2 is a non-empty meaningful value, keep it (no change)
            if p and p.strip():
                return part

            # 5) Unknown: return empty string (caller can filter strictly if needed)
            return ""

        # apply mapping and log the mapping summary for debugging
        df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)
        try:
            vc = df["PartitionName2"].value_counts(dropna=False).to_dict()
            logging.info("APAC PartitionName2 mapping counts example: %s", {k: vc.get(k, 0) for k in list(vc)[:10]})
        except Exception:
            logging.debug("APAC partition mapping counts unavailable")


    # NAMER: normalize PartitionName2 and add LogicalLocation per previous behaviour
    if region_key == "namer" and not df.empty:
        def namer_partition_and_logical(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()
            normalized = part
            logical = "Other"

            if ("US.CO.HQ" in d) or ("HQ" in d and "HQ" in d[:20]) or ("DENVER" in d) or (p == "US.CO.OBS"):
                normalized = "US.CO.OBS"
                logical = "Denver-HQ"
            elif "AUSTIN" in d or "AUSTIN TX" in d or p == "USA/CANADA DEFAULT":
                normalized = "USA/Canada Default"
                logical = "Austin Texas"
            elif "MIAMI" in d or p == "US.FL.MIAMI":
                normalized = "US.FL.Miami"
                logical = "Miami"
            elif "NYC" in d or "NEW YORK" in d or p == "US.NYC":
                normalized = "US.NYC"
                logical = "New York"
            else:
                if p == "US.CO.OBS":
                    normalized = "US.CO.OBS"; logical = "Denver-HQ"
                elif p == "USA/CANADA DEFAULT":
                    normalized = "USA/Canada Default"; logical = "Austin Texas"
                elif p == "US.FL.MIAMI":
                    normalized = "US.FL.Miami"; logical = "Miami"
                elif p == "US.NYC":
                    normalized = "US.NYC"; logical = "New York"
                else:
                    normalized = part
                    logical = "Other"
            return pd.Series({"PartitionName2": normalized, "LogicalLocation": logical})

        mapped = df.apply(namer_partition_and_logical, axis=1)
        df["PartitionName2"] = mapped["PartitionName2"].astype(str)
        df["LogicalLocation"] = mapped["LogicalLocation"].astype(str)

    # ensure PartitionName2 column exists as string
    if "PartitionName2" not in df.columns:
        df["PartitionName2"] = ""

    # ensure LogicalLocation exists
    if "LogicalLocation" not in df.columns:
        df["LogicalLocation"] = ""

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]

# ---------------------------------------------------------------------
# compute_daily_durations (single robust implementation)



def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    """
    Robust, defensive implementation that:
     - accepts a swipes dataframe (may be empty)
     - ensures expected columns exist
     - deduplicates near-duplicate swipes (rounded to seconds)
     - assigns Date from LocaleMessageTime (strict local wall-time date)
     - groups by person_uid + Date and computes first/last, counts and durations
     - returns dataframe with stable output columns (same order as earlier code)
    """
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "DurationMinutes", "DurationDisplay", "DurationHMS",
        "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    # quick return for empty input
    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    # work on a copy
    df = swipes_df.copy()

    # ensure expected columns exist so later code can always reference them
    expected = [
        "EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
        "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"
    ]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes robustly
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # drop near-duplicate swipes (round to seconds)
    try:
        df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
        dedupe_subset = ["person_uid", "_lts_rounded", "CardNumber", "Door"]
        # If person_uid missing, fallback to EmployeeIdentity + LocaleMessageTime
        if df["person_uid"].isnull().all():
            dedupe_subset = ["EmployeeIdentity", "_lts_rounded", "CardNumber", "Door"]
        df = df.drop_duplicates(subset=dedupe_subset, keep="first").copy()
    except Exception:
        # best-effort fallback
        try:
            df = df.drop_duplicates(subset=["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"], keep="first")
        except Exception:
            pass

    # Date assignment (strict local date)
    try:
        df["Date"] = df["LocaleMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure person_uid column exists; fallback to a stable join key if missing
    if "person_uid" not in df.columns or df["person_uid"].isnull().all():
        def make_person_uid(row):
            try:
                # prefer canonical EmployeeID-like tokens
                for cand in ("EmployeeID", "Int1", "Text12"):
                    if cand in row and row.get(cand) not in (None, '', float('nan')):
                        s = str(row.get(cand)).strip()
                        if s:
                            return s
                # fallback to EmployeeIdentity
                if row.get("EmployeeIdentity") not in (None, '', float('nan')):
                    return str(row.get("EmployeeIdentity")).strip()
                # fallback to name
                if row.get("EmployeeName") not in (None, '', float('nan')):
                    return str(row.get("EmployeeName")).strip()
            except Exception:
                pass
            return None
        df["person_uid"] = df.apply(make_person_uid, axis=1)

    # drop rows with no person_uid or no Date (they are not groupable)
    df = df[df["person_uid"].notna() & df["Date"].notna()].copy()
    if df.empty:
        return pd.DataFrame(columns=out_cols)

    # sort then group to pick first/last and other aggregations
    try:
        df = df.sort_values(["person_uid", "Date", "LocaleMessageTime"])
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", lambda s: _pick_first_non_guid_value(s) if not s.empty else None),
            EmployeeName=("EmployeeName", lambda s: _pick_first_non_guid_value(s) if not s.empty else None),
            CardNumber=("CardNumber", lambda s: _pick_first_non_guid_value(s) if not s.empty else None),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        # fallback single-group aggregator (safer for unexpected frames)
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            empid = _pick_first_non_guid_value(g_sorted["EmployeeID"]) if "EmployeeID" in g_sorted else first.get("EmployeeID")
            ename = _pick_first_non_guid_value(g_sorted["EmployeeName"]) if "EmployeeName" in g_sorted else first.get("EmployeeName")
            cnum = _pick_first_non_guid_value(g_sorted["CardNumber"]) if "CardNumber" in g_sorted else first.get("CardNumber")
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": empid,
                "EmployeeName": ename,
                "CardNumber": cnum,
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    # compute duration seconds using first/last swipes (wall-clock)
    grouped["DurationSeconds"] = (pd.to_datetime(grouped["LastSwipe"]) - pd.to_datetime(grouped["FirstSwipe"])).dt.total_seconds().clip(lower=0)

    # format Duration as H:MM string (hours can exceed 24)
    def _seconds_to_hhmm(seconds_val):
        try:
            if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
                return None
            total = int(round(float(seconds_val)))
            hours = total // 3600
            minutes = (total % 3600) // 60
            return f"{hours}:{minutes:02d}"
        except Exception:
            return None

    grouped["Duration"] = grouped["DurationSeconds"].apply(_seconds_to_hhmm)

    # human readable minutes and hms representations
    def _format_minutes_to_hhmm(seconds_val):
        try:
            if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
                return None
            total_minutes = int(round(float(seconds_val) / 60.0))
            h = total_minutes // 60
            m = total_minutes % 60
            return f"{h}h {m}m"
        except Exception:
            return None

    grouped["DurationMinutes"] = grouped["DurationSeconds"].apply(lambda s: int(round(float(s) / 60.0)) if pd.notna(s) else None)
    grouped["DurationDisplay"] = grouped["DurationSeconds"].apply(_format_minutes_to_hhmm)
    grouped["DurationHMS"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    # ensure all requested out_cols exist (stable ordering)
    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    # drop helper column if present
    if "_lts_rounded" in grouped.columns:
        try:
            grouped = grouped.drop(columns=["_lts_rounded"])
        except Exception:
            pass

    # final column ordering and return
    return grouped[out_cols]



def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    # Defensive: normalize incoming regions list
    try:
        requested_regions = [r.lower() for r in (regions or []) if r]
    except Exception:
        requested_regions = []

    # If user provided a city/site, try to map that city to one or more region keys
    def _normalize_token(s: str) -> str:
        return re.sub(r'[^a-z0-9]', '', str(s or '').strip().lower())

    if city:
        city_raw = str(city).strip()
        city_norm = _normalize_token(city_raw)

        # find regions whose partitions or logical_like look like this city
        matched_regions = []
        for rkey, rc in (REGION_CONFIG or {}).items():
            parts = rc.get("partitions", []) or []
            likes = rc.get("logical_like", []) or []
            tokens = set()
            for p in parts:
                if not p:
                    continue
                tokens.add(_normalize_token(p))
                # also split on punctuation/dot and add parts (e.g. "LT.Vilnius" -> "vilnius")
                for part_piece in re.split(r'[.\-/\s]', str(p)):
                    if part_piece:
                        tokens.add(_normalize_token(part_piece))
            for lk in likes:
                tokens.add(_normalize_token(lk))
            # also include server/database names as a fallback
            if city_norm and city_norm in tokens:
                matched_regions.append(rkey)

        # If we could map city to specific region(s), only run those
        if matched_regions:
            requested_regions = [m for m in matched_regions]

    # fallback to all regions in config if none requested
    try:
        if not requested_regions:
            requested_regions = [k.lower() for k in list(REGION_CONFIG.keys())]
    except Exception:
        requested_regions = ['apac']

    results: Dict[str, Any] = {}
    for r in requested_regions:
        if not r:
            continue
        rkey = r.lower()
        if rkey not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", rkey, target_date)
        try:
            swipes = fetch_swipes_for_region(rkey, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", rkey)
            swipes = pd.DataFrame()

        # If a city was requested, apply a strict (but defensive) city filter
        if city and not swipes.empty:
            city_raw = str(city).strip()
            city_norm = _normalize_token(city_raw)

            alt_tokens = set()
            alt_tokens.add(city_raw)
            alt_tokens.add(city_raw.replace('-', ' '))
            alt_tokens.add(city_raw.replace('_', ' '))
            alt_tokens.add(city_raw.replace('.', ' '))
            alt_tokens.add(city_raw.replace(' ', '-'))
            alt_tokens.update({t.title() for t in list(alt_tokens)})
            if city_norm:
                alt_tokens.add(city_norm)

            def _norm_for_cmp(s):
                try:
                    if s is None:
                        return ''
                    return re.sub(r'[^a-z0-9]', '', str(s).strip().lower())
                except Exception:
                    return ''

            # precompute normalized columns (safe defaults)
            try:
                part_norm = swipes["PartitionName2"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "PartitionName2" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
                door_norm = swipes["Door"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "Door" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
                pl_norm = swipes["PrimaryLocation"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "PrimaryLocation" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
            except Exception:
                part_norm = pd.Series([''] * len(swipes), index=swipes.index)
                door_norm = pd.Series([''] * len(swipes), index=swipes.index)
                pl_norm = pd.Series([''] * len(swipes), index=swipes.index)

            # Build mask: start False
            mask = pd.Series(False, index=swipes.index)



            # 1) Strict: prefer PartitionName2 exact matches (canonical names from normalize)
            for t in alt_tokens:
                t_norm = _norm_for_cmp(t)
                if not t_norm:
                    continue
                try:
                    mask = mask | (part_norm == t_norm)
                except Exception:
                    continue

            # 2) For rows where PartitionName2 is empty / unknown, allow Door token contains match (permissive)
            try:
                no_part_mask = part_norm.fillna('').astype(str) == ''
                if no_part_mask.any():
                    door_mask = pd.Series(False, index=swipes.index)
                    for t in alt_tokens:
                        t_norm = _norm_for_cmp(t)
                        if not t_norm:
                            continue
                        door_mask = door_mask | door_norm.str.contains(t_norm, na=False)
                    # only accept door matches when partition is empty
                    mask = mask | (door_mask & no_part_mask)
            except Exception:
                logging.debug("Door-based fallback match failed for city filter in region %s", rkey)

            # 3) Allow PrimaryLocation match ONLY for rows with empty PartitionName2 and not already matched
            try:
                remaining_mask = ~mask
                pl_mask = pd.Series(False, index=swipes.index)
                for t in alt_tokens:
                    t_norm = _norm_for_cmp(t)
                    if not t_norm:
                        continue
                    pl_mask = pl_mask | pl_norm.str.contains(t_norm, na=False)
                mask = mask | (pl_mask & (part_norm.fillna('') == '') & remaining_mask)
            except Exception:
                logging.debug("PrimaryLocation fallback match failed for city filter in region %s", rkey)



            # --- Special-case matching for MY.Kuala Lumpur requested by user ---
            # Many KL door names are like "APAC_MY_KL_MAIN ENTRANCE DOOR" which normalize to
            # 'apacmyklmainentrancedoor' and won't contain 'mykualalumpur' as a contiguous token.
            # So when user asked for MY.Kuala Lumpur, accept any door that contains APAC + MY + (KL | KUALA | KUALALUMPUR).
            try:
                # city_norm is already normalized (non-alphanum removed). compare against expected KL token
                if city_norm in ("mykualalumpur", "mykuala", "kualalumpur", "kuala"):
                    kl_mask = (
                        door_norm.str.contains("apac", na=False)
                        & door_norm.str.contains("my", na=False)
                        & (
                            door_norm.str.contains("kl", na=False)
                            | door_norm.str.contains("kuala", na=False)
                            | door_norm.str.contains("kualalumpur", na=False)
                        )
                    )
                    # Accept KL door matches even if PartitionName2 is empty (safe & strict)
                    mask = mask | kl_mask
            except Exception:
                logging.debug("KL special-case matching failed for city filter")



            # 4) Finally, also check Door and EmployeeName contains across all rows (additional permissive matches)
            try:
                for col in ("Door", "EmployeeName"):
                    if col in swipes.columns:
                        col_norm = swipes[col].fillna("").astype(str).str.lower().apply(_norm_for_cmp)
                        col_mask = pd.Series(False, index=swipes.index)
                        for t in alt_tokens:
                            t_norm = _norm_for_cmp(t)
                            if not t_norm:
                                continue
                            col_mask = col_mask | col_norm.str.contains(t_norm, na=False)
                        mask = mask | col_mask
            except Exception:
                logging.debug("Door/EmployeeName contains-match fallback failed for city filter in region %s", rkey)

            # Apply the mask strictly
            before = len(swipes)
            swipes = swipes[mask].copy()
            logging.info("City filter '%s' applied for region %s: rows before=%d after=%d", city_raw, rkey, before, len(swipes))

        # compute durations for this region
        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", rkey)
            durations = pd.DataFrame()

        # frontend-friendly display columns for swipes
        try:
            if "LocaleMessageTime" in swipes.columns:
                swipes["LocaleMessageTime"] = pd.to_datetime(swipes["LocaleMessageTime"], errors="coerce")
                swipes["DateOnly"] = swipes["LocaleMessageTime"].dt.date
                swipes["Time"] = swipes["LocaleMessageTime"].dt.strftime("%H:%M:%S")
            else:
                if "Date" in swipes.columns and "Time" in swipes.columns:
                    swipes["DateOnly"] = swipes["Date"]
        except Exception:
            logging.debug("Frontend display enrichment failed for region %s", rkey)

        if "AdjustedMessageTime" not in swipes.columns:
            swipes["AdjustedMessageTime"] = pd.NaT

        # write outputs
        try:
            csv_path = outdir_path / f"{rkey}_duration_{target_date.strftime('%Y%m%d')}.csv"
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", rkey)
        try:
            swipes_csv_path = outdir_path / f"{rkey}_swipes_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", rkey)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", rkey, csv_path if 'csv_path' in locals() else '<unknown>', len(durations) if durations is not None else 0)
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", rkey, swipes_csv_path if 'swipes_csv_path' in locals() else '<unknown>', len(swipes) if swipes is not None else 0)

        results[rkey] = {"swipes": swipes, "durations": durations}

    return results


# end of file











check where is issue and fix it carefully and share me updated Snippet 



Check frontend alos 

<!doctype html>
<html>

<head>
  <meta charset="utf-8" />
  <title>Behaviour Analysis — Dashboard</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <!-- React + ReactDOM + Babel (quick prototyping) -->
  <script crossorigin src="https://unpkg.com/react@18/umd/react.development.js"></script>
  <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.development.js"></script>
  <script crossorigin src="https://unpkg.com/babel-standalone@6.26.0/babel.min.js"></script>

  <!-- Chart.js -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

  <!-- Flatpickr -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/flatpickr/dist/flatpickr.min.css">
  <script src="https://cdn.jsdelivr.net/npm/flatpickr"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.1/font/bootstrap-icons.css">
  <link rel="stylesheet" href="style.css">
</head>

<body>
  <div id="root"></div>

  <script type="text/babel">
    (function () {
      const { useState, useEffect, useRef } = React;

      // CHANGE IF YOUR API HOST DIFFERS
      const API_BASE = "http://localhost:8002";

      function resolveApiImageUrl(imgUrl) {
        if (!imgUrl) return null;
        try {
          if (imgUrl.startsWith('http://') || imgUrl.startsWith('https://')) return imgUrl;
          // ensure API_BASE has no trailing slash
          var base = API_BASE.replace(/\/$/, '');
          if (imgUrl.startsWith('/')) return base + imgUrl;
          return base + '/' + imgUrl;
        } catch (e) {
          return imgUrl;
        }
      }


      // --- Region / Location mapping copied from backend/duration_report.REGION_CONFIG (friendly / UI names) ---
      // Keep the keys lowercase to match backend region keys.
      const REGION_OPTIONS = {
        "apac": {
          label: "APAC",
          // Friendly names used by backend normalisation (duration_report) for APAC
          partitions: ["Pune", "Quezon City", "Taguig City", "MY.Kuala Lumpur", "IN.HYD", "SG.Singapore"]
        },
        "emea": {
          label: "EMEA",
          partitions: ["LT.Vilnius", "IT.Rome", "UK.London", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
        },
        "laca": {
          label: "LACA",
          partitions: ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "PA.Panama City", "PE.Lima", "MX.Mexico City"]
        },
        "namer": {
          label: "NAMER",
          // show friendly names to the user, but we map them to backend partition tokens before sending
          partitions: ["Denver", "Austin Texas", "Miami", "New York"]
        }
      };

      // Map from UI-friendly location label -> backend search token (used for &city=)
      // For APAC, friendly labels match backend PartitionName2 normalised values so they map to themselves.
      // For NAMER, backend normalisation sets PartitionName2 to tokens like "US.CO.OBS", "USA/Canada Default" etc.
      const LOCATION_QUERY_VALUE = {
        "apac": {
          "Pune": "Pune",
          "Quezon City": "Quezon City",
          "Taguig City": "Taguig City",
          "MY.Kuala Lumpur": "MY.Kuala Lumpur",
          "IN.HYD": "IN.HYD",
          "SG.Singapore": "SG.Singapore"
        },
        "namer": {
          // friendly->backend tokens (this matches the backend duration_report normalisation)
          "Denver": "US.CO.OBS",
          "Austin Texas": "USA/Canada Default",
          "Miami": "US.FL.Miami",
          "New York": "US.NYC"
        },
        // default passthrough for other regions 
        "emea": {},
        "laca": {}
      };

      // Map risk text/colors (same as backend map_score_to_label buckets)
      const RISK_COLORS = {
        "Low": "#10b981",
        "Low Medium": "#86efac",
        "Medium": "#facc15",
        "Medium High": "#fb923c",
        "High": "#ef4444"
      };
      const RISK_LABELS = ["Low", "Low Medium", "Medium", "Medium High", "High"];

      // (rest unchanged) Explanations...
      const SCENARIO_EXPLANATIONS = {
        "long_gap_>=4.5h": "Long gap between swipes (>=4.5 hours).",
        "short_duration_<4h": "Short total presence (<4 hours).",
        "coffee_badging": "Multiple short swipes — possible coffee badging.",
        "low_swipe_count_<=5": "Very few swipes recorded for day (<=2).",
        "single_door": "Single door used during day.",
        "only_in": "Only IN events present.",
        "only_out": "Only OUT events present.",
        "overtime_>=14h": "Overtime (>=14 hours).",
        "very_long_duration_>=16h": "Very long presence (>=16 hours).",
        "unusually_high_swipes": "Unusually high number of swipes vs history.",
        "repeated_short_breaks": "Multiple short breaks in day.",
        "multiple_location_same_day": "Multiple locations used same day.",
        "weekend_activity": "Activity on weekend.",
        "repeated_rejection_count": "Multiple rejections.",
        "badge_sharing_suspected": "Same card used by multiple persons on same day.",
        "early_arrival_before_06": "First swipe earlier than 06:00.",
        "late_exit_after_23": "Last swipe after 23:30.",
        "shift_inconsistency": "Duration inconsistent with historical shift.",
        "trending_decline": "Historical trending decline.",
        "consecutive_absent_days": "Consecutive absent days historically.",
        "high_variance_duration": "High variance in durations historically.",
        "short_duration_on_high_presence_days": "Short duration despite high typical presence.",
        "swipe_overlap": "Swipes overlapping other users (possible tailgating).",
        "shortstay_longout_repeat": "Short in -> long out -> short return pattern."
      };

      // small utilities
      function pad(n) { return n.toString().padStart(2, '0'); }
      function formatDateISO(d) {
        if (!d) return "";
        const dt = (d instanceof Date) ? d : new Date(d);
        return dt.getFullYear() + "-" + pad(dt.getMonth() + 1) + "-" + pad(dt.getDate());
      }


      function safeDateDisplay(val) {
        if (!val && val !== 0) return "";
        try {
          const d = (val instanceof Date) ? val : new Date(val);
          if (isNaN(d.getTime())) return String(val);
          return d.toLocaleString();
        } catch (e) {
          return String(val);
        }
      }

      function sanitizeName(row) {
        if (!row) return "";
        // prefer feature/duration versions if present
        return row.EmployeeName_feat || row.EmployeeName_dur || row.EmployeeName || row.ObjectName1 || row.objectname1 || row.employee_name || row.person_uid || "";
      }


      function downloadCSV(rows, filename) {
        if (!rows || !rows.length) { alert("No rows to export"); return; }
        var cols = Object.keys(rows[0]);
        var lines = [cols.join(",")];
        rows.forEach(function (r) {
          var row = cols.map(function (c) {
            var v = (r[c] === undefined || r[c] === null) ? "" : String(r[c]).replace(/\n/g, ' ');
            return JSON.stringify(v);
          }).join(",");
          lines.push(row);
        });
        var blob = new Blob([lines.join("\n")], { type: 'text/csv' });
        var url = URL.createObjectURL(blob);
        var a = document.createElement('a'); a.href = url; a.download = filename || 'export.csv'; a.click(); URL.revokeObjectURL(url);
      }

      // duration formatting helper
      function formatSecondsToHmsJS(seconds) {
        if (seconds === null || seconds === undefined || seconds === '') return "-";
        const n = Number(seconds);
        if (isNaN(n) || !isFinite(n)) return "-";
        const s = Math.max(0, Math.floor(n));
        const hh = Math.floor(s / 3600);
        const mm = Math.floor((s % 3600) / 60);
        const ss = s % 60;
        return pad(hh) + ":" + pad(mm) + ":" + pad(ss);
      }



      // duration formatting helper (HH:MM) — used for Duration fields (strict HH:MM)
      function formatSecondsToHmJS(seconds) {
        if (seconds === null || seconds === undefined || seconds === '') return "-";
        const n = Number(seconds);
        if (isNaN(n) || !isFinite(n)) return "-";
        const s = Math.max(0, Math.floor(n));
        const hh = Math.floor(s / 3600);
        const mm = Math.floor((s % 3600) / 60);
        // return HH:MM (hours may be >23)
        return String(hh) + ":" + String(mm).padStart(2, '0');
      }


      // ----- Day-boundary helpers -----
      // Backend assigns Date using LocaleMessageTime.date() (no 2AM shift).
      // Keep frontend day-boundary at 0 so logical dates match backend.
      const DAY_BOUNDARY_HOUR = 0;

      function logicalDateForTs(dt, boundaryHour = DAY_BOUNDARY_HOUR) {
        if (!dt || !(dt instanceof Date) || isNaN(dt.getTime())) return null;
        const hour = dt.getHours();
        const year = dt.getFullYear();
        const month = dt.getMonth();
        const day = dt.getDate();
        const out = new Date(year, month, day, 0, 0, 0, 0);
        // with boundaryHour = 0, this never subtracts a day -> matches backend date assignment
        if (hour < boundaryHour) {
          out.setDate(out.getDate() - 1);
        }
        const y = out.getFullYear();
        const m = String(out.getMonth() + 1).padStart(2, '0');
        const d = String(out.getDate()).padStart(2, '0');
        return `${y}-${m}-${d}`;
      }

      function makeLocalDateFromRow(r) {
        try {
          if (!r) return null;

          // backend usually includes LocaleMessageTime (ISO string). Prefer that.
          if (r.LocaleMessageTime) {
            const t = new Date(r.LocaleMessageTime);
            if (!isNaN(t.getTime())) return t;
          }

          function toInt(v, fallback = 0) {
            const n = Number(v);
            return Number.isFinite(n) ? n : fallback;
          }

          // Backend also supplies DateOnly + Time for frontend convenience — use those if present.
          if (r.DateOnly && r.Time) {
            try {
              // DateOnly might be a Date object or 'YYYY-MM-DD' string.
              const dateStr = String(r.DateOnly).slice(0, 10).replace(/\//g, '-');
              const dateParts = dateStr.split('-').map(p => toInt(p, NaN));
              if (dateParts.length === 3 && !isNaN(dateParts[0])) {
                const year = dateParts[0];
                const month = dateParts[1];
                const day = dateParts[2];

                const timeRaw = String(r.Time).split(/[.+Z ]/)[0];
                const timeParts = timeRaw.split(':').map(p => toInt(p, 0));
                const hh = timeParts[0] || 0;
                const mm = timeParts[1] || 0;
                const ss = timeParts[2] || 0;

                return new Date(year, month - 1, day, hh, mm, ss, 0);
              }
            } catch (e) { /* fallthrough */ }
          }

          // fallback: if Date and Time fields exist (older API formats)
          if (r.Date && r.Time) {
            try {
              const dateStr = String(r.Date).slice(0, 10).replace(/\//g, '-');
              const dateParts = dateStr.split('-').map(p => toInt(p, NaN));
              if (dateParts.length === 3 && !isNaN(dateParts[0])) {
                const year = dateParts[0];
                const month = dateParts[1];
                const day = dateParts[2];

                const timeRaw = String(r.Time).split(/[.+Z ]/)[0];
                const timeParts = timeRaw.split(':').map(p => toInt(p, 0));
                const hh = timeParts[0] || 0;
                const mm = timeParts[1] || 0;
                const ss = timeParts[2] || 0;

                return new Date(year, month - 1, day, hh, mm, ss, 0);
              }
            } catch (e) { /* fallthrough */ }
          }

          // if only DateOnly present, return midnight of that date
          if (r.DateOnly) {
            try {
              const parts = String(r.DateOnly).slice(0, 10).replace(/\//g, '-').split('-');
              if (parts.length === 3) {
                const y = toInt(parts[0], NaN);
                const m = toInt(parts[1], NaN);
                const d = toInt(parts[2], NaN);
                if (!isNaN(y)) return new Date(y, m - 1, d, 0, 0, 0, 0);
              }
            } catch (e) { /* fallthrough */ }
          }

          // if only Date present, use that
          if (r.Date) {
            try {
              const parts = String(r.Date).slice(0, 10).replace(/\//g, '-').split('-');
              if (parts.length === 3) {
                const y = toInt(parts[0], NaN);
                const m = toInt(parts[1], NaN);
                const d = toInt(parts[2], NaN);
                if (!isNaN(y)) return new Date(y, m - 1, d, 0, 0, 0, 0);
              }
            } catch (e) { /* fallthrough */ }
          }

        } catch (e) { }
        return null;
      }

      function App() {
        var yesterday = new Date();
        yesterday.setDate(yesterday.getDate() - 1);

        const [dateFrom, setDateFrom] = useState(formatDateISO(yesterday));
        const [dateTo, setDateTo] = useState(formatDateISO(new Date()));
        const [loading, setLoading] = useState(false);
        const [summary, setSummary] = useState({ rows: 0, flagged_rows: 0, files: [], end_date: null });
        const [rows, setRows] = useState([]);
        const [reasonsCount, setReasonsCount] = useState({});
        const [riskCounts, setRiskCounts] = useState({ "Low": 0, "Low Medium": 0, "Medium": 0, "Medium High": 0, "High": 0 });
        const [filterText, setFilterText] = useState("");
        const [page, setPage] = useState(1);
        const [selectedReason, setSelectedReason] = useState("");
        const [reasonFilterText, setReasonFilterText] = useState("");
        const [modalRow, setModalRow] = useState(null);
        const [modalDetails, setModalDetails] = useState(null);
        const [modalLoading, setModalLoading] = useState(false);
        const [collapseDuplicates, setCollapseDuplicates] = useState(true);
        const [selectedRiskFilter, setSelectedRiskFilter] = useState("");

        // New: region & location
        const [selectedRegion, setSelectedRegion] = useState("apac");
        const [selectedLocation, setSelectedLocation] = useState("All locations");

        // NEW: Employee ID (optional) — minimal addition
        const [employeeId, setEmployeeId] = useState("");

        const pageSize = 25;
        const chartRef = useRef(null);
        const chartInst = useRef(null);

        const fromRef = useRef(null);
        const toRef = useRef(null);
        const fromFp = useRef(null);
        const toFp = useRef(null);


        // Chat state
        const [chatOpen, setChatOpen] = useState(false);
        const [chatMessages, setChatMessages] = useState([]);
        const [chatInput, setChatInput] = useState("");
        const [chatLoading, setChatLoading] = useState(false);




        useEffect(function () {
          if (window.flatpickr && fromRef.current && toRef.current) {
            try { if (fromFp.current) fromFp.current.destroy(); } catch (e) { }
            try { if (toFp.current) toFp.current.destroy(); } catch (e) { }
            fromFp.current = window.flatpickr(fromRef.current, {
              dateFormat: "Y-m-d",
              defaultDate: dateFrom,
              allowInput: true,
              onChange: function (selectedDates, str) {
                if (selectedDates && selectedDates.length) {
                  const iso = formatDateISO(selectedDates[0]);
                  setDateFrom(iso);
                  try { if (toFp.current) toFp.current.set('minDate', iso); } catch (e) { }
                  if (dateTo && new Date(iso) > new Date(dateTo)) {
                    setDateTo(iso);
                    try { if (toFp.current) toFp.current.setDate(iso, true); } catch (e) { }
                  }
                }
              }
            });
            toFp.current = window.flatpickr(toRef.current, {
              dateFormat: "Y-m-d",
              defaultDate: dateTo,
              allowInput: true,
              onChange: function (selectedDates, str) {
                if (selectedDates && selectedDates.length) {
                  const iso = formatDateISO(selectedDates[0]);
                  setDateTo(iso);
                  try { if (fromFp.current) fromFp.current.set('maxDate', iso); } catch (e) { }
                  if (dateFrom && new Date(iso) < new Date(dateFrom)) {
                    setDateFrom(iso);
                    try { if (fromFp.current) fromFp.current.setDate(iso, true); } catch (e) { }
                  }
                }
              }
            });
            try { if (fromFp.current) fromFp.current.set('maxDate', dateTo); if (toFp.current) toFp.current.set('minDate', dateFrom); } catch (e) { }
          }
          loadLatest();
          return function () { try { if (fromFp.current) fromFp.current.destroy(); } catch (e) { } try { if (toFp.current) toFp.current.destroy(); } catch (e) { } };
          // eslint-disable-next-line
        }, []);









        useEffect(function () {
          try { if (fromFp.current && dateFrom) fromFp.current.setDate(dateFrom, false); } catch (e) { }
          try { if (toFp.current && dateTo) toFp.current.setDate(dateTo, false); } catch (e) { }
          try { if (fromFp.current) fromFp.current.set('maxDate', dateTo); } catch (e) { }
          try { if (toFp.current) toFp.current.set('minDate', dateFrom); } catch (e) { }
        }, [dateFrom, dateTo]);

        // When region changes, reset location to "All locations"
        useEffect(() => {
          setSelectedLocation("All locations");
        }, [selectedRegion]);

        async function runForRange() {
          setLoading(true);
          setRows([]);
          setSummary({ rows: 0, flagged_rows: 0, files: [], end_date: null });
          setReasonsCount({});
          setRiskCounts({ "Low": 0, "Low Medium": 0, "Medium": 0, "Medium High": 0, "High": 0 });
          try {
            const start = encodeURIComponent(dateFrom);
            const end = encodeURIComponent(dateTo);
            // include selected region & city if provided
            let url = API_BASE + "/run?start=" + start + "&end=" + end + "&full=true";
            if (selectedRegion) {
              url += "&region=" + encodeURIComponent(selectedRegion);
            }
            if (selectedLocation && selectedLocation !== "All locations") {
              // send backend-aware partition token (use mapping)
              const mapForRegion = LOCATION_QUERY_VALUE[selectedRegion] || {};
              const queryCity = mapForRegion[selectedLocation] || selectedLocation;
              url += "&city=" + encodeURIComponent(queryCity);
            }

            // NEW: include employee_id if provided (minimal addition)
            if (employeeId && String(employeeId).trim() !== "") {
              url += "&employee_id=" + encodeURIComponent(String(employeeId).trim());
            }

            let r = await fetch(url, { method: 'GET' });
            if (!r.ok) { const txt = await r.text(); throw new Error("API returned " + r.status + ": " + txt); }
            let js = await r.json();

            const totalRows = (typeof js.aggregated_unique_persons === 'number') ? js.aggregated_unique_persons
              : (typeof js.rows === 'number') ? js.rows : 0;
            const totalFlagged = (typeof js.flagged_rows === 'number') ? js.flagged_rows : 0;
            const files = js.files || [];

            const sample = Array.isArray(js.flagged_persons) && js.flagged_persons.length ? js.flagged_persons
              : (Array.isArray(js.sample) ? js.sample : []);
            setRows(sample);

            setSummary({ rows: totalRows, flagged_rows: totalFlagged, files: files, end_date: formatDateISO(new Date(dateTo)) });

            if (js.reasons_count && Object.keys(js.reasons_count).length > 0) {
              setReasonsCount(js.reasons_count);
            } else {
              computeReasonsAndRisks(sample);
            }
            if (js.risk_counts && Object.keys(js.risk_counts).length > 0) {
              const all = { "Low": 0, "Low Medium": 0, "Medium": 0, "Medium High": 0, "High": 0 };
              Object.keys(js.risk_counts).forEach(k => { all[k] = js.risk_counts[k]; });
              setRiskCounts(all);
            } else {
              computeReasonsAndRisks(sample);
            }
            setPage(1);
          } catch (err) {
            alert("Error: " + err.message);
            console.error(err);
          } finally {
            setLoading(false);
          }
        }

        function pushChatMessage(msg) {
          setChatMessages(prev => [...prev, msg]);
          setTimeout(() => {
            const el = document.querySelector('.chat-body');
            if (el) el.scrollTop = el.scrollHeight;
          }, 50);
        }



        // New: group rows by person and compute unique per-person reason + highest-severity risk
        function computeReasonsAndRisks(dataRows) {
          // helper severity mapping (higher => more severe)
          function severityForLabel(label) {
            const map = { "Low": 1, "Low Medium": 2, "Medium": 3, "Medium High": 4, "High": 5 };
            if (!label) return 1;
            return map[String(label)] || 1;
          }

          var personMap = {}; // key -> { rows:[], reasonsSet:Set, maxSeverity, chosenLabel }

          (dataRows || []).forEach(function (r) {
            try {
              var key = r.EmployeeID || r.person_uid || (sanitizeName(r) + '|' + (r.CardNumber || r.Card || ''));
              if (!key) {
                // fallback: unique by row index-ish (but try to avoid counting duplicates without id)
                key = JSON.stringify(r); // rare fallback
              }
              if (!personMap[key]) {
                personMap[key] = { rows: [], reasonsSet: new Set(), maxSeverity: 0, chosenLabel: null };
              }
              var p = personMap[key];
              p.rows.push(r);

              // collect reasons (set per person)
              var reasonsField = r.Reasons || r.DetectedScenarios || r.Detected || "";
              String(reasonsField).split(";").map(function (s) { return s && s.trim(); }).filter(Boolean).forEach(function (rs) {
                p.reasonsSet.add(rs);
              });

              // pick highest severity risk label across this person's rows
              var rl = getRiskLabelForRow(r);
              var sev = severityForLabel(rl);
              if (sev > p.maxSeverity) {
                p.maxSeverity = sev;
                p.chosenLabel = rl || "Low";
              }
            } catch (err) {
              // ignore malformed rows
              console.error("computeReasonsAndRisks row error", err);
            }
          });

          // Build aggregated counts: one contribution per person
          var reasonsCounts = {};
          var rcounts = { "Low": 0, "Low Medium": 0, "Medium": 0, "Medium High": 0, "High": 0 };

          Object.keys(personMap).forEach(function (k) {
            var p = personMap[k];
            // reasons: increment each reason once per person
            p.reasonsSet.forEach(function (rn) {
              reasonsCounts[rn] = (reasonsCounts[rn] || 0) + 1;
            });
            // risk: increment chosenLabel once per person (fallback to Low)
            var label = p.chosenLabel || "Low";
            if (!rcounts[label] && rcounts[label] !== 0) rcounts[label] = 0; // ensure key exists
            rcounts[label] = (rcounts[label] || 0) + 1;
          });

          setReasonsCount(reasonsCounts);
          setRiskCounts(rcounts);
        }


        async function loadLatest() {
          setLoading(true);
          try {
            // run for yesterday (to match backend's default behaviour)
            var d = new Date();
            d.setDate(d.getDate() - 1);
            var yesterday = formatDateISO(d);
            setDateFrom(yesterday);
            setDateTo(yesterday);

            const start = encodeURIComponent(yesterday);
            const end = encodeURIComponent(yesterday);
            let url = API_BASE + "/run?start=" + start + "&end=" + end + "&full=true";
            if (selectedRegion) {
              url += "&region=" + encodeURIComponent(selectedRegion);
            }
            if (selectedLocation && selectedLocation !== "All locations") {
              const mapForRegion = LOCATION_QUERY_VALUE[selectedRegion] || {};
              const queryCity = mapForRegion[selectedLocation] || selectedLocation;
              url += "&city=" + encodeURIComponent(queryCity);
            }

            // NEW: include employee_id if provided for loadLatest as well
            if (employeeId && String(employeeId).trim() !== "") {
              url += "&employee_id=" + encodeURIComponent(String(employeeId).trim());
            }

            let r = await fetch(url, { method: 'GET' });
            if (!r.ok) { const txt = await r.text(); throw new Error("API returned " + r.status + ": " + txt); }
            let js = await r.json();

            const totalRows = (typeof js.aggregated_unique_persons === 'number') ? js.aggregated_unique_persons
              : (typeof js.rows === 'number') ? js.rows : 0;
            const totalFlagged = (typeof js.flagged_rows === 'number') ? js.flagged_rows : 0;
            const files = js.files || [];

            const sample = Array.isArray(js.sample) ? js.sample : (Array.isArray(js.flagged_persons) ? js.flagged_persons : []);
            setRows(sample);
            setSummary({ rows: totalRows, flagged_rows: totalFlagged, files: files, end_date: yesterday });

            if (js.reasons_count && Object.keys(js.reasons_count).length > 0) {
              setReasonsCount(js.reasons_count);
            } else {
              computeReasonsAndRisks(sample);
            }
            if (js.risk_counts && Object.keys(js.risk_counts).length > 0) {
              const all = { "Low": 0, "Low Medium": 0, "Medium": 0, "Medium High": 0, "High": 0 };
              Object.keys(js.risk_counts).forEach(k => { all[k] = js.risk_counts[k]; });
              setRiskCounts(all);
            } else {
              computeReasonsAndRisks(sample);
            }
            setPage(1);
          } catch (err) {
            alert("Error: " + err.message);
            console.error(err);
          } finally {
            setLoading(false);
          }
        }




        function getRiskLabelForRow(r) {
          if (!r) return null;
          var rl = r.RiskLevel || r.Risk || null;
          if (rl) return String(rl);
          if (r.RiskScore !== undefined && r.RiskScore !== null) {
            const mapNum = { 1: "Low", 2: "Low Medium", 3: "Medium", 4: "Medium High", 5: "High" };
            return mapNum[String(r.RiskScore)] || null;
          }
          if (r.AnomalyScore !== undefined && r.AnomalyScore !== null) {
            if (r.AnomalyScore >= 5) return "High";
            if (r.AnomalyScore >= 4) return "Medium High";
            if (r.AnomalyScore >= 3) return "Medium";
            if (r.AnomalyScore >= 2) return "Low Medium";
            return "Low";
          }
          return null;
        }

        function buildChart(rcounts) {
          var labels = RISK_LABELS;
          var values = labels.map(l => rcounts && rcounts[l] ? rcounts[l] : 0);
          var colors = labels.map(l => {
            if (selectedRiskFilter) {
              return (l === selectedRiskFilter) ? RISK_COLORS[l] : '#e6edf3';
            } else {
              return RISK_COLORS[l] || '#cccccc';
            }
          });

          var ctx = chartRef.current && chartRef.current.getContext ? chartRef.current.getContext('2d') : null;
          if (!ctx) return;
          try { if (chartInst.current) chartInst.current.destroy(); } catch (e) { }

          chartInst.current = new Chart(ctx, {
            type: 'line',
            data: {
              labels: labels,
              datasets: [{
                label: 'Flagged by Risk Level',
                data: values,
                borderColor: '#2563eb',
                backgroundColor: 'rgba(37,99,235,0.2)',
                fill: true,
                tension: 0.3,
                pointBackgroundColor: colors,
                pointRadius: 5,
                pointHoverRadius: 7
              }]
            },
            options: {
              responsive: true,
              maintainAspectRatio: false,
              plugins: {
                legend: { display: false },
                tooltip: {
                  callbacks: {
                    label: function (context) {
                      return context.parsed.y + ' cases';
                    }
                  }
                }
              },
              onClick: function (evt, elements) {
                if (elements && elements.length > 0) {
                  var idx = elements[0].index;
                  var label = this.data.labels[idx];
                  handleRiskBarClick(label);
                }
              },
              scales: {
                y: { beginAtZero: true, ticks: { precision: 0 } }
              }
            }
          });

        }

        useEffect(function () {
          buildChart(riskCounts);
        }, [riskCounts, selectedRiskFilter]);





        // Filtering & aggregation
        // Keep old filtering behaviour to produce 'filtered' (raw rows matching filters)
        var filtered = (rows || []).filter(function (r) {
          var hay = (sanitizeName(r) + " " + (r.EmployeeID || "") + " " + (r.CardNumber || "") + " " + (r.Reasons || r.DetectedScenarios || "")).toLowerCase();
          var textOk = !filterText || hay.indexOf(filterText.toLowerCase()) !== -1;
          var reasonOk = !selectedReason || (r.Reasons && ((";" + String(r.Reasons) + ";").indexOf(selectedReason) !== -1)) || (r.DetectedScenarios && ((";" + String(r.DetectedScenarios) + ";").indexOf(selectedReason) !== -1));
          var riskOk = true;
          if (selectedRiskFilter) {
            var rl = getRiskLabelForRow(r);
            if (!rl) { riskOk = false; }
            else riskOk = (String(rl) === String(selectedRiskFilter));
          }
          return textOk && reasonOk && riskOk;
        });

        // sort raw filtered (so aggregated picks same order for first-row)
        filtered.sort(function (a, b) {
          var va = Number(a.ViolationDays || a.ViolationDays || 0);
          var vb = Number(b.ViolationDays || b.ViolationDays || 0);
          if (isNaN(va)) va = 0;
          if (isNaN(vb)) vb = 0;
          if (vb !== va) return vb - va;
          return (sanitizeName(a) || "").localeCompare(sanitizeName(b) || "");
        });

        // NEW helper: build normalized timeline rows (reuse for export)
        function getTimelineRows(details) {
          if (!details || !Array.isArray(details.raw_swipes)) return [];
          const arr = details.raw_swipes.map(r => {
            const obj = Object.assign({}, r);
            try { obj.__ts = makeLocalDateFromRow(obj); } catch (_) { obj.__ts = null; }

            let gap = null;
            if (obj.SwipeGapSeconds !== undefined && obj.SwipeGapSeconds !== null) {
              gap = Number(obj.SwipeGapSeconds);
              if (isNaN(gap)) gap = null;
            } else if (obj.SwipeGap) {
              try {
                const parts = String(obj.SwipeGap).split(':').map(p => Number(p));
                if (parts.length === 3 && parts.every(p => !isNaN(p))) gap = parts[0] * 3600 + parts[1] * 60 + parts[2];
              } catch (e) { gap = null; }
            }
            obj.__gap = gap;
            obj.__zone_l = String((obj.Zone || '')).toLowerCase();
            if (obj.__ts) {
              obj.__logical_date = logicalDateForTs(obj.__ts, DAY_BOUNDARY_HOUR);
            } else if (obj.DateOnly) {
              obj.__logical_date = String(obj.DateOnly).slice(0, 10);
            } else if (obj.Date) {
              obj.__logical_date = String(obj.Date).slice(0, 10);
            } else {
              obj.__logical_date = null;
            }
            return obj;
          });

          arr.sort((a, b) => {
            if (a.__ts && b.__ts) return a.__ts - b.__ts;
            if (a.__ts) return -1;
            if (b.__ts) return 1;
            const ka = (a.DateOnly || a.Date || '') + ' ' + (a.Time || '');
            const kb = (b.DateOnly || b.Date || '') + ' ' + (b.Time || '');
            return ka.localeCompare(kb);
          });

          // compute dayStart flags so first-of-day gaps become 0 (mirror render logic)
          const flags = new Array(arr.length).fill(null).map(() => ({ dayStart: false }));
          for (let i = 0; i < arr.length; i++) {
            const cur = arr[i];
            const prev = arr[i - 1];
            const curDate = cur.__logical_date || (cur.DateOnly ? String(cur.DateOnly).slice(0, 10) : (cur.Date ? String(cur.Date).slice(0, 10) : null));
            const prevDate = prev ? (prev.__logical_date || (prev.DateOnly ? String(prev.DateOnly).slice(0, 10) : (prev.Date ? String(prev.Date).slice(0, 10) : null))) : null;
            if (!prev || prevDate !== curDate) flags[i].dayStart = true;
          }
          for (let i = 0; i < arr.length; i++) {
            if (flags[i].dayStart) arr[i].__gap = 0;
          }

          return arr;
        }

        // NEW: export timeline to an Excel-friendly file (HTML table xls)
        function exportTimelineExcel() {
          if (!modalDetails || !modalDetails.raw_swipes || modalDetails.raw_swipes.length === 0) {
            alert("No timeline data to export.");
            return;
          }
          try {
            const rows = getTimelineRows(modalDetails);
            const cols = ["Employee Name", "Employee ID", "Card", "Date", "Time", "SwipeGap", "Door", "Direction", "Zone", "Note"];
            let html = '<html><head><meta charset="utf-8" /></head><body>';
            html += '<table border="1"><thead><tr>';
            cols.forEach(c => { html += '<th>' + c + '</th>'; });
            html += '</tr></thead><tbody>';

            rows.forEach((r, idx) => {
              const g = (r.__gap !== undefined && r.__gap !== null) ? Number(r.__gap) : null;
              const isDayStart = (r.__gap === 0);
              const gapFormatted = isDayStart ? formatSecondsToHmsJS(0) : ((r.SwipeGap && String(r.SwipeGap).trim()) ? String(r.SwipeGap) : (g !== null && g !== undefined ? formatSecondsToHmsJS(g) : "-"));
              const displayDate = r.__logical_date || (r.DateOnly ? String(r.DateOnly).slice(0, 10) : (r.Date ? String(r.Date).slice(0, 10) : '-'));
              const displayTime = r.Time || (r.__ts ? r.__ts.toTimeString().slice(0, 8) : '-');

              html += '<tr>';
              html += '<td>' + (r.EmployeeName || '') + '</td>';
              html += '<td>' + (r.EmployeeID || r.person_uid || '') + '</td>';
              html += '<td>' + (r.CardNumber || r.Card || '') + '</td>';
              html += '<td>' + displayDate + '</td>';
              html += '<td>' + displayTime + '</td>';
              html += '<td>' + gapFormatted + '</td>';
              html += '<td>' + (r.Door || '') + '</td>';
              html += '<td>' + (r.Direction || '') + '</td>';
              html += '<td>' + (r.Zone || '') + '</td>';
              html += '<td>' + (r.Note || '') + '</td>';
              html += '</tr>';
            });

            html += '</tbody></table></body></html>';

            const blob = new Blob([html], { type: 'application/vnd.ms-excel' });
            const pid = (modalRow && (modalRow.EmployeeID || modalRow.person_uid)) ? (modalRow.EmployeeID || modalRow.person_uid) : 'unknown';
            const dateTag = (modalRow && (modalRow.Date || modalRow.DateOnly)) ? String(modalRow.Date || modalRow.DateOnly).slice(0,10).replace(/:/g,'-') : formatDateISO(new Date());
            const filename = `swipe_timeline_${pid}_${dateTag}.xls`;
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a'); a.href = url; a.download = filename; a.click(); URL.revokeObjectURL(url);
          } catch (err) {
            console.error("Export timeline failed", err);
            alert("Failed to export timeline: " + err.message);
          }
        }



// REPLACE the existing buildAggregated(rowsArr) function with this version
function buildAggregated(rowsArr) {
  var map = new Map();
  rowsArr.forEach(function (r) {
    var id = r.EmployeeID || r.person_uid || (sanitizeName(r) + '|' + (r.CardNumber || r.Card || ''));
    var key = String(id);
    if (!map.has(key)) {
      map.set(key, {
        EmployeeName: sanitizeName(r),
        EmployeeID: r.EmployeeID || r.person_uid || "",
        CardNumber: r.CardNumber || r.Card || "",
        ViolationCount: 0,
        ReasonsSet: new Set(),
        ViolationDays: 0,            // init
        ViolationWindowDays: null,
        ViolationDaysLast90: 0,
        RiskLevel: null,            // init chosen risk label
        RiskScore: null,            // optional numeric score if available
        FirstRow: r,
        _rows: []
      });
    }
    var agg = map.get(key);
    agg.ViolationCount += 1;
    agg._rows.push(r);

    // collect reasons
    var reasonsField = r.Reasons || r.DetectedScenarios || r.Detected || "";
    String(reasonsField).split(";").map(function (s) { return s.trim(); }).filter(Boolean).forEach(function (p) { agg.ReasonsSet.add(p); });

    // pick a representative violation count (prefer explicit ViolationDays)
    var candidateCount = null;
    if (r.ViolationDays !== undefined && r.ViolationDays !== null && r.ViolationDays !== "") candidateCount = Number(r.ViolationDays);
    else if (r.ViolationDaysLast !== undefined && r.ViolationDaysLast !== null && r.ViolationDaysLast !== "") candidateCount = Number(r.ViolationDaysLast);
    else if (r.ViolationDaysLast90 !== undefined && r.ViolationDaysLast90 !== null && r.ViolationDaysLast90 !== "") candidateCount = Number(r.ViolationDaysLast90);
    else if (r.ViolationDaysLast_90 !== undefined && r.ViolationDaysLast_90 !== null && r.ViolationDaysLast_90 !== "") candidateCount = Number(r.ViolationDaysLast_90);
    if (candidateCount !== null && !isNaN(candidateCount)) {
      agg.ViolationDays = Math.max(agg.ViolationDays || 0, candidateCount);
    }

    // capture window if present
    if (r.ViolationWindowDays !== undefined && r.ViolationWindowDays !== null && r.ViolationWindowDays !== "") {
      agg.ViolationWindowDays = r.ViolationWindowDays;
    }

    // ---- NEW: pick highest severity risk label for this person across their rows ----
    try {
      var rowRisk = getRiskLabelForRow(r) || null;
      var rowScore = (r.RiskScore !== undefined && r.RiskScore !== null) ? Number(r.RiskScore) : null;
      function severityForLabel(label) {
        var map = { "Low": 1, "Low Medium": 2, "Medium": 3, "Medium High": 4, "High": 5 };
        if (!label) return 1;
        return map[String(label)] || 1;
      }
      if (rowRisk) {
        var currentSeverity = severityForLabel(agg.RiskLevel);
        var thisSeverity = severityForLabel(rowRisk);
        if (!agg.RiskLevel || thisSeverity > currentSeverity) {
          agg.RiskLevel = rowRisk;
        }
      }
      if (rowScore !== null && (!agg.RiskScore || Number(rowScore) > Number(agg.RiskScore))) {
        agg.RiskScore = Number(rowScore);
      }
    } catch (err) {
      // ignore - keep fallbacks
    }
  });

  var out = [];
  map.forEach(function (val, key) {
    out.push({
      EmployeeName: val.EmployeeName,
      EmployeeID: val.EmployeeID,
      CardNumber: val.CardNumber,
      ViolationCount: val.ViolationCount,
      Reasons: Array.from(val.ReasonsSet).join(";"),
      ViolationDays: val.ViolationDays || 0,
      ViolationWindowDays: val.ViolationWindowDays || null,
      ViolationDaysLast90: val.ViolationDaysLast90 || 0,
      // NEW: expose aggregated risk to table rows
      RiskLevel: val.RiskLevel || null,
      RiskScore: val.RiskScore || null,
      FirstRow: val.FirstRow,
      _rows: val._rows
    });
  });

  // sort aggregated by ViolationDays (if present) else ViolationCount, descending, then by name
  out.sort(function (a, b) {
    var aVal = (a.ViolationDays !== undefined && a.ViolationDays !== null) ? Number(a.ViolationDays) : (a.ViolationCount || 0);
    var bVal = (b.ViolationDays !== undefined && b.ViolationDays !== null) ? Number(b.ViolationDays) : (b.ViolationCount || 0);
    if (bVal !== aVal) return bVal - aVal;
    return (a.EmployeeName || "").localeCompare(b.EmployeeName || "");
  });

  return out;
}



        // build aggregatedFiltered only if collapseDuplicates is enabled
        var aggregatedFiltered = collapseDuplicates ? buildAggregated(filtered) : null;

        // set up pagination source
        var sourceForPaging = collapseDuplicates ? (aggregatedFiltered || []) : filtered;

        var totalPages = Math.max(1, Math.ceil((sourceForPaging.length || 0) / pageSize));
        var pageRows = (sourceForPaging || []).slice((page - 1) * pageSize, page * pageSize);




        var totalPages = Math.max(1, Math.ceil(filtered.length / pageSize));
        var pageRows = filtered.slice((page - 1) * pageSize, page * pageSize);


        function exportFiltered() { downloadCSV(collapseDuplicates ? (aggregatedFiltered || []) : filtered, "trend_filtered_export.csv"); }


        function onReasonClick(reason) {
          if (!reason) { setSelectedReason(""); return; }
          if (selectedReason === reason) setSelectedReason(""); else setSelectedReason(reason);
          setPage(1);
        }


        async function openEvidence(row) {
  // open modal quickly with the basic clicked row so UI responds immediately
  setModalRow(row || {});
  setModalDetails(null);
  setModalLoading(true);

  try {
    // fetch full record for this employee
    const q = encodeURIComponent(row.EmployeeID || row.person_uid || "");
    const resp = await fetch(API_BASE + "/record?employee_id=" + q);
    if (!resp.ok) {
      const txt = await resp.text().catch(() => '');
      throw new Error("record failed: " + resp.status + " - " + txt);
    }
    const js = await resp.json();

    // quick image candidate from response or build fallback by id
    const quickImageUrl = js.image_url || js.imageUrl || js.ImageUrl || null;
    const candidateId = (row && (row.EmployeeID || row.person_uid)) || null;
    const builtImage = quickImageUrl || (candidateId ? `/employee/${encodeURIComponent(candidateId)}/image` : null);

    // normalize details
    const details = {
      aggregated_rows: Array.isArray(js.aggregated_rows) ? js.aggregated_rows : (Array.isArray(js.sample) ? js.sample : []),
      raw_swipe_files: Array.isArray(js.raw_swipe_files) ? js.raw_swipe_files : (Array.isArray(js.files) ? js.files : []),
      raw_swipes: Array.isArray(js.raw_swipes) ? js.raw_swipes : []
    };

    // extract email (robustly)
    let newEmail = null;
    try {
      if (details.aggregated_rows && details.aggregated_rows.length) {
        const f = details.aggregated_rows[0];
        newEmail = newEmail || f.EmployeeEmail || f.Email || f.EmailAddress || f.WorkEmail || f.EMail || null;
      }
      if (!newEmail && details.raw_swipes && details.raw_swipes.length) {
        const r0 = details.raw_swipes[0];
        newEmail = newEmail || r0.EmployeeEmail || r0.Email || r0.EmailAddress || r0.WorkEmail || null;
      }
      if (!newEmail && js && js.meta && (js.meta.email || js.meta.Email)) {
        newEmail = js.meta.email || js.meta.Email;
      }
      if (!newEmail && row) {
        newEmail = row.EmployeeEmail || row.Email || null;
      }
    } catch (err) { /* ignore */ }

    // extract card number safely
    let cardNum = null;
    try {
      if (details.aggregated_rows && details.aggregated_rows.length) {
        const f = details.aggregated_rows[0];
        cardNum = f.CardNumber || f.Card || null;
      }
      if (!cardNum && details.raw_swipes && details.raw_swipes.length) {
        const r0 = details.raw_swipes[0];
        cardNum = r0.CardNumber || r0.Card || null;
      }
    } catch (err) { /* ignore */ }

    // choose a source row to pull risk/violation info from (prefer aggregated_rows)
    let srcRow = null;
    if (details.aggregated_rows && details.aggregated_rows.length) srcRow = details.aggregated_rows[0];
    else if (details.raw_swipes && details.raw_swipes.length) srcRow = details.raw_swipes[0];
    else if (js.sample && js.sample.length) srcRow = js.sample[0];
    else if (js.aggregated_rows && js.aggregated_rows.length) srcRow = js.aggregated_rows[0];

    const riskFromSrc = srcRow ? (srcRow.RiskLevel || srcRow.Risk || null) : null;
    const scoreFromSrc = srcRow ? (srcRow.RiskScore || srcRow.Risk || srcRow.AnomalyScore || null) : null;
    const violationFromSrc = srcRow ? (srcRow.ViolationDays || srcRow.ViolationDaysLast90 || srcRow.ViolationDaysLast || 0) : 0;

    // merge gathered values into modalRow so modal shows email/card/risk immediately
    setModalRow(prev => Object.assign({}, prev || {}, {
      EmployeeEmail: newEmail || (prev && prev.EmployeeEmail) || null,
      Email: newEmail || (prev && prev.Email) || null,
      CardNumber: (prev && prev.CardNumber) || cardNum || null,
      ImageUrl: builtImage || (prev && prev.ImageUrl) || null,
      RiskLevel: (prev && prev.RiskLevel) ? prev.RiskLevel : (riskFromSrc || null),
      RiskScore: (prev && prev.RiskScore) ? prev.RiskScore : (scoreFromSrc || null),
      ViolationDays: (prev && (prev.ViolationDays !== undefined)) ? prev.ViolationDays : (violationFromSrc || 0),
      ViolationWindowDays: (prev && prev.ViolationWindowDays) ? prev.ViolationWindowDays : (srcRow && srcRow.ViolationWindowDays ? srcRow.ViolationWindowDays : null)
    }));

    // finally set modalDetails (used by timeline/files rendering)
    setModalDetails(details);
  } catch (e) {
    alert("Failed loading details: " + e.message);
    console.error(e);
  } finally {
    setModalLoading(false);
  }
}

        function closeModal() { setModalRow(null); setModalDetails(null); }

        // helper to render overlap
        function renderOverlapCell(r) {
          var ov = r.OverlapWith || r.swipe_overlap || r.overlap_with || null;
          if (ov && typeof ov === 'string') {
            var parts = ov.split(";").map(function (s) { return s.trim(); }).filter(Boolean);
            if (parts.length === 0) return <span className="muted">—</span>;
            return <span className="pill" title={ov}>{parts.length} overlap</span>;
          }
          return <span className="muted">—</span>;
        }

        function renderReasonChips(reasonText) {
          if (!reasonText) return <span className="muted">—</span>;
          const parts = String(reasonText).split(";").map(s => s.trim()).filter(Boolean);
          return parts.map((p, idx) => (<span key={idx} className="pill" title={SCENARIO_EXPLANATIONS[p] || p}>{p}</span>));
        }

        function renderReasonExplanations(reasonText) {
          if (!reasonText) return <div className="muted">No flags</div>;
          const parts = String(reasonText).split(";").map(s => s.trim()).filter(Boolean);
          return (
            <div>
              {parts.map((p, idx) => (
                <div key={idx} className="why-item" style={{ marginBottom: 8 }}>
                  <b>{p}</b>
                  <div className="small">{SCENARIO_EXPLANATIONS[p] || "No explanation available."}</div>
                </div>
              ))}
            </div>
          );
        }

        async function sendChat(qText, opts = { top_k: 5 }) {
          if (!qText || !qText.toString().trim()) return;
          const text = qText.toString().trim();
          pushChatMessage({ who: 'user', text });
          setChatInput("");
          setChatLoading(true);
          try {
            const payload = Object.assign({ q: text }, opts);
            const resp = await fetch(API_BASE + "/chatbot/query", {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify(payload)
            });
            if (!resp.ok) {
              const t = await resp.text().catch(() => '');
              throw new Error("Server: " + resp.status + " " + t);
            }
            const js = await resp.json();
            const answer = js.answer || js.answer_text || js.result || "No answer returned.";
            const evidence = Array.isArray(js.evidence) ? js.evidence : (js.evidence ? [js.evidence] : []);
            pushChatMessage({ who: 'bot', text: answer, evidence });
          } catch (err) {
            pushChatMessage({ who: 'bot', text: "Error: " + err.message, evidence: [] });
            console.error("chat error", err);
          } finally {
            setChatLoading(false);
            setTimeout(() => {
              const el = document.querySelector('.chat-body');
              if (el) el.scrollTop = el.scrollHeight;
            }, 80);
          }
        }

        const QUICK_PROMPTS = [
          "Who is high risk today",
          "Who is low risk today",
          "Show me 320172 last 90 days",
          "Trend details for today — top reasons",
          "Explain repeated_short_breaks"
        ];
        function useQuickPrompt(q) {
          setChatOpen(true);
          sendChat(q);
        }

        // Swipe timeline rendering uses DAY_BOUNDARY_HOUR = 0 to match backend date assignment
        function renderSwipeTimeline(details, modalRow) {
          if (!details || !details.raw_swipes || details.raw_swipes.length === 0) {
            return <div className="muted">No raw swipe evidence available (person not flagged or raw file missing).</div>;
          }

          const all = details.raw_swipes.map(r => {
            const obj = Object.assign({}, r);
            try { obj.__ts = makeLocalDateFromRow(obj); } catch (e) { obj.__ts = null; }

            let gap = null;
            if (obj.SwipeGapSeconds !== undefined && obj.SwipeGapSeconds !== null) {
              gap = Number(obj.SwipeGapSeconds);
              if (isNaN(gap)) gap = null;
            } else if (obj.SwipeGap) {
              try {
                const parts = String(obj.SwipeGap).split(':').map(p => Number(p));
                if (parts.length === 3 && parts.every(p => !isNaN(p))) gap = parts[0] * 3600 + parts[1] * 60 + parts[2];
              } catch (e) { gap = null; }
            }
            obj.__gap = gap;
            obj.__zone_l = String((obj.Zone || '')).toLowerCase();

            // Prefer backend-provided date fields (DateOnly) or computed timestamp
            if (obj.__ts) {
              obj.__logical_date = logicalDateForTs(obj.__ts, DAY_BOUNDARY_HOUR);
            } else if (obj.DateOnly) {
              // DateOnly may be a string or object; coerce to YYYY-MM-DD
              obj.__logical_date = String(obj.DateOnly).slice(0, 10);
            } else if (obj.Date) {
              obj.__logical_date = String(obj.Date).slice(0, 10);
            } else {
              obj.__logical_date = null;
            }
            return obj;
          }).sort((a, b) => {
            // Primary sort: parsed timestamp if present
            if (a.__ts && b.__ts) return a.__ts - b.__ts;
            if (a.__ts) return -1;
            if (b.__ts) return 1;
            // Fallback: use DateOnly + Time or Date+Time strings
            const ka = (a.DateOnly || a.Date || '') + ' ' + (a.Time || '');
            const kb = (b.DateOnly || b.Date || '') + ' ' + (b.Time || '');
            return ka.localeCompare(kb);
          });

          // flags: dayStart for first row OR when logical date changes between rows
          const flags = new Array(all.length).fill(null).map(() => ({ dayStart: false, outReturn: false }));
          for (let i = 0; i < all.length; i++) {
            const cur = all[i];
            const prev = all[i - 1];
            const curDate = cur.__logical_date || (cur.DateOnly ? String(cur.DateOnly).slice(0, 10) : (cur.Date ? String(cur.Date).slice(0, 10) : null));
            const prevDate = prev ? (prev.__logical_date || (prev.DateOnly ? String(prev.DateOnly).slice(0, 10) : (prev.Date ? String(prev.Date).slice(0, 10) : null))) : null;
            if (!prev || prevDate !== curDate) {
              flags[i].dayStart = true;
            }
          }

          const OUT_RETURN_GAP_SECONDS = 60 * 60;
          for (let i = 0; i < all.length - 1; i++) {
            const a = all[i], b = all[i + 1];
            const aZone = a.__zone_l || ''; const bZone = b.__zone_l || ''; const bGap = b.__gap || 0;
            if (aZone.includes('out of office') || aZone.includes('out_of_office') || aZone.includes('out of')) {
              if (!bZone.includes('out of office') && (bGap >= OUT_RETURN_GAP_SECONDS || (bGap === null && aZone.includes('out')))) {
                flags[i].outReturn = true; flags[i + 1].outReturn = true;
              }
            }
          }

          for (let i = 0; i < all.length; i++) {
            if (flags[i].dayStart) {
              all[i].__gap = 0;
            }
          }

          return (
            <div className="table-scroll">
              <table className="evidence-table" role="table" aria-label="Swipe timeline">
                <thead>
                  <tr>
                    <th>Employee Name</th>
                    <th>Employee ID</th>
                    <th>Card</th>
                    <th>Date</th>
                    <th>Time</th>
                    <th>SwipeGap</th>
                    <th>Door</th>
                    <th>Direction</th>
                    <th>Zone</th>
                    <th>Note</th>
                  </tr>
                </thead>
                <tbody>
                  {all.map((rObj, idx) => {
                    const r = rObj || {};
                    const g = (r.__gap !== undefined && r.__gap !== null) ? Number(r.__gap) : null;
                    const isDayStart = flags[idx].dayStart;
                    const gapFormatted = (isDayStart)
                      ? formatSecondsToHmsJS(0)
                      : (
                        (r.SwipeGap && String(r.SwipeGap).trim())
                          ? String(r.SwipeGap)
                          : (g !== null && g !== undefined)
                            ? formatSecondsToHmsJS(g)
                            : "-"
                      );

                    // display date: prefer logical (backend date), then DateOnly, then Date
                    const displayDate = r.__logical_date || (r.DateOnly ? String(r.DateOnly).slice(0, 10) : (r.Date ? String(r.Date).slice(0, 10) : '-'));
                    const displayTime = r.Time || (r.__ts ? r.__ts.toTimeString().slice(0, 8) : '-');

                    const cls = [];
                    if (isDayStart) cls.push('row-day-start');
                    if (flags[idx].outReturn) cls.push('row-out-return');
                    const rowStyle = isDayStart ? { background: '#e6ffed' } : {};
                    let extraNote = "";
                    try {
                      const originalDate = r.Date ? String(r.Date).slice(0, 10) : null;
                      const logical = r.__logical_date || null;
                      if (originalDate && logical && originalDate !== logical) {
                        extraNote = `Orig: ${originalDate}`;
                        if ((String(r.Direction || '').toLowerCase().indexOf('out') !== -1)) {
                          extraNote += " — Out";
                        }
                      }
                    } catch (e) { extraNote = ""; }

                    return (
                      <tr key={idx} className={cls.join(' ')} style={rowStyle}>
                        <td className="small">{r.EmployeeName || '-'}</td>
                        <td className="small">{r.EmployeeID || '-'}</td>
                        <td className="small">{r.CardNumber || r.Card || '-'}</td>
                        <td className="small">{displayDate}</td>
                        <td className="small">{displayTime}</td>
                        <td className="small">{gapFormatted}</td>
                        <td className="small" style={{ minWidth: 160 }}>{r.Door || '-'}</td>
                        <td className="small">{r.Direction || '-'}</td>
                        <td className="small">{r.Zone || '-'}</td>
                        <td className="small">{r.Note || '-'}{r._source ? <span className="muted"> ({r._source})</span> : null}
                          {extraNote ? <div className="muted" style={{ fontSize: 11, marginTop: 4 }}>{extraNote}</div> : null}
                        </td>
                      </tr>
                    );
                  })}
                </tbody>
              </table>
            </div>
          );
        }

        function handleRiskBarClick(label) {
          if (!label) return;
          if (selectedRiskFilter === label) {
            setSelectedRiskFilter("");
          } else {
            setSelectedRiskFilter(label);
          }
          setPage(1);
        }

        function clearRiskFilter() {
          setSelectedRiskFilter("");
        }


        var rowsCount = (summary && typeof summary.rows === 'number') ? summary.rows : (rows ? rows.length : 0);

        // compute unique flagged persons (dedupe by same key used above)
        var _flaggedKeys = new Set();
        (rows || []).forEach(function (r) {
          if (r && (r.Reasons || r.DetectedScenarios)) {
            var k = r.EmployeeID || r.person_uid || (sanitizeName(r) + '|' + (r.CardNumber || r.Card || ''));
            if (!k) k = JSON.stringify(r);
            _flaggedKeys.add(k);
          }
        });
        var flaggedCount = (summary && typeof summary.flagged_rows === 'number' && summary.flagged_rows > 0) ? summary.flagged_rows : _flaggedKeys.size;

        var flaggedPct = rowsCount ? Math.round((flaggedCount * 100) / (rowsCount || 1)) : 0;



        // helper to get display label for current region
        function regionDisplayLabel(key) {
          if (!key) return '';
          return (REGION_OPTIONS[key] && REGION_OPTIONS[key].label) ? REGION_OPTIONS[key].label : key.toUpperCase();
        }




        return (
          <div className="container" aria-live="polite">
            {loading && (
              <div className="spinner-overlay" role="status" aria-label="Loading">
                <div className="spinner-box">
                  <div className="spinner" />
                  <div style={{ fontWeight: 700 }}>Loading…</div>
                </div>
              </div>
            )}

            <div className="topbar" role="banner">
              <div className="wu-brand" aria-hidden={false}>
                <div className="wu-logo">WU</div>
                <div className="title-block">
                  <h1>Western Union — Trend Analysis</h1>
                  <p style={{ margin: 0, fontSize: 13 }}>
                    {regionDisplayLabel(selectedRegion)} {selectedLocation && selectedLocation !== "All locations" ? "— " + selectedLocation : ""}
                  </p>
                </div>
              </div>

              <div className="header-actions" role="region" aria-label="controls">
                <div className="control">
                  <label className="small" htmlFor="fromDate">From</label>
                  <input id="fromDate" ref={fromRef} className="date-input" type="text" placeholder="YYYY-MM-DD" />
                </div>

                <div className="control">
                  <label className="small" htmlFor="toDate">To</label>
                  <input id="toDate" ref={toRef} className="date-input" type="text" placeholder="YYYY-MM-DD" />
                </div>

                <button className="btn-primary" onClick={runForRange} disabled={loading}>Run</button>
                <button className="btn-ghost" onClick={loadLatest} disabled={loading}>Load latest</button>
              </div>
            </div>

            <div className="card-shell">
              <div className="cards" aria-hidden={loading}>
                <div className="card" title="Rows analysed">
                  <div className="card-content">
                    <div className="card-text">
                      <h3>{(rowsCount !== undefined && rowsCount !== null) ? rowsCount.toLocaleString() : 0}</h3>
                      <p>Rows analysed</p>
                    </div>
                  </div>
                </div>
                <div className="card card-flagged" title="Flagged rows">
                  <div className="card-content">
                    <div className="card-text">
                      <h3>{(flaggedCount !== undefined && flaggedCount !== null) ? flaggedCount.toLocaleString() : 0}</h3>
                      <p>Flagged rows</p>
                    </div>
                  </div>
                </div>
                <div className="card card-rate" title="Flagged rate">
                  <div className="card-content">
                    <div className="card-text">
                      <h3>{flaggedPct}%</h3>
                      <p>Flagged rate</p>
                    </div>
                  </div>
                </div>
              </div>

              <div className="main">
                <div className="left">
                  <div className="chart-wrap" aria-label="Risk level chart">
                    <canvas ref={chartRef}></canvas>
                  </div>




                  <div style={{ display: 'flex', alignItems: 'center', gap: 8, marginTop: 6 }}>
                    <input placeholder="Search name, employee id, card or reason..." value={filterText} onChange={function (e) { setFilterText(e.target.value); setPage(1); }} style={{ flex: 1, padding: 10, borderRadius: 6, border: '1px solid #e6edf3' }} />

                    <label style={{ display: 'flex', alignItems: 'center', gap: 8, marginRight: 8 }}>
                      <input type="checkbox" checked={collapseDuplicates} onChange={(e) => { setCollapseDuplicates(e.target.checked); setPage(1); }} />
                      <span className="small muted">Collapse duplicates</span>
                    </label>

                    <div className="muted">
                      Showing {collapseDuplicates ? (Array.isArray(aggregatedFiltered) ? aggregatedFiltered.length : filtered.length) : filtered.length} / {rows.length} rows
                    </div>

                    <button className="small-button" onClick={exportFiltered}>Export filtered</button>
                    {selectedRiskFilter ? <button className="small-button" onClick={clearRiskFilter}>Clear risk filter</button> : null}
                  </div>



                  <div style={{ marginTop: 10 }} className="table-scroll" role="region" aria-label="results table">
                    <table>
                      <thead>
                        <tr>
                          <th>Employee</th>
                          <th className="small">ID</th>
                          <th className="small">Card</th>
                          <th className="small">Date</th>
                          <th className="small">Duration</th>
                          <th className="small">Violation Days</th>


                          <th className="small">Reasons</th>
                          <th className="small">Evidence</th>
                        </tr>
                      </thead>
                      <tbody>







{pageRows.map(function (r, idx) {
  // if aggregated mode, r will be aggregated object (has ViolationCount and FirstRow/_rows)
  var isAgg = collapseDuplicates && r && r.ViolationCount !== undefined;
  var displayRow = isAgg ? r.FirstRow : r; // use FirstRow for details/evidence
  var empName = isAgg ? (r.EmployeeName || sanitizeName(displayRow)) : sanitizeName(r);
  var empId = isAgg ? (r.EmployeeID || displayRow.EmployeeID || "") : (r.EmployeeID || "");
  var card = isAgg ? (r.CardNumber || displayRow.CardNumber || "") : (r.CardNumber || "");
  var displayDate = safeDateDisplay(displayRow.DisplayDate || displayRow.Date || displayRow.DateOnly || displayRow.FirstSwipe || displayRow.LastSwipe);

  var durText = (displayRow.Duration)
    || (displayRow.DurationSeconds ? formatSecondsToHmJS(Number(displayRow.DurationSeconds))
      : (displayRow.DurationMinutes ? formatSecondsToHmJS(Number(displayRow.DurationMinutes) * 60) : ""));

  var reasonsText = isAgg ? (r.Reasons || displayRow.Reasons || displayRow.DetectedScenarios) : (r.Reasons || r.DetectedScenarios);

  // --- compute violationCount & window BEFORE returning JSX (this fixes the Babel parse error) ---
  var violationCount = "";
  var violationWindow = null;

  if (isAgg) {
    // prefer aggregated ViolationDays (set in buildAggregated), then fallbacks
    if (r.ViolationDays !== undefined && r.ViolationDays !== null && r.ViolationDays !== "") violationCount = String(r.ViolationDays);
    else if (r.ViolationDaysLast !== undefined && r.ViolationDaysLast !== null && r.ViolationDaysLast !== "") violationCount = String(r.ViolationDaysLast);
    else if (r.ViolationDaysLast90 !== undefined && r.ViolationDaysLast90 !== null && r.ViolationDaysLast90 !== "") violationCount = String(r.ViolationDaysLast90);
    else if (r.ViolationDaysLast_90 !== undefined && r.ViolationDaysLast_90 !== null && r.ViolationDaysLast_90 !== "") violationCount = String(r.ViolationDaysLast_90);
    violationWindow = r.ViolationWindowDays || r.ViolationWindow || null;
  } else {
    // single-row displayRow: prefer ViolationDays, then ViolationDaysLast, then last90 variants
    if (displayRow.ViolationDays !== undefined && displayRow.ViolationDays !== null && displayRow.ViolationDays !== "") violationCount = String(displayRow.ViolationDays);
    else if (displayRow.ViolationDaysLast !== undefined && displayRow.ViolationDaysLast !== null && displayRow.ViolationDaysLast !== "") violationCount = String(displayRow.ViolationDaysLast);
    else if (displayRow.ViolationDaysLast90 !== undefined && displayRow.ViolationDaysLast90 !== null && displayRow.ViolationDaysLast90 !== "") violationCount = String(displayRow.ViolationDaysLast90);
    else if (displayRow.ViolationDaysLast_90 !== undefined && displayRow.ViolationDaysLast_90 !== null && displayRow.ViolationDaysLast_90 !== "") violationCount = String(displayRow.ViolationDaysLast_90);
    violationWindow = displayRow.ViolationWindowDays || displayRow.ViolationWindow || null;
  }

  return (
    <tr key={idx} className={(displayRow.Reasons && String(displayRow.Reasons).trim()) ? "flagged-row" : ""}>
      <td className="row-click" onClick={function () { openEvidence(displayRow); }}>
        {empName || <span className="muted">—</span>}
        {isAgg ? <div className="small muted" style={{ marginTop: 4 }}>violation Count - {r.ViolationCount}</div> : null}
      </td>
      <td className="small">{empId}</td>
      <td className="small">{card}</td>
      <td className="small">{displayDate}</td>
      <td className="small">{isAgg ? (displayRow.Duration || (displayRow.DurationSeconds ? formatSecondsToHmJS(Number(displayRow.DurationSeconds)) : "-")) : durText}</td>

      <td className="small">
        {violationCount !== "" ? violationCount : <span className="muted">—</span>}
        {/* NOTE: intentionally not showing "window: {violationWindow}d" in table per request */}
      </td>

      <td className="small">{renderReasonChips(reasonsText)}</td>
      <td className="small">
        <button className="evidence-btn" onClick={function () { openEvidence(displayRow); }}>Evidence</button>
        {isAgg ? <span className="muted" style={{ marginLeft: 8 }}>({r.ViolationCount} rows)</span> : null}
      </td>
    </tr>
  );
})}



                      </tbody>
                    </table>
                  </div>

                  <div style={{ display: 'flex', gap: 8, alignItems: 'center', marginTop: 10 }}>
                    <button onClick={function () { setPage(function (p) { return Math.max(1, p - 1); }); }} disabled={page <= 1}>Prev</button>
                    <div className="muted">Page {page} / {totalPages}</div>
                    <button onClick={function () { setPage(function (p) { return Math.min(totalPages, p + 1); }); }} disabled={page >= totalPages}>Next</button>
                  </div>
                </div>

                <aside className="right" aria-label="side panel">

                  {/* NEW: Region & Location controls */}
                  <div className="sidebar-section" style={{ marginBottom: 12 }}>
                    <strong>Risk filters</strong>
                    <div className="small muted" style={{ marginTop: 6 }}>Select Region and Location to scope the run.</div>

                    <div style={{ display: 'flex', gap: 8, marginTop: 8, alignItems: 'center' }}>
                      <div style={{ flex: 1 }}>
                        <label className="small">Region</label>
                        <select
                          value={selectedRegion}
                          onChange={(e) => { setSelectedRegion(e.target.value); setPage(1); }}
                          style={{ width: '100%', padding: '6px 8px', borderRadius: 6, border: '1px solid #e2e8f0' }}
                        >
                          {Object.keys(REGION_OPTIONS).map(k => (
                            <option key={k} value={k}>{REGION_OPTIONS[k].label}</option>
                          ))}
                        </select>
                      </div>

                      <div style={{ flex: 1 }}>
                        <label className="small">Location</label>
                        <select
                          value={selectedLocation}
                          onChange={(e) => { setSelectedLocation(e.target.value); setPage(1); }}
                          style={{ width: '100%', padding: '6px 8px', borderRadius: 6, border: '1px solid #e2e8f0' }}
                        >
                          <option key="__all" value="All locations">All locations</option>
                          {(REGION_OPTIONS[selectedRegion] && REGION_OPTIONS[selectedRegion].partitions || []).map(loc => (
                            <option key={loc} value={loc}>{loc}</option>
                          ))}
                        </select>
                      </div>


                      {/* NEW: Employee ID input placed next to Region & Location */}

                      <div style={{ flex: 1 }}>
                        <label className="small">Employee ID </label>
                        <input
                          value={employeeId}
                          onChange={(e) => { setEmployeeId(e.target.value); setPage(1); }}
                          placeholder="Enter Employee ID e.g. 326131"
                          style={{ width: '100%', padding: '6px 8px', borderRadius: 6, border: '1px solid #e2e8f0' }}
                        />
                      </div>
                    </div>
                  </div>



                  {/* existing risk chips */}
                  <div className="sidebar-section">
                    <div className="risk-filter-list" style={{ marginTop: 8 }}>
                      {RISK_LABELS.map((lab) => {
                        const cnt = (riskCounts && riskCounts[lab]) ? riskCounts[lab] : 0;
                        const active = selectedRiskFilter === lab;
                        return (
                          <div key={lab} role="button" tabIndex={0} aria-pressed={active} className={"risk-chip " + (active ? "active" : "")} onClick={function () { handleRiskBarClick(lab); }} onKeyDown={function (e) { if (e.key === 'Enter' || e.key === ' ') { handleRiskBarClick(lab); } }}>
                            <div style={{ width: 10, height: 10, borderRadius: 999, background: RISK_COLORS[lab], boxShadow: '0 2px 6px rgba(0,0,0,0.08)' }}></div>
                            <div style={{ fontSize: 13 }}>{lab} <span className="muted" style={{ marginLeft: 6 }}>({cnt})</span></div>
                          </div>
                        );
                      })}
                    </div>

                    <div style={{ marginTop: 8 }}>
                      <button className="small-button" onClick={clearRiskFilter}>Clear risk filter</button>
                    </div>
                  </div>


                  <div className="sidebar-section" style={{ marginTop: 12 }}>
                    <strong>Top reasons summary</strong>
                    <div className="small muted" style={{ marginTop: 6 }}>Click a reason to filter the table by that reason. Click again to clear.</div>

                    <div style={{ marginTop: 8, display: 'flex', gap: 8 }}>
                      <input placeholder="Filter reason list..." value={reasonFilterText} onChange={function (e) { setReasonFilterText(e.target.value); }} style={{ flex: 1, padding: '6px 8px', borderRadius: 6, border: '1px solid #e2e8f0' }} />
                      <button className="small-button" onClick={function () { setSelectedReason(''); setReasonFilterText(''); }}>Clear</button>
                    </div>

                    <div style={{ marginTop: 8, maxHeight: 320, overflow: 'auto' }}>
                      {Object.keys(reasonsCount).length === 0 && <div className="muted">No flags found</div>}
                      {Object.entries(reasonsCount).sort(function (a, b) { return b[1] - a[1]; }).filter(function (kv) {
                        var name = kv[0];
                        if (!reasonFilterText) return true;
                        return name.toLowerCase().indexOf(reasonFilterText.toLowerCase()) !== -1;
                      }).slice(0, 50).map(function (kv) {
                        var name = kv[0], count = kv[1];
                        var active = selectedReason === name;
                        return (
                          <div key={name} style={{ display: 'flex', alignItems: 'center', justifyContent: 'space-between', gap: 8, marginBottom: 6 }}>
                            <button className={"chip " + (active ? "active" : "")} style={{ textAlign: 'left', flex: 1 }} onClick={function () { onReasonClick(name); }}>
                              {name}
                            </button>
                            <div style={{ minWidth: 48, textAlign: 'right' }} className="small"><b>{count}</b></div>
                          </div>
                        );
                      })}

                    </div>
                  </div>
                </aside>
              </div>
            </div>




            {modalRow &&
              <div className="modal" onClick={closeModal}>
                <div className="modal-inner" onClick={function (e) { e.stopPropagation(); }}>
                  <div className="modal-header">
                    <div className="header-content">
                      <div className="header-icon">
                        <i className="bi bi-clipboard2-data-fill"></i>
                      </div>
                      <div className="header-text">
                        <h3>Details — Evidence</h3>
                        <div className="header-subtitle small">Evidence & explanation for selected row</div>
                      </div>
                    </div>
                    <button className="close-btn" onClick={closeModal}>
                      <i className="bi bi-x-lg"></i>
                      Close
                    </button>
                  </div>
                  <div className="modal-body">
                    {modalLoading && (
                      <div className="loading-state">
                        <div className="loading-spinner"></div>
                        <span>Loading evidence…</span>
                      </div>
                    )}
                    <div className="modal-top" role="region" aria-label="evidence summary">
                      <div className="image-section">
                        <div className="image-container">
                          <div className="multi-color-border">
                            <div className="color-ring color-1"></div>
                            <div className="color-ring color-2"></div>
                            <div className="color-ring color-3"></div>
                            <div className="color-ring color-4"></div>
                            <div className="image-content">







                              {/* Improved modal image block — prefer ObjectID/GUID and try more fallbacks */}

                              {(modalDetails && ((modalDetails.aggregated_rows && modalDetails.aggregated_rows.length > 0) || (modalDetails.raw_swipes && modalDetails.raw_swipes.length > 0))) ? (

                                (() => {
                                  // helper: candidate image key names (same as before)
                                  const candidateImageKeys = ['imageUrl', 'image_url', 'ImageUrl', 'image', 'Image', 'img', 'imgUrl', 'ImagePath', 'Photo', 'PhotoUrl', 'EmployeePhoto', 'photo', 'photoUrl'];

                                  // pick first md (aggregated/raw) row if exists
                                  const md = (modalDetails && modalDetails.aggregated_rows && modalDetails.aggregated_rows[0])
                                    || (modalDetails && modalDetails.raw_swipes && modalDetails.raw_swipes[0])
                                    || {};

                                  // try to obtain an explicit image path from md first
                                  let imgPath = candidateImageKeys.map(k => (md && md[k]) ? md[k] : null).find(Boolean) || null;

                                  // if not present yet, use any image_url included in full record response (js.image_url -> set below in openEvidence)
                                  if (!imgPath && modalRow && modalRow.ImageUrl) {
                                    imgPath = modalRow.ImageUrl;
                                  }

                                  // Build ordered id candidates preferring modalRow
                                  const idCandidates = [];
                                  if (modalRow && modalRow.EmployeeID) idCandidates.push(String(modalRow.EmployeeID));
                                  if (modalRow && modalRow.person_uid) idCandidates.push(String(modalRow.person_uid));
                                  if (md && md.EmployeeID) idCandidates.push(String(md.EmployeeID));
                                  if (md && md.person_uid) idCandidates.push(String(md.person_uid));
                                  if (md && md.ObjectID) idCandidates.push(String(md.ObjectID));
                                  if (md && md.GUID) idCandidates.push(String(md.GUID));

                                  const uniqIds = idCandidates.filter((v, i) => v && idCandidates.indexOf(v) === i);

                                  // If still no explicit path, build a likely image endpoint from the top id candidate
                                  if (!imgPath && uniqIds.length) {
                                    imgPath = `/employee/${encodeURIComponent(uniqIds[0])}/image`;
                                  }

                                  if (imgPath) {
                                    const imgSrc = resolveApiImageUrl(imgPath) || imgPath;
                                    return (
                                      <img
                                        className="modal-image"
                                        src={imgSrc}
                                        alt={sanitizeName(modalRow) || "Employee image"}
                                        onLoad={(e) => { try { console.info("employee image loaded:", e.target.src); } catch (e) { } }}
                                        onError={async (e) => {
                                          try {
                                            e.target.onerror = null;
                                            console.warn("image load failed for:", e.target.src);

                                            // cache-busted retry + API variants for all uniqIds
                                            const tryUrls = [];
                                            const original = e.target.src;
                                            tryUrls.push(original + (original.indexOf('?') === -1 ? '?cb=' + Date.now() : '&cb=' + Date.now()));
                                            uniqIds.forEach(id => {
                                              if (!id) return;
                                              const a = resolveApiImageUrl(`/api/employees/${encodeURIComponent(id)}/image`);
                                              const b = resolveApiImageUrl(`/employee/${encodeURIComponent(id)}/image`);
                                              if (a && tryUrls.indexOf(a) === -1) tryUrls.push(a);
                                              if (b && tryUrls.indexOf(b) === -1) tryUrls.push(b);
                                            });

                                            let found = null;
                                            for (const url of tryUrls) {
                                              try {
                                                const getr = await fetch(url, { method: 'GET', cache: 'no-store' });
                                                if (getr && getr.ok) {
                                                  const ct = (getr.headers.get('content-type') || '').toLowerCase();
                                                  if (ct.startsWith('image')) { found = url; break; }
                                                }
                                              } catch (err) { /* ignore */ }
                                            }

                                            if (found) {
                                              e.target.src = found + (found.indexOf('?') === -1 ? ('?cb=' + Date.now()) : ('&cb=' + Date.now()));
                                              return;
                                            }

                                            // final fallback SVG
                                            const svg = '<svg xmlns="http://www.w3.org/2000/svg" width="160" height="160"><rect fill="#eef2f7" width="100%" height="100%"/><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle" fill="#64748b" font-size="18">No image</text></svg>';
                                            e.target.src = 'data:image/svg+xml;utf8,' + encodeURIComponent(svg);
                                          } catch (err) {
                                            try { e.target.style.display = 'none'; } catch (err2) { }
                                            console.error("image fallback error", err);
                                          }
                                        }}
                                      />
                                    );
                                  } else {
                                    // no id/path available -> simple placeholder (unchanged)
                                    return (
                                      <div className="modal-image-placeholder">
                                        <i className="bi bi-person-square"></i>
                                        <span>No image</span>
                                      </div>
                                    );
                                  }
                                })()

                              ) : (
                                <div className="modal-image-placeholder">
                                  <i className="bi bi-person-square"></i>
                                  <span>No image</span>
                                </div>

                              )}

                            </div>
                          </div>
                        </div>
                      </div>

                      <div className="modal-details">
                        <div className="details-header">
                          <div className="emp-info">
                            <div className="emp-name">
                              {sanitizeName(modalRow) || "—"}
                              <span
                                className="risk-badge"
                                style={{
                                  marginLeft: "12px",
                                  background:
                                    RISK_COLORS[modalRow.RiskLevel] ||
                                    RISK_COLORS[getRiskLabelForRow(modalRow)] ||
                                    RISK_COLORS["Low"],
                                }}
                              >
                                {modalRow.RiskLevel ||
                                  (modalRow.RiskScore ? "Score " + modalRow.RiskScore : "Low")}
                              </span>
                            </div>
                            <div className="emp-badge">
                              <i className="bi bi-person-badge"></i>
                              ID: {modalRow.EmployeeID || "—"}
                            </div>
                          </div>
                        </div>
                        <div className="details-grid">
                          <div className="detail-item">
                            <div className="detail-icon">
                              <i className="bi bi-credit-card"></i>
                            </div>
                            <div className="detail-content">
                              <label>Card Number</label>
                              <span>{modalRow.CardNumber || "—"}</span>
                            </div>
                          </div>
                          <div className="detail-item">
                            <div className="detail-icon">
                              <i className="bi bi-envelope"></i>
                            </div>
                            <div className="detail-content">
                              <label>EmployeeEmail</label>

                              <span>
                                {(
                                  // prefer aggregated_rows[0], then raw_swipes[0], then modalRow
                                  (modalDetails && modalDetails.aggregated_rows && modalDetails.aggregated_rows[0] && (modalDetails.aggregated_rows[0].EmployeeEmail || modalDetails.aggregated_rows[0].Email))
                                  || (modalDetails && modalDetails.raw_swipes && modalDetails.raw_swipes[0] && (modalDetails.raw_swipes[0].EmployeeEmail || modalDetails.raw_swipes[0].Email))
                                  || modalRow.EmployeeEmail
                                  || modalRow.Email
                                  || (modalDetails && modalDetails.aggregated_rows && modalDetails.aggregated_rows[0] && (modalDetails.aggregated_rows[0].WorkEmail || modalDetails.aggregated_rows[0].EMail))
                                ) ? (
                                  (modalDetails && modalDetails.aggregated_rows && modalDetails.aggregated_rows[0] && (modalDetails.aggregated_rows[0].EmployeeEmail || modalDetails.aggregated_rows[0].Email))
                                  || (modalDetails && modalDetails.raw_swipes && modalDetails.raw_swipes[0] && (modalDetails.raw_swipes[0].EmployeeEmail || modalDetails.raw_swipes[0].Email))
                                  || modalRow.EmployeeEmail
                                  || modalRow.Email
                                  || (modalDetails && modalDetails.aggregated_rows && modalDetails.aggregated_rows[0] && (modalDetails.aggregated_rows[0].WorkEmail || modalDetails.aggregated_rows[0].EMail))
                                ) : <span className="muted">—</span>}
                              </span>





                            </div>
                          </div>
                          <div className="detail-item">
                            <div className="detail-icon">
                              <i className="bi bi-calendar-date"></i>
                            </div>
                            <div className="detail-content">
                              <label>Date</label>
                              <span>{safeDateDisplay(modalRow.DisplayDate || modalRow.Date || modalRow.DateOnly || modalRow.FirstSwipe)}</span>
                            </div>
                          </div>
                          <div className="detail-item">
                            <div className="detail-icon">
                              <i className="bi bi-clock"></i>
                            </div>
                            <div className="detail-content">
                              <label>Duration</label>

                              <span className="duration-badge">
                                {modalRow.Duration
                                  || (modalRow.DurationSeconds ? formatSecondsToHmJS(Number(modalRow.DurationSeconds))
                                    : (modalRow.DurationMinutes ? formatSecondsToHmJS(Number(modalRow.DurationMinutes) * 60) : "—"))}
                              </span>


                            </div>
                            <div style={{ marginTop: 8, textAlign: 'right' }}>



                              <div className="muted">
                                Violation days
                              </div>
                              <div style={{ fontWeight: 700 }}>
                                {(modalRow && modalRow.ViolationDays !== undefined && modalRow.ViolationDays !== null) ? modalRow.ViolationDays
                                  : (modalRow && modalRow.ViolationDaysLast !== undefined && modalRow.ViolationDaysLast !== null) ? modalRow.ViolationDaysLast
                                    : (modalRow && modalRow.ViolationDaysLast90 !== undefined && modalRow.ViolationDaysLast90 !== null) ? modalRow.ViolationDaysLast90
                                      : 0}
                              </div>


                            </div>
                          </div>
                        </div>
                      </div>

                      <div className="modal-reasons">
                        <div className="explanation-section" style={{ marginTop: 12 }}>
                          <div style={{ fontWeight: 700 }}>Explanation</div>
                          <div style={{
                            marginTop: 8,
                            maxHeight: 160,
                            overflow: 'auto',
                            background: '#fff',
                            border: '1px solid #eef2f7',
                            padding: 8,
                            borderRadius: 6
                          }}>
                            {(modalRow.Explanation || modalRow.ViolationExplanation)
                              ? <div style={{ whiteSpace: 'pre-wrap' }}>{modalRow.Explanation || modalRow.ViolationExplanation}</div>
                              : <div className="muted">No explanation provided.</div>}

                          </div>
                        </div>
                        <div className="reasons-section">
                          <div className="section-title">
                            <i className="bi bi-list-check"></i>
                            Reasons Flagged
                          </div>
                          <div className="reasons-list">
                            {renderReasonChips(modalRow.Reasons || modalRow.DetectedScenarios)}
                          </div>
                        </div>
                      </div>
                    </div>


                    
                  

                    <div className="timeline-section">



  <div className="section-header" style={{ display: 'flex', alignItems: 'center', gap: 8 }}>
                      <i className="bi bi-clock-history"></i>
                      <div style={{ display: 'flex', flexDirection: 'column' }}>
                        <h4 style={{ margin: 0 }}>Swipe Timeline</h4>
                        <span className="subtitle">Filtered for this person/date</span>
                      </div>

                      {/* Export button added here (Excel-friendly .xls HTML table) */}
                      <div style={{ marginLeft: 'auto' }}>
                        <button
                          className="small-button"
                          onClick={exportTimelineExcel}
                          disabled={!modalDetails || !(modalDetails.raw_swipes && modalDetails.raw_swipes.length)}
                        >
                          Export report
                        </button>
                      </div>
                    </div>


                      <div className="timeline-content">
                        {modalDetails ? renderSwipeTimeline(modalDetails, modalRow) : (
                          <div className="loading-timeline">
                            <i className="bi bi-hourglass-split"></i>
                            <span>Evidence not loaded yet.</span>
                          </div>
                        )}
                      </div>
                    </div>

                    <div className="raw-json-section">
                      <label className="toggle-label">
                        <input
                          type="checkbox"
                          id="showraw"
                          onChange={function (e) {
                            const el = document.getElementById('rawpayload');
                            if (el) el.style.display = e.target.checked ? 'block' : 'none';
                          }}
                        />
                        <span className="toggle-slider"></span>
                        <span className="toggle-text">
                          <i className="bi bi-code-slash"></i>
                          Show raw aggregated JSON
                        </span>
                      </label>
                      <div id="rawpayload" className="raw-json" style={{ display: 'none' }}>
                        <pre>{JSON.stringify(modalRow, null, 2)}</pre>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            }

            <button className="chat-fab" title="Ask Trend Details (Ask Me )" onClick={() => setChatOpen(true)} aria-label="Open chat">
              <span className="meta-icon"><img src="chat-bot.png" alt="" /></span>
            </button>


            {chatOpen && (
              <div className="chat-modal" role="dialog" aria-modal="true" aria-label="Trend Chatbot">
                <div className="chat-header">
                  <div style={{ display: 'flex', alignItems: 'center', gap: 8 }}>
                    <div style={{ width: 36, height: 36, borderRadius: 8, background: '#', display: 'flex', alignItems: 'center', justifyContent: 'center', color: '#2563eb', fontWeight: 800 }}><img src="chat-bot.png" alt="" style={{ width: 36, height: 36, }} /></div>
                    <div>
                      <div className="title">Ask me — Trend Details</div>
                      <div style={{ fontSize: 12, opacity: 0.85 }}>Ask trend & risk questions</div>
                    </div>
                  </div>
                  <div style={{ marginLeft: 'auto' }}>
                    <button className="small-button bot-close" onClick={() => { setChatOpen(false); }}>Close</button>
                  </div>
                </div>

                <div className="chat-body">
                  {chatMessages.length === 0 && (
                    <div style={{ color: '#64748b', fontSize: 13 }}>
                      Hi — ask about trends (e.g. "Who is high risk today"). Use the quick prompts below.
                    </div>
                  )}
                  {chatMessages.map((m, i) => (
                    <div key={i} style={{ display: 'block' }}>
                      <div className={"chat-bubble " + (m.who === 'user' ? 'user' : 'bot')}>
                        {m.text}
                        {m.who === 'bot' && m.evidence && m.evidence.length > 0 && (
                          <div className="chat-evidence">
                            <strong>Evidence</strong>
                            <div style={{ marginTop: 6 }}>{m.evidence.slice(0, 5).map((e, j) => (<div key={j}>{typeof e === 'string' ? e : JSON.stringify(e)}</div>))}</div>
                          </div>
                        )}
                      </div>
                    </div>
                  ))}

                  {chatLoading && <div className="chat-loading" style={{ marginTop: 6 }}>Thinking…</div>}
                  <div style={{ marginTop: 8 }} className="quick-prompts" aria-hidden={chatLoading}>
                    {QUICK_PROMPTS.map((q, idx) => (
                      <button key={idx} onClick={() => useQuickPrompt(q)} disabled={chatLoading}>{q}</button>
                    ))}
                  </div>
                </div>

                <div className="chat-input-row">
                  <input
                    className="chat-input"
                    placeholder="Type a question, e.g. 'Who is high risk today'…"
                    value={chatInput}
                    onChange={(e) => setChatInput(e.target.value)}
                    onKeyDown={(e) => { if (e.key === 'Enter' && !e.shiftKey) { e.preventDefault(); sendChat(chatInput); } }}
                  />
                  <button className="chat-send-btn" onClick={() => sendChat(chatInput)} disabled={chatLoading}>Send</button>
                </div>
              </div>
            )}

          </div>
        );
      }

      const root = ReactDOM.createRoot(document.getElementById('root'));
      root.render(React.createElement(App));
    })();
  </script>
</body>

</html>
