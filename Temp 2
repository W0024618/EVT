# ccure_compare_service.py
"""
Compare CCURE profiles/stats with local sheets + compute visit averages.

Provides:
 - compare_ccure_vs_sheets(...)  (existing compare endpoint)
 - compute_visit_averages(timeout=6)  (used by /ccure/averages)
"""

import os
import re
import uuid
import traceback
from datetime import datetime
from typing import List, Dict, Any, Optional

import pandas as pd
import numpy as np
import logging

logger = logging.getLogger("ccure_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

from db import SessionLocal
from models import ActiveEmployee, ActiveContractor
from settings import OUTPUT_DIR

# ---------- small helpers ----------------------------------------------------

def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        normalized = digits.lstrip('0') or digits
        return normalized
    except Exception:
        return None

def _normalize_name(s) -> Optional[str]:
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

def _sanitize_for_json(value):
    """VERY conservative JSON sanitizer to avoid NaN / numpy types / timestamps."""
    try:
        import pandas as _pd
        import numpy as _np
    except Exception:
        _pd = None
        _np = None

    if value is None:
        return None
    if isinstance(value, (str, bool)):
        return value
    if isinstance(value, int):
        return value
    if isinstance(value, float):
        if _np is not None and not _np.isfinite(value):
            return None
        return float(value)
    if _np is not None and isinstance(value, (_np.integer,)):
        return int(value)
    if _np is not None and isinstance(value, (_np.floating,)):
        v = float(value)
        if not _np.isfinite(v):
            return None
        return v
    if isinstance(value, dict):
        out = {}
        for k, v in value.items():
            try:
                key = str(k)
            except Exception:
                key = repr(k)
            out[key] = _sanitize_for_json(v)
        return out
    if isinstance(value, (list, tuple, set)):
        return [_sanitize_for_json(v) for v in value]
    try:
        return str(value)
    except Exception:
        return None

# ---------- CCURE wrappers --------------------------------------------------

def _fetch_ccure_stats() -> Optional[Dict[str, Any]]:
    try:
        import ccure_client
        if hasattr(ccure_client, "get_global_stats"):
            return ccure_client.get_global_stats()
    except Exception:
        logger.debug("ccure_client.get_global_stats not available", exc_info=True)
        return None
    return None

def _fetch_ccure_profiles() -> List[Dict[str, Any]]:
    """Try to fetch full profiles list if ccure_client exposes a method."""
    try:
        import ccure_client
        # earlier implementations provide fetch_all_employees_full() or fetch_all_employees()
        for fn in ("fetch_all_employees_full", "fetch_all_employees", "fetch_all_profiles", "fetch_profiles", "fetch_all"):
            if hasattr(ccure_client, fn):
                try:
                    res = getattr(ccure_client, fn)()
                    if isinstance(res, list):
                        return res
                except Exception:
                    continue
    except Exception:
        pass
    return []

# ---------- region client usage ---------------------------------------------

def _fetch_live_region_summary(timeout: int = 6):
    """
    Uses region_clients.fetch_all_regions() and fetch_all_details() (best-effort).
    Returns (regions_info_list, details_list)
    """
    try:
        import region_clients
    except Exception:
        logger.debug("region_clients not available", exc_info=True)
        return [], []

    regions_info = []
    details = []
    try:
        if hasattr(region_clients, "fetch_all_regions"):
            regions_info = region_clients.fetch_all_regions()
    except Exception:
        logger.exception("fetch_all_regions failed")

    try:
        if hasattr(region_clients, "fetch_all_details"):
            details = region_clients.fetch_all_details()
    except Exception:
        logger.exception("fetch_all_details failed")

    return regions_info or [], details or []

# ---------- compute_visit_averages ------------------------------------------

def compute_visit_averages(timeout: int = 6) -> Dict[str, Any]:
    """
    Returns a JSON-safe dict:
    {
      "live_today": {"employee": int, "contractor": int, "unknown": int, "total": int},
      "ccure_active": {"active_employees_reported": int|None, "active_contractors_reported": int|None,
                       "derived_active_employees": int|None, "derived_active_contractors": int|None},
      "averages": {"employee_pct": float|None, "contractor_pct": float|None, "overall_pct": float|None, "avg_per_site": float|None},
      "sites_queried": int,
      "notes": "..."
    }
    """
    notes = []
    # 1) live summary from region_clients
    regions_info, details = _fetch_live_region_summary(timeout=timeout)

    sites_queried = 0
    live_total = 0
    try:
        # regions_info expected: [{"region": "namer", "count": 123}, ...]
        sites_queried = len(regions_info)
        for r in regions_info:
            try:
                c = r.get("count") if isinstance(r, dict) else None
                if c is None:
                    continue
                ci = _safe_int(c)
                if ci is None:
                    continue
                live_total += int(ci)
            except Exception:
                continue
    except Exception:
        logger.exception("Error computing live_total from regions_info")

    # 2) try to classify details into employee/contractor/unknown
    live_emp = 0
    live_contractor = 0
    live_unknown = 0

    if isinstance(details, list) and len(details) > 0:
        for d in details:
            try:
                if not isinstance(d, dict):
                    live_unknown += 1
                    continue
                # possible keys containing personnel type
                candidate_keys = [
                    "PersonnelType", "personnelType", "personnel_type", "Personnel Type",
                    "PersonnelTypeName", "Personnel", "Type", "personnel", "PersonType", "personType"
                ]
                found = None
                for k in candidate_keys:
                    if k in d and d.get(k) is not None:
                        found = str(d.get(k)).strip()
                        break
                if not found:
                    # some live-summary detail items may carry "personnelType" nested or 'Personnel' under different casing
                    # try scanning all string values for the words 'employee' or 'contractor'
                    looked = False
                    for v in d.values():
                        if isinstance(v, str):
                            sv = v.strip().lower()
                            if "employee" in sv or "contractor" in sv:
                                found = v
                                looked = True
                                break
                    if not looked:
                        found = None

                if found:
                    low = found.strip().lower()
                    if "employee" in low:
                        live_emp += 1
                    elif "contractor" in low:
                        live_contractor += 1
                    else:
                        live_unknown += 1
                else:
                    # fallback: try to use presence of employee id or worker id fields
                    if any(k in d for k in ("EmployeeID","employee_id","EmpID","Worker System Id","Worker System ID","worker_system_id")):
                        # ambiguous, but treat as employee by default if we see EmployeeID
                        # if it looks like a contractor id (prefix W or such) we'd need more logic; keep unknown
                        live_unknown += 1
                    else:
                        live_unknown += 1
            except Exception:
                live_unknown += 1
    else:
        notes.append("No 'details' available from region_clients; only total counts used (breakdown unknown).")

    # If the details-based total differs from the regions_info total, we keep both but prefer region total for overall
    # if details classified sum > 0 we compute derived_total_details
    derived_details_total = live_emp + live_contractor + live_unknown
    if derived_details_total > 0 and live_total != derived_details_total:
        notes.append(f"Region totals ({live_total}) differ from classified detail rows ({derived_details_total}). Using region totals for overall counts and details for breakdown where available.")

    # If details unavailable, attempt to estimate breakdown proportionally using CCURE reported counts (as fallback)
    ccure_stats = _fetch_ccure_stats()
    reported_active_emps = None
    reported_active_contractors = None
    if isinstance(ccure_stats, dict):
        reported_active_emps = _safe_int(ccure_stats.get("ActiveEmployees"))
        reported_active_contractors = _safe_int(ccure_stats.get("ActiveContractors"))

    # Also try deriving active counts from profiles if accessible
    derived_active_emps = None
    derived_active_contractors = None
    try:
        profiles = _fetch_ccure_profiles()
        if isinstance(profiles, list) and len(profiles) > 0:
            # scan profiles
            emp_count = 0
            contractor_count = 0
            for p in profiles:
                try:
                    if not isinstance(p, dict):
                        continue
                    # detect personnel type
                    pt = None
                    for k in ("PersonnelType","personnelType","Personnel","Type"):
                        if k in p and p.get(k):
                            pt = str(p.get(k)).strip().lower()
                            break
                    # detect status
                    st = None
                    for k in ("Employee_Status","Employee Status","Status","Profile_Disabled"):
                        if k in p and p.get(k) is not None:
                            st = p.get(k)
                            break
                    active = None
                    if isinstance(st, bool):
                        active = (not st) if isinstance(st, bool) else None  # handle Profile_Disabled boolean patterns (if Profile_Disabled False -> active)
                        # but because sources differ, we'll not rely solely on this
                    elif isinstance(st, str):
                        s = st.strip().lower()
                        active = (s == "active")
                    else:
                        active = None
                    if pt and active:
                        if pt.startswith("employee"):
                            emp_count += 1
                        elif pt.startswith("contractor"):
                            contractor_count += 1
                except Exception:
                    continue
            if emp_count + contractor_count > 0:
                derived_active_emps = int(emp_count)
                derived_active_contractors = int(contractor_count)
    except Exception:
        logger.exception("Fetching/deriving profiles failed")

    # If we have no detail breakdown but have region total and ccure counts -> estimate breakdown:
    if derived_details_total == 0 and live_total and (reported_active_emps or reported_active_contractors):
        # allocate proportionally using reported CCURE active counts
        re = reported_active_emps or 0
        rc = reported_active_contractors or 0
        denom = (re + rc) or None
        if denom and denom > 0:
            try:
                prop_e = float(re) / float(denom)
                prop_c = float(rc) / float(denom)
                est_emp = int(round(live_total * prop_e))
                est_contractor = int(round(live_total * prop_c))
                # avoid overshoot
                if est_emp + est_contractor != live_total:
                    # adjust by difference
                    diff = live_total - (est_emp + est_contractor)
                    est_emp += diff  # put remainder to employees (arbitrary but deterministic)
                live_emp = est_emp
                live_contractor = est_contractor
                live_unknown = 0
                notes.append("No per-person 'details' available; breakdown estimated proportionally from CCURE reported active counts.")
            except Exception:
                notes.append("Failed to estimate breakdown from CCURE counts.")
        else:
            notes.append("No per-person details and insufficient CCURE counts to estimate employee/contractor breakdown.")

    # Now compute percentages (safe math)
    def safe_pct(n, denom):
        try:
            if n is None or denom is None:
                return None
            dn = float(denom)
            if dn == 0:
                return None
            return round((float(n) / dn) * 100.0, 2)
        except Exception:
            return None

    # overall denominator (ccure reported active total prefer reported then derived)
    cc_emp = reported_active_emps
    cc_con = reported_active_contractors
    if cc_emp is None or cc_con is None:
        # try derived
        if derived_active_emps is not None and derived_active_contractors is not None:
            cc_emp = derived_active_emps if cc_emp is None else cc_emp
            cc_con = derived_active_contractors if cc_con is None else cc_con

    denom_all = None
    if cc_emp is not None and cc_con is not None:
        denom_all = (cc_emp + cc_con) if (isinstance(cc_emp, int) and isinstance(cc_con, int)) else None

    employee_pct = safe_pct(live_emp, cc_emp) if cc_emp is not None else None
    contractor_pct = safe_pct(live_contractor, cc_con) if cc_con is not None else None
    overall_pct = safe_pct(live_total, denom_all) if denom_all is not None else None

    avg_per_site = None
    try:
        if sites_queried and sites_queried > 0:
            avg_per_site = round(float(live_total) / float(sites_queried), 2)
    except Exception:
        avg_per_site = None

    # Build result dict
    result = {
        "live_today": {
            "employee": int(live_emp),
            "contractor": int(live_contractor),
            "unknown": int(live_unknown),
            "total": int(live_total)
        },
        "ccure_active": {
            "active_employees_reported": _safe_int(reported_active_emps),
            "active_contractors_reported": _safe_int(reported_active_contractors),
            "derived_active_employees_from_profiles": _safe_int(derived_active_emps),
            "derived_active_contractors_from_profiles": _safe_int(derived_active_contractors)
        },
        "averages": {
            "employee_pct": _sanitize_for_json(employee_pct),
            "contractor_pct": _sanitize_for_json(contractor_pct),
            "overall_pct": _sanitize_for_json(overall_pct),
            "avg_per_site": _sanitize_for_json(avg_per_site)
        },
        "sites_queried": int(sites_queried),
        "notes": " | ".join(notes) if notes else None
    }

    return _sanitize_for_json(result)


# ---------- compare function (kept minimal here, you can extend) -------------

def compare_ccure_vs_sheets(mode: str = "full", stats_detail: str = "ActiveProfiles", limit_list: int = 200, export: bool = False) -> Dict[str, Any]:
    """
    Lightweight wrapper that uses previous logic - if you already have a longer implementation, keep that.
    For now this function attempts to load ccure stats and sheet counts and returns basic diffs + samples.
    """

    try:
        session = SessionLocal()
    except Exception as e:
        return {"error": f"DB session failed: {e}", "trace": traceback.format_exc()}

    # build sheet counts
    try:
        act_rows = session.query(ActiveEmployee).all()
        contractor_rows = session.query(ActiveContractor).all()
        sheet_emp_count = len(act_rows)
        sheet_contractor_count = len(contractor_rows)
    except Exception as e:
        session.close()
        return {"error": f"Failed to read sheets: {e}", "trace": traceback.format_exc()}

    ccure_stats = _fetch_ccure_stats()
    ccure_profile_count = None
    if isinstance(ccure_stats, dict) and ccure_stats.get("TotalProfiles") is not None:
        try:
            ccure_profile_count = int(ccure_stats.get("TotalProfiles"))
        except Exception:
            ccure_profile_count = None

    # Basic difference summary
    reported_active_emps = None
    reported_active_contractors = None
    if isinstance(ccure_stats, dict):
        reported_active_emps = _safe_int(ccure_stats.get("ActiveEmployees"))
        reported_active_contractors = _safe_int(ccure_stats.get("ActiveContractors"))

    diff = {
        "sheet_counts": {
            "employees": int(sheet_emp_count),
            "contractors": int(sheet_contractor_count),
            "total_profiles": int(sheet_emp_count + sheet_contractor_count)
        },
        "ccure": ccure_stats,
        "ccure_profile_count": _safe_int(ccure_profile_count),
        "differences": {
            "ccure_active_employees": _safe_int(reported_active_emps),
            "ccure_active_contractors": _safe_int(reported_active_contractors),
            "delta_employees": (reported_active_emps - sheet_emp_count) if (isinstance(reported_active_emps, int) and isinstance(sheet_emp_count, int)) else None,
            "delta_contractors": (reported_active_contractors - sheet_contractor_count) if (isinstance(reported_active_contractors, int) and isinstance(sheet_contractor_count, int)) else None
        },
        "samples": {
            "in_ccure_employees_not_in_sheet": [],
            "in_ccure_contractors_not_in_sheet": []
        }
    }

    session.close()
    return _sanitize_for_json(diff)







http://localhost:8000/ccure/averages








Calculate Average and percentage initailly and give me correct API responece

C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\app.py

from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Query
from fastapi.responses import JSONResponse, FileResponse
import shutil, uuid, json
from settings import UPLOAD_DIR, OUTPUT_DIR
from pathlib import Path
import logging

app = FastAPI(title="Attendance Analytics")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

@app.get("/ccure/compare")
def ccure_compare(
    mode: str = Query("full", description="full or stats"),
    stats_detail: str = Query("ActiveProfiles", description="when mode=stats use this"),
    limit_list: int = Query(200, ge=1, le=5000, description="max rows returned in list samples"),
    export: bool = Query(False, description="if true, writes Excel report to server and returns report_path")
):
    try:
        from ccure_compare_service import compare_ccure_vs_sheets
    except Exception as e:
        logger.exception("ccure_compare_service import failed")
        raise HTTPException(status_code=500, detail=f"compare service unavailable: {e}")
    # run compare (the function itself is defensive)
    res = compare_ccure_vs_sheets(mode=mode, stats_detail=stats_detail, limit_list=limit_list, export=export)
    # Ensure result is a dict for JSONResponse
    if not isinstance(res, dict):
        return JSONResponse({"error": "compare service returned unexpected result"}, status_code=500)
    return JSONResponse(res)


# NEW: averages endpoint (calls compute_visit_averages)
@app.get("/ccure/averages")
def ccure_averages(timeout: int = Query(6, description="timeout seconds for live-summary requests")):
    """
    Returns:
    {
      "live_today": { "employee": int, "contractor": int, "total": int },
      "ccure_active": { "active_employees": int|None, "active_contractors": int|None },
      "averages": { "employee_pct": float|None, "contractor_pct": float|None, "overall_pct": float|None },
      "sites_queried": int,
      "notes": null | str
    }
    """
    try:
        from ccure_compare_service import compute_visit_averages
    except Exception as e:
        logger.exception("compute_visit_averages import failed")
        raise HTTPException(status_code=500, detail=f"compute_visit_averages unavailable: {e}")

    try:
        res = compute_visit_averages(timeout=timeout)
    except Exception as e:
        logger.exception("compute_visit_averages execution failed")
        raise HTTPException(status_code=500, detail=f"compute_visit_averages failed: {e}")

    if not isinstance(res, dict):
        return JSONResponse({"error": "compute_visit_averages returned unexpected result"}, status_code=500)
    return JSONResponse(res)


@app.get("/ccure/report/{filename}")
def ccure_report_download(filename: str):
    try:
        safe_name = Path(filename).name
        full = Path(OUTPUT_DIR) / safe_name
        if not full.exists() or not full.is_file():
            raise HTTPException(status_code=404, detail="Report not found")
        return FileResponse(
            str(full),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            filename=safe_name
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Failed to serve report")
        raise HTTPException(status_code=500, detail=f"Failed to serve report: {e}")


@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = Path(UPLOAD_DIR) / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_employee_excel
    except Exception as e:
        logger.exception("ingest_excel import failed")
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_employee_excel(dest)
    return {"status":"ok", "path": str(dest)}

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls', '.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = Path(UPLOAD_DIR) / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    try:
        from ingest_excel import ingest_contractor_excel
    except Exception as e:
        logger.exception("ingest_excel import failed")
        raise HTTPException(status_code=500, detail=f"ingest_excel import failed: {e}")
    ingest_contractor_excel(dest)
    return {"status":"ok", "path": str(dest)}

# Keep other endpoints unchanged (ingest/fetch-all, reports/daily)...
# If you want, I can provide the rest verbatim — I left them unchanged to minimize merge issues.








C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\compare_service.py

# compare_service.py
import pandas as pd
import numpy as np
from datetime import datetime, date, timezone
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary
import re
from dateutil import parser as dateutil_parser
import traceback
import logging

logger = logging.getLogger("compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# --- Helpers -----------------------------------------------------------------
def _to_native(value):
    if value is None:
        return None
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    if isinstance(value, (np.integer,)):
        return int(value)
    if isinstance(value, (np.floating,)):
        return float(value)
    if isinstance(value, (np.bool_, bool)):
        return bool(value)
    try:
        import datetime as _dt
        if isinstance(value, _dt.datetime):
            try:
                if value.tzinfo is not None:
                    utc = value.astimezone(timezone.utc)
                    return utc.replace(tzinfo=None).isoformat() + "Z"
                else:
                    return value.isoformat()
            except Exception:
                return str(value)
        if hasattr(value, 'isoformat'):
            try:
                return value.isoformat()
            except Exception:
                return str(value)
    except Exception:
        pass
    return value

def _normalize_employee_key(x):
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s):
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _normalize_name(s):
    if s is None:
        return None
    try:
        t = str(s).strip().lower()
        t = re.sub(r'[^\w\s]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t if t else None
    except Exception:
        return None

# timestamp parsing helpers (unchanged)
def _parse_timestamp_from_value(val):
    if val is None:
        return None
    import datetime as _dt
    if isinstance(val, _dt.datetime):
        dt = val
        try:
            if dt.tzinfo is not None:
                return dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            return dt
    try:
        import numpy as _np
        if isinstance(val, (int, float, _np.integer, _np.floating)):
            v = int(val)
            if v > 1e12:
                return _dt.fromtimestamp(v / 1000.0, tz=timezone.utc).replace(tzinfo=None)
            if v > 1e9:
                return _dt.fromtimestamp(v, tz=timezone.utc).replace(tzinfo=None)
    except Exception:
        pass
    if isinstance(val, str):
        s = val.strip()
        if s == "":
            return None
        try:
            dt = dateutil_parser.parse(s)
            if dt.tzinfo is not None:
                dt = dt.astimezone(timezone.utc).replace(tzinfo=None)
            return dt
        except Exception:
            fmts = ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M:%S.%f",
                    "%d/%m/%Y %H:%M:%S", "%d-%m-%Y %H:%M:%S",
                    "%Y-%m-%dT%H:%M:%S")
            for fmt in fmts:
                try:
                    return _dt.strptime(s, fmt)
                except Exception:
                    pass
    return None

def _extract_timestamp_from_detail(detail):
    fields = [
        "LocaleMessageDateTime", "LocalMessageDateTime", "LocaleMessageTime", "LocalMessageTime",
        "LocaleMessageDate", "Timestamp", "timestamp", "Time", "LocaleTime", "LocalTime",
        "time", "date", "LocaleMessageDateTimeUtc", "LocalMessageDateTimeUtc",
        "Swipe_Time", "SwipeTime", "SwipeTimeLocal", "SwipeTimestamp", "SwipeDateTime"
    ]
    if isinstance(detail, dict):
        for k in fields:
            if k in detail:
                dt = _parse_timestamp_from_value(detail.get(k))
                if dt is not None:
                    return dt
        for v in detail.values():
            dt = _parse_timestamp_from_value(v)
            if dt is not None:
                return dt
    else:
        return _parse_timestamp_from_value(detail)
    return None



# --- Main functions ----------------------------------------------------------

def ingest_live_details_list(details_list):
    """Persist details_list into LiveSwipe. returns counts."""
    from db import SessionLocal as _SessionLocal
    inserted = 0
    skipped = 0
    with _SessionLocal() as db:
        for d in details_list:
            try:
                ts_parsed = _extract_timestamp_from_detail(d)
            except Exception:
                ts_parsed = None
            if ts_parsed is None:
                # skip rows without parseable timestamp
                skipped += 1
                continue

            # robust extraction of employee id and card fields (many alias names)
            emp = None
            for k in ("EmployeeID", "employee_id", "employeeId", "Employee Id", "EmpID", "Emp Id"):
                if isinstance(d, dict) and k in d:
                    emp = d.get(k)
                    break
            emp = _normalize_employee_key(emp)

            card = None
            for k in ("CardNumber", "card_number", "Card", "Card No", "CardNo", "Badge", "BadgeNo", "badge_number", "IPassID", "iPass ID"):
                if isinstance(d, dict) and k in d:
                    card = d.get(k)
                    break
            card = _normalize_card_like(card)

            full_name = None
            for k in ("ObjectName1", "FullName", "full_name", "EmpName", "Name"):
                if isinstance(d, dict) and k in d:
                    full_name = d.get(k)
                    break

            partition = None
            for k in ("PartitionName2", "PartitionName1", "Partition", "PartitionName", "Region"):
                if isinstance(d, dict) and k in d:
                    partition = d.get(k)
                    break

            floor = d.get("Floor") if isinstance(d, dict) else None
            door = None
            for k in ("Door", "DoorName", "door"):
                if isinstance(d, dict) and k in d:
                    door = d.get(k)
                    break

            region = d.get("__region") if isinstance(d, dict) and "__region" in d else d.get("Region") if isinstance(d, dict) else None

            try:
                rec = LiveSwipe(
                    timestamp=ts_parsed,
                    employee_id=emp,
                    card_number=card,
                    full_name=full_name,
                    partition=partition,
                    floor=floor,
                    door=door,
                    region=region,
                    raw=d
                )
                db.add(rec)
                inserted += 1
            except Exception:
                # skip insertion errors but continue
                db.rollback()
                skipped += 1
                continue
        db.commit()
    print(f"[ingest_live_details_list] inserted={inserted} skipped={skipped}")
    return {"inserted": inserted, "skipped_invalid_timestamp": skipped}


def compute_daily_attendance(target_date: date):
    """Build AttendanceSummary rows for target_date (upserts)."""
    with SessionLocal() as db:
        start = datetime.combine(target_date, datetime.min.time())
        end = datetime.combine(target_date, datetime.max.time())
        swipes = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
        if not swipes:
            print(f"[compute_daily_attendance] no swipes for {target_date}")
            return []

        rows = []
        for s in swipes:
            rows.append({
                "id": s.id,
                "timestamp": s.timestamp,
                "employee_id": _normalize_employee_key(s.employee_id),
                "card_number": _normalize_card_like(s.card_number),
                "full_name": s.full_name,
                "partition": s.partition,
                "floor": s.floor,
                "door": s.door
            })
        df = pd.DataFrame(rows)
        if df.empty:
            print(f"[compute_daily_attendance] dataframe empty after rows -> {target_date}")
            return []

        # create grouping key: prefer employee_id, otherwise card_number
        df['key'] = df['employee_id'].fillna(df['card_number'])
        df = df[df['key'].notna()]
        if df.empty:
            print("[compute_daily_attendance] no usable keys after filling employee_id/card")
            return []

        grouped = df.groupby('key', dropna=False).agg(
            presence_count=('id', 'count'),
            first_seen=('timestamp', 'min'),
            last_seen=('timestamp', 'max'),
            full_name=('full_name', 'first'),
            partition=('partition', 'first'),
            card_number=('card_number', 'first')
        ).reset_index().rename(columns={'key': 'employee_id'})

        # upsert AttendanceSummary rows (merge)
        for _, row in grouped.iterrows():
            try:
                derived_obj = {
                    "partition": (row.get('partition') or None),
                    "full_name": (row.get('full_name') or None),
                    "card_number": (row.get('card_number') or None)
                }
                rec = AttendanceSummary(
                    employee_id=str(row['employee_id']) if pd.notna(row['employee_id']) else None,
                    date=target_date,
                    presence_count=int(row['presence_count']),
                    first_seen=row['first_seen'],
                    last_seen=row['last_seen'],
                    derived=derived_obj
                )
                db.merge(rec)
            except Exception as e:
                print("[compute_daily_attendance] upsert error:", e)
                continue
        db.commit()
        print(f"[compute_daily_attendance] built {len(grouped)} attendance keys for {target_date}")
        return grouped.to_dict(orient='records')


def compare_with_active(target_date: date):
    """Compare AttendanceSummary for date with ActiveEmployee & ActiveContractor and return json-safe dict."""
    # NOTE: we intentionally do NOT import get_global_stats_or_none from ccure_client;
    # a local helper wrapper below will call ccure_client.get_global_stats() safely.
    with SessionLocal() as db:
        att_rows = db.query(AttendanceSummary).filter(AttendanceSummary.date == target_date).all()
        if not att_rows:
            att_df = pd.DataFrame(columns=["employee_id", "presence_count", "first_seen", "last_seen", "card_number", "partition", "full_name"])
        else:
            att_df = pd.DataFrame([{
                "employee_id": _normalize_employee_key(a.employee_id),
                "presence_count": a.presence_count,
                "first_seen": a.first_seen,
                "last_seen": a.last_seen,
                "card_number": _normalize_card_like(a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None),
                "partition": (a.derived.get('partition') if (a.derived and isinstance(a.derived, dict)) else None),
                "full_name": (a.derived.get('full_name') if (a.derived and isinstance(a.derived, dict)) else None)
            } for a in att_rows])

        act_rows = db.query(ActiveEmployee).all()
        contractor_rows = db.query(ActiveContractor).all()

        # Build maps & active list
        act_list = []
        card_to_emp = {}
        name_to_emp = {}

        # Employees
        for e in act_rows:
            emp_id_norm = _normalize_employee_key(e.employee_id)
            # extract card-like from raw if present
            card_from_raw = None
            try:
                rr = e.raw_row or {}
                if isinstance(rr, dict):
                    ck_list = [
                        "CardNumber","card_number","Card","Card No","CardNo","IPassID","IpassID","iPass ID","IPASSID",
                        "Badge Number","BadgeNo","Badge"
                    ]
                    for ck in ck_list:
                        v = rr.get(ck)
                        if v:
                            ckey = _normalize_card_like(v)
                            if ckey:
                                card_from_raw = ckey
                                break
                    # fallback: scan all values for numeric candidate
                    if not card_from_raw:
                        for v in rr.values():
                            try:
                                tmp = _normalize_card_like(v)
                                if tmp and 3 <= len(tmp) <= 12:
                                    card_from_raw = tmp
                                    break
                            except Exception:
                                pass
            except Exception:
                card_from_raw = None

            act_list.append({
                "employee_id": emp_id_norm,
                "full_name": e.full_name,
                "location_city": e.location_city,
                "status": e.current_status,
                "card_number": card_from_raw
            })
            if emp_id_norm:
                card_to_emp[emp_id_norm] = emp_id_norm
            if card_from_raw:
                card_to_emp[card_from_raw] = emp_id_norm
            n = _normalize_name(e.full_name)
            if n:
                name_to_emp[n] = emp_id_norm

        # Contractors
        for c in contractor_rows:
            worker_id = _normalize_employee_key(c.worker_system_id)
            ipass = _normalize_employee_key(c.ipass_id)
            w_ipass = ("W" + ipass) if ipass and not str(ipass).startswith("W") else ipass
            primary_id = worker_id or ipass or None
            act_list.append({
                "employee_id": primary_id,
                "full_name": c.full_name,
                "location_city": c.location,
                "status": c.status,
                "card_number": None
            })
            if primary_id:
                card_to_emp[primary_id] = primary_id
            if ipass:
                card_to_emp[ipass] = primary_id
            if w_ipass:
                card_to_emp[w_ipass] = primary_id
            try:
                rr = c.raw_row or {}
                if isinstance(rr, dict):
                    for ck in ("Worker System Id","Worker System ID","iPass ID","IPASSID","CardNumber","card_number"):
                        if ck in rr and rr.get(ck):
                            key = _normalize_card_like(rr.get(ck))
                            if key:
                                card_to_emp[key] = primary_id
            except Exception:
                pass
            n = _normalize_name(c.full_name)
            if n:
                name_to_emp[n] = primary_id

        act_df = pd.DataFrame(act_list)

        # If no active rows, return attendance-only view
        if act_df.empty:
            if att_df.empty:
                return {"by_location": [], "merged": [], "ccure": get_global_stats_or_none()}
            att_df['partition'] = att_df.get('partition').fillna('Unknown')
            att_df['presence_count'] = att_df['presence_count'].fillna(0)
            att_df['present_today'] = att_df['presence_count'].apply(lambda x: bool(x and x != 0))
            loc_group = att_df.groupby('partition', dropna=False).agg(
                total_n=('employee_id', 'count'),
                present_n=('present_today', 'sum')
            ).reset_index().rename(columns={'partition':'location_city'})
            loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
            by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]
            merged_list = []
            for r in att_df.to_dict(orient='records'):
                merged_list.append({
                    "employee_id": _to_native(r.get('employee_id')),
                    "presence_count": _to_native(r.get('presence_count')),
                    "first_seen": _to_native(r.get('first_seen')),
                    "last_seen": _to_native(r.get('last_seen')),
                    "full_name": _to_native(r.get('full_name')),
                    "location_city": _to_native(r.get('partition')),
                    "present_today": _to_native(r.get('present_today'))
                })
            return {"by_location": by_location, "merged": merged_list, "ccure": get_global_stats_or_none()}

        # normalize columns
        act_df['employee_id'] = act_df['employee_id'].astype(object).apply(_normalize_employee_key)
        att_df['employee_id'] = att_df['employee_id'].astype(object).apply(_normalize_employee_key)
        act_df['card_number'] = act_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in act_df.columns else pd.Series([pd.NA]*len(act_df))
        att_df['card_number'] = att_df.get('card_number').astype(object).apply(_normalize_card_like) if 'card_number' in att_df.columns else pd.Series([pd.NA]*len(att_df))

        # ensure card_to_emp includes act_df card_numbers
        for r in act_df.to_dict(orient='records'):
            c = r.get('card_number')
            eid = r.get('employee_id')
            if c and eid:
                card_to_emp[c] = eid
            if eid:
                # also map numeric-only forms of eid
                n = re.sub(r'\D','', str(eid))
                if n:
                    card_to_emp[n.lstrip('0') or n] = eid

        # mapping function tries multiple strategies
        emp_set = set([x for x in act_df['employee_id'].dropna().astype(str)])

        def numeric_variants(s):
            s = str(s)
            clean = re.sub(r'\D','', s)
            variants = set()
            if clean:
                variants.add(clean)
                variants.add(clean.lstrip('0') or clean)
                if not s.startswith('W'):
                    variants.add('W' + clean)
            return list(variants)

        def remap_att_key(row):
            primary = row.get('employee_id') or None
            card = row.get('card_number') or None

            primary_norm = _normalize_employee_key(primary)
            card_norm = _normalize_card_like(card)

            # 1) exact employee id exists in active list
            if primary_norm and primary_norm in emp_set:
                return primary_norm

            # 2) numeric-variants of primary may map to card_to_emp
            if primary_norm:
                for v in numeric_variants(primary_norm):
                    if v in card_to_emp:
                        return card_to_emp[v]
                if primary_norm in card_to_emp:
                    return card_to_emp[primary_norm]

            # 3) direct card mapping
            if card_norm:
                if card_norm in card_to_emp:
                    return card_to_emp[card_norm]
                if (card_norm.lstrip('0') or card_norm) in card_to_emp:
                    return card_to_emp[card_norm.lstrip('0') or card_norm]
                if ('W' + card_norm) in card_to_emp:
                    return card_to_emp['W' + card_norm]

            # 4) name matching fallback
            fname = _normalize_name(row.get('full_name') or row.get('full_name_att') or None)
            if fname and fname in name_to_emp:
                return name_to_emp[fname]

            # 5) last resort - return primary_norm (maybe non-mapped) so it still shows up
            return primary_norm or card_norm or None

        att_df['mapped_employee_id'] = att_df.apply(remap_att_key, axis=1)

        # drop original employee_id column to avoid duplicate label conflict
        att_merge_df = att_df.drop(columns=['employee_id'], errors='ignore').copy()

        # merge left: act_df left_on employee_id, right_on mapped_employee_id
        merged = pd.merge(
            act_df,
            att_merge_df,
            left_on='employee_id',
            right_on='mapped_employee_id',
            how='left',
            suffixes=('', '_att')
        )

        # fill and finalize
        merged['presence_count'] = merged.get('presence_count', pd.Series([0]*len(merged))).fillna(0)
        # ensure ints when possible
        def safe_int(v):
            try:
                if pd.isna(v):
                    return 0
                iv = int(float(v))
                return iv
            except Exception:
                return v
        merged['presence_count'] = merged['presence_count'].apply(safe_int)
        merged['present_today'] = merged['presence_count'].apply(lambda x: bool(x and x != 0))
        merged['location_city'] = merged.get('location_city').fillna('Unknown')

        # by_location
        loc_group = merged.groupby('location_city', dropna=False).agg(
            total_n=('employee_id', 'count'),
            present_n=('present_today', 'sum')
        ).reset_index()
        loc_group['percent_present'] = loc_group.apply(lambda row: round((row['present_n']/row['total_n'])*100,2) if row['total_n'] and row['total_n']>0 else 0.0, axis=1)
        by_location = [{k:_to_native(v) for k,v in r.items()} for r in loc_group.to_dict(orient='records')]

        merged_list = []
        for r in merged.to_dict(orient='records'):
            clean = {k:_to_native(v) for k,v in r.items()}
            # unify keys for clarity in API response
            clean['mapped_employee_id'] = clean.get('mapped_employee_id')
            clean['card_number_att'] = clean.get('card_number') or clean.get('card_number_att') or None
            # include status if present
            if 'status' not in clean:
                clean['status'] = None
            # ensure employee_id key exists
            if 'employee_id' not in clean:
                clean['employee_id'] = None
            merged_list.append(clean)

        # CCURE stats fetch (best-effort)
        ccure_stats = get_global_stats_or_none()

        # compare counts summary between CCure and Active sheets
        try:
            ccure_summary = ccure_stats or {}
            cc_total_profiles = ccure_summary.get('TotalProfiles')
            cc_active_profiles = ccure_summary.get('ActiveProfiles')
            cc_active_emps = ccure_summary.get('ActiveEmployees')
            cc_active_contractors = ccure_summary.get('ActiveContractors')
        except Exception:
            cc_total_profiles = cc_active_profiles = cc_active_emps = cc_active_contractors = None

        # local sheet counts
        active_emp_count = len(act_rows)
        active_contract_count = len(contractor_rows)

        diff = {
            "active_sheet_employee_count": active_emp_count,
            "active_sheet_contractor_count": active_contract_count,
            "ccure_active_employees": cc_active_emps,
            "ccure_active_contractors": cc_active_contractors,
            "delta_employees": (cc_active_emps - active_emp_count) if (isinstance(cc_active_emps, int) and isinstance(active_emp_count, int)) else None,
            "delta_contractors": (cc_active_contractors - active_contract_count) if (isinstance(cc_active_contractors, int) and isinstance(active_contract_count, int)) else None
        }

        result = {
            "by_location": by_location,
            "merged": merged_list,
            "ccure": ccure_stats,
            "count_comparison": diff
        }
        return result

# Helper wrapper
def get_global_stats_or_none():
    try:
        from ccure_client import get_global_stats
        return get_global_stats()
    except Exception:
        logger.debug("ccure_client.get_global_stats not available", exc_info=True)
        return None






# region_clients.py
import requests
from requests.exceptions import RequestException

# Edit endpoints if your hosts/ports differ
endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/live-summary",
    "emea":  "http://10.199.22.57:3007/api/occupancy/live-summary",
    "laca":  "http://10.199.22.57:4000/api/occupancy/live-summary",
    "apac":  "http://10.199.22.57:3008/api/occupancy/live-summary"
}

def fetch_all_regions():
    """Return list of dicts: [{region: name, count: N}, ...]"""
    results = []
    for region, url in endpoints.items():
        try:
            r = requests.get(url, timeout=6)
            r.raise_for_status()
            data = r.json()
            realtime = data.get("realtime", {}) if isinstance(data, dict) else {}
            total = 0
            for site in realtime.values():
                try:
                    total += int(site.get("total", 0))
                except Exception:
                    pass
            results.append({"region": region, "count": total})
        except RequestException as e:
            # log to console for now
            print(f"[region_clients] error fetching {region} @ {url}: {e}")
            results.append({"region": region, "count": None})
    return results

def fetch_all_details():
    """
    Return flattened 'details' list across all regions (tagged with '__region').
    Returns an empty list if none available.
    """
    all_details = []
    for region, url in endpoints.items():
        try:
            r = requests.get(url, timeout=6)
            r.raise_for_status()
            data = r.json()
            details = data.get("details", []) if isinstance(data, dict) else []
            for d in details:
                d2 = dict(d)
                d2["__region"] = region
                all_details.append(d2)
        except RequestException as e:
            print(f"[region_clients] warning: cannot fetch details from {region}@{url}: {e}")
            continue
        except Exception as e:
            print(f"[region_clients] unexpected error for {region}: {e}")
            continue
    return all_details






