from fastapi import FastAPI, Query, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import logging
from datetime import date, datetime, timedelta
from typing import Optional, Dict, Any, List
import asyncio

from ccure_compare_service import compute_visit_averages, map_to_unified_summary

app = FastAPI(title="Attendance Analytics - Single Averages API")

logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# Tunables (kept in sync with compute module)
COMPUTE_WAIT_TIMEOUT_SECONDS = 60
REGION_TIMEOUT_SECONDS = 20

@app.get("/ccure/averages")
async def ccure_averages(
    raw: bool = Query(False, description="include raw compute payload"),
    start_date: Optional[str] = Query(None, description="YYYY-MM-DD start date (inclusive)"),
    end_date: Optional[str] = Query(None, description="YYYY-MM-DD end date (inclusive)")
):
    """
    Single consolidated API endpoint - uses compute_visit_averages (history-first for headcount).
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    try:
        if loop and loop.is_running():
            detailed = await asyncio.wait_for(
                loop.run_in_executor(None, compute_visit_averages, start_date, end_date, REGION_TIMEOUT_SECONDS),
                timeout=COMPUTE_WAIT_TIMEOUT_SECONDS
            )
        else:
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
    except asyncio.TimeoutError:
        logger.warning("compute_visit_averages timed out; attempting fallback compute synchronously")
        try:
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
        except Exception:
            logger.exception("Fallback compute failed after timeout")
            raise HTTPException(status_code=500, detail="compute timed out and fallback failed")
    except Exception:
        logger.exception("compute_visit_averages raised; attempting fallback compute synchronously")
        try:
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
        except Exception:
            logger.exception("Fallback compute failed")
            raise HTTPException(status_code=500, detail="compute failed and fallback failed")

    if not isinstance(detailed, dict):
        raise HTTPException(status_code=500, detail="compute returned unexpected result")

    summary = map_to_unified_summary(detailed, include_raw=raw)
    return JSONResponse(summary)








import re
import json
from datetime import date, datetime, timedelta
from typing import List, Dict, Any, Optional, Set
import time
import logging

logger = logging.getLogger("ccure_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

from db import SessionLocal
from models import AttendanceSummary, LiveSwipe
from settings import OUTPUT_DIR

# ---------- small helpers ----------------------------------------------------

def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

def _sanitize_for_json(value):
    try:
        import numpy as _np
    except Exception:
        _np = None
    if value is None:
        return None
    if isinstance(value, (str, bool, int)):
        return value
    if isinstance(value, float):
        if _np is not None and not _np.isfinite(value):
            return None
        return float(value)
    if _np is not None and isinstance(value, (_np.integer,)):
        return int(value)
    if isinstance(value, dict):
        out = {}
        for k, v in value.items():
            try:
                key = str(k)
            except Exception:
                key = repr(k)
            out[key] = _sanitize_for_json(v)
        return out
    if isinstance(value, (list, tuple, set)):
        return [_sanitize_for_json(v) for v in value]
    try:
        return str(value)
    except Exception:
        return None

# ---------- ccure helpers ---------------------------------------------------

def _fetch_ccure_stats():
    try:
        import ccure_client
        if hasattr(ccure_client, "get_global_stats"):
            return ccure_client.get_global_stats()
    except Exception:
        logger.debug("ccure_client.get_global_stats not available", exc_info=True)
    return None

def _fetch_ccure_profiles():
    try:
        import ccure_client
        for fn in ("fetch_all_employees_full", "fetch_all_employees", "fetch_all_profiles", "fetch_profiles", "fetch_all"):
            if hasattr(ccure_client, fn):
                try:
                    res = getattr(ccure_client, fn)()
                    if isinstance(res, list):
                        return res
                except Exception:
                    continue
    except Exception:
        pass
    return []

def _extract_ccure_locations_from_profiles(profiles: List[dict]) -> Set[str]:
    locs = set()
    for p in profiles:
        if not isinstance(p, dict):
            continue
        for k in ("Partition", "PartitionName", "Location", "Location City", "location_city", "location", "Site", "BaseLocation"):
            v = p.get(k) if isinstance(p, dict) else None
            if v and isinstance(v, str) and v.strip():
                locs.add(v.strip())
    return locs

# ---------- classification & partition helpers ------------------------------

def classify_personnel_from_detail(detail: dict) -> str:
    try:
        if not isinstance(detail, dict):
            return "contractor"
        candidate_keys = [
            "PersonnelType", "personnelType", "personnel_type", "Personnel Type",
            "PersonnelTypeName", "Personnel", "Type", "personnel", "PersonType", "personType"
        ]
        val = None
        for k in candidate_keys:
            if k in detail and detail.get(k) is not None:
                val = str(detail.get(k)).strip().lower()
                break
        status_keys = ["Employee_Status", "Employee Status", "Status", "Profile_Disabled"]
        status_val = None
        for k in status_keys:
            if k in detail and detail.get(k) is not None:
                status_val = str(detail.get(k)).strip().lower()
                break

        if status_val is not None and "terminated" in status_val:
            return "employee"
        if val is None or val == "":
            return "contractor"
        if "employee" in val:
            return "employee"
        contractor_terms = ["contractor", "visitor", "property", "temp", "temp badge", "tempbadge"]
        for t in contractor_terms:
            if t in val:
                return "contractor"
        if "contract" in val or "visitor" in val:
            return "contractor"
        return "contractor"
    except Exception:
        return "contractor"

def pick_partition_from_detail(detail: dict) -> str:
    if not isinstance(detail, dict):
        return "Unknown"
    for k in ("PartitionName2","PartitionName1","Partition","PartitionName","Region","Location","Site","location_city","Location City"):
        if k in detail and detail.get(k):
            try:
                return str(detail.get(k)).strip()
            except Exception:
                continue
    if "__region" in detail and detail.get("__region"):
        return str(detail.get("__region")).strip()
    return "Unknown"

# ---------- small resilient fetch helper for region_clients -----------------
def _attempt_region_call(fn, timeout, attempts=2, backoff=0.5):
    last_exc = None
    for i in range(attempts):
        try:
            res = fn(timeout=timeout)
            if res is not None:
                return res
        except Exception as e:
            last_exc = e
            logger.warning("[region_clients] attempt %d/%d failed: %s", i+1, attempts, e)
            time.sleep(backoff * (i+1))
            continue
    logger.warning("[region_clients] all %d attempts failed for %s: last_exc=%s", attempts, getattr(fn, "__name__", str(fn)), last_exc)
    return None

# ---------- utility: fallback headcount builder from LiveSwipe --------------

def build_headcount_from_liveswipes_for_today(session) -> (int, Dict[str, Dict[str, int]]):
    start = datetime.combine(date.today(), datetime.min.time())
    end = datetime.combine(date.today(), datetime.max.time())
    swipes = session.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
    if not swipes:
        return 0, {}
    seen_keys = {}
    per_loc = {}
    for s in swipes:
        key = _normalize_employee_key(s.employee_id) or _normalize_card_like(s.card_number)
        if not key:
            key = f"nokey_{s.id}"
        rec = seen_keys.get(key)
        ts = s.timestamp
        if rec is None:
            seen_keys[key] = {"first_seen": ts, "last_seen": ts, "partition": (s.partition or "Unknown"), "class": None, "card": s.card_number, "raw": s.raw}
        else:
            if ts and rec.get("first_seen") and ts < rec["first_seen"]:
                rec["first_seen"] = ts
            if ts and rec.get("last_seen") and ts > rec["last_seen"]:
                rec["last_seen"] = ts
    for k, v in seen_keys.items():
        loc = v.get("partition") or "Unknown"
        if not isinstance(loc, str) or not loc.strip():
            loc = "Unknown"
        if loc not in per_loc:
            per_loc[loc] = {"total": 0, "employee": 0, "contractor": 0}
        per_loc[loc]["total"] += 1
        classified = "contractor"
        raw = v.get("raw")
        if isinstance(raw, dict):
            try:
                classified = classify_personnel_from_detail(raw)
            except Exception:
                classified = "contractor"
        per_loc[loc][classified] += 1
    total = sum(p["total"] for p in per_loc.values())
    return int(total), per_loc

# ---------- main compute function -----------------------------------------

def compute_visit_averages(start_date: Optional[str] = None, end_date: Optional[str] = None, timeout: int = 6) -> Dict[str, Any]:
    """
    Compute visit averages for an inclusive date range.
    History-first for headcount (uses region_clients.fetch_all_history for today's headcount if available).
    """
    notes = []
    today = date.today()

    def _parse_date_param(s):
        if not s:
            return None
        try:
            return datetime.strptime(s, "%Y-%m-%d").date()
        except Exception:
            try:
                return date.fromisoformat(s)
            except Exception:
                return None

    start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
    end_obj = _parse_date_param(end_date) if end_date else today
    if start_obj is None or end_obj is None or start_obj > end_obj:
        start_obj = today - timedelta(days=6)
        end_obj = today
        notes.append("Invalid or missing date range; defaulted to last 7 calendar days inclusive.")

    # gather CCURE stats
    ccure_stats = _fetch_ccure_stats()
    reported_active_emps = _safe_int(ccure_stats.get("ActiveEmployees")) if isinstance(ccure_stats, dict) else None
    reported_active_contractors = _safe_int(ccure_stats.get("ActiveContractors")) if isinstance(ccure_stats, dict) else None

    # Try to build HEADCOUNT from region history endpoints for TODAY first (strict requirement)
    head_total = 0
    head_per_location: Dict[str, Dict[str, int]] = {}
    key_map_head: Dict[str, Dict[str, Any]] = {}
    key_map_live: Dict[str, Dict[str, Any]] = {}

    try:
        import region_clients
        history_entries = _attempt_region_call(region_clients.fetch_all_history, timeout=timeout) or []
        # parse today's entries' partitions
        today_iso = today.isoformat()
        used_history_for_headcount = False
        for e in history_entries:
            try:
                dstr = e.get("date") or e.get("day") or None
                if isinstance(dstr, datetime):
                    dstr = dstr.date().isoformat()
                if not dstr:
                    # some history entries may include top-level date differently - try normalized forms
                    # skip if not today's date
                    continue
                if dstr != today_iso:
                    continue
                parts = e.get("partitions") if isinstance(e.get("partitions"), dict) else {}
                # If partitions present, use them for headcount by location
                if parts:
                    used_history_for_headcount = True
                    for pname, pstat in parts.items():
                        try:
                            p_emp = _safe_int(pstat.get("Employee")) or 0
                            p_con = _safe_int(pstat.get("Contractor")) or 0
                            p_tot = _safe_int(pstat.get("total"))
                            if p_tot is None:
                                p_tot = p_emp + p_con
                            if pname not in head_per_location:
                                head_per_location[pname] = {"total": 0, "employee": 0, "contractor": 0}
                            head_per_location[pname]["employee"] += int(p_emp)
                            head_per_location[pname]["contractor"] += int(p_con)
                            head_per_location[pname]["total"] += int(p_tot)
                            head_total += int(p_tot)
                        except Exception:
                            continue
                else:
                    # If no partitions but a region-level total exists, create a synthetic partition entry using region name
                    region_name = None
                    try:
                        region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                        if region_obj:
                            region_name = region_obj.get("name") or region_obj.get("region") or e.get("__region")
                            r_emp = _safe_int(region_obj.get("Employee")) or _safe_int(e.get("Employee")) or 0
                            r_con = _safe_int(region_obj.get("Contractor")) or _safe_int(e.get("Contractor")) or 0
                            r_tot = _safe_int(region_obj.get("total")) or (r_emp + r_con)
                        else:
                            region_name = e.get("__region") or e.get("region") or "Unknown"
                            r_emp = _safe_int(e.get("Employee")) or 0
                            r_con = _safe_int(e.get("Contractor")) or 0
                            r_tot = _safe_int(e.get("total")) or (r_emp + r_con)
                        if region_name:
                            pname = f"{region_name}"
                            if pname not in head_per_location:
                                head_per_location[pname] = {"total": 0, "employee": 0, "contractor": 0}
                            head_per_location[pname]["employee"] += int(r_emp)
                            head_per_location[pname]["contractor"] += int(r_con)
                            head_per_location[pname]["total"] += int(r_tot)
                            head_total += int(r_tot)
                            used_history_for_headcount = True
                    except Exception:
                        continue
        if used_history_for_headcount:
            notes.append("Headcount (visited today) derived from region history endpoints (occupancy/history).")
    except Exception:
        logger.exception("region_clients.fetch_all_history failed for headcount; will fall back to DB/LiveSwipe")

    # If history didn't provide today's headcount, fall back to AttendanceSummary DB and LiveSwipe fallback
    if not head_per_location:
        try:
            session = SessionLocal()
            att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
            if not att_rows_today:
                # try to build from compute_daily_attendance (if available), else fallback to LiveSwipe
                try:
                    from compare_service import compute_daily_attendance as _compute_daily_attendance
                    built = _compute_daily_attendance(today)
                    if isinstance(built, list) and built:
                        att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                        notes.append("AttendanceSummary was missing; built from LiveSwipe via compute_daily_attendance().")
                except Exception:
                    pass

                if not att_rows_today:
                    built_total, built_per_loc = build_headcount_from_liveswipes_for_today(session)
                    head_total = built_total
                    head_per_location = built_per_loc
                    if head_total > 0:
                        notes.append("AttendanceSummary for today empty; built headcount from LiveSwipe rows (non-persistent fallback).")
            if att_rows_today:
                for a in att_rows_today:
                    key = _normalize_employee_key(a.employee_id) or _normalize_card_like((a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None))
                    partition = None
                    try:
                        if a.derived and isinstance(a.derived, dict):
                            partition = a.derived.get("partition")
                    except Exception:
                        partition = None
                    loc = partition or "Unknown"
                    if not isinstance(loc, str) or not loc.strip():
                        loc = "Unknown"
                    if (a.presence_count or 0) > 0:
                        cls = "contractor"
                        try:
                            if a.derived and isinstance(a.derived, dict):
                                cls = classify_personnel_from_detail(a.derived)
                        except Exception:
                            cls = "contractor"
                        if key:
                            key_map_head[key] = {"loc": loc, "cls": cls}
                        if loc not in head_per_location:
                            head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                        head_per_location[loc][cls] += 1
                        head_per_location[loc]["total"] += 1
                        head_total += 1
            session.expunge_all()
        except Exception:
            logger.exception("Error computing HeadCount from DB")
            notes.append("Failed to compute HeadCount from DB; see server logs.")
        finally:
            try:
                session.close()
            except Exception:
                pass

    # --- LIVE HEADCOUNT via region_clients (realtime) ----------
    live_total = 0
    live_per_location: Dict[str, Dict[str, int]] = {}
    sites_queried = 0
    derived_detail_sum = 0
    details: List[dict] = []
    try:
        import region_clients
        regions_info = _attempt_region_call(region_clients.fetch_all_regions, timeout=timeout) or []
        details = _attempt_region_call(region_clients.fetch_all_details, timeout=timeout) or []
        sites_queried = len(regions_info) if isinstance(regions_info, list) else 0

        # sum region totals
        if regions_info:
            for r in regions_info:
                try:
                    c = r.get("count") if isinstance(r, dict) else None
                    ci = _safe_int(c)
                    if ci is not None:
                        live_total += int(ci)
                except Exception:
                    continue

        # parse per-person details for breakdown and dedupe
        if details and isinstance(details, list):
            for d in details:
                try:
                    loc = pick_partition_from_detail(d) or "Unknown"
                    if not isinstance(loc, str) or not loc.strip():
                        loc = "Unknown"
                    pclass = classify_personnel_from_detail(d)
                    key = _normalize_employee_key(d.get("EmployeeID")) or _normalize_card_like(d.get("CardNumber")) or (d.get("PersonGUID") if d.get("PersonGUID") else None)
                    if not key:
                        key = _normalize_employee_key(d.get("employee_id")) or _normalize_card_like(d.get("Card")) or None
                    if not key:
                        key = f"detail_{derived_detail_sum}_{str(hash(json.dumps(d, default=str)))}"
                    key = str(key)
                    key_map_live[key] = {"loc": loc, "cls": pclass}
                    if loc not in live_per_location:
                        live_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                    live_per_location[loc]["total"] += 1
                    live_per_location[loc][pclass] += 1
                    derived_detail_sum += 1
                except Exception:
                    continue

            # reconciliation: prefer region totals for overall, details for breakdown
            if live_total == 0 and derived_detail_sum > 0:
                live_total = derived_detail_sum
            else:
                if live_total != derived_detail_sum:
                    notes.append(f"Region totals ({live_total}) differ from detail rows ({derived_detail_sum}); using region totals for overall and details for breakdown.")
        else:
            notes.append("No per-person details available from region_clients; live breakdown unavailable.")
    except Exception:
        logger.exception("Error computing Live HeadCount")
        notes.append("Failed to compute Live HeadCount; see logs.")
        live_total = live_total or 0

    # ---------- Ensure headcount is union(head_keys, live_keys) ----------
    try:
        head_keys = set(k for k in key_map_head.keys() if k)
        live_keys = set(k for k in key_map_live.keys() if k)
        union_keys = head_keys.union(live_keys)

        unified_head_per_location: Dict[str, Dict[str, int]] = {}
        for k in union_keys:
            if k in key_map_head:
                loc = key_map_head[k].get("loc") or "Unknown"
                cls = key_map_head[k].get("cls") or "contractor"
            else:
                loc = key_map_live.get(k, {}).get("loc") or "Unknown"
                cls = key_map_live.get(k, {}).get("cls") or "contractor"
            if not isinstance(loc, str) or not loc.strip():
                loc = "Unknown"
            if loc not in unified_head_per_location:
                unified_head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
            unified_head_per_location[loc]["total"] += 1
            unified_head_per_location[loc][cls] += 1

        if union_keys:
            # If union created, prefer the union for head_per_location, and set head_total accordingly
            head_per_location = unified_head_per_location
            head_total = len(union_keys)
    except Exception:
        logger.exception("Error reconciling headcount union with live details")

    # --- Averages from AttendanceSummary (DB) - per-location & overall (range-based) ---
    avg_headcount_last_range = None
    avg_headcount_per_site_last_range = None
    avg_by_location_last_range: Dict[str, Dict[str, Any]] = {}

    try:
        session = SessionLocal()
        days = []
        days_count = (end_obj - start_obj).days + 1
        for i in range(days_count):
            days.append(start_obj + timedelta(days=i))

        loc_day_vals: Dict[str, Dict[str, List[int]]] = {}
        for d in days:
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            per_loc_counts: Dict[str, Dict[str, int]] = {}
            if rows:
                for r in rows:
                    try:
                        if (r.presence_count or 0) <= 0:
                            continue
                        partition = None
                        try:
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                        except Exception:
                            partition = None
                        loc = partition or "Unknown"
                        if not isinstance(loc, str) or not loc.strip():
                            loc = "Unknown"
                        if loc not in per_loc_counts:
                            per_loc_counts[loc] = {"employee": 0, "contractor": 0, "total": 0}
                        cls = "contractor"
                        try:
                            if r.derived and isinstance(r.derived, dict):
                                cls = classify_personnel_from_detail(r.derived)
                        except Exception:
                            cls = "contractor"
                        per_loc_counts[loc][cls] += 1
                        per_loc_counts[loc]["total"] += 1
                    except Exception:
                        continue
            for loc, counts in per_loc_counts.items():
                if loc not in loc_day_vals:
                    loc_day_vals[loc] = {"employee": [], "contractor": [], "total": []}
                loc_day_vals[loc]["employee"].append(counts.get("employee", 0))
                loc_day_vals[loc]["contractor"].append(counts.get("contractor", 0))
                loc_day_vals[loc]["total"].append(counts.get("total", 0))

        for loc, lists in loc_day_vals.items():
            emp_list = lists.get("employee", [])
            con_list = lists.get("contractor", [])
            tot_list = lists.get("total", [])
            days_counted = len(tot_list)
            if days_counted == 0:
                continue
            avg_emp = round(sum(emp_list) / float(days_counted), 2)
            avg_con = round(sum(con_list) / float(days_counted), 2)
            avg_tot = round(sum(tot_list) / float(days_counted), 2)
            avg_by_location_last_range[loc] = {
                "history_days_counted": int(days_counted),
                "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
            }

        days_totals = []
        for d in days:
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            day_total = 0
            if rows:
                for r in rows:
                    if (r.presence_count or 0) > 0:
                        day_total += 1
            days_totals.append(day_total)
        if days_totals:
            avg_headcount_last_range = round(sum(days_totals) / float(len(days_totals)), 2)
            if sites_queried and sites_queried > 0:
                avg_headcount_per_site_last_range = round((sum(days_totals) / float(len(days_totals))) / float(sites_queried), 2)
        session.close()
    except Exception:
        logger.exception("Error computing averages from AttendanceSummary")
        notes.append("Failed to compute historical averages from AttendanceSummary; partial results only.")

    # --- HISTORY AVERAGES via region_clients.fetch_all_history (range-based) ----------
    history_emp_avg = None
    history_contractor_avg = None
    history_overall_avg = None
    history_days = 0
    history_avg_by_location: Dict[str, Dict[str, Any]] = {}
    history_today_emp = None
    history_today_con = None

    try:
        import region_clients
        if hasattr(region_clients, "fetch_all_history"):
            entries = _attempt_region_call(region_clients.fetch_all_history, timeout=timeout) or []
            agg_by_date = {}
            agg_partitions_by_date = {}
            for e in entries:
                try:
                    dstr = e.get("date")
                    if not dstr:
                        dstr = e.get("day") or e.get("timestamp") or None
                        if isinstance(dstr, datetime):
                            dstr = dstr.date().isoformat()
                    if not dstr:
                        continue
                    region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                    emp = None
                    con = None
                    tot = None
                    if region_obj and isinstance(region_obj, dict):
                        emp = _safe_int(region_obj.get("Employee"))
                        con = _safe_int(region_obj.get("Contractor"))
                        tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                    else:
                        emp = _safe_int(e.get("Employee"))
                        con = _safe_int(e.get("Contractor"))
                        tot = _safe_int(e.get("total"))
                    if emp is None and con is None:
                        try:
                            robj = e.get("region") or {}
                            if isinstance(robj, dict):
                                emp = _safe_int(robj.get("Employee"))
                                con = _safe_int(robj.get("Contractor"))
                                tot = _safe_int(robj.get("total"))
                        except Exception:
                            pass
                    if emp is None and con is None:
                        continue
                    if tot is None:
                        tot = (emp or 0) + (con or 0)
                    if dstr not in agg_by_date:
                        agg_by_date[dstr] = {"employee": 0, "contractor": 0, "total": 0, "counted_regions": 0}
                    agg_by_date[dstr]["employee"] += (emp or 0)
                    agg_by_date[dstr]["contractor"] += (con or 0)
                    agg_by_date[dstr]["total"] += (tot or 0)
                    agg_by_date[dstr]["counted_regions"] += 1

                    parts = e.get("partitions") if isinstance(e.get("partitions"), dict) else {}
                    if dstr not in agg_partitions_by_date:
                        agg_partitions_by_date[dstr] = {}
                    for pname, pstat in parts.items():
                        try:
                            p_emp = _safe_int(pstat.get("Employee"))
                            p_con = _safe_int(pstat.get("Contractor"))
                            p_tot = _safe_int(pstat.get("total")) or ((p_emp or 0) + (p_con or 0))
                            if pname not in agg_partitions_by_date[dstr]:
                                agg_partitions_by_date[dstr][pname] = {"employee": 0, "contractor": 0, "total": 0}
                            agg_partitions_by_date[dstr][pname]["employee"] += (p_emp or 0)
                            agg_partitions_by_date[dstr][pname]["contractor"] += (p_con or 0)
                            agg_partitions_by_date[dstr][pname]["total"] += (p_tot or 0)
                        except Exception:
                            continue
                except Exception:
                    continue

            today_iso = today.isoformat()
            if today_iso in agg_by_date:
                history_today_emp = agg_by_date[today_iso].get("employee", 0)
                history_today_con = agg_by_date[today_iso].get("contractor", 0)

            day_vals_emp = []
            day_vals_con = []
            day_vals_tot = []
            selected_dates = []
            for i in range(0, (end_obj - start_obj).days + 1):
                dcheck = (start_obj + timedelta(days=i)).isoformat()
                entry = agg_by_date.get(dcheck)
                if entry:
                    day_vals_emp.append(entry.get("employee", 0))
                    day_vals_con.append(entry.get("contractor", 0))
                    day_vals_tot.append(entry.get("total", 0))
                    selected_dates.append(dcheck)

            if day_vals_emp:
                history_emp_avg = round(sum(day_vals_emp) / float(len(day_vals_emp)), 2)
            if day_vals_con:
                history_contractor_avg = round(sum(day_vals_con) / float(len(day_vals_con)), 2)
            if day_vals_tot:
                history_overall_avg = round(sum(day_vals_tot) / float(len(day_vals_tot)), 2)
            history_days = len(day_vals_tot)
            if history_days == 0:
                notes.append("History endpoints returned no usable rows in requested range; history averages not available.")

            # per-partition averages
            partition_day_values = {}
            for d_iso in selected_dates:
                per_parts = agg_partitions_by_date.get(d_iso, {})
                for pname, pvals in per_parts.items():
                    if pname not in partition_day_values:
                        partition_day_values[pname] = {"employee": [], "contractor": [], "total": []}
                    partition_day_values[pname]["employee"].append(pvals.get("employee", 0))
                    partition_day_values[pname]["contractor"].append(pvals.get("contractor", 0))
                    partition_day_values[pname]["total"].append(pvals.get("total", 0))
            for pname, lists in partition_day_values.items():
                emp_list = lists.get("employee", [])
                con_list = lists.get("contractor", [])
                tot_list = lists.get("total", [])
                days_counted = len(tot_list)
                if days_counted == 0:
                    continue
                avg_emp = round(sum(emp_list) / float(days_counted), 2)
                avg_con = round(sum(con_list) / float(days_counted), 2)
                avg_tot = round(sum(tot_list) / float(days_counted), 2)
                history_avg_by_location[pname] = {
                    "history_days_counted": int(days_counted),
                    "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                    "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                    "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
                }

            logger.debug("history: dates collected=%d partitions_sample=%d", len(selected_dates), len(history_avg_by_location))
    except Exception:
        logger.exception("Error fetching/processing history endpoints")
        notes.append("Failed to compute history averages from region history endpoints; partial results.")

    # Merge DB-derived per-location (avg_by_location_last_range) with history per-location (history_avg_by_location)
    try:
        merged_history = dict(history_avg_by_location)
        for loc, dbvals in (avg_by_location_last_range or {}).items():
            if loc in merged_history:
                continue
            try:
                merged_history[loc] = {
                    "history_days_counted": int(dbvals.get("history_days_counted") or 0),
                    "avg_employee_last_7_days": _sanitize_for_json(dbvals.get("avg_employee_last_7_days")),
                    "avg_contractor_last_7_days": _sanitize_for_json(dbvals.get("avg_contractor_last_7_days")),
                    "avg_overall_last_7_days": _sanitize_for_json(dbvals.get("avg_overall_last_7_days"))
                }
            except Exception:
                merged_history[loc] = {
                    "history_days_counted": int(dbvals.get("history_days_counted") or 0),
                    "avg_employee_last_7_days": _sanitize_for_json(dbvals.get("avg_employee_last_7_days") or None),
                    "avg_contractor_last_7_days": _sanitize_for_json(dbvals.get("avg_contractor_last_7_days") or None),
                    "avg_overall_last_7_days": _sanitize_for_json(dbvals.get("avg_overall_last_7_days") or None)
                }
        history_avg_by_location = merged_history
    except Exception:
        logger.exception("Failed to normalize history_avg_by_location")

    # Fallback: if DB-based avg empty, use history_overall_avg
    if (not avg_headcount_last_range or avg_headcount_last_range == 0) and history_overall_avg:
        try:
            avg_headcount_last_range = history_overall_avg
            avg_headcount_per_site_last_range = round(history_overall_avg / float(sites_queried), 2) if sites_queried and sites_queried > 0 else None
            notes.append("avg_headcount_last_range derived from region history endpoints due to missing AttendanceSummary historical data.")
        except Exception:
            pass

    # compute percentages vs ccure
    def safe_pct(n, denom):
        try:
            if n is None or denom is None:
                return None
            if float(denom) == 0.0:
                return None
            return round((float(n) / float(denom)) * 100.0, 2)
        except Exception:
            return None

    cc_emp_denom = reported_active_emps
    cc_con_denom = reported_active_contractors
    cc_total_denom = None
    if isinstance(cc_emp_denom, int) and isinstance(cc_con_denom, int):
        cc_total_denom = cc_emp_denom + cc_con_denom

    head_emp_total = sum(v.get("employee", 0) for v in head_per_location.values())
    head_con_total = sum(v.get("contractor", 0) for v in head_per_location.values())
    live_emp_total = sum(v.get("employee", 0) for v in live_per_location.values())
    live_con_total = sum(v.get("contractor", 0) for v in live_per_location.values())

    head_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_emp_total, cc_emp_denom))
    head_contractor_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_con_total, cc_con_denom))
    head_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_total, cc_total_denom))

    live_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_emp_total, cc_emp_denom))
    live_contractor_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_con_total, cc_con_denom))
    live_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_total, cc_total_denom))

    history_emp_pct_vs_ccure = _sanitize_for_json(safe_pct(history_emp_avg, cc_emp_denom))
    history_contractor_pct_vs_ccure = _sanitize_for_json(safe_pct(history_contractor_avg, cc_con_denom))
    history_overall_pct_vs_ccure = _sanitize_for_json(safe_pct(history_overall_avg, cc_total_denom))

    history_today_employee_pct_vs_ccure = _sanitize_for_json(safe_pct(history_today_emp, cc_emp_denom))
    history_today_contractor_pct_vs_ccure = _sanitize_for_json(safe_pct(history_today_con, cc_con_denom))

    # Final result
    result = {
        "date": today.isoformat(),
        "headcount": {
            "total_visited_today": int(head_total),
            "employee": int(head_emp_total),
            "contractor": int(head_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in head_per_location.items() }
        },
        "live_headcount": {
            "currently_present_total": int(live_total),
            "employee": int(live_emp_total),
            "contractor": int(live_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in live_per_location.items() }
        },
        "ccure_active": {
            "ccure_active_employees_reported": _safe_int(reported_active_emps),
            "ccure_active_contractors_reported": _safe_int(reported_active_contractors)
        },
        "averages": {
            "head_emp_pct_vs_ccure_today": head_emp_pct_vs_ccure_today,
            "head_contractor_pct_vs_ccure_today": head_contractor_pct_vs_ccure_today,
            "headcount_overall_pct_vs_ccure_today": head_overall_pct_vs_ccure_today,
            "live_employee_pct_vs_ccure": live_emp_pct_vs_ccure_today,
            "live_contractor_pct_vs_ccure": _sanitize_for_json(safe_pct(live_con_total, cc_con_denom)),
            "live_overall_pct_vs_ccure": live_overall_pct_vs_ccure_today,
            "avg_headcount_last_7_days": _sanitize_for_json(avg_headcount_last_range),
            "avg_headcount_per_site_last_7_days": _sanitize_for_json(avg_headcount_per_site_last_range),
            "avg_live_per_site": _sanitize_for_json(round(live_total / sites_queried, 2) if sites_queried and sites_queried > 0 else None),
            "history_avg_employee_last_7_days": _sanitize_for_json(history_emp_avg),
            "history_avg_contractor_last_7_days": _sanitize_for_json(history_contractor_avg),
            "history_avg_overall_last_7_days": _sanitize_for_json(history_overall_avg),
            "history_days_counted": int(history_days) if history_days is not None else None,
            "history_employee_pct_vs_ccure": history_emp_pct_vs_ccure,
            "history_contractor_pct_vs_ccure": history_contractor_pct_vs_ccure,
            "history_overall_pct_vs_ccure": history_overall_pct_vs_ccure,
            "history_today_employee_count": int(history_today_emp) if history_today_emp is not None else None,
            "history_today_contractor_count": int(history_today_con) if history_today_con is not None else None,
            "history_today_employee_pct_vs_ccure": history_today_employee_pct_vs_ccure,
            "history_today_contractor_pct_vs_ccure": history_today_contractor_pct_vs_ccure,
            "avg_by_location_last_7_days": _sanitize_for_json(avg_by_location_last_range),
            "history_avg_by_location_last_7_days": _sanitize_for_json(history_avg_by_location)
        },
        "sites_queried": int(sites_queried),
        "notes": " | ".join(notes) if notes else f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}"
    }

    return _sanitize_for_json(result)


# mapping helper used by app.py
def map_to_unified_summary(detailed: Dict[str, Any], include_raw: bool = False) -> Dict[str, Any]:
    today = detailed.get("date")
    head = detailed.get("headcount", {}) or {}
    live = detailed.get("live_headcount", {}) or {}
    cc = detailed.get("ccure_active", {}) or {}
    av = detailed.get("averages", {}) or {}
    notes = detailed.get("notes")

    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    cc_emp = to_int(cc.get("ccure_active_employees_reported") or cc.get("active_employees"))
    cc_con = to_int(cc.get("ccure_active_contractors_reported") or cc.get("active_contractors"))

    head_total = to_int(head.get("total_visited_today") or 0)
    head_emp = to_int(head.get("employee") or 0)
    head_con = to_int(head.get("contractor") or 0)

    live_total = to_int(live.get("currently_present_total") or 0)
    live_emp = to_int(live.get("employee") or 0)
    live_con = to_int(live.get("contractor") or 0)

    def pct(n, d):
        try:
            if n is None or d is None:
                return None
            if float(d) == 0:
                return None
            return round((float(n) / float(d)) * 100.0, 2)
        except Exception:
            return None

    summary = {
        "date": today,
        "ccure_reported": {
            "employees": cc_emp,
            "contractors": cc_con,
            "total_reported": (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None
        },
        "headcount_attendance_summary": {
            "total_visited_today": head_total,
            "employee": head_emp,
            "contractor": head_con,
            "by_location": head.get("by_location", {})
        },
        "live_headcount_region_clients": {
            "currently_present_total": live_total,
            "employee": live_emp,
            "contractor": live_con,
            "by_location": live.get("by_location", {})
        },
        "percentages_vs_ccure": {
            "head_employee_pct_vs_ccure_today": pct(head_emp, cc_emp),
            "head_contractor_pct_vs_ccure_today": pct(head_con, cc_con),
            "head_overall_pct_vs_ccure_today": pct(head_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "live_employee_pct_vs_ccure_today": pct(live_emp, cc_emp),
            "live_contractor_pct_vs_ccure_today": pct(live_con, cc_con),
            "live_overall_pct_vs_ccure_today": pct(live_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "history_employee_pct_vs_ccure": av.get("history_employee_pct_vs_ccure") or av.get("head_emp_pct_vs_ccure_today") or None,
            "history_contractor_pct_vs_ccure": av.get("history_contractor_pct_vs_ccure") or None,
            "history_overall_pct_vs_ccure": av.get("history_overall_pct_vs_ccure") or None
        },
        "averages": {
            "history_avg_employee_last_7_days": av.get("history_avg_employee_last_7_days"),
            "history_avg_contractor_last_7_days": av.get("history_avg_contractor_last_7_days"),
            "history_avg_overall_last_7_days": av.get("history_avg_overall_last_7_days"),
            "avg_headcount_last_7_days_db": av.get("avg_headcount_last_7_days"),
            "avg_headcount_per_site_last_7_days": av.get("avg_headcount_per_site_last_7_days"),
            "avg_live_per_site": av.get("avg_live_per_site"),
            "avg_by_location_last_7_days": av.get("avg_by_location_last_7_days"),
            "history_avg_by_location_last_7_days": av.get("history_avg_by_location_last_7_days"),
            "history_days_counted": av.get("history_days_counted"),
            "history_today_employee_count": av.get("history_today_employee_count"),
            "history_today_contractor_count": av.get("history_today_contractor_count"),
            "history_today_employee_pct_vs_ccure": av.get("history_today_employee_pct_vs_ccure"),
            "history_today_contractor_pct_vs_ccure": av.get("history_today_contractor_pct_vs_ccure"),
        },
        "notes": notes
    }

    if include_raw:
        summary["raw"] = detailed

    return _sanitize_for_json(summary)











import requests
from requests.exceptions import RequestException
from datetime import datetime
import logging
import time

logger = logging.getLogger("region_clients")
logger.setLevel(logging.INFO)
if not logger.handlers:
    import sys
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Edit endpoints if your hosts/ports differ
endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/live-summary",
    "emea":  "http://10.199.22.57:3007/api/occupancy/live-summary",
    "laca":  "http://10.199.22.57:4000/api/occupancy/live-summary",
    "apac":  "http://10.199.22.57:3008/api/occupancy/live-summary"
}

history_endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/history",
    "emea":  "http://10.199.22.57:3007/api/occupancy/history",
    "apac":  "http://10.199.22.57:3008/api/occupancy/history",
    "laca":  "http://10.199.22.57:4000/api/occupancy/history"
}

DEFAULT_ATTEMPTS = 2
DEFAULT_BACKOFF = 0.8

def _do_get_with_retries(url, timeout, attempts=DEFAULT_ATTEMPTS, backoff=DEFAULT_BACKOFF):
    last_err = None
    for attempt in range(1, attempts + 1):
        try:
            r = requests.get(url, timeout=timeout)
            r.raise_for_status()
            try:
                return r.json()
            except ValueError:
                try:
                    return {"_raw_text": r.text}
                except Exception:
                    return None
        except RequestException as e:
            last_err = e
            logger.warning(f"[region_clients] attempt {attempt}/{attempts} failed for {url}: {e}")
            if attempt < attempts:
                time.sleep(backoff * attempt)
            continue
    logger.warning(f"[region_clients] all {attempts} attempts failed for {url}: {last_err}")
    return None

def fetch_all_regions(timeout=6):
    results = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            realtime = {}
            if isinstance(data, dict):
                realtime = data.get("realtime", {}) or {}
            total = 0
            for site in realtime.values():
                try:
                    total += int(site.get("total", 0))
                except Exception:
                    pass
            # fallback: some live-summary payloads return top-level partitions directly
            if total == 0 and isinstance(data, dict):
                for k, v in data.items():
                    if isinstance(v, dict) and "total" in v:
                        try:
                            total += int(v.get("total", 0))
                        except Exception:
                            pass
            results.append({"region": region, "count": total})
        except Exception as e:
            logger.exception(f"[region_clients] unexpected error fetching live-summary for {region}: {e}")
            results.append({"region": region, "count": None})
    return results

def fetch_all_details(timeout=6):
    all_details = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            details = []
            if isinstance(data, dict):
                details = data.get("details", []) or []
                if not details:
                    for k, v in data.items():
                        if k in ("details", "list", "people", "items") and isinstance(v, list):
                            details = v
                            break
            for d in details:
                try:
                    d2 = dict(d)
                    d2["__region"] = region
                    all_details.append(d2)
                except Exception:
                    continue
        except Exception as e:
            logger.warning(f"[region_clients] warning: cannot fetch details from {region}@{url}: {e}")
            continue
    return all_details

def fetch_history_for_region(region, timeout=6):
    url = history_endpoints.get(region)
    if not url:
        logger.debug(f"[fetch_history_for_region] no history endpoint for {region}")
        return []
    try:
        data = _do_get_with_retries(url, timeout=timeout) or {}
        summary = []
        if isinstance(data, dict):
            candidates = []
            for key in ("summaryByDate", "summary", "data", "entries"):
                if key in data and isinstance(data.get(key), list):
                    candidates = data.get(key)
                    break
            if not candidates:
                if isinstance(data.get("results"), list):
                    candidates = data.get("results")
                else:
                    if "date" in data:
                        candidates = [data]
            for s in candidates:
                try:
                    s2 = dict(s)
                    s2["__region"] = region
                    summary.append(s2)
                except Exception:
                    continue
        elif isinstance(data, list):
            for s in data:
                try:
                    s2 = dict(s)
                    s2["__region"] = region
                    summary.append(s2)
                except Exception:
                    continue
        return summary
    except Exception as e:
        logger.warning(f"[region_clients] cannot fetch history for {region}@{url}: {e}")
        return []

def fetch_all_history(timeout=6):
    all_entries = []
    for region in history_endpoints.keys():
        try:
            entries = fetch_history_for_region(region, timeout=timeout)
            if entries:
                all_entries.extend(entries)
        except Exception:
            logger.exception(f"[region_clients] error aggregating history for {region}")
            continue
    return all_entries













one last time i will again explain you APi resspone one by one ...


1)http://10.199.22.57:3008/api/occupancy/history

uSING THIS api BUILD HEADcOUNT sUMMARY, vISITED tODAY , HeadCount )Strickly 
suppose today is 27 th 
then Display 
This data why u display realtime Headcount ...
      "date": "2025-08-27",
      "day": "Wednesday",
      "region": {
        "name": "APAC",
        "total": 441,
        "Employee": 350,
        "Contractor": 91

      "partitions": {
        "Quezon City": {
          "total": 360,
          "Employee": 322,
          "Contractor": 38

        "Pune": {
          "total": 51,
          "Employee": 2,
          "Contractor": 49
  
        "JP.Tokyo": {
          "total": 9,
          "Employee": 8,
          "Contractor": 1
    
        "Taguig City": {
          "total": 16,
          "Employee": 14,
          "Contractor": 2
     
        "MY.Kuala Lumpur": {
          "total": 5,
          "Employee": 4,
          "Contractor": 1
        




lets check below Api responce ...

{
  "success": true,
  "summaryByDate": [
    {
      "date": "2025-08-20",
      "day": "Wednesday",
      "region": {
        "name": "APAC",
        "total": 1402,
        "Employee": 1244,
        "Contractor": 158
      },
      "partitions": {
        "Pune": {
          "total": 1026,
          "Employee": 914,
          "Contractor": 112
        },
        "Quezon City": {
          "total": 335,
          "Employee": 296,
          "Contractor": 39
        },
        "Taguig City": {
          "total": 26,
          "Employee": 21,
          "Contractor": 5
        },
        "JP.Tokyo": {
          "total": 8,
          "Employee": 7,
          "Contractor": 1
        },
        "MY.Kuala Lumpur": {
          "total": 7,
          "Employee": 6,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-21",
      "day": "Thursday",
      "region": {
        "name": "APAC",
        "total": 1199,
        "Employee": 1052,
        "Contractor": 147
      },
      "partitions": {
        "Quezon City": {
          "total": 283,
          "Employee": 245,
          "Contractor": 38
        },
        "Pune": {
          "total": 901,
          "Employee": 794,
          "Contractor": 107
        },
        "JP.Tokyo": {
          "total": 9,
          "Employee": 8,
          "Contractor": 1
        },
        "MY.Kuala Lumpur": {
          "total": 6,
          "Employee": 5,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-22",
      "day": "Friday",
      "region": {
        "name": "APAC",
        "total": 713,
        "Employee": 575,
        "Contractor": 138
      },
      "partitions": {
        "Quezon City": {
          "total": 230,
          "Employee": 187,
          "Contractor": 43
        },
        "Pune": {
          "total": 469,
          "Employee": 377,
          "Contractor": 92
        },
        "MY.Kuala Lumpur": {
          "total": 3,
          "Employee": 2,
          "Contractor": 1
        },
        "JP.Tokyo": {
          "total": 9,
          "Employee": 8,
          "Contractor": 1
        },
        "Taguig City": {
          "total": 2,
          "Employee": 1,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-23",
      "day": "Saturday",
      "region": {
        "name": "APAC",
        "total": 180,
        "Employee": 102,
        "Contractor": 78
      },
      "partitions": {
        "Quezon City": {
          "total": 136,
          "Employee": 100,
          "Contractor": 36
        },
        "Pune": {
          "total": 42,
          "Employee": 1,
          "Contractor": 41
        },
        "JP.Tokyo": {
          "total": 1,
          "Employee": 1,
          "Contractor": 0
        },
        "Taguig City": {
          "total": 1,
          "Employee": 0,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-24",
      "day": "Sunday",
      "region": {
        "name": "APAC",
        "total": 140,
        "Employee": 58,
        "Contractor": 82
      },
      "partitions": {
        "Quezon City": {
          "total": 91,
          "Employee": 56,
          "Contractor": 35
        },
        "Pune": {
          "total": 48,
          "Employee": 1,
          "Contractor": 47
        },
        "JP.Tokyo": {
          "total": 1,
          "Employee": 1,
          "Contractor": 0
        }
      }
    },
    {
      "date": "2025-08-25",
      "day": "Monday",
      "region": {
        "name": "APAC",
        "total": 1080,
        "Employee": 929,
        "Contractor": 151
      },
      "partitions": {
        "Quezon City": {
          "total": 136,
          "Employee": 103,
          "Contractor": 33
        },
        "Pune": {
          "total": 929,
          "Employee": 813,
          "Contractor": 116
        },
        "JP.Tokyo": {
          "total": 9,
          "Employee": 8,
          "Contractor": 1
        },
        "MY.Kuala Lumpur": {
          "total": 6,
          "Employee": 5,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-26",
      "day": "Tuesday",
      "region": {
        "name": "APAC",
        "total": 1172,
        "Employee": 1022,
        "Contractor": 150
      },
      "partitions": {
        "Quezon City": {
          "total": 292,
          "Employee": 251,
          "Contractor": 41
        },
        "Pune": {
          "total": 852,
          "Employee": 747,
          "Contractor": 105
        },
        "MY.Kuala Lumpur": {
          "total": 3,
          "Employee": 2,
          "Contractor": 1
        },
        "Taguig City": {
          "total": 15,
          "Employee": 13,
          "Contractor": 2
        },
        "JP.Tokyo": {
          "total": 10,
          "Employee": 9,
          "Contractor": 1
        }
      }
    },
    {
      "date": "2025-08-27",
      "day": "Wednesday",
      "region": {
        "name": "APAC",
        "total": 441,
        "Employee": 350,
        "Contractor": 91
      },
      "partitions": {
        "Quezon City": {
          "total": 360,
          "Employee": 322,
          "Contractor": 38
        },
        "Pune": {
          "total": 51,
          "Employee": 2,
          "Contractor": 49
        },
        "JP.Tokyo": {
          "total": 9,
          "Employee": 8,
          "Contractor": 1
        },
        "Taguig City": {
          "total": 16,
          "Employee": 14,
          "Contractor": 2
        },
        "MY.Kuala Lumpur": {
          "total": 5,
          "Employee": 4,
          "Contractor": 1
        }
      }
    }
  ],
  "details": [






and 2) for realtime HeadCount and their average use
http://10.199.22.57:3008/api/occupancy/live-summary

{
  "success": true,
  "today": {
    "total": 442,
    "Employee": 350,
    "Contractor": 92
  },
  "realtime": {
    "Quezon City": {
      "total": 172,
      "Employee": 162,
      "Contractor": 10,
      "floors": {
        "7th Floor": 123,
        "6th Floor": 49
      },
      "zones": {
        "7th Floor": 123,
        "6th Floor": 49
      }
    },
    "Pune": {
      "total": 39,
      "Employee": 2,
      "Contractor": 37,
      "floors": {
        "Tower B": 9,
        "Podium Floor": 30
      },
      "zones": {
        "Tower B": 9,
        "Red Zone": 7,
        "Reception Area": 8,
        "Green Zone": 2,
        "Yellow Zone": 10,
        "Red Zone - Outer Area": 1,
        "Yellow Zone - Outer Area": 2
      }
    },
    "JP.Tokyo": {
      "total": 9,
      "Employee": 8,
      "Contractor": 1,
      "floors": {
        "Tokyo": 9
      },
      "zones": {
        "Tokyo": 9
      }




Strickly display this Summary please.....

i have already explain you many time solve this one issue carefully and give me fully updated file carefully..





http://localhost:8000/ccure/averages
{
  "date": "2025-08-27",
  "ccure_reported": {
    "employees": 8609,
    "contractors": 658,
    "total_reported": 9267
  },
  "headcount_attendance_summary": {
    "total_visited_today": 888,
    "employee": 807,
    "contractor": 81,
    "by_location": {
      "AUT.Vienna": {
        "total": 50,
        "employee": 48,
        "contractor": 2
      },
      "LT.Vilnius": {
        "total": 382,
        "employee": 372,
        "contractor": 10
      },
      "Pune": {
        "total": 41,
        "employee": 2,
        "contractor": 39
      },
      "Taguig City": {
        "total": 16,
        "employee": 14,
        "contractor": 2
      },
      "ES.Madrid": {
        "total": 56,
        "employee": 55,
        "contractor": 1
      },
      "UK.London": {
        "total": 23,
        "employee": 23,
        "contractor": 0
      },
      "DU.Abu Dhab": {
        "total": 21,
        "employee": 20,
        "contractor": 1
      },
      "Quezon City": {
        "total": 199,
        "employee": 187,
        "contractor": 12
      },
      "IE.Dublin": {
        "total": 22,
        "employee": 22,
        "contractor": 0
      },
      "MY.Kuala Lumpur": {
        "total": 5,
        "employee": 4,
        "contractor": 1
      },
      "IT.Rome": {
        "total": 27,
        "employee": 25,
        "contractor": 2
      },
      "MA.Casablanca": {
        "total": 23,
        "employee": 22,
        "contractor": 1
      },
      "JP.Tokyo": {
        "total": 9,
        "employee": 8,
        "contractor": 1
      },
      "RU.Moscow": {
        "total": 4,
        "employee": 4,
        "contractor": 0
      },
      "US.CO.OBS": {
        "total": 1,
        "employee": 0,
        "contractor": 1
      },
      "AR.Cordoba": {
        "total": 3,
        "employee": 0,
        "contractor": 3
      },
      "CR.Costa Rica Partition": {
        "total": 4,
        "employee": 0,
        "contractor": 4
      },
      "BR.Sao Paulo": {
        "total": 2,
        "employee": 1,
        "contractor": 1
      }
    }
  },
  "live_headcount_region_clients": {
    "currently_present_total": 876,
    "employee": 807,
    "contractor": 89,
    "by_location": {
      "US.CO.OBS": {
        "total": 1,
        "employee": 0,
        "contractor": 1
      },
      "LT.Vilnius": {
        "total": 383,
        "employee": 372,
        "contractor": 11
      },
      "MA.Casablanca": {
        "total": 23,
        "employee": 22,
        "contractor": 1
      },
      "AUT.Vienna": {
        "total": 52,
        "employee": 48,
        "contractor": 4
      },
      "IE.Dublin": {
        "total": 23,
        "employee": 22,
        "contractor": 1
      },
      "UK.London": {
        "total": 23,
        "employee": 23,
        "contractor": 0
      },
      "ES.Madrid": {
        "total": 57,
        "employee": 55,
        "contractor": 2
      },
      "IT.Rome": {
        "total": 27,
        "employee": 25,
        "contractor": 2
      },
      "DU.Abu Dhab": {
        "total": 21,
        "employee": 20,
        "contractor": 1
      },
      "RU.Moscow": {
        "total": 5,
        "employee": 4,
        "contractor": 1
      },
      "CR.Costa Rica Partition": {
        "total": 4,
        "employee": 0,
        "contractor": 4
      },
      "AR.Cordoba": {
        "total": 5,
        "employee": 0,
        "contractor": 5
      },
      "BR.Sao Paulo": {
        "total": 2,
        "employee": 1,
        "contractor": 1
      },
      "Quezon City": {
        "total": 199,
        "employee": 187,
        "contractor": 12
      },
      "Pune": {
        "total": 41,
        "employee": 2,
        "contractor": 39
      },
      "JP.Tokyo": {
        "total": 9,
        "employee": 8,
        "contractor": 1
      },
      "MY.Kuala Lumpur": {
        "total": 5,
        "employee": 4,
        "contractor": 1
      },
      "Taguig City": {
        "total": 16,
        "employee": 14,
        "contractor": 2
      }
    }
  },
  "percentages_vs_ccure": {
    "head_employee_pct_vs_ccure_today": 9.37,
    "head_contractor_pct_vs_ccure_today": 12.31,
    "head_overall_pct_vs_ccure_today": 9.58,
    "live_employee_pct_vs_ccure_today": 9.37,
    "live_contractor_pct_vs_ccure_today": 13.53,
    "live_overall_pct_vs_ccure_today": 9.45,
    "history_employee_pct_vs_ccure": 20.87,
    "history_contractor_pct_vs_ccure": 39.82,
    "history_overall_pct_vs_ccure": 22.22
  },
  "averages": {
    "history_avg_employee_last_7_days": 1796.86,
    "history_avg_contractor_last_7_days": 262,
    "history_avg_overall_last_7_days": 2058.86,
    "avg_headcount_last_7_days_db": 2058.86,
    "avg_headcount_per_site_last_7_days": 514.72,
    "avg_live_per_site": 219,
    "avg_by_location_last_7_days": {

    },
    "history_avg_by_location_last_7_days": {
      "US.CO.OBS": {
        "history_days_counted": 7,
        "avg_employee_last_7_days": 252.71,
        "avg_contractor_last_7_days": 25.29,
        "avg_overall_last_7_days": 278
      },
      "US.FL.Miami": {
        "history_days_counted": 4,
        "avg_employee_last_7_days": 17.75,
        "avg_contractor_last_7_days": 2.75,
        "avg_overall_last_7_days": 20.5
      },
      "USA/Canada Default": {
        "history_days_counted": 6,
        "avg_employee_last_7_days": 26.5,
        "avg_contractor_last_7_days": 2,
        "avg_overall_last_7_days": 28.5
      },
      "US.NYC": {
        "history_days_counted": 5,
        "avg_employee_last_7_days": 20.8,
        "avg_contractor_last_7_days": 0.8,
        "avg_overall_last_7_days": 21.6
      },
      "LT.Vilnius": {
        "history_days_counted": 7,
        "avg_employee_last_7_days": 299.71,
        "avg_contractor_last_7_days": 17,
        "avg_overall_last_7_days": 316.71
      },
      "UK.London": {
        "history_days_counted": 5,
        "avg_employee_last_7_days": 14,
        "avg_contractor_last_7_days": 1.2,
        "avg_overall_last_7_days": 15.2
      },
      "AUT.Vienna": {
        "history_days_counted": 7,
        "avg_employee_last_7_days": 26.57,
        "avg_contractor_last_7_days": 2.86,
        "avg_overall_last_7_days": 29.43
      },
      "IE.Dublin": {
        "history_days_counted": 7,
        "avg_employee_last_7_days": 18.29,
        "avg_contractor_last_7_days": 1.86,
        "avg_overall_last_7_days": 20.14
      },
      "ES.Madrid": {
        "history_days_counted": 5,
        "avg_employee_last_7_days": 43,
        "avg_contractor_last_7_days": 3,
        "avg_overall_last_7_days": 46
      },
      "DU.Abu Dhab": {
        "history_days_counted": 5,
        "avg_employee_last_7_days": 31.2,
        "avg_contractor_last_7_days": 1.2,
        "avg_overall_last_7_days": 32.4
      },
      "IT.Rome": {
        "history_days_counted": 5,
        "avg_employee_last_7_days": 19.8,
        "avg_contractor_last_7_days": 1.6,
        "avg_overall_last_7_days": 21.4
      },
      "RU.Moscow": {
        "history_days_counted": 6,
        "avg_employee_last_7_days": 4,
        "avg_contractor_last_7_days": 1.33,
        "avg_overall_last_7_days": 5.33
      },
      "Quezon City": {
        "history_days_counted": 7,
        "avg_employee_last_7_days": 180.57,
        "avg_contractor_last_7_days": 37.71,
        "avg_overall_last_7_days": 218.29
      },
      "Pune": {
        "history_days_counted": 7,
        "avg_employee_last_7_days": 390.71,
        "avg_contractor_last_7_days": 79.57,
        "avg_overall_last_7_days": 470.29
      },
      "JP.Tokyo": {
        "history_days_counted": 7,
        "avg_employee_last_7_days": 6.14,
        "avg_contractor_last_7_days": 0.71,
        "avg_overall_last_7_days": 6.86
      },
      "MY.Kuala Lumpur": {
        "history_days_counted": 5,
        "avg_employee_last_7_days": 3.6,
        "avg_contractor_last_7_days": 1,
        "avg_overall_last_7_days": 4.6
      },
      "CR.Costa Rica Partition": {
        "history_days_counted": 7,
        "avg_employee_last_7_days": 302.43,
        "avg_contractor_last_7_days": 44.14,
        "avg_overall_last_7_days": 346.57
      },
      "AR.Cordoba": {
        "history_days_counted": 7,
        "avg_employee_last_7_days": 100.57,
        "avg_contractor_last_7_days": 28.29,
        "avg_overall_last_7_days": 128.86
      },
      "PA.Panama City": {
        "history_days_counted": 5,
        "avg_employee_last_7_days": 14,
        "avg_contractor_last_7_days": 2.2,
        "avg_overall_last_7_days": 16.2
      },
      "BR.Sao Paulo": {
        "history_days_counted": 6,
        "avg_employee_last_7_days": 32,
        "avg_contractor_last_7_days": 6.67,
        "avg_overall_last_7_days": 38.67
      },
      "MX.Mexico City": {
        "history_days_counted": 4,
        "avg_employee_last_7_days": 30.25,
        "avg_contractor_last_7_days": 4.25,
        "avg_overall_last_7_days": 34.5
      },
      "PE.Lima": {
        "history_days_counted": 5,
        "avg_employee_last_7_days": 27,
        "avg_contractor_last_7_days": 3.6,
        "avg_overall_last_7_days": 30.6
      },
      "MA.Casablanca": {
        "history_days_counted": 6,
        "avg_employee_last_7_days": 12,
        "avg_contractor_last_7_days": 0.83,
        "avg_overall_last_7_days": 12.83
      },
      "Taguig City": {
        "history_days_counted": 4,
        "avg_employee_last_7_days": 7,
        "avg_contractor_last_7_days": 1.5,
        "avg_overall_last_7_days": 8.5
      }
    },
    "history_days_counted": 7,
    "history_today_employee_count": 1110,
    "history_today_contractor_count": 146,
    "history_today_employee_pct_vs_ccure": 12.89,
    "history_today_contractor_pct_vs_ccure": 22.19
  },
  "notes": "Region totals (876) differ from detail rows (896); using region totals for overall and details for breakdown. | avg_headcount_last_range derived from region history endpoints due to missing AttendanceSummary historical data."
}






as per Requirnment update all file and give me fully updated file carefully...





# app.py
from fastapi import FastAPI, Query, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import logging
from datetime import date, datetime, timedelta
from typing import Optional, Dict, Any, List, Set
import re
import json
import time
import asyncio

# DB/models - keep these as in your project
from db import SessionLocal
from models import AttendanceSummary, LiveSwipe

# optional settings
try:
    from settings import UPLOAD_DIR, OUTPUT_DIR
except Exception:
    UPLOAD_DIR = "./uploads"
    OUTPUT_DIR = "./output"

app = FastAPI(title="Attendance Analytics - Single Averages API")

# logging
logger = logging.getLogger("attendance_app")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Timeouts / retries
REGION_TIMEOUT_SECONDS = 20
COMPUTE_WAIT_TIMEOUT_SECONDS = 60  # allow compute to run longer
REGION_ATTEMPTS = 2
REGION_BACKOFF = 0.5

# CORS
_allowed_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
    "http://localhost:3008"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# ----- helpers -----
def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

def _sanitize_for_json(value):
    # keep simple sanitizer to ensure JSON safe values (numbers, strings, dict/list)
    if value is None:
        return None
    if isinstance(value, (str, bool, int, float)):
        return value
    if isinstance(value, dict):
        return { str(k): _sanitize_for_json(v) for k, v in value.items() }
    if isinstance(value, (list, tuple, set)):
        return [ _sanitize_for_json(v) for v in value ]
    try:
        return str(value)
    except Exception:
        return None

# classification & partition helpers
def classify_personnel_from_detail(detail: dict) -> str:
    try:
        if not isinstance(detail, dict):
            return "contractor"
        candidate_keys = [
            "PersonnelType", "personnelType", "personnel_type", "Personnel Type",
            "PersonnelTypeName", "Personnel", "Type", "personnel", "PersonType", "personType"
        ]
        val = None
        for k in candidate_keys:
            if k in detail and detail.get(k) is not None:
                val = str(detail.get(k)).strip().lower()
                break
        status_keys = ["Employee_Status", "Employee Status", "Status", "Profile_Disabled"]
        status_val = None
        for k in status_keys:
            if k in detail and detail.get(k) is not None:
                status_val = str(detail.get(k)).strip().lower()
                break

        if status_val is not None and "terminated" in status_val:
            return "employee"
        if val is None or val == "":
            return "contractor"
        if "employee" in val:
            return "employee"
        contractor_terms = ["contractor", "visitor", "property", "temp", "temp badge", "tempbadge"]
        for t in contractor_terms:
            if t in val:
                return "contractor"
        if "contract" in val or "visitor" in val:
            return "contractor"
        return "contractor"
    except Exception:
        return "contractor"

def pick_partition_from_detail(detail: dict) -> str:
    if not isinstance(detail, dict):
        return "Unknown"
    for k in ("PartitionName2","PartitionName1","Partition","PartitionName","Region","Location","Site","location_city","Location City"):
        if k in detail and detail.get(k):
            try:
                return str(detail.get(k)).strip()
            except Exception:
                continue
    if "__region" in detail and detail.get("__region"):
        return str(detail.get("__region")).strip()
    return "Unknown"

def _attempt_region_call(fn, timeout, attempts=REGION_ATTEMPTS, backoff=REGION_BACKOFF):
    last_exc = None
    for i in range(attempts):
        try:
            # many region_clients functions accept a timeout kwarg
            res = fn(timeout=timeout)
            if res is not None:
                return res
        except TypeError:
            # try calling without timeout if signature differs
            try:
                res = fn()
                if res is not None:
                    return res
            except Exception as e:
                last_exc = e
                logger.warning("[region_clients] attempt %d/%d failed: %s", i+1, attempts, e)
        except Exception as e:
            last_exc = e
            logger.warning("[region_clients] attempt %d/%d failed: %s", i+1, attempts, e)
        time.sleep(backoff * (i+1))
    logger.warning("[region_clients] all %d attempts failed for %s: last_exc=%s", attempts, getattr(fn, "__name__", str(fn)), last_exc)
    return None

def build_headcount_from_liveswipes_for_today(session) -> (int, Dict[str, Dict[str, int]]):
    start = datetime.combine(date.today(), datetime.min.time())
    end = datetime.combine(date.today(), datetime.max.time())
    swipes = session.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
    if not swipes:
        return 0, {}
    seen_keys = {}
    per_loc = {}
    for s in swipes:
        key = _normalize_employee_key(s.employee_id) or _normalize_card_like(s.card_number)
        if not key:
            key = f"nokey_{s.id}"
        rec = seen_keys.get(key)
        ts = s.timestamp
        if rec is None:
            seen_keys[key] = {"first_seen": ts, "last_seen": ts, "partition": (s.partition or "Unknown"), "class": None, "card": s.card_number, "raw": s.raw}
        else:
            if ts and rec.get("first_seen") and ts < rec["first_seen"]:
                rec["first_seen"] = ts
            if ts and rec.get("last_seen") and ts > rec["last_seen"]:
                rec["last_seen"] = ts
    for k, v in seen_keys.items():
        loc = v.get("partition") or "Unknown"
        if not isinstance(loc, str) or not loc.strip():
            loc = "Unknown"
        if loc not in per_loc:
            per_loc[loc] = {"total": 0, "employee": 0, "contractor": 0}
        per_loc[loc]["total"] += 1
        classified = "contractor"
        raw = v.get("raw")
        if isinstance(raw, dict):
            try:
                classified = classify_personnel_from_detail(raw)
            except Exception:
                classified = "contractor"
        per_loc[loc][classified] += 1
    total = sum(p["total"] for p in per_loc.values())
    return int(total), per_loc

# ----- core compute (merged, resilient) -----
def compute_visit_averages(start_date: Optional[str] = None, end_date: Optional[str] = None, timeout: int = REGION_TIMEOUT_SECONDS) -> Dict[str, Any]:
    """
    Returns a detailed dict with keys:
      - date, headcount, live_headcount, ccure_active, averages, sites_queried, notes
    This function uses AttendanceSummary and LiveSwipe and optionally region_clients + ccure_client.
    """
    notes: List[str] = []
    today = date.today()

    def _parse_date_param(s):
        if not s:
            return None
        try:
            return datetime.strptime(s, "%Y-%m-%d").date()
        except Exception:
            try:
                return date.fromisoformat(s)
            except Exception:
                return None

    start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
    end_obj = _parse_date_param(end_date) if end_date else today
    if start_obj is None or end_obj is None or start_obj > end_obj:
        start_obj = today - timedelta(days=6)
        end_obj = today
        notes.append("Invalid or missing date range; defaulted to last 7 calendar days inclusive.")

    # ccure stats and profiles (optional)
    try:
        import ccure_client  # may be missing
        ccure_stats = ccure_client.get_global_stats() if hasattr(ccure_client, "get_global_stats") else {}
    except Exception:
        ccure_stats = {}
    reported_active_emps = _safe_int(ccure_stats.get("ActiveEmployees")) if isinstance(ccure_stats, dict) else None
    reported_active_contractors = _safe_int(ccure_stats.get("ActiveContractors")) if isinstance(ccure_stats, dict) else None

    # HEADCOUNT from AttendanceSummary (preferred) or fallback to compute from LiveSwipe
    head_total = 0
    head_per_location: Dict[str, Dict[str, int]] = {}
    key_map_head = {}
    key_map_live = {}

    try:
        session = SessionLocal()
        att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
        if not att_rows_today:
            # try building from compute_daily_attendance (if installed), else fallback to LiveSwipe
            try:
                from compare_service import compute_daily_attendance as _compute_daily_attendance
                built = _compute_daily_attendance(today)
                # compute_daily_attendance in your code saves to AttendanceSummary; re-query
                if isinstance(built, list) and built:
                    att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                    notes.append("AttendanceSummary was missing; built from LiveSwipe via compute_daily_attendance().")
            except Exception:
                # not critical, fallback below
                pass

            if not att_rows_today:
                built_total, built_per_loc = build_headcount_from_liveswipes_for_today(session)
                head_total = built_total
                head_per_location = built_per_loc
                if head_total > 0:
                    notes.append("AttendanceSummary for today empty; built headcount from LiveSwipe rows (non-persistent fallback).")

        if att_rows_today:
            for a in att_rows_today:
                key = _normalize_employee_key(a.employee_id) or _normalize_card_like((a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None))
                partition = None
                try:
                    if a.derived and isinstance(a.derived, dict):
                        partition = a.derived.get("partition")
                except Exception:
                    partition = None
                loc = partition or "Unknown"
                if not isinstance(loc, str) or not loc.strip():
                    loc = "Unknown"
                if (a.presence_count or 0) > 0:
                    cls = "contractor"
                    try:
                        if a.derived and isinstance(a.derived, dict):
                            cls = classify_personnel_from_detail(a.derived)
                    except Exception:
                        cls = "contractor"
                    if key:
                        key_map_head[key] = {"loc": loc, "cls": cls}
                    if loc not in head_per_location:
                        head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                    head_per_location[loc][cls] += 1
                    head_per_location[loc]["total"] += 1
                    head_total += 1
        session.expunge_all()
    except Exception:
        logger.exception("Error computing HeadCount")
        notes.append("Failed to compute HeadCount from DB; see server logs.")
    finally:
        try:
            session.close()
        except Exception:
            pass

    # LIVE HEADCOUNT via region_clients (resilient)
    live_total = 0
    live_per_location: Dict[str, Dict[str, int]] = {}
    sites_queried = 0
    derived_detail_sum = 0
    details: List[dict] = []
    try:
        import region_clients
        regions_info = _attempt_region_call(region_clients.fetch_all_regions, timeout=timeout) or []
        details = _attempt_region_call(region_clients.fetch_all_details, timeout=timeout) or []
        sites_queried = len(regions_info) if isinstance(regions_info, list) else 0

        # sum region totals
        if regions_info:
            for r in regions_info:
                try:
                    c = r.get("count") if isinstance(r, dict) else None
                    ci = _safe_int(c)
                    if ci is not None:
                        live_total += int(ci)
                except Exception:
                    continue

        # parse per-person details for breakdown and dedupe
        if details and isinstance(details, list):
            for d in details:
                try:
                    loc = pick_partition_from_detail(d) or "Unknown"
                    if not isinstance(loc, str) or not loc.strip():
                        loc = "Unknown"
                    pclass = classify_personnel_from_detail(d)
                    key = _normalize_employee_key(d.get("EmployeeID")) or _normalize_card_like(d.get("CardNumber")) or (d.get("PersonGUID") if d.get("PersonGUID") else None)
                    if not key:
                        key = _normalize_employee_key(d.get("employee_id")) or _normalize_card_like(d.get("Card")) or None
                    if not key:
                        key = f"detail_{derived_detail_sum}_{str(hash(json.dumps(d, default=str)))}"
                    key = str(key)
                    key_map_live[key] = {"loc": loc, "cls": pclass}
                    if loc not in live_per_location:
                        live_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                    live_per_location[loc]["total"] += 1
                    live_per_location[loc][pclass] += 1
                    derived_detail_sum += 1
                except Exception:
                    continue

            # reconciliation: prefer region totals for overall, details for breakdown
            if live_total == 0 and derived_detail_sum > 0:
                live_total = derived_detail_sum
            else:
                if live_total != derived_detail_sum:
                    notes.append(f"Region totals ({live_total}) differ from detail rows ({derived_detail_sum}); using region totals for overall and details for breakdown.")
        else:
            notes.append("No per-person details available from region_clients; live breakdown unavailable.")
    except Exception:
        logger.exception("Error computing Live HeadCount")
        notes.append("Failed to compute Live HeadCount; see logs.")
        live_total = live_total or 0

    # union of head (AttendanceSummary) and live details to reconcile per-person unique keys
    try:
        head_keys = set(k for k in key_map_head.keys() if k)
        live_keys = set(k for k in key_map_live.keys() if k)
        union_keys = head_keys.union(live_keys)
        unified_head_per_location: Dict[str, Dict[str, int]] = {}
        for k in union_keys:
            if k in key_map_head:
                loc = key_map_head[k].get("loc") or "Unknown"
                cls = key_map_head[k].get("cls") or "contractor"
            else:
                loc = key_map_live.get(k, {}).get("loc") or "Unknown"
                cls = key_map_live.get(k, {}).get("cls") or "contractor"
            if not isinstance(loc, str) or not loc.strip():
                loc = "Unknown"
            if loc not in unified_head_per_location:
                unified_head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
            unified_head_per_location[loc]["total"] += 1
            unified_head_per_location[loc][cls] += 1
        if union_keys:
            head_per_location = unified_head_per_location
            head_total = len(union_keys)
    except Exception:
        logger.exception("Error reconciling headcount union with live details")

    # Averages from AttendanceSummary DB over requested range
    avg_headcount_last_range = None
    avg_headcount_per_site_last_range = None
    avg_by_location_last_range: Dict[str, Dict[str, Any]] = {}
    try:
        session = SessionLocal()
        days = []
        days_count = (end_obj - start_obj).days + 1
        for i in range(days_count):
            days.append(start_obj + timedelta(days=i))

        loc_day_vals: Dict[str, Dict[str, List[int]]] = {}
        for d in days:
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            per_loc_counts: Dict[str, Dict[str, int]] = {}
            if rows:
                for r in rows:
                    try:
                        if (r.presence_count or 0) <= 0:
                            continue
                        partition = None
                        try:
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                        except Exception:
                            partition = None
                        loc = partition or "Unknown"
                        if not isinstance(loc, str) or not loc.strip():
                            loc = "Unknown"
                        if loc not in per_loc_counts:
                            per_loc_counts[loc] = {"employee": 0, "contractor": 0, "total": 0}
                        cls = "contractor"
                        try:
                            if r.derived and isinstance(r.derived, dict):
                                cls = classify_personnel_from_detail(r.derived)
                        except Exception:
                            cls = "contractor"
                        per_loc_counts[loc][cls] += 1
                        per_loc_counts[loc]["total"] += 1
                    except Exception:
                        continue
            for loc, counts in per_loc_counts.items():
                if loc not in loc_day_vals:
                    loc_day_vals[loc] = {"employee": [], "contractor": [], "total": []}
                loc_day_vals[loc]["employee"].append(counts.get("employee", 0))
                loc_day_vals[loc]["contractor"].append(counts.get("contractor", 0))
                loc_day_vals[loc]["total"].append(counts.get("total", 0))

        for loc, lists in loc_day_vals.items():
            emp_list = lists.get("employee", [])
            con_list = lists.get("contractor", [])
            tot_list = lists.get("total", [])
            days_counted = len(tot_list)
            if days_counted == 0:
                continue
            avg_emp = round(sum(emp_list) / float(days_counted), 2)
            avg_con = round(sum(con_list) / float(days_counted), 2)
            avg_tot = round(sum(tot_list) / float(days_counted), 2)
            avg_by_location_last_range[loc] = {
                "history_days_counted": int(days_counted),
                "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
            }

        days_totals = []
        for d in days:
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            day_total = 0
            if rows:
                for r in rows:
                    if (r.presence_count or 0) > 0:
                        day_total += 1
            days_totals.append(day_total)
        if days_totals:
            avg_headcount_last_range = round(sum(days_totals) / float(len(days_totals)), 2)
            if sites_queried and sites_queried > 0:
                avg_headcount_per_site_last_range = round((sum(days_totals) / float(len(days_totals))) / float(sites_queried), 2)
        session.close()
    except Exception:
        logger.exception("Error computing averages from AttendanceSummary")
        notes.append("Failed to compute historical averages from AttendanceSummary; partial results only.")

    # HISTORY averages via region_clients.fetch_all_history
    history_emp_avg = None
    history_contractor_avg = None
    history_overall_avg = None
    history_days = 0
    history_avg_by_location: Dict[str, Dict[str, Any]] = {}
    history_today_emp = None
    history_today_con = None

    try:
        import region_clients
        if hasattr(region_clients, "fetch_all_history"):
            entries = _attempt_region_call(region_clients.fetch_all_history, timeout=timeout) or []
            agg_by_date = {}
            agg_partitions_by_date = {}
            for e in entries:
                try:
                    dstr = e.get("date")
                    if not dstr:
                        dstr = e.get("day") or e.get("timestamp") or None
                        if isinstance(dstr, datetime):
                            dstr = dstr.date().isoformat()
                    if not dstr:
                        continue
                    region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                    emp = None
                    con = None
                    tot = None
                    if region_obj:
                        emp = _safe_int(region_obj.get("Employee"))
                        con = _safe_int(region_obj.get("Contractor"))
                        tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                    else:
                        emp = _safe_int(e.get("Employee"))
                        con = _safe_int(e.get("Contractor"))
                        tot = _safe_int(e.get("total"))
                    if emp is None and con is None:
                        try:
                            robj = e.get("region") or {}
                            if isinstance(robj, dict):
                                emp = _safe_int(robj.get("Employee"))
                                con = _safe_int(robj.get("Contractor"))
                                tot = _safe_int(robj.get("total"))
                        except Exception:
                            pass
                    if emp is None and con is None:
                        continue
                    if tot is None:
                        tot = (emp or 0) + (con or 0)
                    if dstr not in agg_by_date:
                        agg_by_date[dstr] = {"employee": 0, "contractor": 0, "total": 0, "counted_regions": 0}
                    agg_by_date[dstr]["employee"] += (emp or 0)
                    agg_by_date[dstr]["contractor"] += (con or 0)
                    agg_by_date[dstr]["total"] += (tot or 0)
                    agg_by_date[dstr]["counted_regions"] += 1

                    parts = e.get("partitions") if isinstance(e.get("partitions"), dict) else {}
                    if dstr not in agg_partitions_by_date:
                        agg_partitions_by_date[dstr] = {}
                    for pname, pstat in parts.items():
                        try:
                            p_emp = _safe_int(pstat.get("Employee"))
                            p_con = _safe_int(pstat.get("Contractor"))
                            p_tot = _safe_int(pstat.get("total")) or ((p_emp or 0) + (p_con or 0))
                            if pname not in agg_partitions_by_date[dstr]:
                                agg_partitions_by_date[dstr][pname] = {"employee": 0, "contractor": 0, "total": 0}
                            agg_partitions_by_date[dstr][pname]["employee"] += (p_emp or 0)
                            agg_partitions_by_date[dstr][pname]["contractor"] += (p_con or 0)
                            agg_partitions_by_date[dstr][pname]["total"] += (p_tot or 0)
                        except Exception:
                            continue
                except Exception:
                    continue

            today_iso = today.isoformat()
            if today_iso in agg_by_date:
                history_today_emp = agg_by_date[today_iso].get("employee", 0)
                history_today_con = agg_by_date[today_iso].get("contractor", 0)

            day_vals_emp = []
            day_vals_con = []
            day_vals_tot = []
            selected_dates = []
            for i in range((end_obj - start_obj).days + 1):
                dcheck = (start_obj + timedelta(days=i)).isoformat()
                entry = agg_by_date.get(dcheck)
                if entry:
                    day_vals_emp.append(entry.get("employee", 0))
                    day_vals_con.append(entry.get("contractor", 0))
                    day_vals_tot.append(entry.get("total", 0))
                    selected_dates.append(dcheck)

            if day_vals_emp:
                history_emp_avg = round(sum(day_vals_emp) / float(len(day_vals_emp)), 2)
            if day_vals_con:
                history_contractor_avg = round(sum(day_vals_con) / float(len(day_vals_con)), 2)
            if day_vals_tot:
                history_overall_avg = round(sum(day_vals_tot) / float(len(day_vals_tot)), 2)
            history_days = len(day_vals_tot)
            if history_days == 0:
                notes.append("History endpoints returned no usable rows in requested range; history averages not available.")

            # per-partition averages
            partition_day_values = {}
            for d_iso in selected_dates:
                per_parts = agg_partitions_by_date.get(d_iso, {})
                for pname, pvals in per_parts.items():
                    if pname not in partition_day_values:
                        partition_day_values[pname] = {"employee": [], "contractor": [], "total": []}
                    partition_day_values[pname]["employee"].append(pvals.get("employee", 0))
                    partition_day_values[pname]["contractor"].append(pvals.get("contractor", 0))
                    partition_day_values[pname]["total"].append(pvals.get("total", 0))
            for pname, lists in partition_day_values.items():
                emp_list = lists.get("employee", [])
                con_list = lists.get("contractor", [])
                tot_list = lists.get("total", [])
                days_counted = len(tot_list)
                if days_counted == 0:
                    continue
                avg_emp = round(sum(emp_list) / float(days_counted), 2)
                avg_con = round(sum(con_list) / float(days_counted), 2)
                avg_tot = round(sum(tot_list) / float(days_counted), 2)
                history_avg_by_location[pname] = {
                    "history_days_counted": int(days_counted),
                    "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                    "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                    "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
                }
    except Exception:
        logger.exception("Error fetching/processing history endpoints")
        notes.append("Failed to compute history averages from region history endpoints; partial results.")

    # Merge avg_by_location_last_range (DB) with history_avg_by_location (history)
    try:
        merged_history = dict(history_avg_by_location)
        for loc, dbvals in (avg_by_location_last_range or {}).items():
            if loc in merged_history:
                continue
            try:
                merged_history[loc] = {
                    "history_days_counted": int(dbvals.get("history_days_counted") or 0),
                    "avg_employee_last_7_days": _sanitize_for_json(dbvals.get("avg_employee_last_7_days")),
                    "avg_contractor_last_7_days": _sanitize_for_json(dbvals.get("avg_contractor_last_7_days")),
                    "avg_overall_last_7_days": _sanitize_for_json(dbvals.get("avg_overall_last_7_days"))
                }
            except Exception:
                merged_history[loc] = {
                    "history_days_counted": int(dbvals.get("history_days_counted") or 0),
                    "avg_employee_last_7_days": _sanitize_for_json(dbvals.get("avg_employee_last_7_days") or None),
                    "avg_contractor_last_7_days": _sanitize_for_json(dbvals.get("avg_contractor_last_7_days") or None),
                    "avg_overall_last_7_days": _sanitize_for_json(dbvals.get("avg_overall_last_7_days") or None)
                }
        history_avg_by_location = merged_history
    except Exception:
        logger.exception("Failed to normalize history_avg_by_location")

    # Fallback: if DB-based avg empty, prefer history_overall_avg
    if (not avg_headcount_last_range or avg_headcount_last_range == 0) and history_overall_avg:
        try:
            avg_headcount_last_range = history_overall_avg
            avg_headcount_per_site_last_range = round(history_overall_avg / float(sites_queried), 2) if sites_queried and sites_queried > 0 else None
            notes.append("avg_headcount_last_range derived from region history endpoints due to missing AttendanceSummary historical data.")
        except Exception:
            pass

    # compute percentages vs ccure
    def safe_pct(n, denom):
        try:
            if n is None or denom is None:
                return None
            if float(denom) == 0.0:
                return None
            return round((float(n) / float(denom)) * 100.0, 2)
        except Exception:
            return None

    cc_emp_denom = reported_active_emps
    cc_con_denom = reported_active_contractors
    cc_total_denom = None
    if isinstance(cc_emp_denom, int) and isinstance(cc_con_denom, int):
        cc_total_denom = cc_emp_denom + cc_con_denom

    head_emp_total = sum(v.get("employee", 0) for v in head_per_location.values())
    head_con_total = sum(v.get("contractor", 0) for v in head_per_location.values())
    live_emp_total = sum(v.get("employee", 0) for v in live_per_location.values())
    live_con_total = sum(v.get("contractor", 0) for v in live_per_location.values())

    head_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_emp_total, cc_emp_denom))
    head_contractor_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_con_total, cc_con_denom))
    head_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_total, cc_total_denom))

    live_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_emp_total, cc_emp_denom))
    live_contractor_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_con_total, cc_con_denom))
    live_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_total, cc_total_denom))

    history_emp_pct_vs_ccure = _sanitize_for_json(safe_pct(history_emp_avg, cc_emp_denom))
    history_contractor_pct_vs_ccure = _sanitize_for_json(safe_pct(history_contractor_avg, cc_con_denom))
    history_overall_pct_vs_ccure = _sanitize_for_json(safe_pct(history_overall_avg, cc_total_denom))
    history_today_employee_pct_vs_ccure = _sanitize_for_json(safe_pct(history_today_emp, cc_emp_denom))
    history_today_contractor_pct_vs_ccure = _sanitize_for_json(safe_pct(history_today_con, cc_con_denom))

    # Build final detailed result structure (compatible with earlier compute outputs)
    detailed = {
        "date": today.isoformat(),
        "headcount": {
            "total_visited_today": int(head_total),
            "employee": int(head_emp_total),
            "contractor": int(head_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in head_per_location.items() }
        },
        "live_headcount": {
            "currently_present_total": int(live_total),
            "employee": int(live_emp_total),
            "contractor": int(live_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in live_per_location.items() }
        },
        "ccure_active": {
            "ccure_active_employees_reported": _safe_int(reported_active_emps),
            "ccure_active_contractors_reported": _safe_int(reported_active_contractors)
        },
        "averages": {
            "head_emp_pct_vs_ccure_today": head_emp_pct_vs_ccure_today,
            "head_contractor_pct_vs_ccure_today": head_contractor_pct_vs_ccure_today,
            "headcount_overall_pct_vs_ccure_today": head_overall_pct_vs_ccure_today,
            "live_employee_pct_vs_ccure": live_emp_pct_vs_ccure_today,
            "live_contractor_pct_vs_ccure": _sanitize_for_json(safe_pct(live_con_total, cc_con_denom)),
            "live_overall_pct_vs_ccure": live_overall_pct_vs_ccure_today,
            "avg_headcount_last_7_days": _sanitize_for_json(avg_headcount_last_range),
            "avg_headcount_per_site_last_7_days": _sanitize_for_json(avg_headcount_per_site_last_range),
            "avg_live_per_site": _sanitize_for_json(round(live_total / sites_queried, 2) if sites_queried and sites_queried > 0 else None),
            "history_avg_employee_last_7_days": _sanitize_for_json(history_emp_avg),
            "history_avg_contractor_last_7_days": _sanitize_for_json(history_contractor_avg),
            "history_avg_overall_last_7_days": _sanitize_for_json(history_overall_avg),
            "history_days_counted": int(history_days) if history_days is not None else None,
            "history_employee_pct_vs_ccure": history_emp_pct_vs_ccure,
            "history_contractor_pct_vs_ccure": history_contractor_pct_vs_ccure,
            "history_overall_pct_vs_ccure": history_overall_pct_vs_ccure,
            "history_today_employee_count": int(history_today_emp) if history_today_emp is not None else None,
            "history_today_contractor_count": int(history_today_con) if history_today_con is not None else None,
            "history_today_employee_pct_vs_ccure": history_today_employee_pct_vs_ccure,
            "history_today_contractor_pct_vs_ccure": history_today_contractor_pct_vs_ccure,
            "avg_by_location_last_7_days": _sanitize_for_json(avg_by_location_last_range),
            "history_avg_by_location_last_7_days": _sanitize_for_json(history_avg_by_location)
        },
        "sites_queried": int(sites_queried),
        "notes": " | ".join(notes) if notes else f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}"
    }

    return _sanitize_for_json(detailed)

# ----- map detailed -> final unified response like the verify example -----
def map_to_unified_summary(detailed: Dict[str, Any], include_raw: bool = False) -> Dict[str, Any]:
    today = detailed.get("date")
    head = detailed.get("headcount", {}) or {}
    live = detailed.get("live_headcount", {}) or {}
    cc = detailed.get("ccure_active", {}) or {}
    av = detailed.get("averages", {}) or {}
    notes = detailed.get("notes")

    def to_int(v):
        try:
            return None if v is None else int(v)
        except Exception:
            try:
                return int(float(v))
            except Exception:
                return None

    cc_emp = to_int(cc.get("ccure_active_employees_reported") or cc.get("active_employees"))
    cc_con = to_int(cc.get("ccure_active_contractors_reported") or cc.get("active_contractors"))

    head_total = to_int(head.get("total_visited_today") or 0)
    head_emp = to_int(head.get("employee") or 0)
    head_con = to_int(head.get("contractor") or 0)

    live_total = to_int(live.get("currently_present_total") or 0)
    live_emp = to_int(live.get("employee") or 0)
    live_con = to_int(live.get("contractor") or 0)

    def pct(n, d):
        try:
            if n is None or d is None:
                return None
            if float(d) == 0:
                return None
            return round((float(n) / float(d)) * 100.0, 2)
        except Exception:
            return None

    summary = {
        "date": today,
        "ccure_reported": {
            "employees": cc_emp,
            "contractors": cc_con,
            "total_reported": (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None
        },
        "headcount_attendance_summary": {
            "total_visited_today": head_total,
            "employee": head_emp,
            "contractor": head_con,
            "by_location": head.get("by_location", {})
        },
        "live_headcount_region_clients": {
            "currently_present_total": live_total,
            "employee": live_emp,
            "contractor": live_con,
            "by_location": live.get("by_location", {})
        },
        "percentages_vs_ccure": {
            "head_employee_pct_vs_ccure_today": pct(head_emp, cc_emp),
            "head_contractor_pct_vs_ccure_today": pct(head_con, cc_con),
            "head_overall_pct_vs_ccure_today": pct(head_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "live_employee_pct_vs_ccure_today": pct(live_emp, cc_emp),
            "live_contractor_pct_vs_ccure_today": pct(live_con, cc_con),
            "live_overall_pct_vs_ccure_today": pct(live_total, (cc_emp + cc_con) if (cc_emp is not None and cc_con is not None) else None),
            "history_employee_pct_vs_ccure": av.get("history_employee_pct_vs_ccure") or av.get("history_emp_pct_vs_ccure") or None,
            "history_contractor_pct_vs_ccure": av.get("history_contractor_pct_vs_ccure") or av.get("history_contractor_pct_vs_ccure") or None,
            "history_overall_pct_vs_ccure": av.get("history_overall_pct_vs_ccure") or None
        },
        "averages": {
            "history_avg_employee_last_7_days": av.get("history_avg_employee_last_7_days"),
            "history_avg_contractor_last_7_days": av.get("history_avg_contractor_last_7_days"),
            "history_avg_overall_last_7_days": av.get("history_avg_overall_last_7_days"),
            "avg_headcount_last_7_days_db": av.get("avg_headcount_last_7_days"),
            "avg_headcount_per_site_last_7_days": av.get("avg_headcount_per_site_last_7_days"),
            "avg_live_per_site": av.get("avg_live_per_site"),
            "avg_by_location_last_7_days": av.get("avg_by_location_last_7_days"),
            "history_avg_by_location_last_7_days": av.get("history_avg_by_location_last_7_days"),
            "history_days_counted": av.get("history_days_counted"),
            "history_today_employee_count": av.get("history_today_employee_count"),
            "history_today_contractor_count": av.get("history_today_contractor_count"),
            "history_today_employee_pct_vs_ccure": av.get("history_today_employee_pct_vs_ccure"),
            "history_today_contractor_pct_vs_ccure": av.get("history_today_contractor_pct_vs_ccure"),
        },
        "notes": notes
    }

    if include_raw:
        summary["raw"] = detailed

    return _sanitize_for_json(summary)

# ----- single public endpoint -----
@app.get("/ccure/averages")
async def ccure_averages(
    raw: bool = Query(False, description="include raw compute payload"),
    start_date: Optional[str] = Query(None, description="YYYY-MM-DD start date (inclusive)"),
    end_date: Optional[str] = Query(None, description="YYYY-MM-DD end date (inclusive)")
):
    """
    Single consolidated API endpoint that returns:
      - headcount (AttendanceSummary unioned with live details)
      - live_headcount (region_clients)
      - ccure_active (ccure_client get_global_stats)
      - averages (history, db averages, per-location)
      - percentages vs ccure
    Accepts optional date range (defaults to last 7 days) and raw flag.
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    # run compute_visit_averages in executor with timeout
    try:
        if loop and loop.is_running():
            detailed = await asyncio.wait_for(
                loop.run_in_executor(None, compute_visit_averages, start_date, end_date, REGION_TIMEOUT_SECONDS),
                timeout=COMPUTE_WAIT_TIMEOUT_SECONDS
            )
        else:
            # no running loop (unlikely in FastAPI but safe)
            detailed = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
    except asyncio.TimeoutError:
        logger.warning("compute_visit_averages timed out after %.1fs; returning fallback minimal response.", COMPUTE_WAIT_TIMEOUT_SECONDS)
        # If compute timed out, attempt to call a quicker, lighter fallback (build minimal from DB)
        try:
            fallback = compute_visit_averages(start_date, end_date, timeout=REGION_TIMEOUT_SECONDS)
            # (we attempted; still returning fallback)
            summary = map_to_unified_summary(fallback, include_raw=raw)
            return JSONResponse(summary)
        except Exception:
            logger.exception("Fallback compute also failed after timeout")
            raise HTTPException(status_code=500, detail="compute timeout and fallback failed")
    except Exception:
        logger.exception("compute_visit_averages() raised; attempt fallback.")
        try:
            fallback = compute_visit_averages(start_date, end_date, REGION_TIMEOUT_SECONDS)
            summary = map_to_unified_summary(fallback, include_raw=raw)
            return JSONResponse(summary)
        except Exception:
            logger.exception("Fallback compute failed")
            raise HTTPException(status_code=500, detail="compute failed and fallback failed")

    if not isinstance(detailed, dict):
        raise HTTPException(status_code=500, detail="compute returned unexpected result")

    summary = map_to_unified_summary(detailed, include_raw=raw)
    return JSONResponse(summary)

# If you need to add extra endpoints later, add them below.
# For now this single endpoint replaces the prior /ccure/verify and /ccure/averages endpoints.









# ccure_compare_service.py (updated)
import re
import json
from datetime import date, datetime, timedelta
from typing import List, Dict, Any, Optional, Set
import time
import logging

logger = logging.getLogger("ccure_compare_service")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

from db import SessionLocal
from models import AttendanceSummary, LiveSwipe
from settings import OUTPUT_DIR

# ---------- small helpers ----------------------------------------------------

def _normalize_employee_key(x) -> Optional[str]:
    if x is None:
        return None
    try:
        s = str(x).strip()
        if s == "" or s.lower() in ("nan", "none", "na", "null"):
            return None
        return s
    except Exception:
        return None

def _normalize_card_like(s) -> Optional[str]:
    if s is None:
        return None
    try:
        ss = str(s).strip()
        if ss == "":
            return None
        digits = re.sub(r'\D+', '', ss)
        if digits == "":
            return None
        return digits.lstrip('0') or digits
    except Exception:
        return None

def _safe_int(v):
    try:
        if v is None:
            return None
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return None

def _sanitize_for_json(value):
    try:
        import numpy as _np
    except Exception:
        _np = None
    if value is None:
        return None
    if isinstance(value, (str, bool, int)):
        return value
    if isinstance(value, float):
        if _np is not None and not _np.isfinite(value):
            return None
        return float(value)
    if _np is not None and isinstance(value, (_np.integer,)):
        return int(value)
    if isinstance(value, dict):
        out = {}
        for k, v in value.items():
            try:
                key = str(k)
            except Exception:
                key = repr(k)
            out[key] = _sanitize_for_json(v)
        return out
    if isinstance(value, (list, tuple, set)):
        return [_sanitize_for_json(v) for v in value]
    try:
        return str(value)
    except Exception:
        return None

# ---------- ccure helpers ---------------------------------------------------

def _fetch_ccure_stats():
    try:
        import ccure_client
        if hasattr(ccure_client, "get_global_stats"):
            return ccure_client.get_global_stats()
    except Exception:
        logger.debug("ccure_client.get_global_stats not available", exc_info=True)
    return None

def _fetch_ccure_profiles():
    try:
        import ccure_client
        for fn in ("fetch_all_employees_full", "fetch_all_employees", "fetch_all_profiles", "fetch_profiles", "fetch_all"):
            if hasattr(ccure_client, fn):
                try:
                    res = getattr(ccure_client, fn)()
                    if isinstance(res, list):
                        return res
                except Exception:
                    continue
    except Exception:
        pass
    return []

def _extract_ccure_locations_from_profiles(profiles: List[dict]) -> Set[str]:
    locs = set()
    for p in profiles:
        if not isinstance(p, dict):
            continue
        for k in ("Partition", "PartitionName", "Location", "Location City", "location_city", "location", "Site", "BaseLocation"):
            v = p.get(k) if isinstance(p, dict) else None
            if v and isinstance(v, str) and v.strip():
                locs.add(v.strip())
    return locs

# ---------- classification & partition helpers ------------------------------

def classify_personnel_from_detail(detail: dict) -> str:
    """
    Classify 'detail' dict as 'employee' or 'contractor'.
    Looks for several PersonnelType/status keys; default is contractor.
    """
    try:
        if not isinstance(detail, dict):
            return "contractor"
        candidate_keys = [
            "PersonnelType", "personnelType", "personnel_type", "Personnel Type",
            "PersonnelTypeName", "Personnel", "Type", "personnel", "PersonType", "personType"
        ]
        val = None
        for k in candidate_keys:
            if k in detail and detail.get(k) is not None:
                val = str(detail.get(k)).strip().lower()
                break
        status_keys = ["Employee_Status", "Employee Status", "Status", "Profile_Disabled"]
        status_val = None
        for k in status_keys:
            if k in detail and detail.get(k) is not None:
                status_val = str(detail.get(k)).strip().lower()
                break

        # terminated/disabled rows treated as employee (they were employees historically)
        if status_val is not None and "terminated" in status_val:
            return "employee"
        if val is None or val == "":
            return "contractor"
        if "employee" in val:
            return "employee"
        contractor_terms = ["contractor", "visitor", "property", "temp", "temp badge", "tempbadge"]
        for t in contractor_terms:
            if t in val:
                return "contractor"
        if "contract" in val or "visitor" in val:
            return "contractor"
        # default to contractor if unclear
        return "contractor"
    except Exception:
        return "contractor"

def pick_partition_from_detail(detail: dict) -> str:
    if not isinstance(detail, dict):
        return "Unknown"
    for k in ("PartitionName2","PartitionName1","Partition","PartitionName","Region","Location","Site","location_city","Location City"):
        if k in detail and detail.get(k):
            try:
                return str(detail.get(k)).strip()
            except Exception:
                continue
    if "__region" in detail and detail.get("__region"):
        return str(detail.get("__region")).strip()
    return "Unknown"

# ---------- small resilient fetch helper for region_clients -----------------
def _attempt_region_call(fn, timeout, attempts=2, backoff=0.5):
    """
    Attempt calling a region_clients function fn(timeout=...) multiple times.
    fn is a callable that accepts timeout kwarg.
    returns the first successful non-None result or empty default.
    """
    last_exc = None
    for i in range(attempts):
        try:
            res = fn(timeout=timeout)
            if res is not None:
                return res
        except Exception as e:
            last_exc = e
            logger.warning("[region_clients] attempt %d/%d failed: %s", i+1, attempts, e)
            time.sleep(backoff * (i+1))
            continue
    logger.warning("[region_clients] all %d attempts failed for %s: last_exc=%s", attempts, getattr(fn, "__name__", str(fn)), last_exc)
    return None

# ---------- utility: fallback headcount builder from LiveSwipe --------------

def build_headcount_from_liveswipes_for_today(session) -> (int, Dict[str, Dict[str, int]]):
    start = datetime.combine(date.today(), datetime.min.time())
    end = datetime.combine(date.today(), datetime.max.time())
    swipes = session.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end).all()
    if not swipes:
        return 0, {}
    seen_keys = {}
    per_loc = {}
    for s in swipes:
        key = _normalize_employee_key(s.employee_id) or _normalize_card_like(s.card_number)
        if not key:
            key = f"nokey_{s.id}"
        rec = seen_keys.get(key)
        ts = s.timestamp
        if rec is None:
            seen_keys[key] = {"first_seen": ts, "last_seen": ts, "partition": (s.partition or "Unknown"), "class": None, "card": s.card_number, "raw": s.raw}
        else:
            if ts and rec.get("first_seen") and ts < rec["first_seen"]:
                rec["first_seen"] = ts
            if ts and rec.get("last_seen") and ts > rec["last_seen"]:
                rec["last_seen"] = ts
    for k, v in seen_keys.items():
        loc = v.get("partition") or "Unknown"
        if not isinstance(loc, str) or not loc.strip():
            loc = "Unknown"
        if loc not in per_loc:
            per_loc[loc] = {"total": 0, "employee": 0, "contractor": 0}
        per_loc[loc]["total"] += 1
        classified = "contractor"
        raw = v.get("raw")
        if isinstance(raw, dict):
            try:
                classified = classify_personnel_from_detail(raw)
            except Exception:
                classified = "contractor"
        per_loc[loc][classified] += 1
    total = sum(p["total"] for p in per_loc.values())
    return int(total), per_loc

# ---------- main compute function -----------------------------------------

def compute_visit_averages(start_date: Optional[str] = None, end_date: Optional[str] = None, timeout: int = 6) -> Dict[str, Any]:
    """
    Compute visit averages for an inclusive date range.
    Uses only AttendanceSummary, region_clients.fetch_all_details, region_clients.fetch_all_history and ccure stats.
    Defaults to last 7 days if no valid range provided.
    `timeout` is forwarded to region_clients functions (live/history). We use small retries internally.
    """
    notes = []
    today = date.today()

    # parse date strings
    def _parse_date_param(s):
        if not s:
            return None
        try:
            return datetime.strptime(s, "%Y-%m-%d").date()
        except Exception:
            try:
                return date.fromisoformat(s)
            except Exception:
                return None

    start_obj = _parse_date_param(start_date) if start_date else (today - timedelta(days=6))
    end_obj = _parse_date_param(end_date) if end_date else today
    if start_obj is None or end_obj is None or start_obj > end_obj:
        start_obj = today - timedelta(days=6)
        end_obj = today
        notes.append("Invalid or missing date range; defaulted to last 7 calendar days inclusive.")

    # gather CCURE stats & profiles (profiles only used if you want to filter  currently not used to drop partitions)
    ccure_stats = _fetch_ccure_stats()
    reported_active_emps = _safe_int(ccure_stats.get("ActiveEmployees")) if isinstance(ccure_stats, dict) else None
    reported_active_contractors = _safe_int(ccure_stats.get("ActiveContractors")) if isinstance(ccure_stats, dict) else None

    ccure_profiles = _fetch_ccure_profiles()
    ccure_locations = _extract_ccure_locations_from_profiles(ccure_profiles) if isinstance(ccure_profiles, list) else set()

    # ---------- HEADCOUNT (AttendanceSummary fallback logic) ----------
    head_total = 0
    head_per_location: Dict[str, Dict[str, int]] = {}
    key_map_head: Dict[str, Dict[str, Any]] = {}
    key_map_live: Dict[str, Dict[str, Any]] = {}

    try:
        session = SessionLocal()
        att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
        if not att_rows_today:
            # try to build from LiveSwipe compute_daily_attendance if available
            try:
                from compare_service import compute_daily_attendance as _compute_daily_attendance
                try:
                    built = _compute_daily_attendance(today)
                    if isinstance(built, list) and len(built) > 0:
                        att_rows_today = session.query(AttendanceSummary).filter(AttendanceSummary.date == today).all()
                        notes.append("AttendanceSummary was missing; built from LiveSwipe via compute_daily_attendance().")
                except Exception:
                    logger.exception("compute_daily_attendance execution failed; falling back")
            except Exception:
                logger.debug("compare_service.compute_daily_attendance not importable; falling back", exc_info=True)

            if not att_rows_today:
                built_total, built_per_loc = build_headcount_from_liveswipes_for_today(session)
                head_total = built_total
                head_per_location = built_per_loc
                if head_total > 0:
                    notes.append("AttendanceSummary for today empty; built headcount from LiveSwipe rows (non-persistent fallback).")
        if att_rows_today:
            # classify using AttendanceSummary.derived if possible (don't rely on Active sheets)
            for a in att_rows_today:
                key = _normalize_employee_key(a.employee_id) or _normalize_card_like((a.derived.get('card_number') if (a.derived and isinstance(a.derived, dict)) else None)) or None
                partition = None
                try:
                    if a.derived and isinstance(a.derived, dict):
                        partition = a.derived.get("partition")
                except Exception:
                    partition = None
                loc = partition or "Unknown"
                if not isinstance(loc, str) or not loc.strip():
                    loc = "Unknown"
                if (a.presence_count or 0) > 0:
                    cls = "contractor"
                    try:
                        if a.derived and isinstance(a.derived, dict):
                            cls = classify_personnel_from_detail(a.derived)
                    except Exception:
                        cls = "contractor"
                    if key:
                        key_map_head[key] = {"loc": loc, "cls": cls}
                    if loc not in head_per_location:
                        head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                    head_per_location[loc][cls] += 1
                    head_per_location[loc]["total"] += 1
                    head_total += 1
        session.expunge_all()
    except Exception:
        logger.exception("Error computing HeadCount")
        notes.append("Failed to compute HeadCount from DB; see server logs.")
    finally:
        try:
            session.close()
        except Exception:
            pass

    # --- LIVE HEADCOUNT via region_clients (realtime) ----------
    live_total = 0
    live_per_location: Dict[str, Dict[str, int]] = {}
    sites_queried = 0
    details = []
    try:
        import region_clients
        regions_info = []
        # Use resilient wrapper with retries/backoff
        try:
            if hasattr(region_clients, "fetch_all_regions"):
                maybe_regions = _attempt_region_call(region_clients.fetch_all_regions, timeout=timeout, attempts=2, backoff=0.5)
                regions_info = maybe_regions or []
        except Exception:
            logger.exception("region_clients.fetch_all_regions failed")
        try:
            if hasattr(region_clients, "fetch_all_details"):
                maybe_details = _attempt_region_call(region_clients.fetch_all_details, timeout=timeout, attempts=2, backoff=0.5)
                details = maybe_details or []
        except Exception:
            logger.exception("region_clients.fetch_all_details failed")
        sites_queried = len(regions_info) if isinstance(regions_info, list) else 0
        if regions_info:
            for r in regions_info:
                try:
                    c = r.get("count") if isinstance(r, dict) else None
                    ci = _safe_int(c)
                    if ci is not None:
                        live_total += int(ci)
                except Exception:
                    continue
        derived_detail_sum = 0
        if details and isinstance(details, list):
            for d in details:
                try:
                    loc = pick_partition_from_detail(d) or "Unknown"
                    if not isinstance(loc, str) or not loc.strip():
                        loc = "Unknown"
                    pclass = classify_personnel_from_detail(d)
                    # dedupe key
                    key = _normalize_employee_key(d.get("EmployeeID")) or _normalize_card_like(d.get("CardNumber")) or (d.get("PersonGUID") if d.get("PersonGUID") else None)
                    if not key:
                        key = _normalize_employee_key(d.get("employee_id")) or _normalize_card_like(d.get("Card")) or None
                    if not key:
                        # synthetic key for anonymous row
                        key = f"detail_{derived_detail_sum}_{str(hash(json.dumps(d, default=str)))}"
                    key = str(key)
                    key_map_live[key] = {"loc": loc, "cls": pclass}
                    if loc not in live_per_location:
                        live_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
                    live_per_location[loc]["total"] += 1
                    live_per_location[loc][pclass] += 1
                    derived_detail_sum += 1
                except Exception:
                    continue
            # decide final live_total: if regions provided prefer region totals for overall; details used for breakdown
            if live_total == 0 and derived_detail_sum > 0:
                live_total = derived_detail_sum
            else:
                if live_total != derived_detail_sum:
                    notes.append(f"Region totals ({live_total}) differ from detail rows ({derived_detail_sum}); using region totals for overall and details for breakdown.")
        else:
            notes.append("No per-person details available from region_clients; live breakdown unavailable.")
    except Exception:
        logger.exception("Error computing Live HeadCount")
        notes.append("Failed to compute Live HeadCount; see logs.")
        live_total = live_total or 0

    # ---------- Ensure headcount is union(head_keys, live_keys) ----------
    try:
        head_keys = set(k for k in key_map_head.keys() if k)
        live_keys = set(k for k in key_map_live.keys() if k)
        union_keys = head_keys.union(live_keys)

        unified_head_per_location: Dict[str, Dict[str, int]] = {}
        for k in union_keys:
            if k in key_map_head:
                loc = key_map_head[k].get("loc") or "Unknown"
                cls = key_map_head[k].get("cls") or "contractor"
            else:
                loc = key_map_live.get(k, {}).get("loc") or "Unknown"
                cls = key_map_live.get(k, {}).get("cls") or "contractor"
            if not isinstance(loc, str) or not loc.strip():
                loc = "Unknown"
            if loc not in unified_head_per_location:
                unified_head_per_location[loc] = {"total": 0, "employee": 0, "contractor": 0}
            unified_head_per_location[loc]["total"] += 1
            unified_head_per_location[loc][cls] += 1

        if union_keys:
            head_per_location = unified_head_per_location
            head_total = len(union_keys)
    except Exception:
        logger.exception("Error reconciling headcount union with live details")

    # --- Averages from AttendanceSummary (DB) - per-location & overall (range-based) ---
    avg_headcount_last_range = None
    avg_headcount_per_site_last_range = None
    avg_by_location_last_range: Dict[str, Dict[str, Any]] = {}

    try:
        session = SessionLocal()
        # build list of days in range
        days = []
        days_count = (end_obj - start_obj).days + 1
        for i in range(0, days_count):
            days.append(start_obj + timedelta(days=i))

        loc_day_vals: Dict[str, Dict[str, List[int]]] = {}
        for d in days:
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            per_loc_counts: Dict[str, Dict[str, int]] = {}
            if rows:
                for r in rows:
                    try:
                        if (r.presence_count or 0) <= 0:
                            continue
                        partition = None
                        try:
                            if r.derived and isinstance(r.derived, dict):
                                partition = r.derived.get("partition")
                        except Exception:
                            partition = None
                        loc = partition or "Unknown"
                        if not isinstance(loc, str) or not loc.strip():
                            loc = "Unknown"
                        if loc not in per_loc_counts:
                            per_loc_counts[loc] = {"employee": 0, "contractor": 0, "total": 0}
                        # classify via derived if possible
                        cls = "contractor"
                        try:
                            if r.derived and isinstance(r.derived, dict):
                                cls = classify_personnel_from_detail(r.derived)
                        except Exception:
                            cls = "contractor"
                        per_loc_counts[loc][cls] += 1
                        per_loc_counts[loc]["total"] += 1
                    except Exception:
                        continue
            for loc, counts in per_loc_counts.items():
                if loc not in loc_day_vals:
                    loc_day_vals[loc] = {"employee": [], "contractor": [], "total": []}
                loc_day_vals[loc]["employee"].append(counts.get("employee", 0))
                loc_day_vals[loc]["contractor"].append(counts.get("contractor", 0))
                loc_day_vals[loc]["total"].append(counts.get("total", 0))

        for loc, lists in loc_day_vals.items():
            emp_list = lists.get("employee", [])
            con_list = lists.get("contractor", [])
            tot_list = lists.get("total", [])
            days_counted = len(tot_list)
            if days_counted == 0:
                continue
            avg_emp = round(sum(emp_list) / float(days_counted), 2)
            avg_con = round(sum(con_list) / float(days_counted), 2)
            avg_tot = round(sum(tot_list) / float(days_counted), 2)
            avg_by_location_last_range[loc] = {
                "history_days_counted": int(days_counted),
                "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
            }

        days_totals = []
        for d in days:
            rows = session.query(AttendanceSummary).filter(AttendanceSummary.date == d).all()
            day_total = 0
            if rows:
                for r in rows:
                    if (r.presence_count or 0) > 0:
                        day_total += 1
            days_totals.append(day_total)
        if days_totals:
            avg_headcount_last_range = round(sum(days_totals) / float(len(days_totals)), 2)
            if sites_queried and sites_queried > 0:
                avg_headcount_per_site_last_range = round((sum(days_totals) / float(len(days_totals))) / float(sites_queried), 2)
        session.close()
    except Exception:
        logger.exception("Error computing averages from AttendanceSummary")
        notes.append("Failed to compute historical averages from AttendanceSummary; partial results only.")

    # --- HISTORY AVERAGES: use region_clients.fetch_all_history (range-based) ----------
    history_emp_avg = None
    history_contractor_avg = None
    history_overall_avg = None
    history_days = 0
    history_avg_by_location: Dict[str, Dict[str, Any]] = {}
    history_today_emp = None
    history_today_con = None

    try:
        import region_clients
        if hasattr(region_clients, "fetch_all_history"):
            entries = _attempt_region_call(region_clients.fetch_all_history, timeout=timeout, attempts=2, backoff=0.5) or []
            agg_by_date = {}
            agg_partitions_by_date = {}
            for e in entries:
                try:
                    dstr = e.get("date")
                    if not dstr:
                        dstr = e.get("day") or e.get("timestamp") or None
                        if isinstance(dstr, datetime):
                            dstr = dstr.date().isoformat()
                    if not dstr:
                        continue
                    # robust region-level extraction (some endpoints return region key, some return fields at top)
                    region_obj = e.get("region") if isinstance(e.get("region"), dict) else None
                    emp = None
                    con = None
                    tot = None
                    if region_obj and isinstance(region_obj, dict):
                        emp = _safe_int(region_obj.get("Employee"))
                        con = _safe_int(region_obj.get("Contractor"))
                        tot = _safe_int(region_obj.get("total")) or ((emp or 0) + (con or 0))
                    else:
                        emp = _safe_int(e.get("Employee") or (e.get("region") and e.get("region").get("Employee") if isinstance(e.get("region"), dict) else None))
                        con = _safe_int(e.get("Contractor") or (e.get("region") and e.get("region").get("Contractor") if isinstance(e.get("region"), dict) else None))
                        tot = _safe_int(e.get("total") or ((emp or 0) + (con or 0)))
                    if emp is None and con is None:
                        try:
                            robj = e.get("region") or {}
                            if isinstance(robj, dict):
                                emp = _safe_int(robj.get("Employee"))
                                con = _safe_int(robj.get("Contractor"))
                                tot = _safe_int(robj.get("total"))
                        except Exception:
                            pass
                    if emp is None and con is None:
                        continue
                    if tot is None:
                        tot = (emp or 0) + (con or 0)
                    if dstr not in agg_by_date:
                        agg_by_date[dstr] = {"employee": 0, "contractor": 0, "total": 0, "counted_regions": 0}
                    agg_by_date[dstr]["employee"] += (emp or 0)
                    agg_by_date[dstr]["contractor"] += (con or 0)
                    agg_by_date[dstr]["total"] += (tot or 0)
                    agg_by_date[dstr]["counted_regions"] += 1

                    parts = e.get("partitions") if isinstance(e.get("partitions"), dict) else {}
                    if dstr not in agg_partitions_by_date:
                        agg_partitions_by_date[dstr] = {}
                    for pname, pstat in parts.items():
                        try:
                            p_emp = _safe_int(pstat.get("Employee"))
                            p_con = _safe_int(pstat.get("Contractor"))
                            p_tot = _safe_int(pstat.get("total")) or ((p_emp or 0) + (p_con or 0))
                            if pname not in agg_partitions_by_date[dstr]:
                                agg_partitions_by_date[dstr][pname] = {"employee": 0, "contractor": 0, "total": 0}
                            agg_partitions_by_date[dstr][pname]["employee"] += (p_emp or 0)
                            agg_partitions_by_date[dstr][pname]["contractor"] += (p_con or 0)
                            agg_partitions_by_date[dstr][pname]["total"] += (p_tot or 0)
                        except Exception:
                            continue
                except Exception:
                    continue

            # history_today (if present)
            today_iso = today.isoformat()
            if today_iso in agg_by_date:
                history_today_emp = agg_by_date[today_iso].get("employee", 0)
                history_today_con = agg_by_date[today_iso].get("contractor", 0)

            # select dates within requested inclusive range
            day_vals_emp = []
            day_vals_con = []
            day_vals_tot = []
            selected_dates = []
            for i in range(0, (end_obj - start_obj).days + 1):
                dcheck = (start_obj + timedelta(days=i)).isoformat()
                entry = agg_by_date.get(dcheck)
                if entry:
                    day_vals_emp.append(entry.get("employee", 0))
                    day_vals_con.append(entry.get("contractor", 0))
                    day_vals_tot.append(entry.get("total", 0))
                    selected_dates.append(dcheck)

            if day_vals_emp:
                history_emp_avg = round(sum(day_vals_emp) / float(len(day_vals_emp)), 2)
            if day_vals_con:
                history_contractor_avg = round(sum(day_vals_con) / float(len(day_vals_con)), 2)
            if day_vals_tot:
                history_overall_avg = round(sum(day_vals_tot) / float(len(day_vals_tot)), 2)
            history_days = len(day_vals_tot)
            if history_days == 0:
                notes.append("History endpoints returned no usable rows in requested range; history averages not available.")

            # per-partition averages across the selected_dates
            partition_day_values = {}
            for d_iso in selected_dates:
                per_parts = agg_partitions_by_date.get(d_iso, {})
                for pname, pvals in per_parts.items():
                    if pname not in partition_day_values:
                        partition_day_values[pname] = {"employee": [], "contractor": [], "total": []}
                    partition_day_values[pname]["employee"].append(pvals.get("employee", 0))
                    partition_day_values[pname]["contractor"].append(pvals.get("contractor", 0))
                    partition_day_values[pname]["total"].append(pvals.get("total", 0))
            for pname, lists in partition_day_values.items():
                emp_list = lists.get("employee", [])
                con_list = lists.get("contractor", [])
                tot_list = lists.get("total", [])
                days_counted = len(tot_list)
                if days_counted == 0:
                    continue
                avg_emp = round(sum(emp_list) / float(days_counted), 2)
                avg_con = round(sum(con_list) / float(days_counted), 2)
                avg_tot = round(sum(tot_list) / float(days_counted), 2)
                history_avg_by_location[pname] = {
                    "history_days_counted": int(days_counted),
                    "avg_employee_last_7_days": _sanitize_for_json(avg_emp),
                    "avg_contractor_last_7_days": _sanitize_for_json(avg_con),
                    "avg_overall_last_7_days": _sanitize_for_json(avg_tot)
                }

            logger.debug("history: dates collected=%d partitions_sample=%d", len(selected_dates), len(history_avg_by_location))
    except Exception:
        logger.exception("Error fetching/processing history endpoints")
        notes.append("Failed to compute history averages from region history endpoints; partial results.")

    # Merge DB-derived per-location (avg_by_location_last_range) with history per-location (history_avg_by_location)
    try:
        merged_history = dict(history_avg_by_location)  # prefer history where present
        for loc, dbvals in (avg_by_location_last_range or {}).items():
            if loc in merged_history:
                continue
            try:
                merged_history[loc] = {
                    "history_days_counted": int(dbvals.get("history_days_counted") or 0),
                    "avg_employee_last_7_days": _sanitize_for_json(dbvals.get("avg_employee_last_7_days")),
                    "avg_contractor_last_7_days": _sanitize_for_json(dbvals.get("avg_contractor_last_7_days")),
                    "avg_overall_last_7_days": _sanitize_for_json(dbvals.get("avg_overall_last_7_days"))
                }
            except Exception:
                merged_history[loc] = {
                    "history_days_counted": int(dbvals.get("history_days_counted") or 0),
                    "avg_employee_last_7_days": _sanitize_for_json(dbvals.get("avg_employee_last_7_days") or None),
                    "avg_contractor_last_7_days": _sanitize_for_json(dbvals.get("avg_contractor_last_7_days") or None),
                    "avg_overall_last_7_days": _sanitize_for_json(dbvals.get("avg_overall_last_7_days") or None)
                }
        history_avg_by_location = merged_history
    except Exception:
        logger.exception("Failed to normalize history_avg_by_location")

    # Fallback: if DB-based avg empty, use history_overall_avg
    if (not avg_headcount_last_range or avg_headcount_last_range == 0) and history_overall_avg:
        try:
            avg_headcount_last_range = history_overall_avg
            avg_headcount_per_site_last_range = round(history_overall_avg / float(sites_queried), 2) if sites_queried and sites_queried > 0 else None
            notes.append("avg_headcount_last_range derived from region history endpoints due to missing AttendanceSummary historical data.")
        except Exception:
            pass

    # --- compute percentages (head/live vs CCURE reported)
    def safe_pct(n, denom):
        try:
            if n is None or denom is None:
                return None
            if float(denom) == 0.0:
                return None
            return round((float(n) / float(denom)) * 100.0, 2)
        except Exception:
            return None

    cc_emp_denom = reported_active_emps
    cc_con_denom = reported_active_contractors
    cc_total_denom = None
    if isinstance(cc_emp_denom, int) and isinstance(cc_con_denom, int):
        cc_total_denom = cc_emp_denom + cc_con_denom

    head_emp_total = sum(v.get("employee", 0) for v in head_per_location.values())
    head_con_total = sum(v.get("contractor", 0) for v in head_per_location.values())
    live_emp_total = sum(v.get("employee", 0) for v in live_per_location.values())
    live_con_total = sum(v.get("contractor", 0) for v in live_per_location.values())

    head_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_emp_total, cc_emp_denom))
    head_contractor_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_con_total, cc_con_denom))
    head_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(head_total, cc_total_denom))

    live_emp_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_emp_total, cc_emp_denom))
    live_contractor_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_con_total, cc_con_denom))
    live_overall_pct_vs_ccure_today = _sanitize_for_json(safe_pct(live_total, cc_total_denom))

    history_emp_pct_vs_ccure = _sanitize_for_json(safe_pct(history_emp_avg, cc_emp_denom))
    history_contractor_pct_vs_ccure = _sanitize_for_json(safe_pct(history_contractor_avg, cc_con_denom))
    history_overall_pct_vs_ccure = _sanitize_for_json(safe_pct(history_overall_avg, cc_total_denom))

    history_today_emp_pct_vs_ccure = _sanitize_for_json(safe_pct(history_today_emp, cc_emp_denom))
    history_today_contractor_pct_vs_ccure = _sanitize_for_json(safe_pct(history_today_con, cc_con_denom))

    # Final result
    result = {
        "date": today.isoformat(),
        "headcount": {
            "total_visited_today": int(head_total),
            "employee": int(head_emp_total),
            "contractor": int(head_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in head_per_location.items() }
        },
        "live_headcount": {
            "currently_present_total": int(live_total),
            "employee": int(live_emp_total),
            "contractor": int(live_con_total),
            "by_location": { loc: {"total": int(stats.get("total", 0)), "employee": int(stats.get("employee", 0)), "contractor": int(stats.get("contractor", 0))} for loc, stats in live_per_location.items() }
        },
        "ccure_active": {
            "ccure_active_employees_reported": _safe_int(reported_active_emps),
            "ccure_active_contractors_reported": _safe_int(reported_active_contractors)
        },
        "averages": {
            "head_emp_pct_vs_ccure_today": head_emp_pct_vs_ccure_today,
            "head_contractor_pct_vs_ccure_today": head_contractor_pct_vs_ccure_today,
            "headcount_overall_pct_vs_ccure_today": head_overall_pct_vs_ccure_today,
            "live_employee_pct_vs_ccure": live_emp_pct_vs_ccure_today,
            "live_contractor_pct_vs_ccure": _sanitize_for_json(safe_pct(live_con_total, cc_con_denom)),
            "live_overall_pct_vs_ccure": live_overall_pct_vs_ccure_today,
            # range-keys (kept for compatibility)
            "avg_headcount_last_7_days": _sanitize_for_json(avg_headcount_last_range),
            "avg_headcount_per_site_last_7_days": _sanitize_for_json(avg_headcount_per_site_last_range),
            "avg_live_per_site": _sanitize_for_json(round(live_total / sites_queried, 2) if sites_queried and sites_queried > 0 else None),

            # history endpoint range averages
            "history_avg_employee_last_7_days": _sanitize_for_json(history_emp_avg),
            "history_avg_contractor_last_7_days": _sanitize_for_json(history_contractor_avg),
            "history_avg_overall_last_7_days": _sanitize_for_json(history_overall_avg),
            "history_days_counted": int(history_days) if history_days is not None else None,
            "history_employee_pct_vs_ccure": history_emp_pct_vs_ccure,
            "history_contractor_pct_vs_ccure": history_contractor_pct_vs_ccure,
            "history_overall_pct_vs_ccure": history_overall_pct_vs_ccure,

            # history-today specific metrics (if present)
            "history_today_employee_count": int(history_today_emp) if history_today_emp is not None else None,
            "history_today_contractor_count": int(history_today_con) if history_today_con is not None else None,
            "history_today_employee_pct_vs_ccure": history_today_emp_pct_vs_ccure,
            "history_today_contractor_pct_vs_ccure": history_today_contractor_pct_vs_ccure,

            "avg_by_location_last_7_days": _sanitize_for_json(avg_by_location_last_range),
            "history_avg_by_location_last_7_days": _sanitize_for_json(history_avg_by_location)
        },
        "sites_queried": int(sites_queried),
        "notes": " | ".join(notes) if notes else f"Computed over range {start_obj.isoformat()} -> {end_obj.isoformat()}"
    }

    return _sanitize_for_json(result)










import requests
from requests.exceptions import RequestException
from datetime import datetime
import logging
import time

logger = logging.getLogger("region_clients")
logger.setLevel(logging.INFO)
if not logger.handlers:
    import sys
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)

# Edit endpoints if your hosts/ports differ
endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/live-summary",
    "emea":  "http://10.199.22.57:3007/api/occupancy/live-summary",
    "laca":  "http://10.199.22.57:4000/api/occupancy/live-summary",
    "apac":  "http://10.199.22.57:3008/api/occupancy/live-summary"
}

history_endpoints = {
    "namer": "http://10.199.22.57:3006/api/occupancy/history",
    "emea":  "http://10.199.22.57:3007/api/occupancy/history",
    "apac":  "http://10.199.22.57:3008/api/occupancy/history",
    "laca":  "http://10.199.22.57:4000/api/occupancy/history"
}

# default retries/backoff for slow endpoints
DEFAULT_ATTEMPTS = 2
DEFAULT_BACKOFF = 0.8

def _do_get_with_retries(url, timeout, attempts=DEFAULT_ATTEMPTS, backoff=DEFAULT_BACKOFF):
    last_err = None
    for attempt in range(1, attempts + 1):
        try:
            r = requests.get(url, timeout=timeout)
            r.raise_for_status()
            try:
                return r.json()
            except ValueError:
                # JSON decode error: return raw text as fallback
                try:
                    return {"_raw_text": r.text}
                except Exception:
                    return None
        except RequestException as e:
            last_err = e
            logger.warning(f"[region_clients] attempt {attempt}/{attempts} failed for {url}: {e}")
            if attempt < attempts:
                time.sleep(backoff * attempt)
            continue
    logger.warning(f"[region_clients] all {attempts} attempts failed for {url}: {last_err}")
    return None

def fetch_all_regions(timeout=6):
    results = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            realtime = {}
            if isinstance(data, dict):
                realtime = data.get("realtime", {}) or {}
            total = 0
            for site in realtime.values():
                try:
                    total += int(site.get("total", 0))
                except Exception:
                    pass
            # fallback: some live-summary payloads return top-level partitions directly
            if total == 0 and isinstance(data, dict):
                # sum any top-level site shapes that look like dictionaries with 'total' keys
                for k, v in data.items():
                    if isinstance(v, dict) and "total" in v:
                        try:
                            total += int(v.get("total", 0))
                        except Exception:
                            pass
            results.append({"region": region, "count": total})
        except Exception as e:
            logger.exception(f"[region_clients] unexpected error fetching live-summary for {region}: {e}")
            results.append({"region": region, "count": None})
    return results

def fetch_all_details(timeout=6):
    all_details = []
    for region, url in endpoints.items():
        try:
            data = _do_get_with_retries(url, timeout=timeout) or {}
            details = []
            if isinstance(data, dict):
                details = data.get("details", []) or []
                # sometimes providers embed details differently
                if not details:
                    # try to collect any arrays under keys that look like detail lists
                    for k, v in data.items():
                        if k in ("details", "list", "people", "items") and isinstance(v, list):
                            details = v
                            break
            for d in details:
                try:
                    d2 = dict(d)
                    d2["__region"] = region
                    all_details.append(d2)
                except Exception:
                    continue
        except Exception as e:
            logger.warning(f"[region_clients] warning: cannot fetch details from {region}@{url}: {e}")
            continue
    return all_details

def fetch_history_for_region(region, timeout=6):
    url = history_endpoints.get(region)
    if not url:
        logger.debug(f"[fetch_history_for_region] no history endpoint for {region}")
        return []
    try:
        data = _do_get_with_retries(url, timeout=timeout) or {}
        summary = []
        # permissive parsing  callers expect list of date-aggregates under 'summaryByDate' or 'summary'
        if isinstance(data, dict):
            candidates = []
            for key in ("summaryByDate", "summary", "data", "entries"):
                if key in data and isinstance(data.get(key), list):
                    candidates = data.get(key)
                    break
            # if none found, maybe payload is already a list
            if not candidates:
                # sometimes the root is a list
                if isinstance(data.get("results"), list):
                    candidates = data.get("results")
                else:
                    # fallback: if entire response is a list-like (wrapped as dict due to json decode), try to find keys that look like per-day structures
                    # last resort: if top-level keys look like a single day's entry, wrap it
                    if "date" in data:
                        candidates = [data]
            for s in candidates:
                try:
                    s2 = dict(s)
                    s2["__region"] = region
                    summary.append(s2)
                except Exception:
                    continue
        elif isinstance(data, list):
            for s in data:
                try:
                    s2 = dict(s)
                    s2["__region"] = region
                    summary.append(s2)
                except Exception:
                    continue
        return summary
    except Exception as e:
        logger.warning(f"[region_clients] cannot fetch history for {region}@{url}: {e}")
        return []

def fetch_all_history(timeout=6):
    all_entries = []
    for region in history_endpoints.keys():
        try:
            entries = fetch_history_for_region(region, timeout=timeout)
            if entries:
                all_entries.extend(entries)
        except Exception:
            logger.exception(f"[region_clients] error aggregating history for {region}")
            continue
    return all_entries



