Check below Trend Runner .py file 
refer Commented trend Runner .py file carefully read each line struckly and make exact same logic in Uncommented file ..

diffreance between Commented and uncommented file is only Commented file build for only Pune 
and Uncommented file is for Global Multiple location 
so make exact same logic in Uncommented file 
refer Commenteed file and add misisng logic and many more 
make exact mirror copy so i can copy paste it easily 



# # backend/trend_runner.py
# from datetime import date, datetime, time, timedelta
# from pathlib import Path
# import pandas as pd
# import numpy as np
# import logging
# import hashlib
# import math
# import re
# import os
# from collections import defaultdict
# from datetime import datetime as _datetime

# # IMPORTANT: duration_report must exist and expose run_for_date(date, regions, outdir, city)
# from duration_report import run_for_date
# from duration_report import run_for_date, compute_daily_durations

# # alias imported function so local wrapper does not shadow it
# from config.door_zone import map_door_to_zone as config_map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
# from config.door_zone import map_door_to_zone as config_map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE


# # HIST_PATH: try a few likely locations (project config, repository root, absolute path)
# CANDIDATE_HISTORY = [
#     Path(__file__).parent / "config" / "current_analysis.csv",
#     Path(__file__).parent.parent / "config" / "current_analysis.csv",
#     Path.cwd() / "current_analysis.csv",
#     Path(__file__).parent / "current_analysis.csv"
# ]
# HIST_PATH = None
# for p in CANDIDATE_HISTORY:
#     if p.exists():
#         HIST_PATH = p
#         break

# if HIST_PATH is None:
#     logging.warning("Historical profile file current_analysis.csv not found in candidate locations.")
#     HIST_DF = pd.DataFrame()
# else:
#     try:
#         HIST_DF = pd.read_csv(HIST_PATH)
#         logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
#     except Exception as e:
#         logging.warning("Failed to load historical profile: %s", e)
#         HIST_DF = pd.DataFrame()

# OUTDIR = Path("./outputs")
# OUTDIR.mkdir(parents=True, exist_ok=True)
# MODELS_DIR = Path("./models")
# MODELS_DIR.mkdir(parents=True, exist_ok=True)
# logging.basicConfig(level=logging.INFO)

# # ----- small shared helpers: treat empty/placeholder tokens as None -----
# _PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

# def _is_placeholder_str(s: object) -> bool:
#     try:
#         if s is None:
#             return True
#         st = str(s).strip().lower()
#         return st in _PLACEHOLDER_STRS
#     except Exception:
#         return False


# def _normalize_id_val(v):
#     """
#     Normalize an id-like value: strip, convert floats like '320172.0' -> '320172'
#     Return None for NaN/empty/placeholder.
#     """
#     try:
#         if pd.isna(v):
#             return None
#     except Exception:
#         pass
#     if v is None:
#         return None
#     s = str(v).strip()
#     if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
#         return None
#     # strip .0 if integer-like
#     try:
#         if '.' in s:
#             f = float(s)
#             if math.isfinite(f) and f.is_integer():
#                 return str(int(f))
#     except Exception:
#         pass
#     return s


# # prefer to avoid emp:<GUID> person_uids — only treat emp: if value looks like a human id (not GUID)
# _GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')

# def _looks_like_guid(s: object) -> bool:
#     """Return True if s looks like a GUID/UUID string."""
#     if s is None:
#         return False
#     try:
#         st = str(s).strip()
#         if not st:
#             return False
#         return bool(_GUID_RE.match(st))
#     except Exception:
#         return False

# def _looks_like_name(s: object) -> bool:
#     """Heuristic: treat as a plausible human name if it contains letters and not a GUID."""
#     if s is None:
#         return False
#     try:
#         st = str(s).strip()
#         if not st:
#             return False
#         # reject GUIDs and obviously numeric ids
#         if _looks_like_guid(st):
#             return False
#         # require at least one alphabetic character
#         return bool(re.search(r'[A-Za-z]', st))
#     except Exception:
#         return False

# def _pick_first_non_guid_value(series):
#     """Pick the first non-null, non-GUID, non-placeholder value from a pandas Series (as str) or None."""
#     for v in series:
#         if v is None:
#             continue
#         try:
#             s = str(v).strip()
#             if not s:
#                 continue
#             if _is_placeholder_str(s):
#                 continue
#             if _looks_like_guid(s):
#                 continue
#             return s
#         except Exception:
#             continue
#     return None

# def _canonical_person_uid(row):
#     """
#     Create canonical person uid:
#       - prefer EmployeeID (normalized) -> 'emp:<id>' only if it is not a GUID
#       - else EmployeeIdentity -> 'uid:<val>' (GUID allowed)
#       - else EmployeeName -> hash-based 'name:<shorthash>'
#     """
#     empid = row.get('EmployeeID', None)
#     empident = row.get('EmployeeIdentity', None)
#     name = row.get('EmployeeName', None)
#     empid_n = _normalize_id_val(empid)
#     if empid_n and not _looks_like_guid(empid_n):
#         return f"emp:{empid_n}"
#     empident_n = _normalize_id_val(empident)
#     if empident_n:
#         return f"uid:{empident_n}"
#     if name and str(name).strip():
#         # stable short hash of name
#         h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
#         return f"name:{h}"
#     return None


# # small helper to extract Card from XML-like strings
# _CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
# def _extract_card_from_xml(txt):
#     try:
#         if not txt or not isinstance(txt, str):
#             return None
#         m = _CARD_XML_RE.search(txt)
#         if m:
#             return m.group(1).strip()
#         # fallback: look for CHUID ... Card: pattern or Card: 12345
#         m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
#         if m2:
#             return m2.group(1).strip()
#     except Exception:
#         pass
#     return None


# # explicit list of zones considered breaks (fallback local; config.door_zone imported earlier)
# # If config.door_zone defines BREAK_ZONES it's used. Keep a fallback here for safety.
# try:
#     _BREAK_ZONES = BREAK_ZONES
#     _OUT_OF_OFFICE_ZONE = OUT_OF_OFFICE_ZONE
# except Exception:
#     _BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
#     _OUT_OF_OFFICE_ZONE = "Out of office"

# def map_door_to_zone(door: object, direction: object = None) -> str:
#     """
#     Local wrapper that delegates to config's map_door_to_zone (aliased to config_map_door_to_zone)
#     with a defensive fallback if the config function fails.
#     """
#     try:
#         # call the aliased function imported from config
#         return config_map_door_to_zone(door, direction)
#     except Exception:
#         try:
#             if door is None:
#                 return None
#             s = str(door).strip()
#             if not s:
#                 return None
#             s_l = s.lower()
#             if direction and isinstance(direction, str):
#                 d = direction.strip().lower()
#                 if "out" in d:
#                     return _OUT_OF_OFFICE_ZONE
#                 if "in" in d:
#                     return "Reception Area"
#             if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
#                 return _OUT_OF_OFFICE_ZONE
#             return "Working Area"
#         except Exception:
#             return None

# # --- CONFIG for violation window and risk thresholds ---
# VIOLATION_WINDOW_DAYS = 90  # look-back window to count violation days (adjustable)
# # risk thresholds (numeric ranges) -> labels
# RISK_THRESHOLDS = [
#     (0.5, "Low"),
#     (1.5, "Low Medium"),
#     (2.5, "Medium"),
#     (4.0, "Medium High"),
#     (float("inf"), "High"),
# ]

# def map_score_to_label(score: float) -> (int, str):
#     """
#     Map a numeric score to RiskScore (1..5) and RiskLevel label.
#     Returns (risk_bucket, label)
#     """
#     try:
#         if score is None:
#             score = 0.0
#         s = float(score)
#     except Exception:
#         s = 0.0
#     bucket = 1
#     label = "Low"
#     for i, (threshold, lbl) in enumerate(RISK_THRESHOLDS, start=1):
#         if s <= threshold:
#             bucket = i
#             label = lbl
#             break
#     return bucket, label

# # ---------------- SCENARIOS (boolean functions) ----------------
# def scenario_long_gap(row):
#     """
#     Long gap detection — updated per request to flag gaps >= 4.5 hours (4:30).
#     """
#     try:
#         gap = int(row.get('MaxSwipeGapSeconds') or 0)
#         return gap >= int(4.5 * 3600)  # 4.5 hours = 16200 seconds
#     except Exception:
#         return False

# def scenario_short_duration(row):
#     return (row.get('DurationMinutes') or 0) < 240

# def scenario_coffee_badging(row):
#     return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

# def scenario_low_swipe_count(row):
#     return 0 < (row.get('CountSwipes') or 0) <= 2

# def scenario_single_door(row):
#     return (row.get('UniqueDoors') or 0) <= 1

# def scenario_only_in(row):
#     return int(row.get('OnlyIn', 0)) == 1

# def scenario_only_out(row):
#     return int(row.get('OnlyOut', 0)) == 1

# def scenario_overtime(row):
#     return (row.get('DurationMinutes') or 0) >= 10 * 60

# def scenario_very_long_duration(row):
#     return (row.get('DurationMinutes') or 0) >= 16 * 60

# def scenario_zero_swipes(row):
#     return int(row.get('CountSwipes', 0)) == 0

# def scenario_unusually_high_swipes(row):
#     """
#     Flag unusually high swipes *only* when accompanied by short total duration.
#     """
#     cur = int(row.get('CountSwipes') or 0)
#     dur = float(row.get('DurationMinutes') or 0.0)
#     empid = row.get('EmployeeID')

#     try:
#         if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
#             rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
#             median = rec.get('TotalSwipes_median', np.nan)
#             if pd.notna(median) and median > 0:
#                 return (cur > 3 * float(median)) and (dur < 60)
#     except Exception:
#         pass

#     try:
#         if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
#             global_med = HIST_DF['TotalSwipes_median'].median()
#             if pd.notna(global_med) and global_med > 0:
#                 return (cur > 3 * float(global_med)) and (dur < 60)
#     except Exception:
#         pass

#     return (cur > 50) and (dur < 60)

# def scenario_high_swipes_benign(row):
#     cur = int(row.get('CountSwipes') or 0)
#     dur = float(row.get('DurationMinutes') or 0.0)
#     empid = row.get('EmployeeID')
#     try:
#         if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
#             rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
#             median = rec.get('TotalSwipes_median', np.nan)
#             if pd.notna(median) and median > 0:
#                 return (cur > 3 * float(median)) and (dur >= 60)
#     except Exception:
#         pass
#     try:
#         if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
#             global_med = HIST_DF['TotalSwipes_median'].median()
#             if pd.notna(global_med) and global_med > 0:
#                 return (cur > 3 * float(global_med)) and (dur >= 60)
#     except Exception:
#         pass
#     return (cur > 50) and (dur >= 60)


# def scenario_behaviour_shift(row, hist_df=None, minutes_threshold=180):
#     """
#     Flag when today's first swipe time deviates from historical median by more than minutes_threshold (default 180 minutes = 3 hours).
#     """
#     try:
#         if pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None:
#             return False
#         first_ts = pd.to_datetime(row.get('FirstSwipe'))
#         # compute minutes since midnight for today's first swipe
#         today_minutes = first_ts.hour * 60 + first_ts.minute
#         empid = row.get('EmployeeID')
#         hist = hist_df if hist_df is not None else (HIST_DF if (HIST_DF is not None and not HIST_DF.empty) else None)
#         if hist is None or hist.empty or empid is None:
#             return False
#         try:
#             rec = hist[hist['EmployeeID'] == empid]
#             if rec.empty:
#                 return False
#             if 'FirstSwipeMinutes_median' in rec.columns:
#                 median_min = rec.iloc[0].get('FirstSwipeMinutes_median')
#             else:
#                 median_min = rec.iloc[0].get('AvgFirstSwipeMins_median', None)
#             if pd.isna(median_min) or median_min is None:
#                 return False
#             diff = abs(today_minutes - float(median_min))
#             return diff >= int(minutes_threshold)
#         except Exception:
#             return False
#     except Exception:
#         return False



# def scenario_repeated_short_breaks(row):
#     """
#     Revised logic to avoid false positives from a single long out_of_office segment.
#     """
#     try:
#         break_count = int(row.get('BreakCount') or 0)
#         total_break_mins = float(row.get('TotalBreakMinutes') or 0.0)
#         long_break_count = int(row.get('LongBreakCount') or 0)
#         short_gap_count = int(row.get('ShortGapCount') or 0)

#         if break_count >= 2:
#             return True
#         if short_gap_count >= 5:
#             return True
#         if total_break_mins >= 180 and short_gap_count >= 2:
#             return True

#         return False
#     except Exception:
#         return False


# def scenario_multiple_location_same_day(row):
#     return (row.get('UniqueLocations') or 0) > 1

# def scenario_weekend_activity(row):
#     try:
#         d = pd.to_datetime(row['Date'])
#         return d.weekday() >= 5
#     except Exception:
#         return False

# def scenario_repeated_rejection_count(row):
#     return (row.get('RejectionCount') or 0) >= 2

# def scenario_badge_sharing_suspected(row, badge_map=None):
#     card = row.get('CardNumber')
#     d = row.get('Date')
#     if card is None or pd.isna(card) or d is None:
#         return False
#     if badge_map is None:
#         return False
#     return badge_map.get((d, card), 0) > 1

# def scenario_early_arrival_before_06(row):
#     fs = row.get('FirstSwipe')
#     if pd.isna(fs) or fs is None:
#         return False
#     try:
#         t = pd.to_datetime(fs).time()
#         return t < time(hour=6)
#     except Exception:
#         return False

# def scenario_late_exit_after_22(row):
#     ls = row.get('LastSwipe')
#     if pd.isna(ls) or ls is None:
#         return False
#     try:
#         t = pd.to_datetime(ls).time()
#         return t >= time(hour=22)
#     except Exception:
#         return False

# def scenario_shift_inconsistency(row):
#     empid = row.get('EmployeeID')
#     dur = row.get('DurationMinutes') or 0
#     if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
#         rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
#         med = rec.get('AvgDurationMins_median', np.nan)
#         std = rec.get('AvgDurationMins_std', np.nan)
#         if pd.notna(med) and pd.notna(std):
#             return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
#     return False

# def scenario_trending_decline(row):
#     empid = row.get('EmployeeID')
#     if HIST_DF is None or HIST_DF.empty:
#         return False
#     if 'TrendingDecline' in HIST_DF.columns:
#         rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
#         if not rec.empty:
#             val = rec.iloc[0].get('TrendingDecline')
#             return str(val).strip().lower() == 'yes' if pd.notna(val) else False
#     return False

# def scenario_consecutive_absent_days(row):
#     if row.get('CountSwipes') == 0:
#         empid = row.get('EmployeeID')
#         if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
#             rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
#             if not rec.empty:
#                 v = rec.iloc[0].get('ConsecAbsent3Plus')
#                 return str(v).strip().lower() in ('yes', 'true', '1')
#         return False
#     return False

# def scenario_high_variance_duration(row):
#     empid = row.get('EmployeeID')
#     if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
#         rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
#         med = rec.get('AvgDurationMins_median', np.nan)
#         std = rec.get('AvgDurationMins_std', np.nan)
#         if pd.notna(med) and pd.notna(std) and med > 0:
#             return (std / med) > 1.0
#     return False

# def scenario_short_duration_on_high_presence_days(row):
#     days_present = row.get('DaysPresentInWeek') or 0
#     dur = row.get('DurationMinutes') or 0
#     return (days_present >= 4) and (dur < 240)

# def scenario_swipe_overlap(row, swipe_overlap_map=None):
#     d = row.get('Date')
#     uid = row.get('person_uid')
#     if swipe_overlap_map is None or d is None or uid is None:
#         return False
#     return (d, uid) in swipe_overlap_map

# # NEW scenario: the pattern described by the user
# def scenario_shortstay_longout_repeat(row):
#     # Uses the feature computed in compute_features: PatternShortLongRepeat
#     return bool(row.get('PatternShortLongRepeat', False))


# # scenario list (name, fn)
# SCENARIOS = [
#     ("long_gap_>=4.5h", scenario_long_gap),
#     ("short_duration_<4h", scenario_short_duration),
#     ("coffee_badging", scenario_coffee_badging),
#     ("low_swipe_count_<=2", scenario_low_swipe_count),
#     ("single_door", scenario_single_door),
#     ("only_in", scenario_only_in),
#     ("only_out", scenario_only_out),
#     ("overtime_>=10h", scenario_overtime),
#     ("very_long_duration_>=16h", scenario_very_long_duration),
#     ("zero_swipes", scenario_zero_swipes),
#     ("unusually_high_swipes", scenario_unusually_high_swipes),
#     ("repeated_short_breaks", scenario_repeated_short_breaks),
#     ("multiple_location_same_day", scenario_multiple_location_same_day),
#     ("weekend_activity", scenario_weekend_activity),
#     ("repeated_rejection_count", scenario_repeated_rejection_count),
#     ("badge_sharing_suspected", scenario_badge_sharing_suspected),
#     ("early_arrival_before_06", scenario_early_arrival_before_06),
#     ("late_exit_after_22", scenario_late_exit_after_22),
#     ("shift_inconsistency", scenario_shift_inconsistency),
#     ("trending_decline", scenario_trending_decline),
#     ("consecutive_absent_days", scenario_consecutive_absent_days),
#     ("high_variance_duration", scenario_high_variance_duration),
#     ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
#     ("swipe_overlap", scenario_swipe_overlap),
#     ("high_swipes_benign", scenario_high_swipes_benign),
#     # new pattern scenario
#     ("behaviour_shift", scenario_behaviour_shift),
#     ("shortstay_longout_repeat", scenario_shortstay_longout_repeat)
# ]

# # --- Human readable explanations per scenario (short and neutral) ---
# def _fmt_minutes(seconds):
#     try:
#         m = int(round((seconds or 0) / 60.0))
#         return f"{m} min" if m < 60 else f"{m//60} hr {m%60} min"
#     except Exception:
#         return None

# SCENARIO_EXPLANATIONS = {
#     "long_gap_>=4.5h": lambda r: f"Long gap between swipes (~{_fmt_minutes(r.get('MaxSwipeGapSeconds'))}). This may indicate an extended out-of-office absence.",
#     "short_duration_<4h": lambda r: f"Short total presence ({int(round(r.get('DurationMinutes',0)))} min) — less than 4 hours; may indicate short stay or partial-day attendance.",
#     "coffee_badging": lambda r: "Multiple quick swipes in short time (possible 'coffee' or proxy badge use).",
#     "low_swipe_count_<=2": lambda r: "Very few swipes on day — unusually low activity.",
#     "single_door": lambda r: "Only a single door used during the day — possible badge-sharing or single-entry behavior.",
#     "only_in": lambda r: "Only 'IN' events recorded without corresponding 'OUT'.",
#     "only_out": lambda r: "Only 'OUT' events recorded without prior 'IN'.",
#     "overtime_>=10h": lambda r: "Overtime detected (>=10 hours).",
#     "very_long_duration_>=16h": lambda r: "Very long presence (>=16 hours).",
#     "zero_swipes": lambda r: "No swipes recorded on this day.",
#     "unusually_high_swipes": lambda r: "Unusually high number of swipes compared to peers/history.",
#     "repeated_short_breaks": lambda r: "Many short gaps between swipes — repeated short breaks pattern.",
#     "multiple_location_same_day": lambda r: "Multiple locations/partitions used in same day.",
#     "weekend_activity": lambda r: "Activity recorded on weekend day.",
#     "repeated_rejection_count": lambda r: "Multiple rejection events recorded.",
#     "badge_sharing_suspected": lambda r: "Same card used by multiple users on same day — possible badge sharing.",
#     "early_arrival_before_06": lambda r: "First swipe earlier than 06:00.",
#     "late_exit_after_22": lambda r: f"Last swipe after 22:00 ({(pd.to_datetime(r.get('LastSwipe')).time() if pd.notna(r.get('LastSwipe')) else 'time unknown')}).",
#     "shift_inconsistency": lambda r: "Duration deviates from historical shift patterns.",
#     "trending_decline": lambda r: "Employee shows trending decline in presence.",
#     "consecutive_absent_days": lambda r: "Consecutive absent days observed historically.",
#     "high_variance_duration": lambda r: "High variance in daily durations historically.",
#     "short_duration_on_high_presence_days": lambda r: "Short duration despite normally high presence days.",
#     "swipe_overlap": lambda r: "Overlap in swipe times with other persons on same door — suspicious co-located events.",
#     "behaviour_shift": lambda r: "Significant change in arrival time compared to historical baseline — behaviour shift detected.",
#     "shortstay_longout_repeat": lambda r: "Repeated pattern: short in → long out-of-office → short return — may indicate leaving site for extended period between brief visits."
# }

# def _explain_scenarios_detected(row, detected_list):
#     pieces = []
#     name = row.get('EmployeeName') or row.get('EmployeeID') or row.get('person_uid') or "Employee"
#     prefix = f"{name} - "
#     for sc in detected_list:
#         sc = sc.strip()
#         fn = SCENARIO_EXPLANATIONS.get(sc)
#         try:
#             if fn:
#                 # let each lambda produce a sentence
#                 pieces.append(fn(row))
#             else:
#                 pieces.append(sc.replace("_", " ").replace(">=", "≥"))
#         except Exception:
#             pieces.append(sc)
#     if not pieces:
#         return None
#     # Join as sentences for clarity.
#     explanation = " ".join([p if p.endswith('.') else p + '.' for p in pieces])
#     return prefix + " " + explanation

# # --- compute_features (replaced/updated) ---
# def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
#     """
#     Compute per person-per-date features used by scenarios.
#     Returns DataFrame per (person_uid, Date) with feature columns and normalized IDs/names.
#     """
#     if swipes is None or swipes.empty:
#         return pd.DataFrame()

#     sw = swipes.copy()

#     # Build lowercase->actual column map for flexible column detection
#     cols_lower = {c.lower(): c for c in sw.columns}

#     # detect time column
#     time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
#     found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)
#     if found_time_col:
#         sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
#         sw['Date'] = sw['LocaleMessageTime'].dt.date
#     else:
#         if 'Date' in sw.columns:
#             sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
#         else:
#             sw['Date'] = None

#     # find these earlier in compute_features — prefer Int1/Text12 for EmployeeID and CHUID/Card for CardNumber
#     name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
#     employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
#     card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
#     door_candidates = ['door', 'doorname', 'door_name']
#     direction_candidates = ['direction', 'directionname', 'direction_name']

#     name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
#     empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
#     card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
#     door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
#     dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    
#     # --- normalise commonly used column names so downstream code can rely on them ---
#     # map detected source columns to canonical names used throughout this function
#     try:
#         if dir_col and dir_col in sw.columns:
#             sw['Direction'] = sw[dir_col]
#         if door_col and door_col in sw.columns:
#             sw['Door'] = sw[door_col]
#         if empid_col and empid_col in sw.columns:
#             sw['EmployeeID'] = sw[empid_col]
#         if name_col and name_col in sw.columns:
#             sw['EmployeeName'] = sw[name_col]
#         if card_col and card_col in sw.columns:
#             sw['CardNumber'] = sw[card_col]

#         # ensure Date exists and use LocaleMessageTime if available
#         if 'LocaleMessageTime' in sw.columns:
#             sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
#         elif 'Date' in sw.columns:
#             sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
#     except Exception:
#         logging.exception("Normalization of swipe columns failed.")


#     # Filter personnel types: tolerant matching to avoid accidental row drops
#     if 'PersonnelTypeName' in sw.columns:
#         sw['PersonnelTypeName'] = sw['PersonnelTypeName'].astype(str).str.strip()
#         mask = sw['PersonnelTypeName'].str.lower().str.contains(r'employee|terminated', na=False)
#         logging.info("PersonnelTypeName values example: %s", list(sw['PersonnelTypeName'].dropna().unique()[:6]))
#         before = len(sw)
#         sw = sw[mask].copy()
#         logging.info("PersonnelTypeName filter applied: before=%d after=%d", before, len(sw))
#     elif 'PersonnelType' in sw.columns:
#         sw['PersonnelType'] = sw['PersonnelType'].astype(str).str.strip()
#         mask = sw['PersonnelType'].str.lower().str.contains(r'employee|terminated', na=False)
#         before = len(sw)
#         sw = sw[mask].copy()
#         logging.info("PersonnelType filter applied: before=%d after=%d", before, len(sw))


#     # else keep everything

#     if sw.empty:
#         logging.info("compute_features: no rows after PersonnelType filter")
#         return pd.DataFrame()

#     # ensure stable person_uid (canonical)
#     if 'person_uid' not in sw.columns:
#         def make_person_uid_local(r):
#             # prefer canonical EmployeeID (normalized, non-GUID) then EmployeeIdentity then EmployeeName
#             empid_val = None
#             if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
#                 empid_val = r.get(empid_col)
#             elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
#                 empid_val = r.get('EmployeeID')

#             empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
#             name_val = None
#             if name_col and name_col in r:
#                 name_val = r.get(name_col)
#             elif 'EmployeeName' in r:
#                 name_val = r.get('EmployeeName')
#             elif 'ObjectName1' in r:
#                 name_val = r.get('ObjectName1')

#             return _canonical_person_uid({
#                 'EmployeeID': empid_val,
#                 'EmployeeIdentity': empident_val,
#                 'EmployeeName': name_val
#             })
#         sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

#     # selection columns for aggregation: include discovered columns
#     sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
#                     'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
#                     'EmployeeIdentity'])
#     if name_col:
#         sel_cols.add(name_col)
#     if empid_col:
#         sel_cols.add(empid_col)
#     if card_col:
#         sel_cols.add(card_col)
#     if door_col:
#         sel_cols.add(door_col)
#     if dir_col:
#         sel_cols.add(dir_col)
#     sel_cols = [c for c in sel_cols if c in sw.columns]

#     def agg_swipe_group(g):
#         # g is a DataFrame for one person_uid + date
#         times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
#         gaps = []
#         short_gap_count = 0
#         for i in range(1, len(times)):
#             s = (times[i] - times[i-1]).total_seconds()
#             gaps.append(s)
#             if s <= 5*60:
#                 short_gap_count += 1
#         max_gap = int(max(gaps)) if gaps else 0

#         # Direction counts (default to column names present)
#         in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
#         out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
#         unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
#         unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
#         rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

#         # pick first non-placeholder, non-guid card number if present (prefer cardnumber/chuid)
#         card_numbers = []
#         # 1) direct known column
#         if card_col and card_col in g.columns:
#             card_numbers = list(pd.unique(g[card_col].dropna()))
#         # 2) explicit 'CardNumber' output column (from SQL COALESCE)
#         if not card_numbers and 'CardNumber' in g.columns:
#             card_numbers = list(pd.unique(g['CardNumber'].dropna()))
#         # 3) some XML-shred columns may appear as 'value' or other column names
#         if not card_numbers:
#             for c in g.columns:
#                 cl = c.lower()
#                 if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
#                     try:
#                         vals = list(pd.unique(g[c].dropna()))
#                         if vals:
#                             card_numbers.extend(vals)
#                     except Exception:
#                         continue
#         # 4) lastly try to extract from XmlMessage fields
#         if not card_numbers:
#             for c in g.columns:
#                 cl = c.lower()
#                 if 'xml' in cl or 'xmlmessage' in cl or 'xml_msg' in cl or 'xmlmessage' in cl:
#                     for raw in g[c].dropna().astype(str):
#                         extracted = _extract_card_from_xml(raw)
#                         if extracted:
#                             card_numbers.append(extracted)
#         # 5) final unique
#         card_numbers = list(dict.fromkeys(card_numbers))  # preserve order, unique

#         card_number = None
#         for c in card_numbers:
#             n = _normalize_id_val(c)
#             # explicitly reject GUIDs as card numbers
#             if n and not _looks_like_guid(n):
#                 card_number = n
#                 break

#         # stable id/name from the group using discovered columns first
#         employee_id = None
#         employee_name = None
#         employee_identity = None
#         personnel_type = None

#         # Employee ID: prefer Int1/Text12 then EmployeeID; DO NOT use EmployeeIdentity as EmployeeID
#         # use _pick_first_non_guid_value to skip GUIDs automatically
#         if empid_col and empid_col in g.columns:
#             vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
#             employee_id = _pick_first_non_guid_value(vals)
#             if employee_id is None and not vals.empty:
#                 v0 = vals.iloc[0]
#                 normalized = _normalize_id_val(v0)
#                 if normalized and not _looks_like_guid(normalized):
#                     employee_id = normalized
#         elif 'EmployeeID' in g.columns:
#             vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
#             employee_id = _pick_first_non_guid_value(vals)
#             if employee_id is None and not vals.empty:
#                 v0 = vals.iloc[0]
#                 normalized = _normalize_id_val(v0)
#                 if normalized and not _looks_like_guid(normalized):
#                     employee_id = normalized

#         # If still no employee_id and PersonnelType indicates contractor -> prefer Text12 explicitly
#         if (not employee_id) and 'PersonnelType' in g.columns:
#             try:
#                 pvals = g['PersonnelType'].dropna().astype(str)
#                 if not pvals.empty:
#                     p0 = pvals.iloc[0]
#                     if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
#                         # look for text12 explicitly (case-insensitive)
#                         for c in g.columns:
#                             if c.lower() == 'text12':
#                                 vals = g[c].dropna().astype(str).map(lambda x: x.strip())
#                                 employee_id = _pick_first_non_guid_value(vals)
#                                 if employee_id:
#                                     break
#             except Exception:
#                 pass

#         # Employee identity (GUID) — keep but do not promote to EmployeeID
#         if 'EmployeeIdentity' in g.columns:
#             vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
#             if not vals.empty:
#                 employee_identity = vals.iloc[0]

#         # Employee name: pick non-GUID candidate
#         candidate_name_vals = None
#         if name_col and name_col in g.columns:
#             candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
#         elif 'EmployeeName' in g.columns:
#             candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
#         elif 'ObjectName1' in g.columns:
#             candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

#         if candidate_name_vals is not None and not candidate_name_vals.empty:
#             employee_name = _pick_first_non_guid_value(candidate_name_vals)
#             if employee_name is None:
#                 # accept any value that looks like a name
#                 for v in candidate_name_vals:
#                     if _looks_like_name(v) and not _is_placeholder_str(v):
#                         employee_name = str(v).strip()
#                         break

#         # personnel type
#         if 'PersonnelTypeName' in g.columns:
#             vals = g['PersonnelTypeName'].dropna()
#             if not vals.empty:
#                 personnel_type = vals.iloc[0]
#         elif 'PersonnelType' in g.columns:
#             vals = g['PersonnelType'].dropna()
#             if not vals.empty:
#                 personnel_type = vals.iloc[0]

#         # First/Last swipe times
#         first_swipe = None
#         last_swipe = None
#         if times:
#             first_swipe = times[0]
#             last_swipe = times[-1]

#         # ----------------- NEW: break/out-of-office sequence analysis -----------------
#         # Build a timeline of (time, door, direction, zone)
#         timeline = []
#         for _, row in g.sort_values('LocaleMessageTime').iterrows():
#             t = row.get('LocaleMessageTime')
#             dname = None
#             if door_col and door_col in row and pd.notna(row.get(door_col)):
#                 dname = row.get(door_col)
#             elif 'Door' in row and pd.notna(row.get('Door')):
#                 dname = row.get('Door')
#             direction = None
#             if dir_col and dir_col in row and pd.notna(row.get(dir_col)):
#                 direction = row.get(dir_col)
#             elif 'Direction' in row and pd.notna(row.get('Direction')):
#                 direction = row.get('Direction')
#             zone = map_door_to_zone(dname, direction)
#             timeline.append((t, dname, direction, zone))

#         # compress timeline into segments with labels: 'work', 'break', 'out_of_office'
#         segments = []
#         if timeline:
#             cur_zone = None
#             seg_start = timeline[0][0]
#             seg_label = None
#             for (t, dname, direction, zone) in timeline:
#                 # determine label
#                 if zone in _BREAK_ZONES:
#                     lbl = 'break'
#                 elif zone == _OUT_OF_OFFICE_ZONE:
#                     lbl = 'out_of_office'
#                 else:
#                     lbl = 'work'
#                 if cur_zone is None:
#                     cur_zone = zone
#                     seg_label = lbl
#                     seg_start = t
#                 else:
#                     # if label changes, close previous segment
#                     if lbl != seg_label:
#                         segments.append({
#                             'label': seg_label,
#                             'start': seg_start,
#                             'end': t,
#                             'start_zone': cur_zone
#                         })
#                         seg_start = t
#                         seg_label = lbl
#                         cur_zone = zone
#                     else:
#                         # keep current segment (extend)
#                         cur_zone = cur_zone or zone
#             # close last
#             if seg_label is not None:
#                 segments.append({
#                     'label': seg_label,
#                     'start': seg_start,
#                     'end': timeline[-1][0],
#                     'start_zone': cur_zone
#                 })

#         # Compute break metrics: only count "real" breaks (and only if long enough)
#         break_count = 0
#         long_break_count = 0
#         total_break_minutes = 0.0


#         # thresholds (minutes)
#         # Count break segments only if >= 1 hour (60 min) for break zones
#         BREAK_MINUTES_THRESHOLD = 60

#         # Out-of-office needs to be at least 3 hours (180 min) to be counted as a break segment
#         OUT_OFFICE_COUNT_MINUTES = 180

#         # Long-break flag threshold: use 120 minutes (2 hours).
#         # This ensures single long break ≥ 2h will be considered significant,
#         # and two breaks >= 1h each will also be detectable via BreakCount >= 2.
#         LONG_BREAK_FLAG_MINUTES = 120


#         for i, s in enumerate(segments):
#             lbl = s.get('label')
#             start = s.get('start')
#             end = s.get('end')
#             dur_mins = ((end - start).total_seconds() / 60.0) if (start and end) else 0.0

#             if lbl == 'break':
#                 # only count break segments that are actually long enough (>= BREAK_MINUTES_THRESHOLD)
#                 if dur_mins >= BREAK_MINUTES_THRESHOLD:
#                     break_count += 1
#                     total_break_minutes += dur_mins
#                     if dur_mins >= LONG_BREAK_FLAG_MINUTES:
#                         long_break_count += 1

#             elif lbl == 'out_of_office':
#                 # for out_of_office: treat as break only when it's between work segments
#                 # and satisfies out-office threshold (>= OUT_OFFICE_COUNT_MINUTES)
#                 prev_lbl = segments[i-1]['label'] if i > 0 else None
#                 next_lbl = segments[i+1]['label'] if i < len(segments)-1 else None
#                 # Only treat as break if it's between work segments and long enough (>= OUT_OFFICE_COUNT_MINUTES)
#                 if prev_lbl == 'work' and next_lbl == 'work' and dur_mins >= OUT_OFFICE_COUNT_MINUTES:
#                     break_count += 1
#                     total_break_minutes += dur_mins
#                     if dur_mins >= LONG_BREAK_FLAG_MINUTES:
#                         long_break_count += 1

#         # Detect the specific pattern:
#         pattern_flag = False
#         pattern_sequence_readable = None
#         try:
#             # create simplified label list with durations
#             seq = []
#             for s in segments:
#                 dur_mins = (s['end'] - s['start']).total_seconds() / 60.0 if (s['end'] and s['start']) else 0
#                 seq.append((s['label'], int(round(dur_mins))))
          
           
#             # look for the pattern anywhere in sequence (triplet: short work -> long out -> short work)
#             for i in range(len(seq)-2):
#                 a = seq[i]     # first work
#                 b = seq[i+1]   # long out
#                 c = seq[i+2]   # short work
#                 # short work: < 60 min; long out: >= LONG_BREAK_FLAG_MINUTES; short return: < 60 min
#                 if (a[0] == 'work' and a[1] < 60) and \
#                    (b[0] in ('out_of_office','break') and b[1] >= LONG_BREAK_FLAG_MINUTES) and \
#                    (c[0] == 'work' and c[1] < 60):
#                     pattern_flag = True
#                     seq_fragment = [a, b, c]
#                     pattern_sequence_readable = " -> ".join([f"{lbl} ({mins}m)" for lbl, mins in seq_fragment])
#                     break

#         except Exception:
#             pattern_flag = False
#             pattern_sequence_readable = None



            

#         # ----------------- return aggregated metrics (including new ones) -----------------
#         return pd.Series({
#             'CountSwipes': int(len(g)),
#             'MaxSwipeGapSeconds': max_gap,
#             'ShortGapCount': int(short_gap_count),
#             'InCount': in_count,
#             'OutCount': out_count,
#             'UniqueDoors': unique_doors,
#             'UniqueLocations': unique_locations,
#             'RejectionCount': rejection_count,
#             'CardNumber': card_number,
#             'EmployeeID': employee_id,
#             'EmployeeIdentity': employee_identity,
#             'EmployeeName': employee_name,
#             'PersonnelType': personnel_type,
#             'FirstSwipe': first_swipe,
#             'LastSwipe': last_swipe,
#             # new break features
#             'BreakCount': int(break_count),
#             'LongBreakCount': int(long_break_count),
#             'TotalBreakMinutes': float(round(total_break_minutes,1)),
#             'PatternShortLongRepeat': bool(pattern_flag),
#             'PatternSequenceReadable': pattern_sequence_readable,
#             'PatternSequence': None  # keep old field empty for compatibility
#         })

#     grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
#     grouped = grouped.apply(agg_swipe_group).reset_index()

#         # ===== POST-PROCESS MERGE: heuristic to attach tiny early-morning fragments into previous day =====
#     # This reduces spurious "separate day" rows for swipes that occur shortly after midnight (00:00-02:00)
#     try:
#         # convert FirstSwipe/LastSwipe to datetimes (they come back as timestamps)
#         grouped['FirstSwipe_dt'] = pd.to_datetime(grouped['FirstSwipe'], errors='coerce')
#         grouped['LastSwipe_dt']  = pd.to_datetime(grouped['LastSwipe'],  errors='coerce')

#         rows_to_drop = set()
#         # threshold: if a day's first swipe is in 00:00..02:00 (after original midnight), and gap to prior day's LastSwipe <= MERGE_GAP_SECONDS,
#         # then attach current day into previous day for the same person.
#         MERGE_GAP_SECONDS = int(4 * 3600)  # <= 4 hours; tweakable
#         # iterate per person
#         for pid, sub in grouped.sort_values(['person_uid','Date']).groupby('person_uid'):
#             prev_idx = None
#             for idx, r in sub.reset_index().iterrows():
#                 real_idx = int(r['index']) if 'index' in r else r.name
#                 cur_first = pd.to_datetime(grouped.at[real_idx, 'FirstSwipe_dt'])
#                 cur_date = grouped.at[real_idx, 'Date']
#                 if prev_idx is not None:
#                     prev_last = pd.to_datetime(grouped.at[prev_idx, 'LastSwipe_dt'])
#                     # if current first exists and is within MERGE_GAP_SECONDS of prev last, and the current FirstSwipe is early morning (hour < 2)
#                     if (not pd.isna(cur_first)) and (not pd.isna(prev_last)):
#                         gap = (cur_first - prev_last).total_seconds()
#                         if 0 <= gap <= MERGE_GAP_SECONDS and cur_first.time().hour < 2:
#                             try:
#                                 # merge current row into prev row: sum CountSwipes, max gap, extend LastSwipe etc.
#                                 grouped.at[prev_idx, 'CountSwipes'] = int(grouped.at[prev_idx, 'CountSwipes']) + int(grouped.at[real_idx, 'CountSwipes'])
#                                 grouped.at[prev_idx, 'MaxSwipeGapSeconds'] = max(int(grouped.at[prev_idx, 'MaxSwipeGapSeconds'] or 0), int(grouped.at[real_idx, 'MaxSwipeGapSeconds'] or 0), int(gap))
#                                 # extend LastSwipe to current's LastSwipe_dt if later
#                                 if not pd.isna(grouped.at[real_idx, 'LastSwipe_dt']):
#                                     if pd.isna(grouped.at[prev_idx, 'LastSwipe_dt']) or grouped.at[real_idx, 'LastSwipe_dt'] > grouped.at[prev_idx, 'LastSwipe_dt']:
#                                         grouped.at[prev_idx, 'LastSwipe_dt'] = grouped.at[real_idx, 'LastSwipe_dt']
#                                         grouped.at[prev_idx, 'LastSwipe'] = grouped.at[real_idx, 'LastSwipe']
#                                 # recompute CardNumber/UniqueDoors/UniqueLocations conservative approach: prefer prev if present, else use current
#                                 if not grouped.at[prev_idx, 'CardNumber']:
#                                     grouped.at[prev_idx, 'CardNumber'] = grouped.at[real_idx, 'CardNumber']
#                                 grouped.at[prev_idx, 'UniqueDoors'] = int(max(int(grouped.at[prev_idx].get('UniqueDoors') or 0), int(grouped.at[real_idx].get('UniqueDoors') or 0)))
#                                 grouped.at[prev_idx, 'UniqueLocations'] = int(max(int(grouped.at[prev_idx].get('UniqueLocations') or 0), int(grouped.at[real_idx].get('UniqueLocations') or 0)))
#                                 # mark this index to drop
#                                 rows_to_drop.add(real_idx)
#                                 # do not advance prev_idx (we keep same prev to possibly absorb multiple small fragments)
#                                 continue
#                             except Exception:
#                                 pass
#                 prev_idx = real_idx
#         if rows_to_drop:
#             grouped = grouped.drop(index=list(rows_to_drop)).reset_index(drop=True)
#     except Exception:
#         logging.exception("Failed merge-early-morning fragments (non-fatal).")


#     dur = pd.DataFrame() if durations is None else durations.copy()
#     if not dur.empty and 'Date' in dur.columns:
#         dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

#     merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

#     # --- START PATCH: coalesce duplicate columns produced by merge ---
#     def _coalesce_merge_columns(df, bases):
#         for base in bases:
#             x = base + "_x"
#             y = base + "_y"
#             try:
#                 has_base = base in df.columns
#                 base_all_null = False
#                 if has_base:
#                     base_all_null = df[base].isnull().all()
#             except Exception:
#                 has_base = base in df.columns
#                 base_all_null = True

#             if (not has_base) or base_all_null:
#                 if x in df.columns and y in df.columns:
#                     try:
#                         df[base] = df[x].combine_first(df[y])
#                     except Exception:
#                         try:
#                             df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
#                         except Exception:
#                             if x in df.columns:
#                                 df[base] = df[x]
#                             elif y in df.columns:
#                                 df[base] = df[y]
#                 elif x in df.columns:
#                     df[base] = df[x]
#                 elif y in df.columns:
#                     df[base] = df[y]
#     _coalesce_merge_columns(merged, [
#         "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
#     ])
#     drop_cols = [c for c in merged.columns if c.endswith("_x") or c.endswith("_y")]
#     if drop_cols:
#         try:
#             merged.drop(columns=drop_cols, inplace=True)
#         except Exception:
#             for c in drop_cols:
#                 if c in merged.columns:
#                     try:
#                         merged.drop(columns=[c], inplace=True)
#                     except Exception:
#                         pass
#     # --- END PATCH ---

#     # coalesce helpers (ensure column existence)
#     def ensure_col(df, col, default=None):
#         if col not in df.columns:
#             df[col] = default

#     ensure_col(merged, 'DurationSeconds', 0)
#     ensure_col(merged, 'FirstSwipe', pd.NaT)
#     ensure_col(merged, 'LastSwipe', pd.NaT)
#     ensure_col(merged, 'CountSwipes', 0)
#     ensure_col(merged, 'MaxSwipeGapSeconds', 0)
#     ensure_col(merged, 'ShortGapCount', 0)
#     ensure_col(merged, 'RejectionCount', 0)
#     ensure_col(merged, 'UniqueLocations', 0)
#     ensure_col(merged, 'UniqueDoors', 0)
#     ensure_col(merged, 'CardNumber', None)
#     ensure_col(merged, 'EmployeeID', None)
#     ensure_col(merged, 'EmployeeName', None)
#     ensure_col(merged, 'EmployeeIdentity', None)
#     ensure_col(merged, 'PersonnelType', None)
#     ensure_col(merged, 'BreakCount', 0)
#     ensure_col(merged, 'LongBreakCount', 0)
#     ensure_col(merged, 'TotalBreakMinutes', 0.0)
#     ensure_col(merged, 'PatternShortLongRepeat', False)
#     ensure_col(merged, 'PatternSequenceReadable', None)
#     ensure_col(merged, 'PatternSequence', None)

#     # If EmployeeName is missing or a GUID, try to get a better name from durations (durations typically has EmployeeName)
#     if 'EmployeeName' in merged.columns:
#         def choose_best_name(row):
#             gname = row.get('EmployeeName')
#             dname = None
#             for cand in ('EmployeeName', 'employee_name', 'objectname1', 'ObjectName1'):
#                 if cand in row and row.get(cand) is not None:
#                     dname = row.get(cand)
#                     break
#             if _looks_like_name(gname):
#                 return str(gname).strip()
#             if _looks_like_name(dname):
#                 return str(dname).strip()
#             if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
#                 return str(gname).strip()
#             if dname and not _is_placeholder_str(dname):
#                 return str(dname).strip()
#             return None
#         merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
#     else:
#         if not dur.empty:
#             def fill_name_from_dur(row):
#                 gname = row.get('EmployeeName')
#                 if _looks_like_name(gname) and not _is_placeholder_str(gname):
#                     return gname
#                 for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
#                     if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
#                         return row[cand]
#                 return None
#             merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

#     # numeric normalization for EmployeeID: ensure not GUIDs/placeholder, convert floats like '320172.0' -> '320172'
#     def normalize_empid(v):
#         if v is None:
#             return None
#         try:
#             s = str(v).strip()
#             if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
#                 return None
#             if _looks_like_guid(s):
#                 return None
#             try:
#                 if '.' in s:
#                     f = float(s)
#                     if math.isfinite(f) and f.is_integer():
#                         return str(int(f))
#             except Exception:
#                 pass
#             return s
#         except Exception:
#             return None

#     merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

#     # normalize card numbers: reject GUIDs and placeholder tokens
#     def normalize_card(v):
#         if v is None:
#             return None
#         try:
#             s = str(v).strip()
#             if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
#                 return None
#             if _looks_like_guid(s):
#                 return None
#             return s
#         except Exception:
#             return None

#     merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

#     # numeric normalization
#     # If durations DataFrame provided DurationSeconds, use that; else fall back to computed (LastSwipe-FirstSwipe)
#     if 'DurationSeconds' not in merged.columns or merged['DurationSeconds'].isnull().all():
#         try:
#             merged['DurationSeconds'] = (pd.to_datetime(merged['LastSwipe']) - pd.to_datetime(merged['FirstSwipe'])).dt.total_seconds().clip(lower=0).fillna(0)
#         except Exception:
#             merged['DurationSeconds'] = merged.get('DurationSeconds', 0)

#     merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
#     merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
#     merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
#     merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
#     merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
#     merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
#     merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
#     merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)
#     merged['BreakCount'] = merged['BreakCount'].fillna(0).astype(int)
#     merged['LongBreakCount'] = merged['LongBreakCount'].fillna(0).astype(int)
#     merged['TotalBreakMinutes'] = merged['TotalBreakMinutes'].fillna(0.0).astype(float)
#     merged['PatternShortLongRepeat'] = merged['PatternShortLongRepeat'].fillna(False).astype(bool)

#     # ensure FirstSwipe/LastSwipe are datetimes
#     for col in ['FirstSwipe', 'LastSwipe']:
#         try:
#             merged[col] = pd.to_datetime(merged[col], errors='coerce')
#         except Exception:
#             merged[col] = pd.NaT

#     merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
#     merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
#     merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

#     # EmpHistoryPresent
#     hist_map = {}
#     if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
#         hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
#     merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

#     # normalize string columns for safe downstream use; EmployeeName keep as readable-only
#     for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
#         if c in merged.columns:
#             def _clean_str_val(v):
#                 if v is None:
#                     return None
#                 try:
#                     s = str(v).strip()
#                     if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
#                         return None
#                     return s
#                 except Exception:
#                     return None
#             merged[c] = merged[c].apply(_clean_str_val)

#     # EmployeeName: keep None if empty or GUID/placeholder; otherwise string.
#     if 'EmployeeName' in merged.columns:
#         merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

#     return merged


# # ---------------- SCENARIO WEIGHTS (for anomaly scoring) ----------------
# WEIGHTS = {
#     "long_gap_>=4.5h": 0.3,
#     "short_duration_<4h": 1.0,
#     "coffee_badging": 1.0,
#     "low_swipe_count_<=2": 0.5,
#     "single_door": 0.25,
#     "only_in": 0.8,
#     "only_out": 0.8,
#     "overtime_>=10h": 0.2,
#     "very_long_duration_>=16h": 1.5,
#     "zero_swipes": 0.4,
#     "unusually_high_swipes": 1.5,
#     "repeated_short_breaks": 0.5,
#     "multiple_location_same_day": 0.6,
#     "weekend_activity": 0.6,
#     "repeated_rejection_count": 0.8,
#     "badge_sharing_suspected": 2.0,
#     "early_arrival_before_06": 0.4,
#     "late_exit_after_22": 0.4,
#     "shift_inconsistency": 1.2,
#     "trending_decline": 0.7,
#     "consecutive_absent_days": 1.2,
#     "high_variance_duration": 0.8,
#     "short_duration_on_high_presence_days": 1.1,
#     "swipe_overlap": 2.0,
#     "high_swipes_benign": 0.1,

#     # weight for new scenario
#     "shortstay_longout_repeat": 2.0
# }
# ANOMALY_THRESHOLD = 1.5


# def _read_past_trend_csvs(outdir: str, window_days: int, target_date: date):
#     """
#     Read existing trend_pune_*.csv in outdir and return a single DataFrame filtered to the
#     window (target_date - window_days .. target_date-1).
#     """
#     p = Path(outdir)
#     csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
#     if not csvs:
#         return pd.DataFrame()
#     dfs = []
#     cutoff = target_date - timedelta(days=window_days)
#     for fp in csvs:
#         try:
#             df = pd.read_csv(fp, parse_dates=['Date'])
#             # keep only rows with date in (cutoff .. target_date-1)
#             if 'Date' in df.columns:
#                 try:
#                     df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
#                 except Exception:
#                     pass
#                 df = df[df['Date'].apply(lambda d: d is not None and d >= cutoff and d < target_date)]
#             dfs.append(df)
#         except Exception:
#             try:
#                 df = pd.read_csv(fp, dtype=str)
#                 if 'Date' in df.columns:
#                     try:
#                         df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
#                         df = df[df['Date'].apply(lambda d: d is not None and d >= cutoff and d < target_date)]
#                     except Exception:
#                         pass
#                 dfs.append(df)
#             except Exception:
#                 continue
#     if not dfs:
#         return pd.DataFrame()
#     try:
#         out = pd.concat(dfs, ignore_index=True)
#         return out
#     except Exception:
#         return pd.DataFrame()


# def _read_scenario_counts_by_person(outdir: str, window_days: int, target_date: date, scenario_col: str):
#     """
#     Return dict mapping normalized identifier -> count of days where scenario_col == True
#     in the window (target_date - window_days .. target_date-1).
#     Keys include normalized EmployeeID/person_uid/plain numeric tokens (same normalization as compute_violation_days_map).
#     """
#     df = _read_past_trend_csvs(outdir, window_days, target_date)
#     if df is None or df.empty or scenario_col not in df.columns:
#         return {}

#     # normalize Date
#     if 'Date' in df.columns:
#         try:
#             df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
#         except Exception:
#             pass

#     id_cols = [c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in df.columns]

#     out = defaultdict(int)
#     # consider only rows where scenario_col is truthy
#     q = df[df[scenario_col] == True] if df[scenario_col].dtype == bool else df[df[scenario_col].astype(str).str.lower() == 'true']
#     for _, r in q.iterrows():
#         for col in id_cols:
#             try:
#                 raw = r.get(col)
#                 if raw in (None, '', float('nan')):
#                     continue
#                 norm = _normalize_id_val(raw)
#                 if norm:
#                     out[str(norm)] += 1
#                     # also store stripped prefix variant
#                     stripped = _strip_uid_prefix(str(norm))
#                     if stripped != str(norm):
#                         out[str(stripped)] += 1
#             except Exception:
#                 continue
#         # fallback Int1/Text12 fields
#         for fallback in ('Int1', 'Text12'):
#             if fallback in r and r.get(fallback) not in (None, '', 'nan'):
#                 try:
#                     norm = _normalize_id_val(r.get(fallback))
#                     if norm:
#                         out[str(norm)] += 1
#                 except Exception:
#                     continue
#     return dict(out)





# def _strip_uid_prefix(s):
#     """Strip common prefixes like emp:, uid:, name: if present; return original otherwise."""
#     try:
#         if s is None:
#             return s
#         st = str(s)
#         for p in ('emp:', 'uid:', 'name:'):
#             if st.startswith(p):
#                 return st[len(p):]
#         return st
#     except Exception:
#         return s


# def compute_violation_days_map(outdir: str, window_days: int, target_date: date):
#     """
#     Return dict: identifier_string -> count of unique dates flagged as IsFlagged True
#     in the last window_days (excluding target_date).

#     We build a multi-key map so historical rows flagged under different identifier columns
#     (person_uid, EmployeeID, EmployeeIdentity, CardNumber) are all discoverable.
#     """
#     df = _read_past_trend_csvs(outdir, window_days, target_date)
#     if df is None or df.empty:
#         return {}

#     # Make sure Date is a date object
#     if 'Date' in df.columns:
#         try:
#             df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
#         except Exception:
#             pass

#     # Determine which column names are present that we care about
#     id_cols = []
#     for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber'):
#         if c in df.columns:
#             id_cols.append(c)

#     # Ensure IsFlagged exists
#     if 'IsFlagged' not in df.columns:
#         if 'AnomalyScore' in df.columns:
#             df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: float(s) >= ANOMALY_THRESHOLD if not pd.isna(s) else False)
#         else:
#             df['IsFlagged'] = False

#     # Build mapping identifier -> set(dates)
#     ident_dates = defaultdict(set)
#     try:
#         # iterate flagged rows only
#         flagged = df[df['IsFlagged'] == True]
#         for _, r in flagged.iterrows():
#             d = r.get('Date')
#             if d is None:
#                 continue
#             # for each identifier column present, normalize and add date
#             for col in id_cols:
#                 try:
#                     raw = r.get(col)
#                     if raw is None:
#                         continue
#                     norm = _normalize_id_val(raw)
#                     if norm:
#                         # store both the normalized token and stripped prefix variant
#                         ident_dates[str(norm)].add(d)
#                         stripped = _strip_uid_prefix(str(norm))
#                         if stripped != str(norm):
#                             ident_dates[str(stripped)].add(d)
#                 except Exception:
#                     continue
#             # also try any fallback token fields if present (older CSVs may store Int1/Text12)
#             for fallback in ('Int1', 'Text12'):
#                 if fallback in r and r.get(fallback) not in (None, '', 'nan'):
#                     try:
#                         norm = _normalize_id_val(r.get(fallback))
#                         if norm:
#                             ident_dates[str(norm)].add(d)
#                             stripped = _strip_uid_prefix(str(norm))
#                             if stripped != str(norm):
#                                 ident_dates[str(stripped)].add(d)
#                     except Exception:
#                         continue
#     except Exception:
#         logging.exception("Error building violation days map from history.")
#         # best-effort: convert what we have

#     # convert sets -> counts
#     out = {k: int(len(v)) for k, v in ident_dates.items()}
#     return out


# def run_trend_for_date(target_date: date, outdir: str = "./outputs", city='Pune', as_dict: bool = False):
#     logging.info("run_trend_for_date: date=%s (city=%s)", target_date, city)
#     results = run_for_date(target_date, regions=['apac'], outdir=outdir, city=city)
#     apac = results.get('apac', {})
#     swipes = apac.get('swipes', pd.DataFrame())
#     durations = apac.get('durations', pd.DataFrame())

#     # save raw swipes for evidence (full raw) — keep original timestamps in saved raw file
#     try:
#         if swipes is not None and not swipes.empty:
#             sw_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}.csv"
#             swipes.to_csv(sw_out, index=False)
#             logging.info("Saved raw swipes to %s", sw_out)
#     except Exception as e:
#         logging.warning("Failed to save raw swipes: %s", e)


#        # --- Day boundary handling ---
#        # Enable the Pune 02:00 logic by default for Pune runs (backwards compatible with env var).
#     # Rationale: many Pune sites have swipes after midnight that logically belong to the previous duty-day.
#     use_pune_2am_boundary = False
#     try:
#         if city and isinstance(city, str) and 'pun' in city.strip().lower():
#             # default to 2:00 boundary for Pune runs
#             use_pune_2am_boundary = True
#         else:
#             # allow explicit override via env var for non-Pune runs (keeps backward compatibility)
#             if os.getenv("PUNE_2AM_BOUNDARY", "0") == "1":
#                 use_pune_2am_boundary = True
#     except Exception:
#         use_pune_2am_boundary = False


#     # If Pune: create shifted copies for grouping (shift timestamps BACK by 2 hours for grouping)
#     # We will compute durations from the shifted swipes so Date boundaries align with 02:00.
#     sw_for_features = swipes.copy() if swipes is not None else pd.DataFrame()
#     durations_for_features = durations.copy() if durations is not None else pd.DataFrame()

#     if use_pune_2am_boundary and (sw_for_features is not None) and (not sw_for_features.empty):
#         try:
#             # ensure LocaleMessageTime parsed
#             if 'LocaleMessageTime' in sw_for_features.columns:
#                 sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features['LocaleMessageTime'], errors='coerce')
#             else:
#                 # try other timestamp columns defensively
#                 for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
#                     if cand in sw_for_features.columns:
#                         sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features[cand], errors='coerce')
#                         break

#             # PRESERVE original times for evidence + later matching
#             sw_for_features['OriginalLocaleMessageTime'] = sw_for_features['LocaleMessageTime']

#             # shift timestamps backward by 2 hours so the 'date' computed from them moves the day boundary to 02:00
#             sw_for_features['LocaleMessageTime'] = sw_for_features['LocaleMessageTime'] - pd.Timedelta(hours=2)

#             # recompute durations using shifted swipes so FirstSwipe/LastSwipe/Duration match 02:00-day boundary
#             try:
#                 durations_for_features = compute_daily_durations(sw_for_features)
#             except Exception:
#                 logging.exception("Failed to recompute durations for Pune 2AM boundary; falling back to original durations.")
#                 durations_for_features = durations.copy() if durations is not None else pd.DataFrame()

#             # For debugging/evidence linking: optionally write shifted-version of raw swipes
#             try:
#                 sw_shifted_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}_shifted.csv"
#                 # keep essential cols, including the OriginalLocaleMessageTime for traceability
#                 cols_keep = [c for c in sw_for_features.columns if c in ('person_uid','EmployeeID','CardNumber','LocaleMessageTime','OriginalLocaleMessageTime','Door','Direction','PartitionName2')]
#                 sw_for_features[cols_keep].to_csv(sw_shifted_out, index=False)
#             except Exception:
#                 logging.debug("Could not write shifted swipes evidence file (non-fatal).")
#         except Exception:
#             logging.exception("Failed preparing shifted swipes for Pune 2AM logic; using original swipes/durations.")
#             sw_for_features = swipes.copy() if swipes is not None else pd.DataFrame()
#             durations_for_features = durations.copy() if durations is not None else pd.DataFrame()
            

#     # compute features using possibly-shifted data (so grouping uses 02:00 boundary for Pune)
#     features = compute_features(sw_for_features, durations_for_features)
#     if features.empty:
#         logging.warning("run_trend_for_date: no features computed")
#         if as_dict:
#             return {'rows': 0, 'flagged_rows': 0, 'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': 0}
#         return pd.DataFrame()

#     # Provide an explicit DisplayDate/AdjustedDate for frontend:
#     # - If FirstSwipe exists use its local date (this shows the *logical* date after any shifts)
#     # - Otherwise fallback to existing Date value.
#     try:
#         if 'FirstSwipe' in features.columns:
#             features['DisplayDate'] = pd.to_datetime(features['FirstSwipe'], errors='coerce').dt.date
#         else:
#             features['DisplayDate'] = features['Date']
#     except Exception:
#         # safe fallback
#         features['DisplayDate'] = features.get('Date', None)


#     # If we used a shifted timeline for grouping (Pune), restore FirstSwipe/LastSwipe displayed times by adding 2 hours
#     # (this preserves the 02:00-based grouping but shows original times to user)
#     if use_pune_2am_boundary:
#         for dtcol in ('FirstSwipe', 'LastSwipe'):
#             if dtcol in features.columns:
#                 try:
#                     features[dtcol] = pd.to_datetime(features[dtcol], errors='coerce') + pd.Timedelta(hours=2)
#                 except Exception:
#                     # if values are strings or malformed, attempt safe add-by-parsing row-by-row
#                     def _add2h_safe(v):
#                         try:
#                             t = pd.to_datetime(v, errors='coerce')
#                             if pd.isna(t):
#                                 return v
#                             return t + pd.Timedelta(hours=2)
#                         except Exception:
#                             return v
#                     features[dtcol] = features[dtcol].apply(_add2h_safe)

#     # (rest of function unchanged) ----- continue with existing logic -----
#     # ----- New: compute historical scenario counts and weekly short-duration patterns -----
#     try:
#         # read scenario counts for pattern shortstay_longout_repeat
#         hist_pattern_counts = _read_scenario_counts_by_person(outdir, VIOLATION_WINDOW_DAYS, target_date, 'shortstay_longout_repeat')
#         # repeated_short_breaks counts
#         hist_rep_breaks = _read_scenario_counts_by_person(outdir, VIOLATION_WINDOW_DAYS, target_date, 'repeated_short_breaks')
#         # short_duration_<4h counts (scenario name "short_duration_<4h" exists in SCENARIOS)
#         hist_short_duration = _read_scenario_counts_by_person(outdir, VIOLATION_WINDOW_DAYS, target_date, 'short_duration_<4h')

#         def get_hist_count_for_row(row, hist_map):
#             # check several identifiers used historically
#             for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
#                 if k in row and row.get(k) not in (None, '', float('nan')):
#                     try:
#                         norm = _normalize_id_val(row.get(k))
#                         if norm and str(norm) in hist_map:
#                             return int(hist_map.get(str(norm), 0))
#                         stripped = _strip_uid_prefix(str(norm)) if norm else None
#                         if stripped and str(stripped) in hist_map:
#                             return int(hist_map.get(str(stripped), 0))
#                     except Exception:
#                         continue
#             return 0

#         features['HistPatternShortLongCount90'] = features.apply(lambda r: get_hist_count_for_row(r, hist_pattern_counts), axis=1)
#         features['HistRepeatedShortBreakCount90'] = features.apply(lambda r: get_hist_count_for_row(r, hist_rep_breaks), axis=1)
#         features['HistShortDurationCount90'] = features.apply(lambda r: get_hist_count_for_row(r, hist_short_duration), axis=1)

#         # escalate: if PatternShortLong repeat count >=3 -> high risk
#         pat_mask = features['HistPatternShortLongCount90'].fillna(0).astype(int) >= 3
#         if pat_mask.any():
#             features.loc[pat_mask, 'RiskScore'] = 5
#             features.loc[pat_mask, 'RiskLevel'] = 'High'
#             features.loc[pat_mask, 'IsFlagged'] = True

#         # repeated_short_breaks: if history > 5 -> force High (example of heavy violator)
#         rep_mask = features['HistRepeatedShortBreakCount90'].fillna(0).astype(int) >= 5
#         if rep_mask.any():
#             features.loc[rep_mask, 'RiskScore'] = 5
#             features.loc[rep_mask, 'RiskLevel'] = 'High'
#             features.loc[rep_mask, 'IsFlagged'] = True

#     except Exception:
#         logging.exception("Failed to compute historical scenario counts.")



#     # ===== START FIX: reconcile zero CountSwipes with raw swipe files =====
#     try:
#         if swipes is not None and not swipes.empty and 'person_uid' in swipes.columns:
#             tsw = swipes.copy()
#             if 'LocaleMessageTime' in tsw.columns:
#                 tsw['LocaleMessageTime'] = pd.to_datetime(tsw['LocaleMessageTime'], errors='coerce')
#             else:
#                 for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
#                     if cand in tsw.columns:
#                         tsw['LocaleMessageTime'] = pd.to_datetime(tsw[cand], errors='coerce')
#                         break
#             if 'Date' not in tsw.columns:
#                 if 'LocaleMessageTime' in tsw.columns:
#                     tsw['Date'] = tsw['LocaleMessageTime'].dt.date
#                 else:
#                     for cand in ('date','Date'):
#                         if cand in tsw.columns:
#                             try:
#                                 tsw['Date'] = pd.to_datetime(tsw[cand], errors='coerce').dt.date
#                             except Exception:
#                                 tsw['Date'] = None
#                             break

#             try:
#                 grp = tsw.dropna(subset=['person_uid', 'Date']).groupby(['person_uid', 'Date'])
#                 counts = grp.size().to_dict()
#                 firsts = grp['LocaleMessageTime'].min().to_dict()
#                 lasts = grp['LocaleMessageTime'].max().to_dict()
#             except Exception:
#                 counts = {}
#                 firsts = {}
#                 lasts = {}

#             def _fix_row_by_raw(idx, row):
#                 key = (row.get('person_uid'), row.get('Date'))
#                 if key in counts and (row.get('CountSwipes', 0) == 0 or pd.isna(row.get('CountSwipes'))):
#                     try:
#                         c = int(counts.get(key, 0))
#                         features.at[idx, 'CountSwipes'] = c
#                         f = firsts.get(key)
#                         l = lasts.get(key)
#                         if pd.notna(f) and (pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None):
#                             features.at[idx, 'FirstSwipe'] = pd.to_datetime(f)
#                         if pd.notna(l) and (pd.isna(row.get('LastSwipe')) or row.get('LastSwipe') is None):
#                             features.at[idx, 'LastSwipe'] = pd.to_datetime(l)
#                         try:
#                             fs = features.at[idx, 'FirstSwipe']
#                             ls = features.at[idx, 'LastSwipe']
#                             if pd.notna(fs) and pd.notna(ls):
#                                 dursec = (pd.to_datetime(ls) - pd.to_datetime(fs)).total_seconds()
#                                 dursec = max(0, dursec)
#                                 features.at[idx, 'DurationSeconds'] = float(dursec)
#                                 features.at[idx, 'DurationMinutes'] = float(dursec / 60.0)
#                         except Exception:
#                             pass
#                     except Exception:
#                         pass

#             for ix, r in features[features['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
#                 try:
#                     _fix_row_by_raw(ix, r)
#                 except Exception:
#                     logging.debug("Failed to reconcile row %s with raw swipes", ix)
#     except Exception:
#         logging.exception("Error while reconciling aggregated features with raw swipes (zero-swipe fix).")
#     # ===== END FIX =====

#     # Build badge map and swipe overlap maps for higher-severity scenarios
#     badge_map = {}
#     if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
#         tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
#         if not tmp.empty:
#             grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
#             badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}

#     swipe_overlap_map = {}
#     overlap_window_seconds = 2
#     if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
#         tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
#         if not tmp.empty:
#             tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
#             for (d, door), g in tmp.groupby(['Date', 'Door']):
#                 items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
#                 n = len(items)
#                 for i in range(n):
#                     t_i, uid_i = items[i]
#                     j = i+1
#                     while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
#                         uid_j = items[j][1]
#                         if uid_i != uid_j:
#                             swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
#                             swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
#                         j += 1

#     # Evaluate scenarios (use weighting to compute anomaly score)
#     for name, fn in SCENARIOS:
#         if name == "badge_sharing_suspected":
#             features[name] = features.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
#         elif name == "swipe_overlap":
#             features[name] = features.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map), axis=1)
#         else:
#             features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

#     def compute_score(r):
#         score = 0.0
#         detected = []
#         for name, _ in SCENARIOS:
#             val = bool(r.get(name))
#             w = WEIGHTS.get(name, 0.0)
#             if val and w > 0:
#                 score += float(w)
#                 detected.append(name)
#         return score, detected

#     scores = features.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
#     features['AnomalyScore'] = scores['AnomalyScore'].astype(float)
#     features['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
#     features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))
#     scores = features.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
#     features['AnomalyScore'] = scores['AnomalyScore'].astype(float)
#     features['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
#     features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))


#   # ----- NEW: PresentToday flag, historical monitoring note, and post-score weekly adjustments -----
#     try:
#         # PresentToday: considered present if there were any swipes recorded
#         features['PresentToday'] = features['CountSwipes'].fillna(0).astype(int) > 0

#         # Compute violation days map (already computed later normally) but ensure column exists
#         if 'ViolationDaysLast90' not in features.columns:
#             # best-effort default to 0
#             features['ViolationDaysLast90'] = 0

#         # Append monitoring note for persons who have past violations and are present today.
#         def _append_monitor_note(idx, row):
#             try:
#                 vd = int(row.get('ViolationDaysLast90') or 0)
#             except Exception:
#                 vd = 0
#             if vd <= 0:
#                 return row.get('Explanation')  # unchanged
#             if not row.get('PresentToday', False):
#                 return row.get('Explanation')
#             # Prepare note text
#             note = f"Note: Previously flagged {vd} time{'s' if vd!=1 else ''} in the last {VIOLATION_WINDOW_DAYS} days — monitor when present today."
#             ex = row.get('Explanation') or ''
#             if ex and not ex.strip().endswith('.'):
#                 ex = ex.strip() + '.'
#             # Avoid duplicate note if already present
#             if note in ex:
#                 return ex
#             return (ex + ' ' + note).strip()

#         features['Explanation'] = features.apply(lambda r: _append_monitor_note(r.name, r), axis=1)

#         # Add MonitorFlag boolean column so UI can highlight easily
#         features['MonitorFlag'] = features.apply(lambda r: (int(r.get('ViolationDaysLast90') or 0) > 0) and bool(r.get('PresentToday')), axis=1)

#         # Now compute consecutive-week short-duration runs (post scoring)
#         past_df = _read_past_trend_csvs(outdir, VIOLATION_WINDOW_DAYS, target_date)
#         week_runs = _compute_weeks_with_threshold(past_df, person_col='person_uid', date_col='Date', scenario_col='short_duration_<4h', threshold_days=3)

#         def _get_week_run_for_row(r):
#             for k in ('person_uid', 'EmployeeID'):
#                 if k in r and r.get(k):
#                     key = str(r.get(k))
#                     if key in week_runs:
#                         return int(week_runs[key])
#                     # also try stripped prefix
#                     stripped = _strip_uid_prefix(key)
#                     if stripped in week_runs:
#                         return int(week_runs[stripped])
#             return 0

#         features['ConsecWeeksShort4hrs'] = features.apply(_get_week_run_for_row, axis=1)

#         # Apply anomaly score bumps now that AnomalyScore exists
#         # (safe: check column presence)
#         if 'AnomalyScore' not in features.columns:
#             features['AnomalyScore'] = 0.0

#         mask1 = features['ConsecWeeksShort4hrs'].fillna(0).astype(int) >= 1
#         mask2 = features['ConsecWeeksShort4hrs'].fillna(0).astype(int) >= 2

#         if mask1.any():
#             features.loc[mask1, 'AnomalyScore'] = features.loc[mask1, 'AnomalyScore'].astype(float) + 0.5
#         if mask2.any():
#             features.loc[mask2, 'AnomalyScore'] = features.loc[mask2, 'AnomalyScore'].astype(float) + 1.0

#         # Recompute IsFlagged and RiskLevel after bumping AnomalyScore
#         features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

#         def _map_risk_after_bump(r):
#             score = r.get('AnomalyScore') or 0.0
#             bucket, label = map_score_to_label(score)
#             return int(bucket), label
#         rs2 = features.apply(lambda r: pd.Series(_map_risk_after_bump(r), index=['RiskScore', 'RiskLevel']), axis=1)
#         features['RiskScore'] = rs2['RiskScore']
#         features['RiskLevel'] = rs2['RiskLevel']

#     except Exception:
#         logging.exception("Failed post-scoring weekly-run / monitoring augmentation.")



#     def reasons_for_row(r):
#         if not bool(r.get('IsFlagged')):
#             return None
#         ds_raw = r.get('DetectedScenarios')
#         if ds_raw:
#             ds = [s.strip() for s in ds_raw.split(";") if s and s.strip()]
#             # Build natural explanation sentences for the detected scenarios
#             explanation = _explain_scenarios_detected(r, ds)
#             # Also produce compact reasons list (code-style) in Reasons for backwards compatibility
#             reasons_codes = "; ".join(ds) if ds else None
#             return reasons_codes, explanation
#         return None, None


#         # Apply reasons_for_row to populate Reasons (codes) and Explanation (natural text)
#     reason_tuples = features.apply(lambda r: pd.Series(reasons_for_row(r), index=['Reasons', 'Explanation']), axis=1)

#     # sanitize Reasons and Explanation values: convert placeholder tokens (including 'nan') -> None
#     def _sanitize_reason_val(v):
#         if v is None:
#             return None
#         try:
#             s = str(v).strip()
#             if s == "" or _is_placeholder_str(s):
#                 return None
#             return s
#         except Exception:
#             return None

#     # first sanitize raw values
#     features['Reasons'] = reason_tuples['Reasons'].apply(_sanitize_reason_val)
#     features['Explanation'] = reason_tuples['Explanation'].apply(lambda v: None if _is_placeholder_str(v) else (str(v).strip() if v is not None else None))

#     # If a row is flagged but has no explicit Reasons, attempt to derive a safe fallback
#     # (covers cases where AnomalyScore was raised by historical/weekly bumps or other non-scenario logic).
#     def _ensure_reason_for_flagged(row):
#         if bool(row.get('IsFlagged')) and (row.get('Reasons') is None or row.get('Reasons') == ''):
#             # try DetectedScenarios if present (it may exist but was placeholder)
#             ds = row.get('DetectedScenarios')
#             if ds and not _is_placeholder_str(ds):
#                 parts = [p.strip() for p in re.split(r'[;,\|]', str(ds)) if p and not _is_placeholder_str(p)]
#                 if parts:
#                     return "; ".join(parts)
#             # escalate-derived reasons (meaningful and stable codes)
#             if int(row.get('ConsecWeeksShort4hrs') or 0) >= 1:
#                 return "consecutive_short_weeks"
#             if int(row.get('ViolationDaysLast90') or 0) > 0:
#                 return "historical_monitoring"
#             # fallback: leave empty (None) so UI doesn't display 'nan'
#             return None
#         return row.get('Reasons')

#     if 'IsFlagged' in features.columns:
#         features['Reasons'] = features.apply(_ensure_reason_for_flagged, axis=1)
#     else:
#         # still sanitize even if IsFlagged missing
#         features['Reasons'] = features['Reasons'].apply(_sanitize_reason_val)


#     if 'OverlapWith' not in features.columns:
#         def overlap_with_fn(r):
#             d = r.get('Date')
#             uid = r.get('person_uid')
#             if (d, uid) in swipe_overlap_map:
#                 return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
#             return None
#         features['OverlapWith'] = features.apply(overlap_with_fn, axis=1)

#     # compute ViolationDays in past window (person_uid -> count) using existing trend CSVs in outdir
#     try:
#         violation_map = compute_violation_days_map(outdir, VIOLATION_WINDOW_DAYS, target_date)

#         def map_violation_days(r):
#             # Check multiple possible identifiers; normalize them the same way as history map keys
#             candidates = []
#             for k in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
#                 if k in r and r.get(k) not in (None, '', float('nan')):
#                     val = r.get(k)
#                     try:
#                         norm = _normalize_id_val(val)
#                         if norm:
#                             candidates.append(str(norm))
#                             stripped = _strip_uid_prefix(str(norm))
#                             if stripped != str(norm):
#                                 candidates.append(str(stripped))
#                     except Exception:
#                         continue
#             # try also the raw EmployeeID fallback
#             for cand in candidates:
#                 if cand in violation_map:
#                     return int(violation_map[cand])
#             return 0

#         features['ViolationDaysLast90'] = features.apply(map_violation_days, axis=1)
#     except Exception:
#         features['ViolationDaysLast90'] = 0


#     # compute RiskScore (bucket) and RiskLevel label
#     try:
#         def map_risk(r):
#             score = r.get('AnomalyScore') or 0.0
#             bucket, label = map_score_to_label(score)
#             return int(bucket), label
#         rs = features.apply(lambda r: pd.Series(map_risk(r), index=['RiskScore', 'RiskLevel']), axis=1)
#         features['RiskScore'] = rs['RiskScore']
#         features['RiskLevel'] = rs['RiskLevel']
#     except Exception:
#         features['RiskScore'] = 1
#         features['RiskLevel'] = 'Low'

#     # ---- OVERRIDE: force High risk when ViolationDaysLast90 >= 4 ----
#     try:
#         features['ViolationDaysLast90'] = features['ViolationDaysLast90'].fillna(0).astype(int)
#         high_violation_mask = features['ViolationDaysLast90'] >= 4
#         if high_violation_mask.any():
#             features.loc[high_violation_mask, 'RiskScore'] = 5
#             features.loc[high_violation_mask, 'RiskLevel'] = 'High'
#     except Exception:
#         pass
#     # ---------------------------------------------------------------

#     # Remove suffix columns and fix duplicates
#     cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
#     if cols_to_drop:
#         for c in cols_to_drop:
#             base = c[:-2]
#             if base in features.columns:
#                 try:
#                     features.drop(columns=[c], inplace=True)
#                 except Exception:
#                     pass
#             else:
#                 try:
#                     features.rename(columns={c: base}, inplace=True)
#                 except Exception:
#                     pass
#     features = features.loc[:, ~features.columns.duplicated()]

#     # ensure booleans are native Python (avoid numpy.bool_)
#     for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
#         if col in features.columns:
#             features[col] = features[col].astype(bool)

#     # write CSV with native types
#     out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
#     try:
#         write_df = features.copy()
#         # FirstSwipe/LastSwipe -> ISO strings
#         for dtcol in ('FirstSwipe', 'LastSwipe'):
#             if dtcol in write_df.columns:
#                 write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
#         # Date -> ISO date
#         if 'Date' in write_df.columns:
#             try:
#                 write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
#                 write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
#             except Exception:
#                 pass
#         write_df = write_df.where(pd.notnull(write_df), None)
#         write_df.to_csv(out_csv, index=False)

        
#         logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
#     except Exception as e:
#         logging.exception("Failed to write trend CSV: %s", e)

#       # Format DisplayDate (ISO) for output if present
#     if 'DisplayDate' in write_df.columns:
#         try:
#             write_df['DisplayDate'] = pd.to_datetime(write_df['DisplayDate'], errors='coerce').dt.date
#             write_df['DisplayDate'] = write_df['DisplayDate'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
#         except Exception:
#             pass

  
#     # If caller wants dict/json-friendly output (useful for API endpoints), prepare it:
#     if as_dict:
#         try:
#             # prepare a safe records list (convert datetimes to ISO strings)
#             rec_df = features.copy()
#             for dtcol in ('FirstSwipe', 'LastSwipe'):
#                 if dtcol in rec_df.columns:
#                     rec_df[dtcol] = pd.to_datetime(rec_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
#             if 'Date' in rec_df.columns:
#                 try:
#                     rec_df['Date'] = pd.to_datetime(rec_df['Date'], errors='coerce').dt.date
#                     rec_df['Date'] = rec_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
#                 except Exception:
#                     pass
#             rec_df = rec_df.where(pd.notnull(rec_df), None)

#             # total rows & flagged rows (explicit and consistent)
#             total_rows = int(len(rec_df))
#             flagged_rows = int(rec_df['IsFlagged'].sum()) if 'IsFlagged' in rec_df.columns else 0

#             # reasons_count: count reasons only among flagged rows (keeps semantics consistent with flagged_rows)
#             reasons_count = {}
#             flagged_df = rec_df[rec_df['IsFlagged'] == True] if 'IsFlagged' in rec_df.columns else pd.DataFrame()
#             if 'Reasons' in flagged_df.columns and not flagged_df.empty:
#                 for v in flagged_df['Reasons'].dropna().astype(str):
#                     for part in re.split(r'[;,\|]', v):
#                         key = part.strip()
#                         if key:
#                             reasons_count[key] = reasons_count.get(key, 0) + 1

#             # risk_counts: count RiskLevel only among flagged rows (so this sums to flagged_rows)
#             risk_counts = {}
#             if 'RiskLevel' in flagged_df.columns and not flagged_df.empty:
#                 for v in flagged_df['RiskLevel'].fillna('').astype(str):
#                     if v:
#                         risk_counts[v] = risk_counts.get(v, 0) + 1

#             # sample: return flagged rows as sample (keep behaviour)
#             sample_records = flagged_df.to_dict(orient='records') if not flagged_df.empty else []

#             return {
#                 'rows': total_rows,
#                 'flagged_rows': flagged_rows,
#                 'aggregated_unique_persons': total_rows,
#                 'sample': sample_records,
#                 'reasons_count': reasons_count,
#                 'risk_counts': risk_counts
#             }
#         except Exception:
#             logging.exception("Failed to build dict output for run_trend_for_date")
#             return {'rows': len(features), 'flagged_rows': int(features['IsFlagged'].sum() if 'IsFlagged' in features.columns else 0),
#                     'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': len(features)}



#     # default: return DataFrame (existing behavior)
#     return features


# def _compute_weeks_with_threshold(df, person_col='person_uid', date_col='Date', scenario_col='short_duration_<4h', threshold_days=3, lookback_weeks=8):
#     """
#     Return dict person_uid -> max_consecutive_weeks meeting threshold_days for scenario_col
#     df expected to include Date as date type and scenario_col boolean.
#     """
#     out = {}
#     if df is None or df.empty or scenario_col not in df.columns:
#         return out
#     try:
#         dd = df.copy()
#         dd['Date'] = pd.to_datetime(dd[date_col], errors='coerce').dt.date
#         # compute ISO year-week
#         dd['year_week'] = dd['Date'].apply(lambda d: (d.isocalendar()[0], d.isocalendar()[1]) if d else (None, None))
#         # Build per person-week counts
#         grp = dd[dd[scenario_col] == True].groupby([person_col, 'year_week']).size().reset_index(name='cnt')
#         # collect last lookback_weeks weeks values for each person
#         for person, g in grp.groupby(person_col):
#             # sort by year_week
#             weeks = sorted([ (yw, int(cnt)) for yw, cnt in zip(g['year_week'], g['cnt']) if yw is not None ])
#             # transform to list of (year, week, cnt) then determine consecutive weeks that meet threshold
#             week_map = { (y,w): cnt for (y,w),cnt in zip(g['year_week'], g['cnt'])}
#             # get the list of week keys sorted
#             wk_keys = sorted(week_map.keys())
#             # compute max consecutive runs
#             max_run = 0
#             cur_run = 0
#             prev = None
#             for y,w in wk_keys:
#                 cnt = week_map.get((y,w), 0)
#                 if cnt >= threshold_days:
#                     if prev is None:
#                         cur_run = 1
#                     else:
#                         # check consecutive week
#                         # naive next-week calculation:
#                         py, pw = prev
#                         # increment which handles year boundary
#                         from datetime import date as _date
#                         try:
#                             d1 = _date.fromisocalendar(py, pw, 1)
#                             d2 = _date.fromisocalendar(y, w, 1)
#                             diff_weeks = int((d2 - d1).days / 7)
#                         except Exception:
#                             diff_weeks = 1 if (y,w) != prev else 0
#                         if diff_weeks == 1:
#                             cur_run += 1
#                         else:
#                             cur_run = 1
#                     prev = (y,w)
#                 else:
#                     prev = (y,w)
#                     cur_run = 0
#                 max_run = max(max_run, cur_run)
#             out[person] = max_run
#     except Exception:
#         logging.exception("Failed computing weeks with threshold")
#     return out

# def build_90day_training(end_date: date = None, window_days: int = 90, outdir: str = "./outputs",
#                          min_unique_employees: int = 100, city: str = "Pune"):
#     """
#     Build a per-person training CSV aggregated across the last `window_days` days.
#     Output: outdir/training_person_90day.csv (or with window_days in name)
#     Each row aggregates medians/means/sums for numeric metrics and creates binary labels
#     for each scenario if the scenario occurred at least once in the window for that person.
#     """
#     if end_date is None:
#         end_date = datetime.now().date()
#     outdir = Path(outdir)
#     outdir.mkdir(parents=True, exist_ok=True)
#     logging.info("build_90day_training: end_date=%s window_days=%d", end_date, window_days)

#     # read trend CSVs within window (uses existing helper)
#     df = _read_past_trend_csvs(outdir=str(outdir), window_days=window_days, target_date=end_date)
#     if df is None or df.empty:
#         logging.warning("No trend CSVs found in %s for window ending %s", outdir, end_date)
#         return None

#     # ensure Date is date
#     if 'Date' in df.columns:
#         try:
#             df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
#         except Exception:
#             pass

#     # ensure person_uid exists (fallback to EmployeeID/EmployeeIdentity)
#     if 'person_uid' not in df.columns:
#         def make_person_uid_local(row):
#             parts = []
#             for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
#                 v = row.get(c)
#                 if pd.notna(v) and str(v).strip():
#                     parts.append(str(v).strip())
#             return "|".join(parts) if parts else None
#         df['person_uid'] = df.apply(make_person_uid_local, axis=1)

#     # choose scenario columns (boolean daily indicators) existing in df
#     scenario_names = [name for name, _ in SCENARIOS if name in df.columns]

#     # aggregate per person_uid
#     agg_funcs = {
#         'CountSwipes': ['median', 'mean', 'sum'],
#         'DurationMinutes': ['median', 'mean', 'sum'],
#         'MaxSwipeGapSeconds': ['max', 'median'],
#         'ShortGapCount': ['sum'],
#         'UniqueDoors': ['median'],
#         'UniqueLocations': ['median'],
#         'RejectionCount': ['sum']
#     }

#     # ensure numeric columns exist
#     for col in ['CountSwipes', 'DurationMinutes', 'MaxSwipeGapSeconds', 'ShortGapCount', 'UniqueDoors', 'UniqueLocations', 'RejectionCount']:
#         if col not in df.columns:
#             df[col] = 0

#     group_cols = ['person_uid']
#     grp = df.groupby(group_cols, sort=False)

#     person_rows = []
#     unique_persons = set()

#     for person, g in grp:
#         row = {}
#         row['person_uid'] = person
#         row['EmployeeID'] = next((v for v in g.get('EmployeeID', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
#         row['EmployeeName'] = next((v for v in g.get('EmployeeName', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
#         row['days_present'] = int(g.shape[0])
#         row['HistPatternShortLongCount90'] = int(g.get('HistPatternShortLongCount90', pd.Series([0])).sum()) if 'HistPatternShortLongCount90' in g else 0
#         row['HistRepeatedShortBreakCount90'] = int(g.get('HistRepeatedShortBreakCount90', pd.Series([0])).sum()) if 'HistRepeatedShortBreakCount90' in g else 0
#         row['HistShortDurationCount90'] = int(g.get('HistShortDurationCount90', pd.Series([0])).sum()) if 'HistShortDurationCount90' in g else 0



#         # numeric aggregates
#         for col, funcs in agg_funcs.items():
#             if col in g.columns:
#                 for f in funcs:
#                     key = f"{col}_{f}"
#                     try:
#                         val = getattr(g[col], f)()
#                         row[key] = float(val) if pd.notna(val) else None
#                     except Exception:
#                         row[key] = None
#             else:
#                 for f in funcs:
#                     row[f"{col}_{f}"] = None

#         # scenario labels: if scenario column exists in daily rows, label = sum > 0
#         for s in scenario_names:
#             try:
#                 s_count = int(g[s].astype(int).sum()) if s in g.columns else 0
#             except Exception:
#                 s_count = int((g[s].sum()) if s in g.columns else 0)
#             row[f"{s}_days"] = s_count
#             row[f"{s}_label"] = int(s_count > 0)

#         person_rows.append(row)
#         unique_persons.add(person)

#         # early stop if dataset large
#         if len(unique_persons) >= min_unique_employees:
#             logging.info("Reached min unique employees=%d; stopping early", min_unique_employees)
#             break

#     if not person_rows:
#         logging.warning("No aggregated person rows were created.")
#         return None

#     training_df = pd.DataFrame(person_rows)
#     out_name = f"training_person_{window_days}day_{end_date.strftime('%Y%m%d')}.csv"
#     out_path = outdir / out_name
#     training_df.to_csv(out_path, index=False)
#     logging.info("Saved 90-day training CSV to %s (rows=%d)", out_path, len(training_df))
#     return out_path


# # ---------------- training dataset builder (restored) ----------------
# def build_monthly_training(end_date: date = None, months: int = 3, min_unique_employees: int = 1000,
#                            outdir: str = "./outputs", city: str = "Pune"):
#     if end_date is None:
#         end_date = datetime.now().date()
#     logging.info("build_monthly_training: end_date=%s months=%d min_unique=%d", end_date, months, min_unique_employees)
#     outdir = Path(outdir)
#     month_windows = []
#     cur = end_date.replace(day=1)
#     for _ in range(months):
#         start = cur
#         next_month = (cur.replace(day=28) + timedelta(days=4)).replace(day=1)
#         last = next_month - timedelta(days=1)
#         month_windows.append((start, last))
#         cur = (start - timedelta(days=1)).replace(day=1)

#     person_month_rows = []
#     unique_persons = set()

#     for start, last in month_windows:
#         d = start
#         month_dfs = []
#         while d <= last:
#             csv_path = outdir / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
#             if csv_path.exists():
#                 try:
#                     df = pd.read_csv(csv_path)
#                     month_dfs.append(df)
#                 except Exception:
#                     try:
#                         df = pd.read_csv(csv_path, dtype=str)
#                         month_dfs.append(df)
#                     except Exception as e:
#                         logging.warning("Failed reading %s: %s", csv_path, e)
#             else:
#                 # generate the daily trend if missing
#                 logging.info("Monthly builder: trend CSV missing for %s — generating by running run_trend_for_date", d.isoformat())
#                 try:
#                     run_trend_for_date(d, outdir=str(outdir), city=city)
#                     # attempt to read after generating
#                     if csv_path.exists():
#                         try:
#                             df = pd.read_csv(csv_path)
#                             month_dfs.append(df)
#                         except Exception:
#                             try:
#                                 df = pd.read_csv(csv_path, dtype=str)
#                                 month_dfs.append(df)
#                             except Exception as e:
#                                 logging.warning("Failed reading %s after generation: %s", csv_path, e)
#                 except Exception as e:
#                     logging.warning("Failed to generate trend for %s: %s", d, e)
#             d = d + timedelta(days=1)

#         if not month_dfs:
#             logging.info("No daily trend CSVs found for month %s - %s", start.isoformat(), last.isoformat())
#             continue

#         month_df = pd.concat(month_dfs, ignore_index=True)
#         # ensure person_uid exists
#         if 'person_uid' not in month_df.columns:
#             def make_person_uid(row):
#                 parts = []
#                 for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
#                     v = row.get(c)
#                     if pd.notna(v) and str(v).strip():
#                         parts.append(str(v).strip())
#                 return "|".join(parts) if parts else None
#             month_df['person_uid'] = month_df.apply(make_person_uid, axis=1)

#         # convert boolean columns to int for aggregation if necessary
#         for name, _ in SCENARIOS:
#             if name in month_df.columns:
#                 month_df[name] = month_df[name].astype(int)

#         agg_funcs = {
#             'CountSwipes': ['median', 'mean', 'sum'],
#             'DurationMinutes': ['median', 'mean', 'sum'],
#             'MaxSwipeGapSeconds': ['max', 'median'],
#             'ShortGapCount': ['sum'],
#             'UniqueDoors': ['median'],
#             'UniqueLocations': ['median'],
#             'RejectionCount': ['sum']
#         }
#         scenario_cols = [name for name,_ in SCENARIOS if name in month_df.columns]
#         group_cols = ['person_uid']
#         grp = month_df.groupby(group_cols)

#         for person, g in grp:
#             row = {}
#             row['person_uid'] = person
#             row['EmployeeID'] = next((v for v in g.get('EmployeeID', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
#             row['EmployeeName'] = next((v for v in g.get('EmployeeName', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
#             row['MonthStart'] = start.isoformat()
#             row['MonthEnd'] = last.isoformat()
#             for col, funcs in agg_funcs.items():
#                 if col in g.columns:
#                     for f in funcs:
#                         key = f"{col}_{f}"
#                         try:
#                             val = getattr(g[col], f)()
#                             row[key] = float(val) if pd.notna(val) else None
#                         except Exception:
#                             row[key] = None
#                 else:
#                     for f in funcs:
#                         row[f"{col}_{f}"] = None
#             for s in scenario_cols:
#                 row[f"{s}_days"] = int(g[s].sum())
#                 row[f"{s}_label"] = int(g[s].sum() > 0)
#             row['days_present'] = int(g.shape[0])
#             person_month_rows.append(row)
#             unique_persons.add(person)

#         if len(unique_persons) >= min_unique_employees:
#             logging.info("Reached min unique employees=%d, stopping aggregation early", min_unique_employees)
#             break

#     if not person_month_rows:
#         logging.warning("No person-month rows created (no data).")
#         return None

#     training_df = pd.DataFrame(person_month_rows)
#     train_out = outdir / "training_person_month.csv"
#     training_df.to_csv(train_out, index=False)
#     logging.info("Saved training CSV to %s (rows=%d unique_persons=%d)", train_out, len(training_df), len(unique_persons))
#     return train_out


# if __name__ == "__main__":
#     today = datetime.now().date()
#     df = run_trend_for_date(today)
#     print("Completed; rows:", len(df) if df is not None else 0)



# # ------------------------------
# # ### CACHE: 90-day combined cache helpers
# # ------------------------------
# from datetime import datetime as _datetime

# _CACHE_FILENAME = "trend_pune_90day_cache.csv"
# _CACHE_META = "trend_pune_90day_cache_meta.txt"
# _CACHE_REFRESH_SECONDS = 60 * 60 * 6  # refresh every 6 hours by default

# def _cache_paths_for_outdir(outdir: str):
#     p = Path(outdir)
#     return p / _CACHE_FILENAME, p / _CACHE_META

# def build_90day_cache(outdir: str = str(OUTDIR), window_days: int = 90):
#     """
#     Build (or rebuild) a combined cache CSV containing the last `window_days` days of
#     trend_pune_*.csv (up to today). Returns (df, start_date, end_date) or (None, None, None)
#     if no data found.
#     """
#     outdir_p = Path(outdir)
#     outdir_p.mkdir(parents=True, exist_ok=True)
#     target_date = _datetime.now().date() + timedelta(days=1)  # make helper include today
#     try:
#         df = _read_past_trend_csvs(str(outdir_p), window_days, target_date)
#     except Exception:
#         df = pd.DataFrame()

#     if df is None or df.empty:
#         # remove stale cache if exists
#         cpath, mpath = _cache_paths_for_outdir(outdir)
#         try:
#             if cpath.exists():
#                 cpath.unlink()
#             if mpath.exists():
#                 mpath.unlink()
#         except Exception:
#             pass
#         return None, None, None

#     # ensure Date column is date type and normalized
#     if 'Date' in df.columns:
#         try:
#             df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
#         except Exception:
#             pass

#     # determine start/end present in this combined df
#     try:
#         present_dates = sorted([d for d in pd.Series(df['Date'].dropna().unique())])
#         start = present_dates[0] if present_dates else None
#         end = present_dates[-1] if present_dates else None
#     except Exception:
#         start = None
#         end = None

#     # write combined cache CSV (safe write)
#     cpath, mpath = _cache_paths_for_outdir(outdir)
#     try:
#         # convert Date to ISO strings to be robust on read
#         df_to_write = df.copy()
#         if 'Date' in df_to_write.columns:
#             try:
#                 df_to_write['Date'] = df_to_write['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
#             except Exception:
#                 pass
#         df_to_write.to_csv(cpath, index=False)
#         # write meta
#         with open(mpath, "w", encoding="utf8") as f:
#             f.write(f"{start.isoformat() if start else ''}\n")
#             f.write(f"{end.isoformat() if end else ''}\n")
#             f.write(f"{_datetime.now().isoformat()}\n")
#     except Exception:
#         logging.exception("Failed to write 90-day cache to %s", cpath)
#         return df, start, end

#     logging.info("Built 90-day cache %s (%s -> %s, rows=%d)", cpath, start, end, len(df))
#     return df, start, end

# def read_90day_cache(outdir: str = str(OUTDIR), window_days: int = 90, force_refresh: bool = False):
#     """
#     Load the 90-day combined cache if present and fresh; otherwise (re)build it.
#     Returns (df, start_date, end_date) where df has Date as datetime.date values.
#     If no cache and no data available returns (None, None, None).
#     """
#     outdir_p = Path(outdir)
#     cpath, mpath = _cache_paths_for_outdir(outdir)

#     # if cache exists and not forced refresh, check modification time to decide whether to use it
#     try:
#         if cpath.exists() and mpath.exists() and not force_refresh:
#             meta_mtime = cpath.stat().st_mtime
#             # if cache is older than refresh interval, rebuild
#             age = _datetime.now().timestamp() - meta_mtime
#             if age > _CACHE_REFRESH_SECONDS:
#                 # rebuild
#                 return build_90day_cache(outdir=outdir, window_days=window_days)
#             # try to read cache
#             try:
#                 df = pd.read_csv(cpath)
#                 if 'Date' in df.columns:
#                     try:
#                         df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
#                     except Exception:
#                         pass
#                 # read meta if available
#                 start = None
#                 end = None
#                 try:
#                     if mpath.exists():
#                         with open(mpath, "r", encoding="utf8") as f:
#                             lines = [l.strip() for l in f.readlines()]
#                             if len(lines) >= 2:
#                                 start = pd.to_datetime(lines[0]).date() if lines[0] else None
#                                 end = pd.to_datetime(lines[1]).date() if lines[1] else None
#                 except Exception:
#                     start = None
#                     end = None
#                 return df, start, end
#             except Exception:
#                 logging.exception("Failed to read existing 90-day cache; will rebuild.")
#                 return build_90day_cache(outdir=outdir, window_days=window_days)
#         else:
#             # build new cache
#             return build_90day_cache(outdir=outdir, window_days=window_days)
#     except Exception:
#         logging.exception("Error in read_90day_cache; attempting to build cache")
#         return build_90day_cache(outdir=outdir, window_days=window_days)
    
















# backend/trend_runner.py
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re
import os
import calendar
import json
from collections import defaultdict
from datetime import datetime as _datetime
from datetime import timedelta as _timedelta
from typing import Optional, List

# ------------------ personnel enrichment (add near imports) ------------------
def _get_personnel_funcs_lazy():
    """
    Try to import personnel helpers from app.py at call time (avoids circular imports).
    Returns tuple (get_personnel_info_fn_or_None, get_person_image_bytes_fn_or_None).
    """
    try:
        import importlib
        appmod = importlib.import_module('app')  # adjust module name if your app module name is different
        gpi = getattr(appmod, 'get_personnel_info', None)
        gpib = getattr(appmod, 'get_person_image_bytes', None)
        return gpi, gpib
    except Exception:
        # if the app module isn't importable from this process, return None and continue
        return None, None

def _normalize_for_lookup(val):
    """simple normalizer used for building lookup id (avoid GUIDs when possible)."""
    if val is None:
        return None
    s = str(val).strip()
    if not s:
        return None
    # keep numeric-like ids or short strings; drop long GUID-like values if possible
    if len(s) > 36 and '-' in s:
        # likely GUID — still return, but caller may prefer EmployeeID first
        return s
    return s

def _enrich_with_personnel_info(df, image_endpoint_template="/employee/{}/image"):
    """
    Adds columns to df:
      - EmployeeEmail (string or None)
      - imageUrl (string or None) -> path the frontend should fetch (relative or absolute)
    Uses app.get_personnel_info when available; otherwise builds reasonable imageUrl from EmployeeID.
    Returns new DataFrame (copy).
    """
    if df is None or df.empty:
        return df

    # Try to build absolute endpoint if running inside Flask request context
    try:
        from flask import request as _flask_request
        base = (_flask_request.url_root or _flask_request.host_url).rstrip('/')
        if base:
            image_endpoint_template = f"{base}/employee/{{}}/image"
    except Exception:
        # no request context — keep given template (caller may set absolute string)
        pass

    get_personnel_info, get_person_image_bytes = _get_personnel_funcs_lazy()

    emails = []
    image_urls = []

    for _, row in df.iterrows():
        email = None
        image_url = None

        # prefer canonical non-GUID employee id for lookup
        cand_empid = None
        for tok in ('EmployeeID', 'EmployeeID_feat', 'EmployeeID_dur', 'Int1', 'Text12', 'employeeid'):
            if tok in row and row.get(tok) not in (None, '', float('nan')):
                cand_empid = _normalize_for_lookup(row.get(tok))
                if cand_empid:
                    break

        cand_uid = None
        for tok in ('EmployeeIdentity', 'person_uid', 'GUID'):
            if tok in row and row.get(tok) not in (None, '', float('nan')):
                cand_uid = row.get(tok)
                break

        # Try app.get_personnel_info if available (best source for Email + canonical ObjectID/GUID)
        if get_personnel_info:
            try:
                lookup_token = cand_empid or cand_uid
                pi = None
                if lookup_token is not None:
                    pi = get_personnel_info(lookup_token)
                else:
                    # last-resort: try employee name/object name if present
                    ln = row.get('EmployeeName') or row.get('ObjectName1')
                    if ln:
                        pi = get_personnel_info(ln)
                if pi and isinstance(pi, dict):
                    # EmployeeEmail alias from app.get_personnel_info
                    email = pi.get('EmployeeEmail') or pi.get('EmailAddress') or pi.get('Email') or None
                    # determine parent id for image route: prefer ObjectID, then GUID, then EmployeeID
                    parent = pi.get('ObjectID') or pi.get('GUID') or cand_empid or cand_uid
                    if parent is not None:
                        image_url = image_endpoint_template.format(str(parent))
            except Exception:
                # don't propagate enrichment errors
                email = email or None
                image_url = image_url or None

        # fallback when get_personnel_info not available or returned nothing
        if email is None:
            # try to find email-like fields in row if present
            for possible in ('EmployeeEmail', 'Email', 'EmailAddress', 'WorkEmail', 'ManagerEmail', 'EMail'):
                if possible in row and row.get(possible) not in (None, '', float('nan')):
                    email = row.get(possible)
                    break

        if image_url is None:
            # fallback: if we have an EmployeeID use that for image route
            if cand_empid:
                image_url = image_endpoint_template.format(cand_empid)
            elif cand_uid:
                image_url = image_endpoint_template.format(cand_uid)
            else:
                image_url = None

        # normalize simple email string
        if email and isinstance(email, str):
            email = email.strip()

        emails.append(email)
        image_urls.append(image_url)

    out = df.copy()
    out['EmployeeEmail'] = emails
    out['imageUrl'] = image_urls
    return out


def _enrich_with_personnel_info(df, image_endpoint_template="/employee/{}/image"):
    """
    Adds columns to df:
      - EmployeeEmail (string or None)
      - imageUrl (string or None) -> path the frontend should fetch (relative)
    Uses app.get_personnel_info when available; otherwise builds reasonable imageUrl from EmployeeID.
    Returns new DataFrame (copy).
    """
    if df is None or df.empty:
        return df
    get_personnel_info, get_person_image_bytes = _get_personnel_funcs_lazy()

    emails = []
    image_urls = []

    for _, row in df.iterrows():
        email = None
        image_url = None

        # prefer canonical non-GUID employee id for lookup
        cand_empid = None
        for tok in ('EmployeeID', 'EmployeeID_feat', 'EmployeeID_dur', 'Int1', 'Text12'):
            if tok in row and row.get(tok) not in (None, '', float('nan')):
                cand_empid = _normalize_for_lookup(row.get(tok))
                break
        cand_uid = row.get('EmployeeIdentity') or row.get('person_uid') or None

        # Try app.get_personnel_info if available (best source for Email + canonical ObjectID/GUID)
        if get_personnel_info:
            try:
                lookup_token = cand_empid or cand_uid
                pi = None
                if lookup_token is not None:
                    pi = get_personnel_info(lookup_token)
                else:
                    # last-resort: try employee name/object name if present
                    ln = row.get('EmployeeName') or row.get('ObjectName1')
                    if ln:
                        pi = get_personnel_info(ln)
                if pi and isinstance(pi, dict):
                    # EmployeeEmail alias from app.get_personnel_info
                    email = pi.get('EmployeeEmail') or pi.get('EmailAddress') or None
                    # determine parent id for image route: prefer ObjectID, then GUID, then EmployeeID
                    parent = pi.get('ObjectID') or pi.get('GUID') or cand_empid or cand_uid
                    if parent is not None:
                        image_url = image_endpoint_template.format(str(parent))
            except Exception:
                # don't propagate enrichment errors
                email = email or None
                image_url = image_url or None

        # fallback when get_personnel_info not available or returned nothing
        if email is None:
            # try to find email-like fields in row if present
            for possible in ('EmployeeEmail', 'Email', 'EmailAddress', 'ManagerEmail'):
                if possible in row and row.get(possible) not in (None, '', float('nan')):
                    email = row.get(possible)
                    break

        if image_url is None:
            # fallback: if we have an EmployeeID use that for image route
            if cand_empid:
                image_url = image_endpoint_template.format(cand_empid)
            elif cand_uid:
                image_url = image_endpoint_template.format(cand_uid)
            else:
                image_url = None

        emails.append(email)
        image_urls.append(image_url)

    out = df.copy()
    out['EmployeeEmail'] = emails
    out['imageUrl'] = image_urls
    return out
# ---------------------------------------------------------------------------


# duration_report imports (made robust)
try:
    from duration_report import run_for_date, compute_daily_durations, REGION_CONFIG
except Exception:
    try:
        from duration_report import run_for_date, compute_daily_durations
        REGION_CONFIG = {}
    except Exception:
        # minimal fallback: run_for_date must exist in actual environment; otherwise callers will fail later
        try:
            from duration_report import run_for_date
        except Exception:
            run_for_date = None
        compute_daily_durations = None
        REGION_CONFIG = {}

# try to import config door->zone mapping, but keep fallback
try:
    from config.door_zone import map_door_to_zone as config_map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    config_map_door_to_zone = None
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

# HIST_PATH: try a few likely locations (project config, repository root, absolute path)
CANDIDATE_HISTORY = [
    Path(__file__).parent / "config" / "current_analysis.csv",
    Path(__file__).parent.parent / "config" / "current_analysis.csv",
    Path.cwd() / "current_analysis.csv",
    Path(__file__).parent / "current_analysis.csv"
]
HIST_PATH = None
for p in CANDIDATE_HISTORY:
    if p.exists():
        HIST_PATH = p
        break

if HIST_PATH is None:
    logging.warning("Historical profile file current_analysis.csv not found in candidate locations.")
    HIST_DF = pd.DataFrame()
else:
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)

def _slug_city(city: str) -> str:
    if not city:
        return "pune"
    return str(city).strip().lower().replace(" ", "_")



# ----- small shared helpers: treat empty/placeholder tokens as None -----
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _normalize_id_val(v):
    """
    Normalize an id-like value: strip, convert floats like '320172.0' -> '320172'
    Return None for NaN/empty/placeholder.
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    # strip .0 if integer-like
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s

# GUID / name helpers
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')

def _looks_like_guid(s: object) -> bool:
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        if _looks_like_guid(st):
            return False
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    """
    Create canonical person uid:
      - prefer EmployeeID (normalized) -> 'emp:<id>' only if it is not a GUID
      - else EmployeeIdentity -> 'uid:<val>'
      - else EmployeeName -> hash-based 'name:<shorthash>'
    """
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None

# small helper to extract Card from XML-like strings
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None

# use config_map_door_to_zone if available, else fallback
try:
    _BREAK_ZONES = BREAK_ZONES
    _OUT_OF_OFFICE_ZONE = OUT_OF_OFFICE_ZONE
except Exception:
    _BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    _OUT_OF_OFFICE_ZONE = "Out of office"

def map_door_to_zone(door: object, direction: object = None) -> str:
    try:
        if config_map_door_to_zone is not None:
            return config_map_door_to_zone(door, direction)
    except Exception:
        pass
    try:
        if door is None:
            return None
        s = str(door).strip()
        if not s:
            return None
        s_l = s.lower()
        if direction and isinstance(direction, str):
            d = direction.strip().lower()
            if "out" in d:
                return _OUT_OF_OFFICE_ZONE
            if "in" in d:
                return "Reception Area"
        if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
            return _OUT_OF_OFFICE_ZONE
        return "Working Area"
    except Exception:
        return None

# ----- Config and scenarios -----
VIOLATION_WINDOW_DAYS = 90
RISK_THRESHOLDS = [
    (0.5, "Low"),
    (1.5, "Low Medium"),
    (2.5, "Medium"),
    (4.0, "Medium High"),
    (float("inf"), "High"),
]

def map_score_to_label(score: float) -> (int, str):
    try:
        if score is None:
            score = 0.0
        s = float(score)
    except Exception:
        s = 0.0
    bucket = 1
    label = "Low"
    for i, (threshold, lbl) in enumerate(RISK_THRESHOLDS, start=1):
        if s <= threshold:
            bucket = i
            label = lbl
            break
    return bucket, label

# scenario functions (kept from your improved version)
def scenario_long_gap(row):
    try:
        gap = int(row.get('MaxSwipeGapSeconds') or 0)
        return gap >= int(4.5 * 3600)
    except Exception:
        return False

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = int(row.get('CountSwipes') or 0)
    dur = float(row.get('DurationMinutes') or 0.0)
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = rec.get('TotalSwipes_median', np.nan)
            if pd.notna(median) and median > 0:
                return (cur > 3 * float(median)) and (dur < 60)
    except Exception:
        pass
    try:
        if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
            global_med = HIST_DF['TotalSwipes_median'].median()
            if pd.notna(global_med) and global_med > 0:
                return (cur > 3 * float(global_med)) and (dur < 60)
    except Exception:
        pass
    return (cur > 50) and (dur < 60)

def scenario_high_swipes_benign(row):
    cur = int(row.get('CountSwipes') or 0)
    dur = float(row.get('DurationMinutes') or 0.0)
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = rec.get('TotalSwipes_median', np.nan)
            if pd.notna(median) and median > 0:
                return (cur > 3 * float(median)) and (dur >= 60)
    except Exception:
        pass
    try:
        if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
            global_med = HIST_DF['TotalSwipes_median'].median()
            if pd.notna(global_med) and global_med > 0:
                return (cur > 3 * float(global_med)) and (dur >= 60)
    except Exception:
        pass
    return (cur > 50) and (dur >= 60)

def scenario_behaviour_shift(row, hist_df=None, minutes_threshold=180):
    try:
        if pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None:
            return False
        first_ts = pd.to_datetime(row.get('FirstSwipe'))
        today_minutes = first_ts.hour * 60 + first_ts.minute
        empid = row.get('EmployeeID')
        hist = hist_df if hist_df is not None else (HIST_DF if (HIST_DF is not None and not HIST_DF.empty) else None)
        if hist is None or hist.empty or empid is None:
            return False
        try:
            rec = hist[hist['EmployeeID'] == empid]
            if rec.empty:
                return False
            if 'FirstSwipeMinutes_median' in rec.columns:
                median_min = rec.iloc[0].get('FirstSwipeMinutes_median')
            else:
                median_min = rec.iloc[0].get('AvgFirstSwipeMins_median', None)
            if pd.isna(median_min) or median_min is None:
                return False
            diff = abs(today_minutes - float(median_min))
            return diff >= int(minutes_threshold)
        except Exception:
            return False
    except Exception:
        return False

def scenario_repeated_short_breaks(row):
    try:
        break_count = int(row.get('BreakCount') or 0)
        total_break_mins = float(row.get('TotalBreakMinutes') or 0.0)
        long_break_count = int(row.get('LongBreakCount') or 0)
        short_gap_count = int(row.get('ShortGapCount') or 0)
        if break_count >= 2:
            return True
        if short_gap_count >= 5:
            return True
        if total_break_mins >= 180 and short_gap_count >= 2:
            return True
        return False
    except Exception:
        return False

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map

def scenario_shortstay_longout_repeat(row):
    return bool(row.get('PatternShortLongRepeat', False))

SCENARIOS = [
    ("long_gap_>=4.5h", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap),
    ("high_swipes_benign", scenario_high_swipes_benign),
    ("behaviour_shift", scenario_behaviour_shift),
    ("shortstay_longout_repeat", scenario_shortstay_longout_repeat)
]

# --- improved human-readable scenario explanations (use hours for duration/gaps) ---
def _hrs_from_minutes(mins):
    try:
        m = float(mins or 0.0)
        return round(m / 60.0, 1)
    except Exception:
        return None

def _hrs_from_seconds(sec):
    try:
        s = float(sec or 0.0)
        return round(s / 3600.0, 1)
    except Exception:
        return None

SCENARIO_EXPLANATIONS = {
    "long_gap_>=4.5h": lambda r: (
        (lambda h: f"Long gap between swipes (~{h} h)." if h is not None else "Long gap between swipes.")
        (_hrs_from_seconds(r.get('MaxSwipeGapSeconds')))
    ),
    "short_duration_<4h": lambda r: (
        # if duration is zero but we only saw only_in/only_out, be explicit
        "Only 'IN' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyIn', 0)) == 1 else
        "Only 'OUT' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyOut', 0)) == 1 else
        (lambda h: f"Short total presence (~{h} h)." if h is not None else "Short total presence.")(_hrs_from_minutes(r.get('DurationMinutes')))
    ),
    "coffee_badging": lambda r: "Multiple quick swipes in short time.",
    "low_swipe_count_<=2": lambda r: "Very few swipes on day.",
    "single_door": lambda r: "Only a single door used during the day.",
    "only_in": lambda r: "Only 'IN' events recorded.",
    "only_out": lambda r: "Only 'OUT' events recorded.",
    "overtime_>=10h": lambda r: "Overtime detected (>=10 hours).",
    "very_long_duration_>=16h": lambda r: "Very long presence (>=16 hours).",
    "zero_swipes": lambda r: "No swipes recorded on this day.",
    "unusually_high_swipes": lambda r: "Unusually high number of swipes compared to peers/history.",
    "repeated_short_breaks": lambda r: "Many short gaps between swipes.",
    "multiple_location_same_day": lambda r: "Multiple locations/partitions used in same day.",
    "weekend_activity": lambda r: "Activity recorded on weekend day.",
    "repeated_rejection_count": lambda r: "Multiple rejection events recorded.",
    "badge_sharing_suspected": lambda r: "Same card used by multiple users on same day — possible badge sharing.",
    "early_arrival_before_06": lambda r: "First swipe earlier than 06:00.",
    "late_exit_after_22": lambda r: "Last swipe after 22:00.",
    "shift_inconsistency": lambda r: "Duration deviates from historical shift patterns.",
    "trending_decline": lambda r: "Employee shows trending decline in presence.",
    "consecutive_absent_days": lambda r: "Consecutive absent days observed historically.",
    "high_variance_duration": lambda r: "High variance in daily durations historically.",
    "short_duration_on_high_presence_days": lambda r: "Short duration despite normally high presence days.",
    "swipe_overlap": lambda r: "Overlap in swipe times with other persons on same door.",
    "behaviour_shift": lambda r: "Significant change in arrival time compared to historical baseline.",
    "shortstay_longout_repeat": lambda r: "Repeated pattern: short in → long out → short return."
}


# def _explain_scenarios_detected(row, detected_list):
#     pieces = []
#     # prefer non-GUID EmployeeID, then EmployeeName, then person_uid stripped
#     name = None
#     try:
#         # check explicit EmployeeID (non-GUID)
#         empid = row.get('EmployeeID') or row.get('EmployeeID_feat') or row.get('EmployeeID_dur') or row.get('Int1') or row.get('Text12')
#         if empid and not _looks_like_guid(empid) and not _is_placeholder_str(empid):
#             name = str(empid).strip()
#         else:
#             # prefer a human name if present and not a GUID
#             nm = row.get('EmployeeName')
#             if nm and _looks_like_name(nm) and not _is_placeholder_str(nm):
#                 name = str(nm).strip()
#             else:
#                 # try stripping person_uid prefixes like 'emp:' or 'uid:' if present
#                 pu = row.get('person_uid')
#                 if isinstance(pu, str) and (pu.startswith('emp:') or pu.startswith('uid:') or pu.startswith('name:')):
#                     try:
#                         stripped = _strip_uid_prefix(pu)
#                         if stripped and not _looks_like_guid(stripped):
#                             name = str(stripped)
#                     except Exception:
#                         pass
#                 if not name:
#                     # fallback: use EmployeeID if it's non-empty and non-placeholder even if it looks like GUID
#                     if empid and not _is_placeholder_str(empid):
#                         name = str(empid)
#                     else:
#                         name = row.get('EmployeeName') or row.get('person_uid') or row.get('EmployeeID') or "Employee"
#     except Exception:
#         name = row.get('EmployeeName') or row.get('person_uid') or row.get('EmployeeID') or "Employee"

#     prefix = f"{name} - "
#     for sc in detected_list:
#         sc = sc.strip()
#         fn = SCENARIO_EXPLANATIONS.get(sc)
#         try:
#             if fn:
#                 pieces.append(fn(row))
#             else:
#                 pieces.append(sc.replace("_", " ").replace(">=", "≥"))
#         except Exception:
#             pieces.append(sc)
#     if not pieces:
#         return None
#     explanation = " ".join([p if p.endswith('.') else p + '.' for p in pieces])

#     # Replace any GUID that accidentally remained inside explanation with the chosen human identifier
#     try:
#         GUID_IN_TEXT_RE = re.compile(r'[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}')
#         if name and isinstance(explanation, str) and GUID_IN_TEXT_RE.search(explanation):
#             explanation = GUID_IN_TEXT_RE.sub(str(name), explanation)
#     except Exception:
#         pass

#     # return with prefix (keeps old behaviour of starting with "Name - ")
#     return prefix + " " + explanation


def _explain_scenarios_detected(row, detected_list):
    pieces = []
    # derive a human display label consisting of Name and EmployeeID where possible
    try:
        empid = None
        # prefer explicit EmployeeID (non-GUID) from various tokens
        for tok in ('EmployeeID', 'EmployeeID_feat', 'EmployeeID_dur', 'Int1', 'Text12'):
            if tok in row and row.get(tok) not in (None, '', 'nan'):
                val = _normalize_id_val(row.get(tok))
                if val and not _looks_like_guid(val):
                    empid = str(val)
                    break
        # if empid still None, allow non-GUID EmployeeIdentity
        if not empid and row.get('EmployeeIdentity') not in (None, '', 'nan'):
            tmp = _normalize_id_val(row.get('EmployeeIdentity'))
            if tmp and not _looks_like_guid(tmp):
                empid = str(tmp)

        name = None
        try:
            nm = row.get('EmployeeName')
            if nm and _looks_like_name(nm) and not _is_placeholder_str(nm):
                name = str(nm).strip()
            else:
                # fallback: pick first non-guid textual name from common tokens
                for cand in ('EmployeeName', 'EmployeeName_feat', 'EmployeeName_dur', 'ObjectName1'):
                    if cand in row and row.get(cand) not in (None, '', 'nan'):
                        v = row.get(cand)
                        if v and not _looks_like_guid(v) and _looks_like_name(v):
                            name = str(v).strip()
                            break
                if not name:
                    # try to strip person_uid prefixes if present
                    pu = row.get('person_uid')
                    if isinstance(pu, str) and (pu.startswith('emp:') or pu.startswith('uid:') or pu.startswith('name:')):
                        try:
                            stripped = _strip_uid_prefix(pu)
                            if stripped and not _looks_like_guid(stripped):
                                name = str(stripped)
                        except Exception:
                            pass
        except Exception:
            name = None

        # build prefix
        prefix = ""
        if name and empid:
            prefix = f"{name} ({empid}) - "
        elif name:
            prefix = f"{name} - "
        elif empid:
            prefix = f"{empid} - "
        else:
            prefix = ""
    except Exception:
        prefix = ""

    for sc in detected_list:
        sc = sc.strip()
        fn = SCENARIO_EXPLANATIONS.get(sc)
        try:
            if fn:
                pieces.append(fn(row))
            else:
                pieces.append(sc.replace("_", " ").replace(">=", "≥"))
        except Exception:
            pieces.append(sc)
    if not pieces:
        return None
    explanation = " ".join([p if p.endswith('.') else p + '.' for p in pieces])

    # Replace any GUID that accidentally remained inside explanation with the chosen human identifier (without duplicating parentheses)
    try:
        GUID_IN_TEXT_RE = re.compile(r'[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}')
        if prefix and isinstance(explanation, str) and GUID_IN_TEXT_RE.search(explanation):
            # replace GUIDs inside with the prefix label (strip trailing ' - ' from prefix)
            label = prefix.rstrip(' - ')
            explanation = GUID_IN_TEXT_RE.sub(str(label), explanation)
    except Exception:
        pass

    return prefix + explanation


# ---------------- compute_features (robust merged version) ----------------
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)

    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
    else:
        if 'Date' in sw.columns:
            sw['LocaleMessageTime'] = None
            try:
                sw['LocaleMessageTime'] = pd.to_datetime(sw['Date'], errors='coerce')
            except Exception:
                sw['LocaleMessageTime'] = None

    # By default Date comes from LocaleMessageTime (local, human timestamps).
    # However if an AdjustedMessageTime column exists (the Pune 2AM boundary) prefer that
    # for date assignment so trend grouping matches compute_daily_durations().
    if 'AdjustedMessageTime' in sw.columns and sw['AdjustedMessageTime'].notna().any():
        try:
            sw['AdjustedMessageTime'] = pd.to_datetime(sw['AdjustedMessageTime'], errors='coerce')
            # Prefer adjusted date for rows where it exists (this mirrors duration_report logic).
            mask_adj = sw['AdjustedMessageTime'].notna()
            # Ensure LocaleMessageTime parsed for those not adjusted
            sw.loc[~mask_adj, 'Date'] = pd.to_datetime(sw.loc[~mask_adj, 'LocaleMessageTime'], errors='coerce').dt.date
            sw.loc[mask_adj, 'Date']  = sw.loc[mask_adj,  'AdjustedMessageTime'].dt.date
        except Exception:
            # fallback to LocaleMessageTime date
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
    else:
        # normal path
        sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date

    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    try:
        if dir_col and dir_col in sw.columns:
            sw['Direction'] = sw[dir_col]
        if door_col and door_col in sw.columns:
            sw['Door'] = sw[door_col]
        if empid_col and empid_col in sw.columns:
            sw['EmployeeID'] = sw[empid_col]
        if name_col and name_col in sw.columns:
            sw['EmployeeName'] = sw[name_col]
        if card_col and card_col in sw.columns:
            sw['CardNumber'] = sw[card_col]
        if 'LocaleMessageTime' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
        elif 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
    except Exception:
        logging.exception("Normalization of swipe columns failed.")

    # PersonnelType filtering (tolerant) - avoid dropping if column absent
    if 'PersonnelTypeName' in sw.columns:
        sw['PersonnelTypeName'] = sw['PersonnelTypeName'].astype(str).str.strip()
        mask = sw['PersonnelTypeName'].str.lower().str.contains(r'employee|terminated', na=False)
        logging.info("PersonnelTypeName values example: %s", list(sw['PersonnelTypeName'].dropna().unique()[:6]))
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelTypeName filter applied: before=%d after=%d", before, len(sw))
    elif 'PersonnelType' in sw.columns:
        sw['PersonnelType'] = sw['PersonnelType'].astype(str).str.strip()
        mask = sw['PersonnelType'].str.lower().str.contains(r'employee|terminated', na=False)
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelType filter applied: before=%d after=%d", before, len(sw))

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # person_uid canonical
    if 'person_uid' not in sw.columns:
        def make_person_uid_local(r):
            empid_val = None
            if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
                empid_val = r.get(empid_col)
            elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
                empid_val = r.get('EmployeeID')
            empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
            name_val = None
            if name_col and name_col in r:
                name_val = r.get(name_col)
            elif 'EmployeeName' in r:
                name_val = r.get('EmployeeName')
            elif 'ObjectName1' in r:
                name_val = r.get('ObjectName1')
            return _canonical_person_uid({
                'EmployeeID': empid_val,
                'EmployeeIdentity': empident_val,
                'EmployeeName': name_val
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
                    'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
                    'EmployeeIdentity'])
    if name_col: sel_cols.add(name_col)
    if empid_col: sel_cols.add(empid_col)
    if card_col: sel_cols.add(card_col)
    if door_col: sel_cols.add(door_col)
    if dir_col: sel_cols.add(dir_col)
    sel_cols = [c for c in sel_cols if c in sw.columns]

    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0

        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

        # card extraction
        card_numbers = []
        if card_col and card_col in g.columns:
            card_numbers = list(pd.unique(g[card_col].dropna()))
        if not card_numbers and 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna()))
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
                    try:
                        vals = list(pd.unique(g[c].dropna()))
                        if vals:
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'xml' in cl:
                    for raw in g[c].dropna().astype(str):
                        extracted = _extract_card_from_xml(raw)
                        if extracted:
                            card_numbers.append(extracted)
        card_numbers = list(dict.fromkeys(card_numbers))
        card_number = None
        for c in card_numbers:
            n = _normalize_id_val(c)
            if n and not _looks_like_guid(n):
                card_number = n
                break

        # stable id/name
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None

        if empid_col and empid_col in g.columns:
            vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        elif 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized

        if (not employee_id) and 'PersonnelType' in g.columns:
            try:
                pvals = g['PersonnelType'].dropna().astype(str)
                if not pvals.empty:
                    p0 = pvals.iloc[0]
                    if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
                        for c in g.columns:
                            if c.lower() == 'text12':
                                vals = g[c].dropna().astype(str).map(lambda x: x.strip())
                                employee_id = _pick_first_non_guid_value(vals)
                                if employee_id:
                                    break
            except Exception:
                pass

        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]

        candidate_name_vals = None
        if name_col and name_col in g.columns:
            candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
        elif 'EmployeeName' in g.columns:
            candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
        elif 'ObjectName1' in g.columns:
            candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

        if candidate_name_vals is not None and not candidate_name_vals.empty:
            employee_name = _pick_first_non_guid_value(candidate_name_vals)
            if employee_name is None:
                for v in candidate_name_vals:
                    if _looks_like_name(v) and not _is_placeholder_str(v):
                        employee_name = str(v).strip()
                        break

        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        first_swipe = None
        last_swipe = None
        if times:
            first_swipe = times[0]
            last_swipe = times[-1]

        # timeline & segments with mapping to zones
        timeline = []
        for _, row in g.sort_values('LocaleMessageTime').iterrows():
            t = row.get('LocaleMessageTime')
            dname = None
            if door_col and door_col in row and pd.notna(row.get(door_col)):
                dname = row.get(door_col)
            elif 'Door' in row and pd.notna(row.get('Door')):
                dname = row.get('Door')
            direction = None
            if dir_col and dir_col in row and pd.notna(row.get(dir_col)):
                direction = row.get(dir_col)
            elif 'Direction' in row and pd.notna(row.get('Direction')):
                direction = row.get('Direction')
            zone = map_door_to_zone(dname, direction)
            timeline.append((t, dname, direction, zone))

        segments = []
        if timeline:
            cur_zone = None
            seg_start = timeline[0][0]
            seg_label = None
            for (t, dname, direction, zone) in timeline:
                if zone in _BREAK_ZONES:
                    lbl = 'break'
                elif zone == _OUT_OF_OFFICE_ZONE:
                    lbl = 'out_of_office'
                else:
                    lbl = 'work'
                if cur_zone is None:
                    cur_zone = zone
                    seg_label = lbl
                    seg_start = t
                else:
                    if lbl != seg_label:
                        segments.append({
                            'label': seg_label,
                            'start': seg_start,
                            'end': t,
                            'start_zone': cur_zone
                        })
                        seg_start = t
                        seg_label = lbl
                        cur_zone = zone
                    else:
                        cur_zone = cur_zone or zone
            if seg_label is not None:
                segments.append({
                    'label': seg_label,
                    'start': seg_start,
                    'end': timeline[-1][0],
                    'start_zone': cur_zone
                })

        break_count = 0
        long_break_count = 0
        total_break_minutes = 0.0

        BREAK_MINUTES_THRESHOLD = 60
        OUT_OFFICE_COUNT_MINUTES = 180
        LONG_BREAK_FLAG_MINUTES = 120

        for i, s in enumerate(segments):
            lbl = s.get('label')
            start = s.get('start')
            end = s.get('end')
            dur_mins = ((end - start).total_seconds() / 60.0) if (start and end) else 0.0
            if lbl == 'break':
                if dur_mins >= BREAK_MINUTES_THRESHOLD:
                    break_count += 1
                    total_break_minutes += dur_mins
                    if dur_mins >= LONG_BREAK_FLAG_MINUTES:
                        long_break_count += 1
            elif lbl == 'out_of_office':
                prev_lbl = segments[i-1]['label'] if i > 0 else None
                next_lbl = segments[i+1]['label'] if i < len(segments)-1 else None
                if prev_lbl == 'work' and next_lbl == 'work' and dur_mins >= OUT_OFFICE_COUNT_MINUTES:
                    break_count += 1
                    total_break_minutes += dur_mins
                    if dur_mins >= LONG_BREAK_FLAG_MINUTES:
                        long_break_count += 1

        pattern_flag = False
        pattern_sequence_readable = None
        try:
            seq = []
            for s in segments:
                dur_mins = (s['end'] - s['start']).total_seconds() / 60.0 if (s['end'] and s['start']) else 0
                seq.append((s['label'], int(round(dur_mins))))
            for i in range(len(seq)-2):
                a = seq[i]
                b = seq[i+1]
                c = seq[i+2]
                if (a[0] == 'work' and a[1] < 60) and \
                   (b[0] in ('out_of_office','break') and b[1] >= LONG_BREAK_FLAG_MINUTES) and \
                   (c[0] == 'work' and c[1] < 60):
                    pattern_flag = True
                    seq_fragment = [a, b, c]
                    pattern_sequence_readable = " -> ".join([f"{lbl} ({mins}m)" for lbl, mins in seq_fragment])
                    break
        except Exception:
            pattern_flag = False
            pattern_sequence_readable = None

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type,
            'FirstSwipe': first_swipe,
            'LastSwipe': last_swipe,
            'BreakCount': int(break_count),
            'LongBreakCount': int(long_break_count),
            'TotalBreakMinutes': float(round(total_break_minutes,1)),
            'PatternShortLongRepeat': bool(pattern_flag),
            'PatternSequenceReadable': pattern_sequence_readable,
            'PatternSequence': None
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])[sel_cols]
    grouped = grouped.apply(agg_swipe_group).reset_index()

    

    # POST-PROCESS: merge early-morning fragments into previous day (heuristic)
    try:
        grouped['FirstSwipe_dt'] = pd.to_datetime(grouped['FirstSwipe'], errors='coerce')
        grouped['LastSwipe_dt']  = pd.to_datetime(grouped['LastSwipe'],  errors='coerce')
        rows_to_drop = set()
        MERGE_GAP_SECONDS = int(4 * 3600)
        for pid, sub in grouped.sort_values(['person_uid','Date']).groupby('person_uid'):
            prev_idx = None
            for idx, r in sub.reset_index().iterrows():
                real_idx = int(r['index']) if 'index' in r else r.name
                cur_first = pd.to_datetime(grouped.at[real_idx, 'FirstSwipe_dt'])
                if prev_idx is not None:
                    prev_last = pd.to_datetime(grouped.at[prev_idx, 'LastSwipe_dt'])
                    if (not pd.isna(cur_first)) and (not pd.isna(prev_last)):
                        gap = (cur_first - prev_last).total_seconds()
                        if 0 <= gap <= MERGE_GAP_SECONDS and cur_first.time().hour < 2:
                            try:
                                grouped.at[prev_idx, 'CountSwipes'] = int(grouped.at[prev_idx, 'CountSwipes']) + int(grouped.at[real_idx, 'CountSwipes'])
                                grouped.at[prev_idx, 'MaxSwipeGapSeconds'] = max(int(grouped.at[prev_idx, 'MaxSwipeGapSeconds'] or 0), int(grouped.at[real_idx, 'MaxSwipeGapSeconds'] or 0), int(gap))
                                if not pd.isna(grouped.at[real_idx, 'LastSwipe_dt']):
                                    if pd.isna(grouped.at[prev_idx, 'LastSwipe_dt']) or grouped.at[real_idx, 'LastSwipe_dt'] > grouped.at[prev_idx, 'LastSwipe_dt']:
                                        grouped.at[prev_idx, 'LastSwipe_dt'] = grouped.at[real_idx, 'LastSwipe_dt']
                                        grouped.at[prev_idx, 'LastSwipe'] = grouped.at[real_idx, 'LastSwipe']
                                if not grouped.at[prev_idx, 'CardNumber']:
                                    grouped.at[prev_idx, 'CardNumber'] = grouped.at[real_idx, 'CardNumber']
                                grouped.at[prev_idx, 'UniqueDoors'] = int(max(int(grouped.at[prev_idx].get('UniqueDoors') or 0), int(grouped.at[real_idx].get('UniqueDoors') or 0)))
                                grouped.at[prev_idx, 'UniqueLocations'] = int(max(int(grouped.at[prev_idx].get('UniqueLocations') or 0), int(grouped.at[real_idx].get('UniqueLocations') or 0)))
                                rows_to_drop.add(real_idx)
                                continue
                            except Exception:
                                pass
                prev_idx = real_idx
        if rows_to_drop:
            grouped = grouped.drop(index=list(rows_to_drop)).reset_index(drop=True)
    except Exception:
        logging.exception("Failed merge-early-morning fragments (non-fatal).")

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # coalesce duplicated columns (_x/_y) produced by merge
    def _coalesce_merge_columns(df, bases):
        for base in bases:
            x = base + "_x"
            y = base + "_y"
            try:
                has_base = base in df.columns
                base_all_null = False
                if has_base:
                    base_all_null = df[base].isnull().all()
            except Exception:
                has_base = base in df.columns
                base_all_null = True
            if (not has_base) or base_all_null:
                if x in df.columns and y in df.columns:
                    try:
                        df[base] = df[x].combine_first(df[y])
                    except Exception:
                        try:
                            df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
                        except Exception:
                            if x in df.columns:
                                df[base] = df[x]
                            elif y in df.columns:
                                df[base] = df[y]
                elif x in df.columns:
                    df[base] = df[x]
                elif y in df.columns:
                    df[base] = df[y]
    _coalesce_merge_columns(merged, [
        "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
    ])
    drop_cols = [c for c in merged.columns if c.endswith("_x") or c.endswith("_y")]
    if drop_cols:
        try:
            merged.drop(columns=drop_cols, inplace=True)
        except Exception:
            for c in drop_cols:
                if c in merged.columns:
                    try:
                        merged.drop(columns=[c], inplace=True)
                    except Exception:
                        pass

    # ensure columns exist and normalized
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)
    ensure_col(merged, 'BreakCount', 0)
    ensure_col(merged, 'LongBreakCount', 0)
    ensure_col(merged, 'TotalBreakMinutes', 0.0)
    ensure_col(merged, 'PatternShortLongRepeat', False)
    ensure_col(merged, 'PatternSequenceReadable', None)
    ensure_col(merged, 'PatternSequence', None)

    if 'EmployeeName' in merged.columns:
        def choose_best_name(row):
            gname = row.get('EmployeeName')
            dname = None
            for cand in ('EmployeeName', 'employee_name', 'objectname1', 'ObjectName1'):
                if cand in row and row.get(cand) is not None:
                    dname = row.get(cand)
                    break
            if _looks_like_name(gname):
                return str(gname).strip()
            if _looks_like_name(dname):
                return str(dname).strip()
            if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
                return str(gname).strip()
            if dname and not _is_placeholder_str(dname):
                return str(dname).strip()
            return None
        merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
    else:
        if not dur.empty:
            def fill_name_from_dur(row):
                gname = row.get('EmployeeName')
                if _looks_like_name(gname) and not _is_placeholder_str(gname):
                    return gname
                for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
                    if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
                        return row[cand]
                return None
            merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

    def normalize_empid(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            try:
                if '.' in s:
                    f = float(s)
                    if math.isfinite(f) and f.is_integer():
                        return str(int(f))
            except Exception:
                pass
            return s
        except Exception:
            return None

    merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

    def normalize_card(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            return s
        except Exception:
            return None

    merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

    if 'DurationSeconds' not in merged.columns or merged['DurationSeconds'].isnull().all():
        try:
            merged['DurationSeconds'] = (pd.to_datetime(merged['LastSwipe']) - pd.to_datetime(merged['FirstSwipe'])).dt.total_seconds().clip(lower=0).fillna(0)
        except Exception:
            merged['DurationSeconds'] = merged.get('DurationSeconds', 0)

    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)
    merged['BreakCount'] = merged['BreakCount'].fillna(0).astype(int)
    merged['LongBreakCount'] = merged['LongBreakCount'].fillna(0).astype(int)
    merged['TotalBreakMinutes'] = merged['TotalBreakMinutes'].fillna(0.0).astype(float)
    merged['PatternShortLongRepeat'] = merged['PatternShortLongRepeat'].fillna(False).astype(bool)

    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
        if c in merged.columns:
            def _clean_str_val(v):
                if v is None:
                    return None
                try:
                    s = str(v).strip()
                    if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                        return None
                    return s
                except Exception:
                    return None
            merged[c] = merged[c].apply(_clean_str_val)

    if 'EmployeeName' in merged.columns:
        merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

    return merged

# ---------------- SCENARIO WEIGHTS ----------------
WEIGHTS = {
    "long_gap_>=4.5h": 0.3,
    "short_duration_<4h": 1.0,
    "coffee_badging": 1.0,
    "low_swipe_count_<=2": 0.5,
    "single_door": 0.25,
    "only_in": 0.8,
    "only_out": 0.8,
    "overtime_>=10h": 0.2,
    "very_long_duration_>=16h": 1.5,
    "zero_swipes": 0.4,
    "unusually_high_swipes": 1.5,
    "repeated_short_breaks": 0.5,
    "multiple_location_same_day": 0.6,
    "weekend_activity": 0.6,
    "repeated_rejection_count": 0.8,
    "badge_sharing_suspected": 2.0,
    "early_arrival_before_06": 0.4,
    "late_exit_after_22": 0.4,
    "shift_inconsistency": 1.2,
    "trending_decline": 0.7,
    "consecutive_absent_days": 1.2,
    "high_variance_duration": 0.8,
    "short_duration_on_high_presence_days": 1.1,
    "swipe_overlap": 2.0,
    "high_swipes_benign": 0.1,
    "shortstay_longout_repeat": 2.0
}
ANOMALY_THRESHOLD = 1.5

def _read_past_trend_csvs(outdir: str, window_days: int, target_date: date):
    p = Path(outdir)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return pd.DataFrame()
    dfs = []
    cutoff = target_date - timedelta(days=window_days)
    for fp in csvs:
        try:
            df = pd.read_csv(fp, parse_dates=['Date'])
            if 'Date' in df.columns:
                try:
                    df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                except Exception:
                    pass
                # include target_date in the window (cutoff <= Date <= target_date)
                def _date_in_window(d):
                    try:
                        return d is not None and (d >= cutoff and d <= target_date)
                    except Exception:
                        return False
                df = df[df['Date'].apply(_date_in_window)]
            dfs.append(df)
        except Exception:
            try:
                df = pd.read_csv(fp, dtype=str)
                if 'Date' in df.columns:
                    try:
                        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                        # include target_date in the window (cutoff <= Date <= target_date)
                        def _date_in_window(d):
                            try:
                                return d is not None and (d >= cutoff and d <= target_date)
                            except Exception:
                                return False
                        df = df[df['Date'].apply(_date_in_window)]
                    except Exception:
                        pass
                dfs.append(df)
            except Exception:
                continue
    if not dfs:
        return pd.DataFrame()
    try:
        out = pd.concat(dfs, ignore_index=True)
        return out
    except Exception:
        return pd.DataFrame()

def _read_scenario_counts_by_person(outdir: str, window_days: int, target_date: date, scenario_col: str):
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty or scenario_col not in df.columns:
        return {}
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass
    id_cols = [c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in df.columns]
    out = defaultdict(int)
    q = df[df[scenario_col] == True] if df[scenario_col].dtype == bool else df[df[scenario_col].astype(str).str.lower() == 'true']
    for _, r in q.iterrows():
        for col in id_cols:
            try:
                raw = r.get(col)
                if raw in (None, '', float('nan')):
                    continue
                norm = _normalize_id_val(raw)
                if norm:
                    out[str(norm)] += 1
                    stripped = _strip_uid_prefix(str(norm))
                    if stripped != str(norm):
                        out[str(stripped)] += 1
            except Exception:
                continue
        for fallback in ('Int1', 'Text12'):
            if fallback in r and r.get(fallback) not in (None, '', 'nan'):
                try:
                    norm = _normalize_id_val(r.get(fallback))
                    if norm:
                        out[str(norm)] += 1
                except Exception:
                    continue
    return dict(out)

def _compute_weeks_with_threshold(past_df: pd.DataFrame,
                                  person_col: str = 'person_uid',
                                  date_col: str = 'Date',
                                  scenario_col: str = 'short_duration_<4h',
                                  threshold_days: int = 3) -> dict:
    if past_df is None or past_df.empty:
        return {}
    df = past_df.copy()
    if date_col not in df.columns:
        return {}
    try:
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.date
    except Exception:
        pass
    if scenario_col not in df.columns:
        return {}
    try:
        if df[scenario_col].dtype == bool:
            df['__scenario_flag__'] = df[scenario_col].astype(bool)
        else:
            df['__scenario_flag__'] = df[scenario_col].astype(str).str.strip().str.lower().isin({'true', '1', 'yes', 'y', 't'})
    except Exception:
        df['__scenario_flag__'] = df[scenario_col].apply(lambda v: str(v).strip().lower() in ('true','1','yes','y','t') if v is not None else False)
    df = df[df['__scenario_flag__'] == True].copy()
    if df.empty:
        return {}
    if person_col not in df.columns:
        fallback = next((c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in past_df.columns), None)
        if fallback is None:
            return {}
        person_col = fallback
    def _week_monday(d):
        try:
            if d is None or (isinstance(d, float) and np.isnan(d)):
                return None
            iso = d.isocalendar()
            return date.fromisocalendar(iso[0], iso[1], 1)
        except Exception:
            return None
    df['__week_monday__'] = df[date_col].apply(_week_monday)
    df = df.dropna(subset=['__week_monday__', person_col])
    if df.empty:
        return {}
    week_counts = (df.groupby([person_col, '__week_monday__'])
                     .size()
                     .reset_index(name='days_flagged'))
    valid_weeks = week_counts[week_counts['days_flagged'] >= int(threshold_days)].copy()
    if valid_weeks.empty:
        return {}
    person_weeks = {}
    for person, grp in valid_weeks.groupby(person_col):
        wlist = sorted(pd.to_datetime(grp['__week_monday__']).dt.date.unique(), reverse=True)
        person_weeks[str(person)] = wlist
    def _consecutive_week_count(week_dates_desc):
        if not week_dates_desc:
            return 0
        count = 1
        prev = week_dates_desc[0]
        for cur in week_dates_desc[1:]:
            try:
                if (prev - cur).days == 7:
                    count += 1
                    prev = cur
                else:
                    break
            except Exception:
                break
        return count
    out = {}
    for pid, weeks in person_weeks.items():
        c = _consecutive_week_count(weeks)
        out[str(pid)] = int(c)
        try:
            stripped = _strip_uid_prefix(str(pid))
            if stripped and stripped != str(pid):
                out[str(stripped)] = int(c)
        except Exception:
            pass
    return out

def _strip_uid_prefix(s):
    try:
        if s is None:
            return s
        st = str(s)
        for p in ('emp:', 'uid:', 'name:'):
            if st.startswith(p):
                return st[len(p):]
        return st
    except Exception:
        return s

def compute_violation_days_map(outdir: str, window_days: int, target_date: date):
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty:
        return {}
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass
    id_cols = []
    for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber'):
        if c in df.columns:
            id_cols.append(c)
    if 'IsFlagged' not in df.columns:
        if 'AnomalyScore' in df.columns:
            df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: float(s) >= ANOMALY_THRESHOLD if not pd.isna(s) else False)
        else:
            df['IsFlagged'] = False
    ident_dates = defaultdict(set)
    try:
        flagged = df[df['IsFlagged'] == True]
        for _, r in flagged.iterrows():
            d = r.get('Date')
            if d is None:
                continue
            for col in id_cols:
                try:
                    raw = r.get(col)
                    if raw is None:
                        continue
                    norm = _normalize_id_val(raw)
                    if norm:
                        ident_dates[str(norm)].add(d)
                        stripped = _strip_uid_prefix(str(norm))
                        if stripped != str(norm):
                            ident_dates[str(stripped)].add(d)
                except Exception:
                    continue
            for fallback in ('Int1', 'Text12'):
                if fallback in r and r.get(fallback) not in (None, '', 'nan'):
                    try:
                        norm = _normalize_id_val(r.get(fallback))
                        if norm:
                            ident_dates[str(norm)].add(d)
                            stripped = _strip_uid_prefix(str(norm))
                            if stripped != str(norm):
                                ident_dates[str(stripped)].add(d)
                    except Exception:
                        continue
    except Exception:
        logging.exception("Error building violation days map from history.")
    out = {k: int(len(v)) for k, v in ident_dates.items()}
    return out



def score_trends_from_durations(combined_df: pd.DataFrame, swipes_df: Optional[pd.DataFrame] = None, outdir: Optional[str] = None, target_date: Optional[date] = None) -> pd.DataFrame:
    """
    Take combined durations DataFrame and optional swipes DataFrame and compute:
      - scenario boolean columns
      - Reasons (semicolon-separated scenario keys)
      - ViolationExplanation (human text)
      - AnomalyScore (weighted sum)
      - IsFlagged (AnomalyScore >= ANOMALY_THRESHOLD)
      - ViolationDaysLast90 (from history)
    """
    if combined_df is None or combined_df.empty:
        return pd.DataFrame()

    df = combined_df.copy()
    # Ensure person_uid exists
    if 'person_uid' not in df.columns:
        df['person_uid'] = df.apply(lambda r: _canonical_person_uid(r), axis=1)

    # Ensure key columns exist
    for c in ['FirstSwipe','LastSwipe','CountSwipes','DurationMinutes','MaxSwipeGapSeconds','EmployeeID','CardNumber','person_uid','Date']:
        if c not in df.columns:
            df[c] = None

    # apply scenario functions to each row
    scenario_flags = {}
    for sc_code, sc_fn in SCENARIOS:
        try:
            # apply robustly
            flags = []
            for _, r in df.iterrows():
                try:
                    flags.append(bool(sc_fn(r)))
                except Exception:
                    flags.append(False)
            df[sc_code] = flags
            scenario_flags[sc_code] = df[sc_code]
        except Exception:
            df[sc_code] = False
            scenario_flags[sc_code] = df[sc_code]

    # build Reasons (list) and AnomalyScore
    def compute_row_reasons_score(row):
        detected = []
        score = 0.0
        for sc_code, _ in SCENARIOS:
            try:
                if bool(row.get(sc_code)):
                    detected.append(sc_code)
                    score += float(WEIGHTS.get(sc_code, 0.0))
            except Exception:
                continue
        return detected, float(score)

    reasons_list = []
    scores = []
    for _, r in df.iterrows():
        detected, sc = compute_row_reasons_score(r)
        reasons_list.append(detected)
        scores.append(sc)
    df['ReasonsList'] = reasons_list
    df['AnomalyScore'] = scores
    df['AnomalyScore'] = pd.to_numeric(df['AnomalyScore'], errors='coerce').fillna(0.0)

    # human explanation
    def make_explanation(row):
        try:
            detected = row.get('ReasonsList') or []
            if not detected:
                return None
            return _explain_scenarios_detected(row, detected)
        except Exception:
            return None

    df['ViolationExplanation'] = df.apply(make_explanation, axis=1)

    # Reasons text (semi-colon)
    def reasons_to_text(lst):
        try:
            if not lst:
                return None
            return ";".join(lst)
        except Exception:
            return None

    df['Reasons'] = df['ReasonsList'].apply(reasons_to_text)

    # IsFlagged
    df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: float(s) >= ANOMALY_THRESHOLD)

    # ViolationDaysLast90: try to compute from history (best-effort)
    try:
        td = target_date if target_date is not None else (date.today())
        vmap = compute_violation_days_map(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, td)
    except Exception:
        vmap = {}

    def lookup_violation_days(row):
        try:
            candidates = []
            for k in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
                v = row.get(k)
                if v not in (None, '', float('nan')):
                    candidates.append(_normalize_id_val(v))
            for c in candidates:
                if c is None:
                    continue
                if c in vmap:
                    return int(vmap.get(c, 0))
                stripped = _strip_uid_prefix(c)
                if stripped != c and stripped in vmap:
                    return int(vmap.get(stripped, 0))
            return 0
        except Exception:
            return 0

    df['ViolationDaysLast90'] = df.apply(lookup_violation_days, axis=1)

    # tidy / expose common fields: ensure Date is iso / datetime
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass

    # provide a textual Reasons field if missing
    if 'Reasons' not in df.columns:
        df['Reasons'] = df['ReasonsList'].apply(reasons_to_text)

    # keep ordering sensible
    return df

def run_trend_for_date(target_date: date,
                       regions: Optional[List[str]] = None,
                       outdir: str = None,
                       city: str = "pune",
                       as_dict: bool = False) -> pd.DataFrame:
    """
    Enhanced run_trend_for_date: collects swipes + durations, computes features and scores.
    """
    city_slug = _slug_city(city)
    if regions is None:
        try:
            regions = list(REGION_CONFIG.keys()) if isinstance(REGION_CONFIG, dict) and REGION_CONFIG else []
        except Exception:
            regions = []
    regions = [r.lower() for r in regions if r]
    outdir_path = Path(outdir) if outdir else OUTDIR

    # fetch durations & swipes per region
    if run_for_date is None:
        raise RuntimeError("duration_report.run_for_date is not available in this environment.")
    try:
        results = run_for_date(target_date, regions, str(outdir_path), city)
    except TypeError:
        try:
            results = run_for_date(target_date, regions, str(outdir_path))
        except Exception:
            results = run_for_date(target_date)

    # combine durations and swipes across regions
    dur_list = []
    swipe_list = []
    for rkey, rr in (results or {}).items():
        try:
            dfdur = rr.get('durations')
            if dfdur is not None and not dfdur.empty:
                dfdur = dfdur.copy()
                dfdur['region'] = rkey
                dur_list.append(dfdur)
        except Exception:
            continue
        try:
            dfsw = rr.get('swipes')
            if dfsw is not None and not dfsw.empty:
                dfcopy = dfsw.copy()
                dfcopy['region'] = rkey
                swipe_list.append(dfcopy)
        except Exception:
            continue

    combined = pd.concat(dur_list, ignore_index=True) if dur_list else pd.DataFrame()
    sw_combined = pd.concat(swipe_list, ignore_index=True) if swipe_list else pd.DataFrame()

    # compute scoring: use available score_trends_from_durations / compute_features pipeline
    trend_df = combined
    try:
        # try using compute_features if swipes available to enrich features
        if not combined.empty:
            # merge features using compute_features if available
            try:
                features_df = compute_features(sw_combined, combined) if (callable(globals().get('compute_features')) and not sw_combined.empty) else None
                if features_df is not None and not features_df.empty:
                    # merge combined durations with features on person_uid & Date
                    merged = pd.merge(features_df, combined, how='left', on=['person_uid', 'Date'], suffixes=('_feat', '_dur'))
                    # prefer feature-derived columns where present
                    trend_df = merged
                else:
                    trend_df = combined
            except Exception:
                trend_df = combined

            # score rows
            trend_df = score_trends_from_durations(trend_df, swipes_df=sw_combined, outdir=str(outdir_path) if outdir_path else None, target_date=target_date)
        else:
            trend_df = combined
    except Exception:
        trend_df = combined

    # compatibility: if caller asked for dict summary
    if as_dict:
        if isinstance(trend_df, pd.DataFrame):
            total_rows = int(len(trend_df))
            flagged_rows = int(trend_df['IsFlagged'].sum()) if 'IsFlagged' in trend_df.columns else 0
            reasons_count = {}
            risk_counts = {}
            try:
                if 'Reasons' in trend_df.columns:
                    for v in trend_df['Reasons'].dropna().astype(str):
                        for part in re.split(r'[;,\|]', v):
                            key = part.strip()
                            if key:
                                reasons_count[key] = reasons_count.get(key, 0) + 1
                if 'RiskLevel' in trend_df.columns:
                    for v in trend_df['RiskLevel'].fillna('').astype(str):
                        if v:
                            risk_counts[v] = risk_counts.get(v, 0) + 1
            except Exception:
                pass
            sample_records = trend_df.head(20).to_dict(orient='records') if not trend_df.empty else []
            return {
                'rows': total_rows,
                'flagged_rows': flagged_rows,
                'aggregated_unique_persons': total_rows,
                'sample': sample_records,
                'reasons_count': reasons_count,
                'risk_counts': risk_counts,
                'files': []
            }
        else:
            return {
                'rows': 0,
                'flagged_rows': 0,
                'aggregated_unique_persons': 0,
                'sample': [],
                'reasons_count': {},
                'risk_counts': {},
                'files': []
            }

    return trend_df



# ---------------- helper wrappers ----------------
def _ensure_date_obj(d):
    if d is None:
        return None
    if isinstance(d, date):
        return d
    if isinstance(d, _datetime):
        return d.date()
    if isinstance(d, str):
        try:
            return _datetime.strptime(d, "%Y-%m-%d").date()
        except Exception:
            try:
                return _datetime.fromisoformat(d).date()
            except Exception:
                raise ValueError(f"Unsupported date string: {d}")
    raise ValueError(f"Unsupported date type: {type(d)}")

def build_monthly_training(start_date=None, end_date=None, outdir: str = None, city: str = 'Pune', as_dict: bool = False):
    od = Path(outdir) if outdir else OUTDIR
    od.mkdir(parents=True, exist_ok=True)
    if start_date is None and end_date is None:
        today = date.today()
        first = date(today.year, today.month, 1)
        last = date(today.year, today.month, calendar.monthrange(today.year, today.month)[1])
    else:
        if start_date is None:
            raise ValueError("start_date must be provided when end_date is provided")
        first = _ensure_date_obj(start_date)
        if end_date is None:
            last = date(first.year, first.month, calendar.monthrange(first.year, first.month)[1])
        else:
            last = _ensure_date_obj(end_date)
    if last < first:
        raise ValueError("end_date must be >= start_date")
    cur = first
    ran = []
    errors = {}
    total_flagged = 0
    total_rows = 0
    while cur <= last:
        try:
            logging.info("build_monthly_training: running for %s (city=%s)", cur.isoformat(), city)
            res = run_trend_for_date(cur, outdir=str(od), city=city, as_dict=as_dict)
            ran.append({'date': cur.isoformat(), 'result': res})
            if isinstance(res, dict):
                total_flagged += int(res.get('flagged_rows', 0) or 0)
                total_rows += int(res.get('rows', 0) or 0)
        except KeyboardInterrupt:
            raise
        except Exception as e:
            logging.exception("build_monthly_training: failed for %s", cur)
            errors[cur.isoformat()] = str(e)
        cur = cur + _timedelta(days=1)
    summary = {
        'start_date': first.isoformat(),
        'end_date': last.isoformat(),
        'dates_attempted': (last - first).days + 1,
        'dates_succeeded': len([r for r in ran if r.get('result') is not None]),
        'dates_failed': len(errors),
        'errors': errors,
        'total_rows': total_rows,
        'total_flagged': total_flagged
    }
    if as_dict:
        return summary
    return ran

def read_90day_cache(outdir: str = None):
    od = Path(outdir) if outdir else OUTDIR
    fp = od / "90day_cache.json"
    if not fp.exists():
        return {}
    try:
        with fp.open("r", encoding="utf8") as fh:
            return json.load(fh)
    except Exception:
        logging.exception("read_90day_cache: failed to read %s", str(fp))
        return {}

if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today, as_dict=False)
    print("Completed; rows:", len(df) if df is not None else 0)
