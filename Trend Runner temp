# backend/app.py
from flask import Flask, jsonify, request, send_from_directory
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re
import io
import base64
import os
import difflib
from typing import Optional, List, Dict, Any
from duration_report import REGION_CONFIG
from datetime import date, timedelta, datetime
from flask import jsonify, request

from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR
from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE

# ---------- Ensure outputs directory exists early (so OVERRIDES_FILE can be defined safely) ----------
BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)

OVERRIDES_FILE = DEFAULT_OUTDIR / "overrides.csv"


def _load_overrides():
    if not OVERRIDES_FILE.exists():
        return {}
    try:
        df = pd.read_csv(OVERRIDES_FILE, dtype=str)
        out = {}
        for _, r in df.iterrows():
            emp = str(r.get('EmployeeID') or r.get('person_uid') or '').strip()
            if not emp:
                continue
            out[emp] = {
                'level': str(r.get('OverrideLevel') or '').strip(),
                'reason': str(r.get('Reason') or '').strip(),
                'ts': str(r.get('Timestamp') or '').strip()
            }
        return out
    except Exception:
        logging.exception("Failed reading overrides file")
        return {}

def _save_override(employee_key, level, reason):
    now = datetime.now().isoformat()
    row = {'EmployeeID': employee_key, 'OverrideLevel': level, 'Reason': reason or '', 'Timestamp': now}
    try:
        if OVERRIDES_FILE.exists():
            df = pd.read_csv(OVERRIDES_FILE, dtype=str)
            # pandas.DataFrame.append is deprecated -> use concat
            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        else:
            df = pd.DataFrame([row])
        df.to_csv(OVERRIDES_FILE, index=False)
        return True
    except Exception:
        logging.exception("Failed to save override")
        return False

def _slug_city(s):
    """
    Convert a city/site string into a safe slug: lowercase, alphanumeric+hyphen
    """
    if not s:
        return ''
    # Remove special chars, spaces to hyphens, lower
    slug = re.sub(r'[^\w\s-]', '', str(s)).strip().lower()
    slug = re.sub(r'[\s_]+', '-', slug)
    return slug


# --- Use REGION_CONFIG servers to talk to ACVSCore (no separate ACVSCORE_DB_CONFIG) ---
# ODBC driver variable is already defined later: ODBC_DRIVER (safe to reference only at runtime)

_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20

def _get_acvscore_conn():
    """
    Try to connect to ACVSCore by reusing credentials from REGION_CONFIG.
    Loops through REGION_CONFIG entries and attempts:
      1) SQL auth (UID/PWD) to database "ACVSCore" using region server + credentials
      2) If SQL auth fails on that server, try Trusted_Connection (Windows auth) as a fallback
    If that fails, optionally attempt ACVSCORE_DB_CONFIG if defined (safe: checked via globals()).
    Returns first successful pyodbc connection or None.
    Implements a short backoff after recent failure to reduce log noise.
    """
    try:
        import pyodbc
    except Exception:
        logging.exception("pyodbc not installed; ACVSCore lookups unavailable.")
        return None

    # basic backoff: skip attempts if we just failed recently
    from datetime import datetime
    now = datetime.now().timestamp()
    last = _acvscore_backoff.get("ts")
    if last and _acvscore_backoff.get("failed") and (now - last) < _ACVSCORE_BACKOFF_SECONDS:
        logging.debug("Skipping ACVSCore connection attempt (backoff active).")
        return None

    # iterate region servers (use the same credentials defined in REGION_CONFIG)
    tried = []
    for region_key, rc in (REGION_CONFIG or {}).items():
        server = rc.get("server")
        user = rc.get("user")
        pwd = rc.get("password")
        if not server:
            continue

        # Try SQL auth first if credentials present
        if user and pwd:
            tried.append(f"{region_key}@{server}(sql)")
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={server};DATABASE=ACVSCore;UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;"
            )
            try:
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (sql auth).", server, region_key)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("SQL auth to %s failed: %s", server, e)

        # Try Trusted Connection fallback on same server
        tried.append(f"{region_key}@{server}(trusted)")
        conn_str_trusted = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={server};DATABASE=ACVSCore;Trusted_Connection=yes;"
            "TrustServerCertificate=Yes;"
        )
        try:
            conn = pyodbc.connect(conn_str_trusted, autocommit=True, timeout=5)
            logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (trusted connection).", server, region_key)
            _acvscore_backoff["ts"] = None
            _acvscore_backoff["failed"] = False
            return conn
        except Exception as e:
            logging.debug("Trusted connection to %s failed: %s", server, e)
            continue

    # Fallback: if a global ACVSCORE_DB_CONFIG exists, try it (safe check)
    try:
        if 'ACVSCORE_DB_CONFIG' in globals() and isinstance(globals().get('ACVSCORE_DB_CONFIG'), dict):
            cfg = globals().get('ACVSCORE_DB_CONFIG')
            server = cfg.get('server')
            user = cfg.get('user')
            pwd = cfg.get('password')
            database = cfg.get('database', 'ACVSCore')
            tried.append(f"ACVSCORE_DB_CONFIG@{server}")
            try:
                conn_str = (
                    f"DRIVER={{{ODBC_DRIVER}}};"
                    f"SERVER={server};DATABASE={database};UID={user};PWD={pwd};"
                    "TrustServerCertificate=Yes;"
                )
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore using ACVSCORE_DB_CONFIG (%s).", server)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("ACVSCORE_DB_CONFIG connection failed: %s", e)
    except Exception:
        # defensive: do not propagate any errors from fallback logic
        logging.debug("ACVSCORE_DB_CONFIG fallback not available or failed.")

    # record failure to backoff
    _acvscore_backoff["ts"] = now
    _acvscore_backoff["failed"] = True
    logging.error("Failed to connect to ACVSCore using REGION_CONFIG servers. Tried: %s", tried)
    return None


# ODBC driver (keep existing env-based driver)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data


def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    """
    Try to resolve personnel record using a flexible lookup.
    Returns dict with keys: ObjectID (may be None), GUID, Name, EmailAddress, ManagerEmail
    If resolution fails returns empty dict.
    """
    out: Dict[str, Any] = {}
    if candidate_identifier is None:
        return out
    conn = _get_acvscore_conn()
    if conn is None:
        return out

    try:
        cur = conn.cursor()
        # We'll attempt to match on multiple columns: ObjectID, GUID, Int1, Text12, Name
        # Use parameters (all strings) — SQL will handle mismatched types.
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        params = (cand, cand, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()
        if row:
            # columns: ObjectID, GUID, Name, EmailAddress, ManagerEmail
            try:
                out['ObjectID'] = row[0]
                out['GUID'] = row[1]
                out['Name'] = row[2]
                out['EmailAddress'] = row[3]
                out['ManagerEmail'] = row[4]
            except Exception:
                # defensive assignment by index
                out = {
                    'ObjectID': row[0] if len(row) > 0 else None,
                    'GUID': row[1] if len(row) > 1 else None,
                    'Name': row[2] if len(row) > 2 else None,
                    'EmailAddress': row[3] if len(row) > 3 else None,
                    'ManagerEmail': row[4] if len(row) > 4 else None
                }
    except Exception:
        logging.exception("Failed personnel lookup for candidate: %s", candidate_identifier)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out

def get_person_image_bytes(parent_id) -> Optional[bytes]:
    """
    Query ACVSCore.Access.Images for top image where ParentId = parent_id and return raw bytes.
    Returns None if not found or on error.
    """
    conn = _get_acvscore_conn()
    if conn is None:
        return None
    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 AI.Image
            FROM ACVSCore.Access.Images AI
            WHERE AI.ParentId = ?
              AND DATALENGTH(AI.Image) > 0
            ORDER BY AI.ObjectID DESC
        """
        cur.execute(sql, (str(parent_id),))
        row = cur.fetchone()
        if row and row[0] is not None:
            # pyodbc returns buffer/bytearray for varbinary; convert to bytes
            try:
                b = bytes(row[0])
                return b
            except Exception:
                # sometimes row[0] is already bytes-like
                return row[0]
    except Exception:
        logging.exception("Failed to fetch image for ParentId=%s", parent_id)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass
    return None

# ---------- New route to serve employee image ----------
# We'll import send_file later where used; define route after app created.

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

# send_file is needed for Excel responses
from flask import send_file
try:
    # optional import; used for styling
    from openpyxl import load_workbook
    from openpyxl.styles import Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        # convert floats NaN handled above
        return x
    try:
        # fallback to string
        return str(x)
    except Exception:
        return None


_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False


# Helper: format seconds to HH:MM:SS
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return None
        # guard against floats and NaN
        s = int(float(seconds))
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return None


# Placeholder tokens (keep consistent with trend_runner expectations)
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


def _resolve_field_from_record(record: dict, candidate_tokens: list):
    """
    Search a single row `record` (dict) for likely columns listed in candidate_tokens.
    Return first non-placeholder value found (converted to Python scalar), else None.
    """
    if record is None:
        return None

    # 1) exact key matches (case-sensitive & common casing)
    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)

    # 2) case-insensitive contains match
    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)

    # 3) xml / value parsing fallback for CardNumber
    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue

    # 4) final fallback: first non-placeholder value
    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)

    return None


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Clean a dataframe for JSON output (convert NaN -> None, pandas types -> native, format datetimes).
    """
    if df is None or df.empty:
        return []
    df = df.copy()

    # remove duplicate suffix columns
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    # Date normalization
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    # Datetime columns to ISO strings
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # Replace NaN/inf -> None
    df = df.where(pd.notnull(df), None)

    # Convert records to safe Python types
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        # Typical fields
        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        # ----- Schema-aware fallback resolution -----
        if not emp_id:
            emp_tokens = ['Int1', 'Text12', 'EmployeeID', 'empid', 'id']
            resolved_emp = _resolve_field_from_record(r, emp_tokens)
            if resolved_emp is not None:
                try:
                    s = str(resolved_emp).strip()
                    # remove trailing .0 for floats
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    if _looks_like_guid(s):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = s
                        emp_id = s
                except Exception:
                    if _looks_like_guid(resolved_emp):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = resolved_emp
                        emp_id = resolved_emp

        # Prefer Credential.CardNumber / CHUID / Card as CardNumber when missing — reject GUIDs/placeholders
        if out.get('CardNumber') in (None, '', 'nan'):
            card_tokens = ['CardNumber', 'CHUID', 'Card', 'card_no', 'cardnum']
            resolved_card = _resolve_field_from_record(r, card_tokens)
            if resolved_card is not None:
                try:
                    cs = str(resolved_card).strip()
                    if _looks_like_guid(cs) or _is_placeholder_str(cs):
                        out['CardNumber'] = None
                    else:
                        out['CardNumber'] = cs
                except Exception:
                    out['CardNumber'] = None

        # final safety: ensure EmployeeID/CardNumber are not GUID-like tokens
        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None
        if 'CardNumber' in out and isinstance(out['CardNumber'], str) and _looks_like_guid(out['CardNumber']):
            out['CardNumber'] = None

        # If EmployeeName empty or looks like a GUID, prefer EmployeeID (human id) over GUIDs
        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            if emp_id not in (None, '', 'nan') and not _looks_like_guid(emp_id):
                out['EmployeeName'] = str(emp_id)
            else:
                out['EmployeeName'] = None

        cleaned.append(out)
    return cleaned

# ----- Helpers added to match commented (Pune) file functionality but multi-city-aware -----

def _replace_placeholder_strings(obj):
    """
    If obj is a DataFrame, replace known placeholder strings with None (NaN).
    If obj is a scalar/string, return None for placeholder strings else return obj.
    """
    if obj is None:
        return obj
    try:
        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
            for col in df.columns:
                try:
                    # Replace placeholder strings (case-insensitive)
                    df[col] = df[col].apply(lambda x: None if _is_placeholder_str(x) else x)
                except Exception:
                    continue
            return df
        else:
            # scalar
            return None if _is_placeholder_str(obj) else obj
    except Exception:
        return obj

def _normalize_id_local(v):
    """
    Normalize an identifier for robust matching/counting:
    - treat NaN/None/empty as None
    - strip and convert float-like integers to integer strings
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == '' or s.lower() == 'nan':
        return None
    try:
        if '.' in s:
            fv = float(s)
            if math.isfinite(fv) and fv.is_integer():
                s = str(int(fv))
    except Exception:
        pass
    return s

def _find_swipe_files(outdir: Path, date_obj: Optional[date] = None, city_slug: Optional[str] = None):
    """
    Find swipe CSV files in outdir matching the date_obj (if provided) and optional city_slug.
    Returns list of Path objects sorted (most recent first).
    This function is intentionally permissive to support various swipe filename conventions.
    """
    p = Path(outdir)
    files_set = set()
    try:
        if date_obj is None:
            # get recent files, prefer city-specific if provided
            if city_slug:
                # try several patterns
                for fp in p.glob(f"*{city_slug}*.csv"):
                    if fp.name.lower().startswith("swipes") or "swipe" in fp.name.lower():
                        files_set.add(fp)
            # fallback to all swipes
            for fp in p.glob("swipes_*.csv"):
                files_set.add(fp)
            for fp in p.glob("swipes*.csv"):
                files_set.add(fp)
        else:
            target = date_obj.strftime("%Y%m%d")
            # patterns trying to capture common naming: swipes_<city>_<YYYYMMDD>.csv, swipes_*_<YYYYMMDD>.csv, swipes_<YYYYMMDD>.csv
            patterns = []
            if city_slug:
                patterns.append(f"swipes_{city_slug}_{target}.csv")
                patterns.append(f"swipes*{city_slug}*_{target}.csv")
                patterns.append(f"*{city_slug}*_{target}.csv")
            patterns.append(f"swipes_*_{target}.csv")
            patterns.append(f"swipes*_{target}.csv")
            patterns.append(f"*_{target}.csv")
            patterns.append(f"swipes_{target}.csv")
            for pat in patterns:
                for fp in p.glob(pat):
                    files_set.add(fp)
    except Exception:
        pass
    files = sorted(list(files_set), reverse=True)
    return files

# -----------------------
# Routes
# -----------------------
@app.route('/')
def root():
    return "Trend Analysis API — Multi-city"

@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.get_json(force=True) or {}
        else:
            try:
                params = request.form.to_dict() or {}
            except Exception:
                params = {}

    date_str = (params.get('date') or params.get('Date') or '').strip() or None
    start_str = (params.get('start') or params.get('Start') or '').strip() or None
    end_str = (params.get('end') or params.get('End') or '').strip() or None

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    regions_param = params.get('regions') or params.get('region') or ''
    if regions_param:
        regions = [r.strip().lower() for r in re.split(r'[;,|]', str(regions_param)) if r.strip()]
    else:
        try:
            regions = [k.lower() for k in list(REGION_CONFIG.keys())]
        except Exception:
            regions = ['apac']

    valid_regions = []
    for r in regions:
        if r in (REGION_CONFIG or {}):
            valid_regions.append(r)
        else:
            logging.debug("Requested region '%s' not in REGION_CONFIG - skipping", r)
    if not valid_regions:
        valid_regions = [k.lower() for k in REGION_CONFIG.keys()] if REGION_CONFIG else ['apac']
    params['_regions_to_run'] = valid_regions

    city_param = params.get('city') or params.get('site') or params.get('site_name') or None
    city_slug = _slug_city(city_param) if city_param else None
    params['_city'] = city_slug

    combined_rows = []
    files = []

    # ---------------------------
    # Run trend for each requested date
    # ---------------------------
    for d in dates:
        try:
            if run_trend_for_date is None:
                raise RuntimeError("run_trend_for_date helper not available in trend_runner")
            try:
                df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug)
            except TypeError:
                try:
                    df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
                except Exception:
                    # Last-resort: try duration_report fallback if available
                    try:
                        from duration_report import run_for_date as _dr_run_for_date
                        region_results = _dr_run_for_date(d, valid_regions, str(DEFAULT_OUTDIR), city_param)
                        combined_list = []
                        for rkey, res in (region_results or {}).items():
                            try:
                                df_dur = res.get('durations')
                                if df_dur is not None and not df_dur.empty:
                                    combined_list.append(df_dur)
                            except Exception:
                                continue
                        df = pd.concat(combined_list, ignore_index=True) if combined_list else pd.DataFrame()
                    except Exception:
                        raise
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_{city_slug}_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        if df is None or (hasattr(df, 'empty') and df.empty):
            continue

        try:
            df = _replace_placeholder_strings(df)
        except Exception:
            pass

        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        combined_rows.append(df)

    # *** Important: combine after loop to avoid UnboundLocalError and extra repeated concat inside loop ***
    try:
        combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()
    except Exception:
        combined_df = pd.DataFrame()

    try:
        if not combined_df.empty:
            if 'person_uid' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['person_uid'].dropna().astype(str).nunique())
            elif 'EmployeeID' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['EmployeeID'].dropna().astype(str).nunique())
            else:
                raw_unique_person_uids = int(len(combined_df))
        else:
            raw_unique_person_uids = 0
    except Exception:
        raw_unique_person_uids = int(len(combined_df)) if combined_df is not None else 0

    try:
        if not combined_df.empty and 'IsFlagged' in combined_df.columns:
            flagged_df = combined_df[combined_df['IsFlagged'] == True].copy()
        else:
            flagged_df = pd.DataFrame()
    except Exception:
        flagged_df = pd.DataFrame()

    try:
        analysis_count = int(raw_unique_person_uids)
    except Exception:
        analysis_count = int(len(combined_df)) if combined_df is not None else 0

    try:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = float((flagged_count / analysis_count * 100.0) if analysis_count and analysis_count > 0 else 0.0)
    except Exception:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = 0.0

    try:
        sample_source = flagged_df if not flagged_df.empty else combined_df
        samples = _clean_sample_df(sample_source.head(10), max_rows=10) if sample_source is not None and not sample_source.empty else []
    except Exception:
        samples = []

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "aggregated_rows_total_raw": int(len(combined_df)),
        "aggregated_unique_persons": int(analysis_count),
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "flagged_rate_percent": float(flagged_rate_pct),
        "files": files,
        "sample": (samples[:10] if isinstance(samples, list) else samples),
        "reasons_count": {},
        "risk_counts": {},
        "flagged_persons": (samples if samples else []),
        "_raw_unique_person_uids": int(raw_unique_person_uids),
        "regions_run": params.get('_regions_to_run', []),
        "city_used": city_slug
    }

    return jsonify(resp)

# ... rest of file unchanged (routes /latest, /record, /employee/<empid>/image, etc.) ...
# I left the rest of the file unchanged except for the combined_df fix above.
# (Full file continued in your codebase.)














# backend/trend_runner.py
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re
import os
import calendar
import json
from collections import defaultdict
from datetime import datetime as _datetime
from datetime import timedelta as _timedelta
from typing import Optional, List

# ------------------ personnel enrichment (add near imports) ------------------
def _get_personnel_funcs_lazy():
    """
    Try to import personnel helpers from app.py at call time (avoids circular imports).
    Returns tuple (get_personnel_info_fn_or_None, get_person_image_bytes_fn_or_None).
    """
    try:
        import importlib
        appmod = importlib.import_module('app')  # adjust module name if your app module name is different
        gpi = getattr(appmod, 'get_personnel_info', None)
        gpib = getattr(appmod, 'get_person_image_bytes', None)
        return gpi, gpib
    except Exception:
        # if the app module isn't importable from this process, return None and continue
        return None, None

# ... (many helpers and definitions unchanged) ...

# ---------------- compute_features (robust merged version) ----------------
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    # unchanged (keeps computing per-person-per-day features from swipes/durations)
    # [preserve your existing compute_features implementation unchanged]
    # ... (full compute_features function here, unchanged) ...
    pass  # placeholder in this snippet view; in file the function is unchanged

# ---------------- SCENARIO WEIGHTS, helpers, scoring functions (unchanged) ----------------
# ... (rest of file up to run_trend_for_date) ...

def run_trend_for_date(target_date: date,
                       regions: Optional[List[str]] = None,
                       outdir: str = None,
                       city: str = "pune",
                       as_dict: bool = False) -> pd.DataFrame:
    """
    Enhanced run_trend_for_date: collects swipes + durations, computes features and scores.
    Mirrors original Pune-focused logic (including 02:00 boundary) for compatibility.
    """
    city_slug = _slug_city(city)
    if regions is None:
        try:
            regions = list(REGION_CONFIG.keys()) if isinstance(REGION_CONFIG, dict) and REGION_CONFIG else []
        except Exception:
            regions = []
    regions = [r.lower() for r in regions if r]
    outdir_path = Path(outdir) if outdir else OUTDIR

    # fetch durations & swipes per region
    if run_for_date is None:
        raise RuntimeError("duration_report.run_for_date is not available in this environment.")
    try:
        results = run_for_date(target_date, regions, str(outdir_path), city)
    except TypeError:
        try:
            results = run_for_date(target_date, regions, str(outdir_path))
        except Exception:
            results = run_for_date(target_date)

    # combine durations and swipes across regions
    dur_list = []
    swipe_list = []
    for rkey, rr in (results or {}).items():
        try:
            dfdur = rr.get('durations')
            if dfdur is not None and not dfdur.empty:
                dfdur = dfdur.copy()
                dfdur['region'] = rkey
                dur_list.append(dfdur)
        except Exception:
            continue
        try:
            dfsw = rr.get('swipes')
            if dfsw is not None and not dfsw.empty:
                dfcopy = dfsw.copy()
                dfcopy['region'] = rkey
                swipe_list.append(dfcopy)
        except Exception:
            continue

    combined = pd.concat(dur_list, ignore_index=True) if dur_list else pd.DataFrame()
    sw_combined = pd.concat(swipe_list, ignore_index=True) if swipe_list else pd.DataFrame()

    # save raw swipes for evidence (full raw) — keep original timestamps in saved raw file
    try:
        if sw_combined is not None and not sw_combined.empty:
            sw_out = Path(outdir_path) / f"swipes_{city_slug}_{target_date.strftime('%Y%m%d')}.csv"
            sw_combined.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception as e:
        logging.warning("Failed to save raw swipes: %s", e)

    # --- Day boundary handling: Pune 02:00 shift (backwards-compatible) ---
    use_pune_2am_boundary = False
    try:
        if city and isinstance(city, str) and 'pun' in city.strip().lower():
            use_pune_2am_boundary = True
        else:
            if os.getenv("PUNE_2AM_BOUNDARY", "0") == "1":
                use_pune_2am_boundary = True
    except Exception:
        use_pune_2am_boundary = False

    sw_for_features = sw_combined.copy() if sw_combined is not None else pd.DataFrame()
    durations_for_features = combined.copy() if combined is not None else pd.DataFrame()

    if use_pune_2am_boundary and (sw_for_features is not None) and (not sw_for_features.empty):
        try:
            if 'LocaleMessageTime' in sw_for_features.columns:
                sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features['LocaleMessageTime'], errors='coerce')
            else:
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                    if cand in sw_for_features.columns:
                        sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features[cand], errors='coerce')
                        break

            # PRESERVE original times for evidence + later matching
            sw_for_features['OriginalLocaleMessageTime'] = sw_for_features['LocaleMessageTime']

            # shift timestamps backward by 2 hours so the 'date' computed from them moves the day boundary to 02:00
            sw_for_features['LocaleMessageTime'] = sw_for_features['LocaleMessageTime'] - pd.Timedelta(hours=2)

            # recompute durations using shifted swipes so FirstSwipe/LastSwipe/Duration match 02:00-day boundary
            try:
                if callable(compute_daily_durations):
                    durations_for_features = compute_daily_durations(sw_for_features)
                else:
                    durations_for_features = combined.copy() if combined is not None else pd.DataFrame()
            except Exception:
                logging.exception("Failed to recompute durations for Pune 2AM boundary; falling back to original durations.")
                durations_for_features = combined.copy() if combined is not None else pd.DataFrame()

            # For debugging/evidence linking: optionally write shifted-version of raw swipes
            try:
                sw_shifted_out = Path(outdir_path) / f"swipes_{city_slug}_{target_date.strftime('%Y%m%d')}_shifted.csv"
                cols_keep = [c for c in sw_for_features.columns if c in ('person_uid','EmployeeID','CardNumber','LocaleMessageTime','OriginalLocaleMessageTime','Door','Direction','PartitionName2')]
                sw_for_features[cols_keep].to_csv(sw_shifted_out, index=False)
            except Exception:
                logging.debug("Could not write shifted swipes evidence file (non-fatal).")
        except Exception:
            logging.exception("Failed preparing shifted swipes for Pune 2AM logic; using original swipes/durations.")
            sw_for_features = sw_combined.copy() if sw_combined is not None else pd.DataFrame()
            durations_for_features = combined.copy() if combined is not None else pd.DataFrame()

    # compute features using possibly-shifted data (so grouping uses 02:00 boundary for Pune)
    features = compute_features(sw_for_features, durations_for_features)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        if as_dict:
            return {'rows': 0, 'flagged_rows': 0, 'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': 0}
        return pd.DataFrame()

    # Provide an explicit DisplayDate/AdjustedDate for frontend:
    try:
        if 'FirstSwipe' in features.columns:
            features['DisplayDate'] = pd.to_datetime(features['FirstSwipe'], errors='coerce').dt.date
        else:
            features['DisplayDate'] = features['Date']
    except Exception:
        features['DisplayDate'] = features.get('Date', None)

    # ----------------------------
    # NEW: Recompute per-row metrics (gaps/duration/counts) from raw swipes AFTER DisplayDate fixed
    # This removes spurious long gaps created when swipes were split across the 02:00 boundary.
    # ----------------------------
    try:
        if sw_combined is not None and not sw_combined.empty:
            # ensure LocaleMessageTime exists and is parsed
            if 'LocaleMessageTime' not in sw_combined.columns:
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                    if cand in sw_combined.columns:
                        sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined[cand], errors='coerce')
                        break
            else:
                sw_combined['LocaleMessageTime'] = pd.to_datetime(sw_combined['LocaleMessageTime'], errors='coerce')

            # compute display-date key on raw swipes consistent with features DisplayDate
            if use_pune_2am_boundary:
                # For mapping to DisplayDate we must mimic the shifted grouping: adjusted = LocaleMessageTime - 2 hours
                sw_combined['DisplayDateKey'] = (sw_combined['LocaleMessageTime'] - pd.Timedelta(hours=2)).dt.date
            else:
                sw_combined['DisplayDateKey'] = sw_combined['LocaleMessageTime'].dt.date

            # helper to compute metrics per (person_uid, DisplayDateKey)
            def _agg_metrics(g):
                times = list(pd.to_datetime(g['LocaleMessageTime'].dropna()))
                times_sorted = sorted(times)
                count_swipes = len(times_sorted)
                max_gap = 0
                short_gap_count = 0
                if len(times_sorted) >= 2:
                    gaps = []
                    for i in range(1, len(times_sorted)):
                        s = (times_sorted[i] - times_sorted[i-1]).total_seconds()
                        gaps.append(s)
                        if s <= 5*60:
                            short_gap_count += 1
                    max_gap = int(max(gaps)) if gaps else 0
                first_ts = times_sorted[0] if times_sorted else None
                last_ts = times_sorted[-1] if times_sorted else None
                # doors/locations
                unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
                unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
                # pick first non-guid CardNumber/EmployeeID/EmployeeName
                def _pick_non_guid(colname):
                    if colname in g.columns:
                        for v in pd.unique(g[colname].dropna().astype(str).map(lambda x: x.strip())):
                            if v and (not _GUID_RE.match(v)) and v.lower() not in _PLACEHOLDER_STRS:
                                return v
                    return None
                card = _pick_non_guid('CardNumber') or _pick_non_guid('card') or _pick_non_guid('XmlShredValue') or None
                empid = _pick_non_guid('EmployeeID') or _pick_non_guid('Int1') or _pick_non_guid('Text12') or None
                empname = _pick_non_guid('EmployeeName') or _pick_non_guid('ObjectName1') or None
                duration_sec = 0.0
                if first_ts is not None and last_ts is not None:
                    try:
                        duration_sec = float((pd.to_datetime(last_ts) - pd.to_datetime(first_ts)).total_seconds())
                        if duration_sec < 0:
                            duration_sec = 0.0
                    except Exception:
                        duration_sec = 0.0
                return pd.Series({
                    'FirstSwipe_raw': first_ts,
                    'LastSwipe_raw': last_ts,
                    'CountSwipes_raw': int(count_swipes),
                    'DurationSeconds_raw': float(duration_sec),
                    'DurationMinutes_raw': float(duration_sec/60.0),
                    'MaxSwipeGapSeconds_raw': int(max_gap),
                    'ShortGapCount_raw': int(short_gap_count),
                    'UniqueDoors_raw': int(unique_doors),
                    'UniqueLocations_raw': int(unique_locations),
                    'CardNumber_raw': card,
                    'EmployeeID_raw': empid,
                    'EmployeeName_raw': empname
                })

            # group and compute metrics
            grouped_raw = sw_combined.dropna(subset=['person_uid', 'DisplayDateKey'], how='any').groupby(['person_uid', 'DisplayDateKey'])
            if not grouped_raw.ngroups:
                raw_metrics_df = pd.DataFrame(columns=[
                    'person_uid','DisplayDateKey','FirstSwipe_raw','LastSwipe_raw','CountSwipes_raw','DurationSeconds_raw',
                    'DurationMinutes_raw','MaxSwipeGapSeconds_raw','ShortGapCount_raw','UniqueDoors_raw','UniqueLocations_raw',
                    'CardNumber_raw','EmployeeID_raw','EmployeeName_raw'
                ])
            else:
                raw_metrics_df = grouped_raw.apply(_agg_metrics).reset_index()
                # rename DisplayDateKey column to match features
                raw_metrics_df.rename(columns={'DisplayDateKey':'DisplayDate'}, inplace=True)

            # merge metrics back into features keyed by person_uid + DisplayDate
            try:
                # ensure DisplayDate in features is of date type (or comparable)
                features['_DisplayDate_for_merge'] = pd.to_datetime(features['DisplayDate'], errors='coerce').dt.date
                raw_metrics_df['_DisplayDate_for_merge'] = pd.to_datetime(raw_metrics_df['DisplayDate'], errors='coerce').dt.date
                merged_metrics = pd.merge(features, raw_metrics_df, how='left',
                                          left_on=['person_uid','_DisplayDate_for_merge'],
                                          right_on=['person_uid','_DisplayDate_for_merge'],
                                          suffixes=('','_rawagg'))
                # Where raw metrics exist, overwrite feature columns (keeps existing fields otherwise)
                for base_col, raw_col in [
                    ('FirstSwipe','FirstSwipe_raw'),
                    ('LastSwipe','LastSwipe_raw'),
                    ('CountSwipes','CountSwipes_raw'),
                    ('DurationSeconds','DurationSeconds_raw'),
                    ('DurationMinutes','DurationMinutes_raw'),
                    ('MaxSwipeGapSeconds','MaxSwipeGapSeconds_raw'),
                    ('ShortGapCount','ShortGapCount_raw'),
                    ('UniqueDoors','UniqueDoors_raw'),
                    ('UniqueLocations','UniqueLocations_raw'),
                    ('CardNumber','CardNumber_raw'),
                    ('EmployeeID','EmployeeID_raw'),
                    ('EmployeeName','EmployeeName_raw')
                ]:
                    if raw_col in merged_metrics.columns:
                        try:
                            # prefer raw (non-null) value from raw metrics
                            merged_metrics[base_col] = merged_metrics[raw_col].combine_first(merged_metrics.get(base_col))
                        except Exception:
                            # fallback assignment
                            merged_metrics[base_col] = merged_metrics.get(base_col)
                # cleanup and set features to merged_metrics (only columns we need)
                # preserve all original feature columns and override those updated above
                feature_cols = list(features.columns)
                features = merged_metrics[feature_cols].copy()
                # drop helper
                if '_DisplayDate_for_merge' in features.columns:
                    features.drop(columns=['_DisplayDate_for_merge'], inplace=True, errors='ignore')
            except Exception:
                logging.exception("Failed merge of raw-swipe metrics into features (non-fatal)")
    except Exception:
        logging.exception("Failed recomputing per-row metrics from raw swipes (non-fatal)")

    # If we used a shifted timeline for grouping (Pune), restore FirstSwipe/LastSwipe displayed times by adding 2 hours
    if use_pune_2am_boundary:
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in features.columns:
                try:
                    features[dtcol] = pd.to_datetime(features[dtcol], errors='coerce') + pd.Timedelta(hours=2)
                except Exception:
                    def _add2h_safe(v):
                        try:
                            t = pd.to_datetime(v, errors='coerce')
                            if pd.isna(t):
                                return v
                            return t + pd.Timedelta(hours=2)
                        except Exception:
                            return v
                    features[dtcol] = features[dtcol].apply(_add2h_safe)

    # Merge features with durations (prefer features where present)
    try:
        merged = pd.merge(features, combined, how='left', on=['person_uid', 'Date'], suffixes=('_feat', '_dur'))
    except Exception:
        merged = features

    # Score and enrich merged
    trend_df = score_trends_from_durations(merged, swipes_df=sw_combined, outdir=str(outdir_path), target_date=target_date)

    # write CSV with native types (Pune-compatible filename)
    try:
        write_df = trend_df.copy()
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        write_df = write_df.where(pd.notnull(write_df), None)
        out_csv = Path(outdir_path) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception:
        logging.exception("Failed to write trend CSV")

    # Format DisplayDate (ISO) for output if present
    try:
        if 'DisplayDate' in trend_df.columns:
            trend_df['DisplayDate'] = pd.to_datetime(trend_df['DisplayDate'], errors='coerce').dt.date
            trend_df['DisplayDate'] = trend_df['DisplayDate'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
    except Exception:
        pass

    # Return dict-compatible summary if requested
    if as_dict:
        try:
            rec_df = trend_df.copy()
            for dtcol in ('FirstSwipe', 'LastSwipe'):
                if dtcol in rec_df.columns:
                    rec_df[dtcol] = pd.to_datetime(rec_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
            if 'Date' in rec_df.columns:
                try:
                    rec_df['Date'] = pd.to_datetime(rec_df['Date'], errors='coerce').dt.date
                    rec_df['Date'] = rec_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
                except Exception:
                    pass
            rec_df = rec_df.where(pd.notnull(rec_df), None)
            total_rows = int(len(rec_df))
            flagged_rows = int(rec_df['IsFlagged'].sum()) if 'IsFlagged' in rec_df.columns else 0
            reasons_count = {}
            risk_counts = {}
            try:
                if 'Reasons' in rec_df.columns:
                    for v in rec_df['Reasons'].dropna().astype(str):
                        for part in re.split(r'[;,\|]', v):
                            key = part.strip()
                            if key:
                                reasons_count[key] = reasons_count.get(key, 0) + 1
                if 'RiskLevel' in rec_df.columns:
                    for v in rec_df['RiskLevel'].fillna('').astype(str):
                        if v:
                            risk_counts[v] = risk_counts.get(v, 0) + 1
            except Exception:
                pass
            sample_records = rec_df.head(20).to_dict(orient='records') if not rec_df.empty else []
            return {
                'rows': total_rows,
                'flagged_rows': flagged_rows,
                'aggregated_unique_persons': total_rows,
                'sample': sample_records,
                'reasons_count': reasons_count,
                'risk_counts': risk_counts,
                'files': []
            }
        except Exception:
            logging.exception("Failed to build dict output for run_trend_for_date")
            return {'rows': len(trend_df), 'flagged_rows': int(trend_df['IsFlagged'].sum() if 'IsFlagged' in trend_df.columns else 0),
                    'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': len(trend_df)}

    return trend_df









    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined if joined else None















check below 3 file carefully and fix our 3 issue 
# backend/app.py
from flask import Flask, jsonify, request, send_from_directory
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re
import io
import base64
import os
import difflib
from typing import Optional, List, Dict, Any
from duration_report import REGION_CONFIG
from datetime import date, timedelta, datetime
from flask import jsonify, request

from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR
from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE

# ---------- Ensure outputs directory exists early (so OVERRIDES_FILE can be defined safely) ----------
BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)

OVERRIDES_FILE = DEFAULT_OUTDIR / "overrides.csv"


def _load_overrides():
    if not OVERRIDES_FILE.exists():
        return {}
    try:
        df = pd.read_csv(OVERRIDES_FILE, dtype=str)
        out = {}
        for _, r in df.iterrows():
            emp = str(r.get('EmployeeID') or r.get('person_uid') or '').strip()
            if not emp:
                continue
            out[emp] = {
                'level': str(r.get('OverrideLevel') or '').strip(),
                'reason': str(r.get('Reason') or '').strip(),
                'ts': str(r.get('Timestamp') or '').strip()
            }
        return out
    except Exception:
        logging.exception("Failed reading overrides file")
        return {}

def _save_override(employee_key, level, reason):
    now = datetime.now().isoformat()
    row = {'EmployeeID': employee_key, 'OverrideLevel': level, 'Reason': reason or '', 'Timestamp': now}
    try:
        if OVERRIDES_FILE.exists():
            df = pd.read_csv(OVERRIDES_FILE, dtype=str)
            # pandas.DataFrame.append is deprecated -> use concat
            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        else:
            df = pd.DataFrame([row])
        df.to_csv(OVERRIDES_FILE, index=False)
        return True
    except Exception:
        logging.exception("Failed to save override")
        return False

def _slug_city(s):
    """
    Convert a city/site string into a safe slug: lowercase, alphanumeric+hyphen
    """
    if not s:
        return ''
    # Remove special chars, spaces to hyphens, lower
    slug = re.sub(r'[^\w\s-]', '', str(s)).strip().lower()
    slug = re.sub(r'[\s_]+', '-', slug)
    return slug


# --- Use REGION_CONFIG servers to talk to ACVSCore (no separate ACVSCORE_DB_CONFIG) ---
# ODBC driver variable is already defined later: ODBC_DRIVER (safe to reference only at runtime)

_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20

def _get_acvscore_conn():
    """
    Try to connect to ACVSCore by reusing credentials from REGION_CONFIG.
    Loops through REGION_CONFIG entries and attempts:
      1) SQL auth (UID/PWD) to database "ACVSCore" using region server + credentials
      2) If SQL auth fails on that server, try Trusted_Connection (Windows auth) as a fallback
    If that fails, optionally attempt ACVSCORE_DB_CONFIG if defined (safe: checked via globals()).
    Returns first successful pyodbc connection or None.
    Implements a short backoff after recent failure to reduce log noise.
    """
    try:
        import pyodbc
    except Exception:
        logging.exception("pyodbc not installed; ACVSCore lookups unavailable.")
        return None

    # basic backoff: skip attempts if we just failed recently
    from datetime import datetime
    now = datetime.now().timestamp()
    last = _acvscore_backoff.get("ts")
    if last and _acvscore_backoff.get("failed") and (now - last) < _ACVSCORE_BACKOFF_SECONDS:
        logging.debug("Skipping ACVSCore connection attempt (backoff active).")
        return None

    # iterate region servers (use the same credentials defined in REGION_CONFIG)
    tried = []
    for region_key, rc in (REGION_CONFIG or {}).items():
        server = rc.get("server")
        user = rc.get("user")
        pwd = rc.get("password")
        if not server:
            continue

        # Try SQL auth first if credentials present
        if user and pwd:
            tried.append(f"{region_key}@{server}(sql)")
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={server};DATABASE=ACVSCore;UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;"
            )
            try:
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (sql auth).", server, region_key)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("SQL auth to %s failed: %s", server, e)

        # Try Trusted Connection fallback on same server
        tried.append(f"{region_key}@{server}(trusted)")
        conn_str_trusted = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={server};DATABASE=ACVSCore;Trusted_Connection=yes;"
            "TrustServerCertificate=Yes;"
        )
        try:
            conn = pyodbc.connect(conn_str_trusted, autocommit=True, timeout=5)
            logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (trusted connection).", server, region_key)
            _acvscore_backoff["ts"] = None
            _acvscore_backoff["failed"] = False
            return conn
        except Exception as e:
            logging.debug("Trusted connection to %s failed: %s", server, e)
            continue

    # Fallback: if a global ACVSCORE_DB_CONFIG exists, try it (safe check)
    try:
        if 'ACVSCORE_DB_CONFIG' in globals() and isinstance(globals().get('ACVSCORE_DB_CONFIG'), dict):
            cfg = globals().get('ACVSCORE_DB_CONFIG')
            server = cfg.get('server')
            user = cfg.get('user')
            pwd = cfg.get('password')
            database = cfg.get('database', 'ACVSCore')
            tried.append(f"ACVSCORE_DB_CONFIG@{server}")
            try:
                conn_str = (
                    f"DRIVER={{{ODBC_DRIVER}}};"
                    f"SERVER={server};DATABASE={database};UID={user};PWD={pwd};"
                    "TrustServerCertificate=Yes;"
                )
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore using ACVSCORE_DB_CONFIG (%s).", server)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("ACVSCORE_DB_CONFIG connection failed: %s", e)
    except Exception:
        # defensive: do not propagate any errors from fallback logic
        logging.debug("ACVSCORE_DB_CONFIG fallback not available or failed.")

    # record failure to backoff
    _acvscore_backoff["ts"] = now
    _acvscore_backoff["failed"] = True
    logging.error("Failed to connect to ACVSCore using REGION_CONFIG servers. Tried: %s", tried)
    return None


# ODBC driver (keep existing env-based driver)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data


def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    """
    Try to resolve personnel record using a flexible lookup.
    Returns dict with keys: ObjectID (may be None), GUID, Name, EmailAddress, ManagerEmail
    If resolution fails returns empty dict.
    """
    out: Dict[str, Any] = {}
    if candidate_identifier is None:
        return out
    conn = _get_acvscore_conn()
    if conn is None:
        return out

    try:
        cur = conn.cursor()
        # We'll attempt to match on multiple columns: ObjectID, GUID, Int1, Text12, Name
        # Use parameters (all strings) — SQL will handle mismatched types.
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        params = (cand, cand, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()
        if row:
            # columns: ObjectID, GUID, Name, EmailAddress, ManagerEmail
            try:
                out['ObjectID'] = row[0]
                out['GUID'] = row[1]
                out['Name'] = row[2]
                out['EmailAddress'] = row[3]
                out['ManagerEmail'] = row[4]
            except Exception:
                # defensive assignment by index
                out = {
                    'ObjectID': row[0] if len(row) > 0 else None,
                    'GUID': row[1] if len(row) > 1 else None,
                    'Name': row[2] if len(row) > 2 else None,
                    'EmailAddress': row[3] if len(row) > 3 else None,
                    'ManagerEmail': row[4] if len(row) > 4 else None
                }
    except Exception:
        logging.exception("Failed personnel lookup for candidate: %s", candidate_identifier)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out

def get_person_image_bytes(parent_id) -> Optional[bytes]:
    """
    Query ACVSCore.Access.Images for top image where ParentId = parent_id and return raw bytes.
    Returns None if not found or on error.
    """
    conn = _get_acvscore_conn()
    if conn is None:
        return None
    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 AI.Image
            FROM ACVSCore.Access.Images AI
            WHERE AI.ParentId = ?
              AND DATALENGTH(AI.Image) > 0
            ORDER BY AI.ObjectID DESC
        """
        cur.execute(sql, (str(parent_id),))
        row = cur.fetchone()
        if row and row[0] is not None:
            # pyodbc returns buffer/bytearray for varbinary; convert to bytes
            try:
                b = bytes(row[0])
                return b
            except Exception:
                # sometimes row[0] is already bytes-like
                return row[0]
    except Exception:
        logging.exception("Failed to fetch image for ParentId=%s", parent_id)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass
    return None

# ---------- New route to serve employee image ----------
# We'll import send_file later where used; define route after app created.

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

# send_file is needed for Excel responses
from flask import send_file
try:
    # optional import; used for styling
    from openpyxl import load_workbook
    from openpyxl.styles import Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        # convert floats NaN handled above
        return x
    try:
        # fallback to string
        return str(x)
    except Exception:
        return None


_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False


# Helper: format seconds to HH:MM:SS
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return None
        # guard against floats and NaN
        s = int(float(seconds))
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return None


# Placeholder tokens (keep consistent with trend_runner expectations)
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


def _resolve_field_from_record(record: dict, candidate_tokens: list):
    """
    Search a single row `record` (dict) for likely columns listed in candidate_tokens.
    Return first non-placeholder value found (converted to Python scalar), else None.
    """
    if record is None:
        return None

    # 1) exact key matches (case-sensitive & common casing)
    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)

    # 2) case-insensitive contains match
    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)

    # 3) xml / value parsing fallback for CardNumber
    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue

    # 4) final fallback: first non-placeholder value
    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)

    return None


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Clean a dataframe for JSON output (convert NaN -> None, pandas types -> native, format datetimes).
    """
    if df is None or df.empty:
        return []
    df = df.copy()

    # remove duplicate suffix columns
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    # Date normalization
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    # Datetime columns to ISO strings
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # Replace NaN/inf -> None
    df = df.where(pd.notnull(df), None)

    # Convert records to safe Python types
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        # Typical fields
        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        # ----- Schema-aware fallback resolution -----
        if not emp_id:
            emp_tokens = ['Int1', 'Text12', 'EmployeeID', 'empid', 'id']
            resolved_emp = _resolve_field_from_record(r, emp_tokens)
            if resolved_emp is not None:
                try:
                    s = str(resolved_emp).strip()
                    # remove trailing .0 for floats
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    if _looks_like_guid(s):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = s
                        emp_id = s
                except Exception:
                    if _looks_like_guid(resolved_emp):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = resolved_emp
                        emp_id = resolved_emp

        # Prefer Credential.CardNumber / CHUID / Card as CardNumber when missing — reject GUIDs/placeholders
        if out.get('CardNumber') in (None, '', 'nan'):
            card_tokens = ['CardNumber', 'CHUID', 'Card', 'card_no', 'cardnum']
            resolved_card = _resolve_field_from_record(r, card_tokens)
            if resolved_card is not None:
                try:
                    cs = str(resolved_card).strip()
                    if _looks_like_guid(cs) or _is_placeholder_str(cs):
                        out['CardNumber'] = None
                    else:
                        out['CardNumber'] = cs
                except Exception:
                    out['CardNumber'] = None

        # final safety: ensure EmployeeID/CardNumber are not GUID-like tokens
        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None
        if 'CardNumber' in out and isinstance(out['CardNumber'], str) and _looks_like_guid(out['CardNumber']):
            out['CardNumber'] = None

        # If EmployeeName empty or looks like a GUID, prefer EmployeeID (human id) over GUIDs
        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            if emp_id not in (None, '', 'nan') and not _looks_like_guid(emp_id):
                out['EmployeeName'] = str(emp_id)
            else:
                out['EmployeeName'] = None

        cleaned.append(out)
    return cleaned

# ----- Helpers added to match commented (Pune) file functionality but multi-city-aware -----

def _replace_placeholder_strings(obj):
    """
    If obj is a DataFrame, replace known placeholder strings with None (NaN).
    If obj is a scalar/string, return None for placeholder strings else return obj.
    """
    if obj is None:
        return obj
    try:
        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
            for col in df.columns:
                try:
                    # Replace placeholder strings (case-insensitive)
                    df[col] = df[col].apply(lambda x: None if _is_placeholder_str(x) else x)
                except Exception:
                    continue
            return df
        else:
            # scalar
            return None if _is_placeholder_str(obj) else obj
    except Exception:
        return obj

def _normalize_id_local(v):
    """
    Normalize an identifier for robust matching/counting:
    - treat NaN/None/empty as None
    - strip and convert float-like integers to integer strings
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == '' or s.lower() == 'nan':
        return None
    try:
        if '.' in s:
            fv = float(s)
            if math.isfinite(fv) and fv.is_integer():
                s = str(int(fv))
    except Exception:
        pass
    return s

def _find_swipe_files(outdir: Path, date_obj: Optional[date] = None, city_slug: Optional[str] = None):
    """
    Find swipe CSV files in outdir matching the date_obj (if provided) and optional city_slug.
    Returns list of Path objects sorted (most recent first).
    This function is intentionally permissive to support various swipe filename conventions.
    """
    p = Path(outdir)
    files_set = set()
    try:
        if date_obj is None:
            # get recent files, prefer city-specific if provided
            if city_slug:
                # try several patterns
                for fp in p.glob(f"*{city_slug}*.csv"):
                    if fp.name.lower().startswith("swipes") or "swipe" in fp.name.lower():
                        files_set.add(fp)
            # fallback to all swipes
            for fp in p.glob("swipes_*.csv"):
                files_set.add(fp)
            for fp in p.glob("swipes*.csv"):
                files_set.add(fp)
        else:
            target = date_obj.strftime("%Y%m%d")
            # patterns trying to capture common naming: swipes_<city>_<YYYYMMDD>.csv, swipes_*_<YYYYMMDD>.csv, swipes_<YYYYMMDD>.csv
            patterns = []
            if city_slug:
                patterns.append(f"swipes_{city_slug}_{target}.csv")
                patterns.append(f"swipes*{city_slug}*_{target}.csv")
                patterns.append(f"*{city_slug}*_{target}.csv")
            patterns.append(f"swipes_*_{target}.csv")
            patterns.append(f"swipes*_{target}.csv")
            patterns.append(f"*_{target}.csv")
            patterns.append(f"swipes_{target}.csv")
            for pat in patterns:
                for fp in p.glob(pat):
                    files_set.add(fp)
    except Exception:
        pass
    files = sorted(list(files_set), reverse=True)
    return files

# -----------------------
# Routes
# -----------------------
@app.route('/')
def root():
    return "Trend Analysis API — Multi-city"

@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.get_json(force=True) or {}
        else:
            try:
                params = request.form.to_dict() or {}
            except Exception:
                params = {}

    date_str = (params.get('date') or params.get('Date') or '').strip() or None
    start_str = (params.get('start') or params.get('Start') or '').strip() or None
    end_str = (params.get('end') or params.get('End') or '').strip() or None

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    regions_param = params.get('regions') or params.get('region') or ''
    if regions_param:
        regions = [r.strip().lower() for r in re.split(r'[;,|]', str(regions_param)) if r.strip()]
    else:
        try:
            regions = [k.lower() for k in list(REGION_CONFIG.keys())]
        except Exception:
            regions = ['apac']

    valid_regions = []
    for r in regions:
        if r in (REGION_CONFIG or {}):
            valid_regions.append(r)
        else:
            logging.debug("Requested region '%s' not in REGION_CONFIG - skipping", r)
    if not valid_regions:
        valid_regions = [k.lower() for k in REGION_CONFIG.keys()] if REGION_CONFIG else ['apac']
    params['_regions_to_run'] = valid_regions

    city_param = params.get('city') or params.get('site') or params.get('site_name') or None
    city_slug = _slug_city(city_param) if city_param else None
    params['_city'] = city_slug

    combined_rows = []
    files = []

    for d in dates:
        try:
            if run_trend_for_date is None:
                raise RuntimeError("run_trend_for_date helper not available in trend_runner")
            try:
                df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug)
            except TypeError:
                try:
                    df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
                except Exception:
                    # Last-resort: try duration_report fallback if available
                    try:
                        from duration_report import run_for_date as _dr_run_for_date
                        region_results = _dr_run_for_date(d, valid_regions, str(DEFAULT_OUTDIR), city_param)
                        combined_list = []
                        for rkey, res in (region_results or {}).items():
                            try:
                                df_dur = res.get('durations')
                                if df_dur is not None and not df_dur.empty:
                                    combined_list.append(df_dur)
                            except Exception:
                                continue
                        df = pd.concat(combined_list, ignore_index=True) if combined_list else pd.DataFrame()
                    except Exception:
                        raise
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_{city_slug}_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        if df is None or (hasattr(df, 'empty') and df.empty):
            continue

        try:
            df = _replace_placeholder_strings(df)
        except Exception:
            pass

        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        combined_rows.append(df)
        combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()


    
   

    try:
        if not combined_df.empty:
            if 'person_uid' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['person_uid'].dropna().astype(str).nunique())
            elif 'EmployeeID' in combined_df.columns:
                raw_unique_person_uids = int(combined_df['EmployeeID'].dropna().astype(str).nunique())
            else:
                raw_unique_person_uids = int(len(combined_df))
        else:
            raw_unique_person_uids = 0
    except Exception:
        raw_unique_person_uids = int(len(combined_df)) if combined_df is not None else 0

    try:
        if not combined_df.empty and 'IsFlagged' in combined_df.columns:
            flagged_df = combined_df[combined_df['IsFlagged'] == True].copy()
        else:
            flagged_df = pd.DataFrame()
    except Exception:
        flagged_df = pd.DataFrame()

    try:
        analysis_count = int(raw_unique_person_uids)
    except Exception:
        analysis_count = int(len(combined_df)) if combined_df is not None else 0

    try:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = float((flagged_count / analysis_count * 100.0) if analysis_count and analysis_count > 0 else 0.0)
    except Exception:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = 0.0

    try:
        sample_source = flagged_df if not flagged_df.empty else combined_df
        samples = _clean_sample_df(sample_source.head(10), max_rows=10) if sample_source is not None and not sample_source.empty else []
    except Exception:
        samples = []

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "aggregated_rows_total_raw": int(len(combined_df)),
        "aggregated_unique_persons": int(analysis_count),
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "flagged_rate_percent": float(flagged_rate_pct),
        "files": files,
        "sample": (samples[:10] if isinstance(samples, list) else samples),
        "reasons_count": {},
        "risk_counts": {},
        "flagged_persons": (samples if samples else []),
        "_raw_unique_person_uids": int(raw_unique_person_uids),
        "regions_run": params.get('_regions_to_run', []),
        "city_used": city_slug
    }

    return jsonify(resp)

@app.route('/latest', methods=['GET'])
def latest_results():
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]

    start_date_iso = None
    end_date_iso = None
    try:
        m = re.search(r'(\d{8})', latest.name)
        if m:
            ymd = m.group(1)
            dt = datetime.strptime(ymd, "%Y%m%d").date()
            start_date_iso = dt.isoformat()
            end_date_iso = dt.isoformat()
    except Exception:
        start_date_iso = None
        end_date_iso = None

    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)

    df = _replace_placeholder_strings(df)

    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in df.columns), None)

    def _norm_val_for_latest(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                fv = float(s)
                if math.isfinite(fv) and fv.is_integer():
                    s = str(int(fv))
        except Exception:
            pass
        return s

    if id_col is None:
        unique_persons = int(len(df))
    else:
        ids_series = df[id_col].apply(_norm_val_for_latest) if id_col in df.columns else pd.Series([None]*len(df))
        if id_col != 'person_uid' and 'person_uid' in df.columns:
            ids_series = ids_series.fillna(df['person_uid'].astype(str).replace('nan','').replace('None',''))
        unique_persons = int(len(set([x for x in ids_series.unique() if x])))

    # build initial sample (list of dicts)
    sample = _clean_sample_df(df, max_rows=5)  # returns list

    # --- Enrich sample rows with EmployeeEmail and imageUrl (best-effort) ---
    
# --- Enrich sample rows with EmployeeEmail and imageUrl (best-effort) ---
    try:
        if isinstance(sample, list) and sample:
            for s in sample:
                try:
                    # prefer EmployeeID / person_uid / EmployeeName as lookup token
                    lookup_token = (
                        s.get('EmployeeID')
                        or s.get('person_uid')
                        or s.get('EmployeeName')
                        or s.get('EmployeeIdentity')
                    )
                    pi = {}
                    if lookup_token:
                        try:
                            pi = get_personnel_info(lookup_token) or {}
                        except Exception:
                            pi = {}

                    # set email if found from personnel info
                    if pi:
                        email = (
                            pi.get('EmailAddress')
                            or pi.get('EmployeeEmail')
                            or pi.get('Email')
                            or None
                        )
                        if email:
                            s['EmployeeEmail'] = email

                        # prefer ObjectID then GUID for image parent
                        objid = pi.get('ObjectID') or pi.get('GUID') or None
                        if objid:
                            try:
                                base = (request.url_root or request.host_url).rstrip('/')
                            except Exception:
                                base = ''
                            s['imageUrl'] = f"/employee/{objid}/image"

                            # best-effort flag whether image exists (may be False if DB missing)
                            try:
                                b = get_person_image_bytes(objid)
                                if not b:
                                    try:
                                        b = get_person_image_bytes(f"emp:{objid}")
                                    except Exception:
                                        b = None
                                s['HasImage'] = True if b else False
                            except Exception:
                                s['HasImage'] = False

                    # fallback: if still no email, try to find the matching row
                    if not s.get('EmployeeEmail'):
                        try:
                            match_mask = pd.Series(False, index=df.index)
                            if s.get('person_uid') and 'person_uid' in df.columns:
                                match_mask |= df['person_uid'].astype(str).str.strip() == str(s.get('person_uid')).strip()
                            if s.get('EmployeeID') and 'EmployeeID' in df.columns:
                                match_mask |= df['EmployeeID'].astype(str).str.strip() == str(s.get('EmployeeID')).strip()
                            if (
                                not match_mask.any()
                                and s.get('EmployeeName')
                                and 'EmployeeName' in df.columns
                            ):
                                match_mask |= (
                                    df['EmployeeName'].astype(str)
                                    .str.strip()
                                    .str.lower()
                                    == str(s.get('EmployeeName')).strip().lower()
                                )

                            if match_mask.any():
                                idx = df[match_mask].index[0]
                                for col in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                                    if col in df.columns:
                                        val = df.at[idx, col]
                                        if val not in (None, '', 'nan'):
                                            s['EmployeeEmail'] = val
                                            break
                            else:
                                for col in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                                    if col in df.columns:
                                        try:
                                            non_null = df[col].dropna().astype(str).map(lambda x: x.strip())
                                            non_null = non_null[non_null != '']
                                            if len(non_null):
                                                s['EmployeeEmail'] = non_null.iloc[0]
                                                break
                                        except Exception:
                                            continue
                        except Exception:
                            pass

                    # ✅ This must be *inside* the try block
                    # always provide a relative path to avoid duplicated host prefixes
                    if not s.get('imageUrl') and s.get('EmployeeID'):
                        s['imageUrl'] = f"/employee/{s.get('EmployeeID')}/image"

                except Exception:
                    # ensure that an exception for a single sample doesn't break everything
                    continue
    except Exception:
        # defensive: if enrichment fails entirely, continue with original samples
        pass


    resp = {
        
        "file": latest.name,
        "rows_raw": int(len(df)),
        "rows": unique_persons,
        "sample": sample,
        "start_date": start_date_iso,
        "end_date": end_date_iso,
        "city": city_slug
    }
    return jsonify(resp)


@app.route('/record', methods=['GET'])
def get_record():
    q = request.args.get('employee_id') or request.args.get('person_uid')
    include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)
    

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    df_list = []
    for fp in csvs:
        try:
            tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
        except Exception:
            try:
                tmp = pd.read_csv(fp, dtype=str)
                if 'Date' in tmp.columns:
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
            except Exception:
                continue
        df_list.append(tmp)
    if not df_list:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200
    df = pd.concat(df_list, ignore_index=True)

    df = _replace_placeholder_strings(df)

    if q is None:
        cleaned = _clean_sample_df(df, max_rows=10)
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    q_str = str(q).strip()
    def normalize_series(s):
        if s is None:
            return pd.Series([''] * len(df))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    found_mask = pd.Series(False, index=df.index)
    if 'EmployeeID' in df.columns:
        emp_series = normalize_series(df['EmployeeID'])
        found_mask = found_mask | (emp_series == q_str)
    if 'person_uid' in df.columns:
        uid_series = normalize_series(df['person_uid'])
        found_mask = found_mask | (uid_series == q_str)
    if 'Int1' in df.columns and not found_mask.any():
        int1_series = normalize_series(df['Int1'])
        found_mask = found_mask | (int1_series == q_str)

    if not found_mask.any():
        try:
            q_numeric = float(q_str)
            if 'EmployeeID' in df.columns:
                emp_numeric = pd.to_numeric(df['EmployeeID'], errors='coerce')
                found_mask = found_mask | (emp_numeric == q_numeric)
            if 'Int1' in df.columns and not found_mask.any():
                int_numeric = pd.to_numeric(df['Int1'], errors='coerce')
                found_mask = found_mask | (int_numeric == q_numeric)
        except Exception:
            pass

    matched = df[found_mask].copy()
    if matched.empty:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    cleaned_matched = _clean_sample_df(matched, max_rows=len(matched))



    # enrich aggregated rows (email, image, violation days, explanation)
    try:
        try:
            # lazy import from trend_runner (avoid import-time circular issues)
            from trend_runner import compute_violation_days_map, _strip_uid_prefix as _strip_uid_prefix_tr
            violation_map = compute_violation_days_map(str(DEFAULT_OUTDIR), 90, datetime.now().date())
        except Exception:
            violation_map = {}
            _strip_uid_prefix_tr = (lambda x: x)
    except Exception:
        violation_map = {}
        _strip_uid_prefix_tr = (lambda x: x)

    matched_indexed = matched.reset_index(drop=True)
  

    # more robust candidate resolution + explanation/email/image enrichment
    GUID_IN_TEXT_RE = re.compile(
        r'[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}'
    )

    for idx_c, cleaned in enumerate(cleaned_matched):
        candidate_row = None
        try:
            # 1) try exact matches by person_uid, EmployeeID, EmployeeName
            if cleaned.get('person_uid') and 'person_uid' in matched_indexed.columns:
                mr = matched_indexed[matched_indexed['person_uid'].astype(str).str.strip() == str(cleaned['person_uid']).strip()]
                if not mr.empty:
                    candidate_row = mr.iloc[0].to_dict()

            if candidate_row is None and cleaned.get('EmployeeID') and 'EmployeeID' in matched_indexed.columns:
                mr = matched_indexed[matched_indexed['EmployeeID'].astype(str).str.strip() == str(cleaned['EmployeeID']).strip()]
                if not mr.empty:
                    candidate_row = mr.iloc[0].to_dict()

            if candidate_row is None and cleaned.get('EmployeeName') and 'EmployeeName' in matched_indexed.columns:
                mr = matched_indexed[matched_indexed['EmployeeName'].astype(str).str.strip().str.lower() == str(cleaned['EmployeeName']).strip().lower()]
                if not mr.empty:
                    candidate_row = mr.iloc[0].to_dict()

            # 2) try other useful columns (CardNumber, EmployeeIdentity, Int1, Text12, ObjectIdentity1, ObjectID)
            if candidate_row is None and not matched_indexed.empty:
                for ccol in ('CardNumber', 'EmployeeIdentity', 'Int1', 'Text12', 'ObjectIdentity1', 'ObjectID', 'GUID'):
                    if ccol in matched_indexed.columns and cleaned.get(ccol) not in (None, '', 'nan'):
                        mr = matched_indexed[matched_indexed[ccol].astype(str).str.strip() == str(cleaned.get(ccol)).strip()]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                            break

            # 3) last resort - take the first matched row (if any)
            if candidate_row is None and not matched_indexed.empty:
                candidate_row = matched_indexed.iloc[0].to_dict()
        except Exception:
            candidate_row = None

        # Build Explanation: prefer explicit Explanation, else build from Reasons
        violation_expl = None
        try:
            if candidate_row:
                violation_expl = candidate_row.get('Explanation') or candidate_row.get('explanation') or None

            if not violation_expl:
                reasons = cleaned.get('Reasons') or (candidate_row.get('Reasons') if candidate_row else None)
                if reasons:
                    parts = [p.strip() for p in re.split(r'[;,\|]', str(reasons)) if p.strip()]
                    mapped = []
                    for p in parts:
                        try:
                            if 'SCENARIO_EXPLANATIONS' in globals() and p in SCENARIO_EXPLANATIONS:
                                mapped.append(SCENARIO_EXPLANATIONS[p](candidate_row or {}))
                            else:
                                mapped.append(p.replace("_", " ").replace(">=", "≥"))
                        except Exception:
                            mapped.append(p)
                    violation_expl = " ".join(mapped) if mapped else None

            # Replace GUIDS that appear inside longer text with a human name (if available)
            if violation_expl:
                try:
                    # prefer candidate_row name, then cleaned name
                    emp_name_for_expl = None
                    if candidate_row:
                        emp_name_for_expl = candidate_row.get('EmployeeName') or candidate_row.get('employee_name') or candidate_row.get('ObjectName1')
                    if not emp_name_for_expl:
                        emp_name_for_expl = cleaned.get('EmployeeName')
                    if emp_name_for_expl:
                        violation_expl = GUID_IN_TEXT_RE.sub(str(emp_name_for_expl), str(violation_expl))
                except Exception:
                    pass
        except Exception:
            violation_expl = None

        # Violation days lookup (unchanged logic)
        try:
            candidates = []
            for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
                if cleaned.get(k) not in (None, '', 'nan'):
                    candidates.append(cleaned.get(k))
            if candidate_row:
                for k in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
                    if candidate_row.get(k) not in (None,'','nan'):
                        candidates.append(candidate_row.get(k))
            vdays = 0
            if violation_map:
                for c in candidates:
                    if c is None:
                        continue
                    n = _normalize_id_local(c)
                    if n and n in violation_map:
                        vdays = int(violation_map.get(n, 0))
                        break
                    try:
                        stripped = _strip_uid_prefix_tr(str(n))
                        if stripped != n and stripped in violation_map:
                            vdays = int(violation_map.get(stripped, 0))
                            break
                    except Exception:
                        pass
        except Exception:
            vdays = 0

        # personnel lookup (try many tokens)
        personnel_info = {}
        try:
            lookup_candidates = []
            if candidate_row:
                for k in ('EmployeeObjID','EmployeeObjId','EmployeeIdentity','ObjectID','GUID','EmployeeID','Int1','Text12','EmployeeName'):
                    if candidate_row.get(k) not in (None,'','nan'):
                        lookup_candidates.append(candidate_row.get(k))
            for k in ('EmployeeID','person_uid','EmployeeName'):
                if cleaned.get(k) not in (None,'','nan'):
                    lookup_candidates.append(cleaned.get(k))

            # try each candidate till we get a useful personnel_info
            for cand in lookup_candidates:
                if cand is None:
                    continue
                try:
                    info = get_personnel_info(cand)
                    if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None or info.get('ManagerEmail') is not None):
                        personnel_info = info
                        break
                except Exception:
                    continue

            # Extra fallback: if still empty and cleaned has EmployeeID, attempt lookup explicitly by EmployeeID
            if not personnel_info:
                try:
                    cand = cleaned.get('EmployeeID') or cleaned.get('EmployeeIdentity')
                    if cand:
                        info = get_personnel_info(cand)
                        if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None):
                            personnel_info = info
                except Exception:
                    pass
        except Exception:
            personnel_info = {}

        # Attach computed fields
        try:
            cleaned['ViolationDaysLast90'] = int(vdays or 0)
            cleaned['ViolationExplanation'] = violation_expl
            cleaned['Explanation'] = violation_expl or cleaned.get('ViolationExplanation') or None

            # Ensure EmployeeName is present (prefer cleaned, then candidate_row, then personnel_info Name)
            try:
                if (not cleaned.get('EmployeeName')) or _looks_like_guid(cleaned.get('EmployeeName')):
                    if candidate_row and candidate_row.get('EmployeeName') and not _looks_like_guid(candidate_row.get('EmployeeName')):
                        cleaned['EmployeeName'] = candidate_row.get('EmployeeName')
                    elif personnel_info and personnel_info.get('Name'):
                        cleaned['EmployeeName'] = personnel_info.get('Name')
                # final pass: remove any GUIDs remaining in Explanation
                if cleaned.get('Explanation') and GUID_IN_TEXT_RE.search(str(cleaned.get('Explanation'))):
                    ename = cleaned.get('EmployeeName') or ''
                    cleaned['Explanation'] = GUID_IN_TEXT_RE.sub(str(ename), str(cleaned.get('Explanation')))
            except Exception:
                pass

            # Populate email from personnel_info OR candidate_row OR common columns in matched_indexed
            if personnel_info:
                cleaned['EmployeeObjID'] = personnel_info.get('ObjectID')
                cleaned['EmployeeEmail'] = personnel_info.get('EmailAddress')
                cleaned['ManagerEmail'] = personnel_info.get('ManagerEmail')
            else:
                # try to pick an email-like column from candidate_row
                if not cleaned.get('EmployeeEmail') and candidate_row:
                    for fk in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                        if candidate_row.get(fk) not in (None, '', 'nan'):
                            cleaned['EmployeeEmail'] = candidate_row.get(fk)
                            break
                # final attempt: look in matched_indexed columns (if candidate_row didn't have it)
                if not cleaned.get('EmployeeEmail'):
                    for col in ('Email', 'EmailAddress', 'EmployeeEmail', 'WorkEmail', 'EMail'):
                        if col in matched_indexed.columns:
                            try:
                                val = matched_indexed.iloc[0].get(col)
                                if val not in (None, '', 'nan'):
                                    cleaned['EmployeeEmail'] = val
                                    break
                            except Exception:
                                continue

                # image resolution: prefer personnel_info.ObjectID, then candidate_row.ObjectID/EmployeeObjID, otherwise None
                try:
                    img_obj = None
                    if personnel_info and personnel_info.get('ObjectID') is not None:
                        img_obj = personnel_info.get('ObjectID')
                    elif candidate_row:
                        for k in ('EmployeeObjID', 'EmployeeObjId', 'ObjectID', 'ObjectIdentity1'):
                            if candidate_row.get(k) not in (None, '', 'nan'):
                                img_obj = candidate_row.get(k)
                                break

                    if img_obj:
                        # make the image URL absolute so frontend can fetch it regardless of page origin
                        try:
                            base = (request.url_root or request.host_url).rstrip('/')
                        except Exception:
                            base = ''

                        try:
                            # prefer absolute URL when base is available, otherwise use relative path
                            if base:
                                cleaned['imageUrl'] = f"{base}/employee/{img_obj}/image"
                            else:
                                cleaned['imageUrl'] = f"/employee/{img_obj}/image"
                        except Exception:
                            # fallback to relative path if formatting fails for any reason
                            cleaned['imageUrl'] = f"/employee/{img_obj}/image"

                        try:
                            b = get_person_image_bytes(img_obj)
                            cleaned['HasImage'] = True if b else False
                        except Exception:
                            cleaned['HasImage'] = False
                    else:
                        # fallback: if we have an EmployeeID, use that for image route
                        if cleaned.get('EmployeeID'):
                            try:
                                base = (request.url_root or request.host_url).rstrip('/')
                            except Exception:
                                base = ''
                            if base:
                                cleaned['imageUrl'] = f"{base}/employee/{cleaned.get('EmployeeID')}/image"
                            else:
                                cleaned['imageUrl'] = f"/employee/{cleaned.get('EmployeeID')}/image"
                            try:
                                b = get_person_image_bytes(cleaned.get('EmployeeID'))
                                cleaned['HasImage'] = True if b else False
                            except Exception:
                                cleaned['HasImage'] = False
                        else:
                            cleaned['imageUrl'] = None
                            cleaned['HasImage'] = False

                except Exception:
                    # if anything went wrong in the image resolution logic, ensure safe defaults
                    cleaned['imageUrl'] = None
                    cleaned['HasImage'] = False
                   
            # ensure EmployeeID surfaced if present in candidate_row
            if not cleaned.get('EmployeeID') and candidate_row:
                for k in ('EmployeeID','Int1','Text12','EmployeeIdentity'):
                    if candidate_row.get(k) not in (None, '', 'nan'):
                        cleaned['EmployeeID'] = candidate_row.get(k)
                        break
        except Exception:
            pass



    # Build raw_swipes timeline by scanning swipe files
    raw_files = set()
    raw_swipes_out = []
    seen_swipe_keys = set()
    def _append_swipe(out_row, source_name):
        # make safe: avoid .strip() on None
        key = (
            out_row.get('Date') or '',
            out_row.get('Time') or '',
            (out_row.get('Door') or '').strip(),
            (out_row.get('Direction') or '').strip(),
            (out_row.get('CardNumber') or out_row.get('Card') or '').strip()
        )
        if key in seen_swipe_keys:
            return
        seen_swipe_keys.add(key)
        out_row['_source'] = source_name
        raw_swipes_out.append(out_row)

    # collate scan dates
    dates_to_scan = set()
    for _, agg_row in matched.iterrows():
        try:
            if 'Date' in agg_row and pd.notna(agg_row['Date']):
                try:
                    d = pd.to_datetime(agg_row['Date']).date()
                    dates_to_scan.add(d)
                except Exception:
                    pass
            for col in ('FirstSwipe','LastSwipe'):
                if col in agg_row and pd.notna(agg_row[col]):
                    try:
                        d = pd.to_datetime(agg_row[col]).date()
                        dates_to_scan.add(d)
                    except Exception:
                        pass
        except Exception:
            continue
    if not dates_to_scan:
        dates_to_scan = {None}

    for d in dates_to_scan:
        candidates = _find_swipe_files(DEFAULT_OUTDIR, date_obj=d, city_slug=city_slug)
        for fp in candidates:
            raw_files.add(fp.name)
            try:
                try:
                    raw_df = pd.read_csv(fp, parse_dates=['LocaleMessageTime'])
                except Exception:
                    raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

            raw_df = _replace_placeholder_strings(raw_df)
            cols_lower = {c.lower(): c for c in raw_df.columns}
            tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
            emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
            name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
            card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
            door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
            dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
            note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
            person_uid_col = cols_lower.get('person_uid')

            mask = pd.Series(False, index=raw_df.index)
            if person_uid_col and person_uid_col in raw_df.columns:
                mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
            if emp_col and emp_col in raw_df.columns:
                mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
            if not mask.any() and emp_col and emp_col in raw_df.columns:
                try:
                    q_numeric = float(q)
                    emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                    mask = mask | (emp_numeric == q_numeric)
                except Exception:
                    pass
            if not mask.any() and name_col and name_col in raw_df.columns:
                # safe name compare
                try:
                    mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())
                except Exception:
                    pass

            if not mask.any():
                continue

            filtered = raw_df[mask].copy()
            if filtered.empty:
                continue

            if tcol and tcol in filtered.columns:
                try:
                    filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                except Exception:
                    pass

            if tcol and tcol in filtered.columns:
                filtered = filtered.sort_values(by=tcol)
                filtered['_prev_ts'] = filtered[tcol].shift(1)
                try:
                    filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                except Exception:
                    filtered['_swipe_gap_seconds'] = 0.0
                try:
                    cur_dates = filtered[tcol].dt.date
                    prev_dates = cur_dates.shift(1)
                    day_start_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
                    filtered.loc[day_start_mask, '_swipe_gap_seconds'] = 0.0
                except Exception:
                    pass
            else:
                filtered['_swipe_gap_seconds'] = 0.0

            try:
                if door_col and door_col in filtered.columns:
                    if dir_col and dir_col in filtered.columns:
                        # careful: apply needs column names from raw_df
                        filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                    else:
                        filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
                else:
                    filtered['_zone'] = filtered.get('PartitionName2', None)
            except Exception:
                filtered['_zone'] = None

            for _, r in filtered.iterrows():
                out = {}
                out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in filtered.columns else _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else q)
                emp_val = None
                if emp_col and emp_col in filtered.columns:
                    emp_val = _to_python_scalar(r.get(emp_col))
                else:
                    for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                        if cand.lower() in cols_lower:
                            emp_val = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                            if emp_val not in (None,'','nan'):
                                break
                    if emp_val in (None,'','nan'):
                        emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
                if emp_val is not None:
                    try:
                        s = str(emp_val).strip()
                        if '.' in s:
                            f = float(s)
                            if math.isfinite(f) and f.is_integer():
                                s = str(int(f))
                        if _looks_like_guid(s) or _is_placeholder_str(s):
                            emp_val = None
                        else:
                            emp_val = s
                    except Exception:
                        if _looks_like_guid(emp_val):
                            emp_val = None
                out['EmployeeID'] = emp_val

                card_val = None
                if card_col and card_col in filtered.columns:
                    card_val = _to_python_scalar(r.get(card_col))
                else:
                    for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                        if cand.lower() in cols_lower:
                            card_val = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                            if card_val not in (None,'','nan'):
                                break
                    if card_val in (None,'','nan'):
                        card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                if card_val is not None:
                    try:
                        cs = str(card_val).strip()
                        if _looks_like_guid(cs) or _is_placeholder_str(cs):
                            card_val = None
                        else:
                            card_val = cs
                    except Exception:
                        card_val = None
                out['CardNumber'] = card_val
                out['Card'] = card_val

                if tcol and tcol in filtered.columns:
                    ts = r.get(tcol)
                    try:
                        ts_py = pd.to_datetime(ts)
                        out['Date'] = ts_py.date().isoformat()
                        out['Time'] = ts_py.time().isoformat()
                        out['LocaleMessageTime'] = ts_py.isoformat()
                    except Exception:
                        txt = str(r.get(tcol))
                        out['Date'] = txt[:10]
                        out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                        out['LocaleMessageTime'] = txt
                else:
                    out['Date'] = None
                    out['Time'] = None
                    out['LocaleMessageTime'] = None

                out['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
                out['SwipeGap'] = format_seconds_to_hms(out['SwipeGapSeconds'])
                out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
                out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in r else None
                out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
                try:
                    out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else map_door_to_zone(out['Door'], out['Direction'])
                except Exception:
                    out['Zone'] = None
                out['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
                out['_source_file'] = fp.name
                _append_swipe(out, fp.name)

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": sorted(list(raw_files)),
        "raw_swipes": raw_swipes_out
    }), 200

@app.route('/record/export', methods=['GET'])
def export_record_excel():
    q = request.args.get('employee_id') or request.args.get('person_uid')
    date_str = request.args.get('date')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    if not q:
        return jsonify({"error":"employee_id or person_uid is required"}), 400

    p = Path(DEFAULT_OUTDIR)
    files_to_scan = []
    if date_str:
        try:
            dd = pd.to_datetime(date_str).date()
            files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=dd, city_slug=city_slug)
        except Exception:
            return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400
    else:
        files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=city_slug)

    if not files_to_scan:
        return jsonify({"error":"no raw swipe files found for requested date / outputs"}), 404

    all_rows = []
    for fp in files_to_scan:
        try:
            raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
        except Exception:
            try:
                raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

        raw_df = _replace_placeholder_strings(raw_df)
        cols_lower = {c.lower(): c for c in raw_df.columns}
        tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
        emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
        name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
        card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
        door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
        dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
        note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
        person_uid_col = cols_lower.get('person_uid')

        mask = pd.Series(False, index=raw_df.index)
        if person_uid_col and person_uid_col in raw_df.columns:
            mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
        if emp_col and emp_col in raw_df.columns:
            mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
        if not mask.any() and emp_col and emp_col in raw_df.columns:
            try:
                q_numeric = float(q)
                emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                mask = mask | (emp_numeric == q_numeric)
            except Exception:
                pass
        if not mask.any() and name_col and name_col in raw_df.columns:
            mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())

        if not mask.any():
            continue

        filtered = raw_df[mask].copy()
        if filtered.empty:
            continue

        if tcol and tcol in filtered.columns:
            try:
                filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
            except Exception:
                pass

        if tcol and tcol in filtered.columns:
            filtered = filtered.sort_values(by=tcol)
            filtered['_prev_ts'] = filtered[tcol].shift(1)
            try:
                filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
            except Exception:
                filtered['_swipe_gap_seconds'] = 0.0
        else:
            filtered['_swipe_gap_seconds'] = 0.0

        try:
            if door_col and door_col in filtered.columns:
                if dir_col and dir_col in filtered.columns:
                    filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                else:
                    filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
            else:
                filtered['_zone'] = filtered.get('PartitionName2', None)
        except Exception:
            filtered['_zone'] = None

        for _, r in filtered.iterrows():
            row = {}
            row['EmployeeName'] = _to_python_scalar(r.get(name_col)) if (name_col and name_col in filtered.columns) else None
            emp_val = None
            if emp_col and emp_col in filtered.columns:
                emp_val = _to_python_scalar(r.get(emp_col))
            else:
                for cand in ('int1','text12','employeeid','employee_identity','employeeidentity'):
                    if cand in cols_lower and cols_lower[cand] in filtered.columns:
                        emp_val = _to_python_scalar(r.get(cols_lower[cand]))
                        if emp_val:
                            break
            row['EmployeeID'] = emp_val
            row['Card'] = _to_python_scalar(r.get(card_col)) if (card_col and card_col in filtered.columns) else None

            if tcol and tcol in filtered.columns:
                ts = r.get(tcol)
                try:
                    ts_py = pd.to_datetime(ts)
                    row['Date'] = ts_py.date().isoformat()
                    row['Time'] = ts_py.time().isoformat()
                    row['LocaleMessageTime'] = ts_py.isoformat()
                except Exception:
                    txt = str(r.get(tcol))
                    row['Date'] = txt[:10]
                    row['Time'] = txt[11:19] if len(txt) >= 19 else None
                    row['LocaleMessageTime'] = txt
            else:
                row['Date'] = None
                row['Time'] = None
                row['LocaleMessageTime'] = None

            row['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
            row['SwipeGap'] = format_seconds_to_hms(row['SwipeGapSeconds'])
            row['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
            row['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else None
            row['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None
            try:
                zone_val = r.get('_zone') if '_zone' in r else None
                if zone_val is None:
                    zone_val = map_door_to_zone(row['Door'], row['Direction'])
                row['Zone'] = _to_python_scalar(zone_val)
            except Exception:
                row['Zone'] = None
            row['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
            row['_source_file'] = fp.name
            all_rows.append(row)

    if not all_rows:
        return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

    df_out = pd.DataFrame(all_rows)
    details_cols = ['EmployeeName','EmployeeID','Door','Direction','Zone','Date','LocaleMessageTime','SwipeGapSeconds','PartitionName2','_source_file']
    timeline_cols = ['EmployeeName','EmployeeID','Card','Date','Time','SwipeGapSeconds','Door','Direction','Zone','Note','_source_file']

    details_df = df_out[[c for c in details_cols if c in df_out.columns]].copy()
    timeline_df = df_out[[c for c in timeline_cols if c in df_out.columns]].copy()

    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            details_df.to_excel(writer, sheet_name='Details — Evidence', index=False)
            timeline_df.to_excel(writer, sheet_name='Swipe timeline', index=False)
            writer.save()
            output.seek(0)
    except Exception as e:
        logging.exception("Failed to create Excel: %s", e)
        return jsonify({"error":"failed to create excel"}), 500

    if OPENPYXL_AVAILABLE:
        try:
            wb = load_workbook(output)
            thin = Side(border_style="thin", color="000000")
            thick = Side(border_style="medium", color="000000")
            for ws in wb.worksheets:
                header = ws[1]
                for cell in header:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                        cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
                for col in ws.columns:
                    max_len = 0
                    col_letter = col[0].column_letter
                    for cell in col:
                        try:
                            v = str(cell.value) if cell.value is not None else ""
                        except Exception:
                            v = ""
                        if len(v) > max_len:
                            max_len = len(v)
                    width = min(max(10, max_len + 2), 50)
                    ws.column_dimensions[col_letter].width = width
            out2 = io.BytesIO()
            wb.save(out2)
            out2.seek(0)
            return send_file(out2, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        except Exception:
            logging.exception("Excel styling failed, returning raw file")
            output.seek(0)
            return send_file(output, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    else:
        output.seek(0)
        return send_file(output, as_attachment=True,
                         download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                         mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)

@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        if build_monthly_training is None:
            raise RuntimeError("build_monthly_training not available")
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500

# chatbot helpers (kept mostly as-is)
try:
    from trend_runner import _read_past_trend_csvs, _normalize_id_val, SCENARIO_EXPLANATIONS
except Exception:
    _read_past_trend_csvs = None
    _normalize_id_val = None
    SCENARIO_EXPLANATIONS = {}

def _load_latest_trend_df(outdir: Path, city: str = "pune"):
    city_slug = _slug_city(city)
    csvs = sorted(outdir.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(outdir.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return None, None
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    df = _replace_placeholder_strings(df)
    return df, latest.name

def _find_person_rows(identifier: str, days: int = 90, outdir: Path = DEFAULT_OUTDIR):
    if _normalize_id_val:
        norm = _normalize_id_val(identifier)
    else:
        norm = str(identifier).strip()
        if '.' in norm:
            try:
                f = float(norm)
                if f.is_integer():
                    norm = str(int(f))
            except Exception:
                pass
    today = datetime.now().date()
    try:
        if _read_past_trend_csvs:
            past = _read_past_trend_csvs(str(outdir), days, today)
        else:
            files = sorted(Path(outdir).glob("trend_pune_*.csv"), reverse=True)
            dfs = []
            cutoff = today - timedelta(days=days)
            for fp in files:
                try:
                    tmp = pd.read_csv(fp, parse_dates=['Date'])
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                    tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                    dfs.append(tmp)
                except Exception:
                    try:
                        tmp = pd.read_csv(fp, dtype=str)
                        if 'Date' in tmp.columns:
                            tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                            tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                            dfs.append(tmp)
                    except Exception:
                        continue
            past = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
    except Exception:
        past = pd.DataFrame()

    if past is None or past.empty:
        return pd.DataFrame()

    past = _replace_placeholder_strings(past)
    match_mask = pd.Series(False, index=past.index)
    for col in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
        if col in past.columns:
            try:
                match_mask = match_mask | (past[col].astype(str).fillna('').str.strip() == str(norm).strip())
            except Exception:
                continue

    if not match_mask.any():
        try:
            qnum = float(norm)
            for col in ('EmployeeID','Int1'):
                if col in past.columns:
                    try:
                        numcol = pd.to_numeric(past[col], errors='coerce')
                        match_mask = match_mask | (numcol == qnum)
                    except Exception:
                        continue
        except Exception:
            pass

    if not match_mask.any() and 'EmployeeName' in past.columns:
        names = past['EmployeeName'].dropna().astype(str).unique().tolist()
        close = difflib.get_close_matches(str(identifier), names, n=5, cutoff=0.7)
        if close:
            match_mask = match_mask | past['EmployeeName'].astype(str).isin(close)

    return past[match_mask].copy()

def _explain_scenario_code(code):
    if not code:
        return None
    code = str(code).strip()
    if code in SCENARIO_EXPLANATIONS:
        try:
            fn = SCENARIO_EXPLANATIONS.get(code)
            try:
                txt = fn({})
                return txt
            except Exception:
                return code.replace("_", " ").replace(">= ", "≥ ")
        except Exception:
            return code.replace("_", " ").replace(">= ", "≥ ")
    return code.replace("_", " ").replace(">=", "≥")

def _map_score_to_label_fallback(score: float):
    try:
        s = float(score)
    except Exception:
        return (0.0, "Low")
    if s >= 0.75:
        return (s, "High")
    if s >= 0.4:
        return (s, "Medium")
    return (s, "Low")

@app.route('/chatbot/query', methods=['POST'])
def chatbot_query():
    payload = request.get_json(force=True)
    q = (payload.get('q') or '').strip()
    if not q:
        return jsonify({"error":"query text 'q' required"}), 400
    lang = payload.get('lang')
    q_l = q.lower().strip()

    if re.search(r"\bwho is (high|low) risk\b", q_l) or re.search(r"\b(high|low) risk (people|persons|people) (today)?\b", q_l):
        want = 'high' if 'high' in q_l else 'low' if 'low' in q_l else None
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer": "No trend data available.", "evidence": []})
        if 'RiskLevel' not in df.columns:
            if 'RiskScore' in df.columns:
                def _map_rs(s):
                    try:
                        if pd.isna(s):
                            return 'Low'
                    except Exception:
                        pass
                    try:
                        if 'map_score_to_label' in globals() and callable(globals().get('map_score_to_label')):
                            try:
                                return globals().get('map_score_to_label')(float(s))[1]
                            except Exception:
                                pass
                        return _map_score_to_label_fallback(float(s))[1]
                    except Exception:
                        return 'Low'
                df['RiskLevel'] = df['RiskScore'].apply(lambda s: _map_rs(s))
            else:
                df['RiskLevel'] = df.get('RiskLevel', 'Low')
        if want == 'high':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'high']
        elif want == 'low':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'low']
        else:
            sel = df
        names = sel['EmployeeName'].dropna().astype(str).unique().tolist()
        if not names:
            ans = f"No {want} risk persons found in the latest data." if want else "No persons found."
            return jsonify({"answer": ans, "evidence": []})
        else:
            ans = f"{want.capitalize()} risk persons today: " + ", ".join(names[:40])
            sample = _clean_sample_df(sel.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bexplain\s+([A-Za-z0-9_\-]+)\b.*", q_l)
    if m:
        code = m.group(1).strip()
        explanation = _explain_scenario_code(code)
        ans = f"Explanation for '{code}': {explanation}"
        return jsonify({"answer": ans, "evidence": []})

    if 'trend details' in q_l or 'top reasons' in q_l or 'trend details for today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'Reasons' in df.columns:
            reasons = {}
            for v in df['Reasons'].dropna().astype(str):
                for part in re.split(r'[;,\|]', v):
                    key = part.strip()
                    if key and not _is_placeholder_str(key):
                        reasons[key] = reasons.get(key, 0) + 1
            top = sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:10]
            if not top:
                return jsonify({"answer":"No reason counts available today.","evidence":[]})
            ans = "Top reasons today: " + ", ".join([f"{k} ({c})" for k,c in top])
            sample = []
            try:
                top_reasons = [k for k,_ in top]
                mask = df['Reasons'].astype(str).apply(lambda s: any(tr in s for tr in top_reasons))
                sample_df = df[mask].head(10)
                sample = _clean_sample_df(sample_df, max_rows=10)
            except Exception:
                sample = []
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bshow (?:me )?([A-Za-z0-9\-\:\s]+?) (?:for )?(?:last )?(\d+)\s*days\b", q_l)
    if not m:
        m = re.match(r".*\b(show|display)\s+(?:me\s+)?([A-Za-z0-9\-\:\s]+?)\s+last\s+(\d+)\s*days\b", q_l)
    if m:
        if len(m.groups()) == 2:
            identifier, days = m.group(1).strip(), int(m.group(2))
        else:
            identifier = m.group(1).strip()
            days = int(m.group(2))
        rows = _find_person_rows(identifier, days=days, outdir=DEFAULT_OUTDIR)
        if rows is None or rows.empty:
            return jsonify({"answer": f"No records found for '{identifier}' in last {days} days.", "evidence": []})
        flagged = rows[rows.get('IsFlagged', False) == True] if 'IsFlagged' in rows.columns else pd.DataFrame()
        flagged_count = int(len(flagged))
        total_days = int(len(rows))
        latest_row = rows.sort_values('Date', ascending=False).iloc[0].to_dict()
        name = latest_row.get('EmployeeName') or latest_row.get('person_uid') or latest_row.get('EmployeeID')
        ans = f"Found {total_days} day(s) for {name} in the last {days} days. Flagged days: {flagged_count}."
        sample = _clean_sample_df(rows.sort_values('Date', ascending=False).head(10), max_rows=10)
        return jsonify({"answer": ans, "evidence": sample})

    if 'present today' in q_l or 'who is present today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'PresentToday' in df.columns:
            present = df[df['PresentToday'] == True]
            names = present['EmployeeName'].dropna().unique().tolist()
            ans = f"Present today: {', '.join(names[:40]) if names else 'None'}"
            sample = _clean_sample_df(present.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})
        else:
            return jsonify({"answer":"PresentToday field not available in latest trends.","evidence":[]})

    hint = "I can answer: 'Who is high risk today', 'Who is low risk today', 'Show me <EmployeeID|Name> last 90 days', 'Explain <scenario_code>', 'Trend details for today — top reasons'."
    return jsonify({"answer": f"I can help with trend & risk questions. I recognized: {q}. Try: {hint}", "evidence":[]})

@app.route('/employee/<empid>/image', methods=['GET'])
def serve_employee_image(empid):
    if empid is None:
        return jsonify({"error": "employee id required"}), 400
    try:
        img_bytes = get_person_image_bytes(empid)
        # If no image found, return a small inline SVG placeholder so <img> can still render
        if not img_bytes:
            svg = (
                '<svg xmlns="http://www.w3.org/2000/svg" width="160" height="160">'
                '<rect fill="#eef2f7" width="100%" height="100%"/>'
                '<text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle" fill="#64748b" font-size="18">'
                'No image</text></svg>'
            )
            bio = io.BytesIO(svg.encode('utf-8'))
            bio.seek(0)
            resp = send_file(bio, mimetype='image/svg+xml')
            # avoid aggressive caching of placeholder
            resp.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            return resp

        # detect content type heuristically (jpeg/png)
        header = img_bytes[:8] if isinstance(img_bytes, (bytes, bytearray)) else bytes(img_bytes)[:8]
        content_type = 'application/octet-stream'
        try:
            if len(header) >= 2 and header[0] == 0xFF and header[1] == 0xD8:
                content_type = 'image/jpeg'
            elif header.startswith(b'\x89PNG\r\n\x1a\n'):
                content_type = 'image/png'
            elif header.startswith(b'BM'):
                content_type = 'image/bmp'
            else:
                # fallback: try to detect common image signatures, otherwise serve as octet-stream
                content_type = 'application/octet-stream'
        except Exception:
            content_type = 'application/octet-stream'

        bio = io.BytesIO(img_bytes if isinstance(img_bytes, (bytes, bytearray)) else bytes(img_bytes))
        bio.seek(0)
        resp = send_file(bio, mimetype=content_type)
        # small caching is ok for real images
        resp.headers['Cache-Control'] = 'private, max-age=300'
        return resp
    except Exception:
        logging.exception("Error serving image for employee %s", empid)
        return jsonify({"error": "failed to serve image"}), 500



# run
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)











# backend/trend_runner.py
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re
import os
import calendar
import json
from collections import defaultdict
from datetime import datetime as _datetime
from datetime import timedelta as _timedelta
from typing import Optional, List

# ------------------ personnel enrichment (add near imports) ------------------
def _get_personnel_funcs_lazy():
    """
    Try to import personnel helpers from app.py at call time (avoids circular imports).
    Returns tuple (get_personnel_info_fn_or_None, get_person_image_bytes_fn_or_None).
    """
    try:
        import importlib
        appmod = importlib.import_module('app')  # adjust module name if your app module name is different
        gpi = getattr(appmod, 'get_personnel_info', None)
        gpib = getattr(appmod, 'get_person_image_bytes', None)
        return gpi, gpib
    except Exception:
        # if the app module isn't importable from this process, return None and continue
        return None, None

def _normalize_for_lookup(val):
    """simple normalizer used for building lookup id (avoid GUIDs when possible)."""
    if val is None:
        return None
    s = str(val).strip()
    if not s:
        return None
    # keep numeric-like ids or short strings; drop long GUID-like values if possible
    if len(s) > 36 and '-' in s:
        # likely GUID — still return, but caller may prefer EmployeeID first
        return s
    return s

def _enrich_with_personnel_info(df, image_endpoint_template="/employee/{}/image"):
    """
    Adds columns to df:
      - EmployeeEmail (string or None)
      - imageUrl (string or None) -> path the frontend should fetch (relative)
    Uses app.get_personnel_info when available; otherwise builds reasonable imageUrl from EmployeeID.
    Returns new DataFrame (copy).
    """
    if df is None or df.empty:
        return df
    get_personnel_info, get_person_image_bytes = _get_personnel_funcs_lazy()

    emails = []
    image_urls = []

    for _, row in df.iterrows():
        email = None
        image_url = None

        # prefer canonical non-GUID employee id for lookup
        cand_empid = None
        for tok in ('EmployeeID', 'EmployeeID_feat', 'EmployeeID_dur', 'Int1', 'Text12'):
            if tok in row and row.get(tok) not in (None, '', float('nan')):
                cand_empid = _normalize_for_lookup(row.get(tok))
                break
        cand_uid = row.get('EmployeeIdentity') or row.get('person_uid') or None

        # Try app.get_personnel_info if available (best source for Email + canonical ObjectID/GUID)
        if get_personnel_info:
            try:
                lookup_token = cand_empid or cand_uid
                pi = None
                if lookup_token is not None:
                    pi = get_personnel_info(lookup_token)
                else:
                    # last-resort: try employee name/object name if present
                    ln = row.get('EmployeeName') or row.get('ObjectName1')
                    if ln:
                        pi = get_personnel_info(ln)
                if pi and isinstance(pi, dict):
                    # EmployeeEmail alias from app.get_personnel_info
                    email = pi.get('EmployeeEmail') or pi.get('EmailAddress') or None
                    # determine parent id for image route: prefer ObjectID, then GUID, then EmployeeID
                    parent = pi.get('ObjectID') or pi.get('GUID') or cand_empid or cand_uid
                    if parent is not None:
                        image_url = image_endpoint_template.format(str(parent))
            except Exception:
                # don't propagate enrichment errors
                email = email or None
                image_url = image_url or None

        # fallback when get_personnel_info not available or returned nothing
        if email is None:
            # try to find email-like fields in row if present
            for possible in ('EmployeeEmail', 'Email', 'EmailAddress', 'ManagerEmail'):
                if possible in row and row.get(possible) not in (None, '', float('nan')):
                    email = row.get(possible)
                    break

        if image_url is None:
            # fallback: if we have an EmployeeID use that for image route
            if cand_empid:
                image_url = image_endpoint_template.format(cand_empid)
            elif cand_uid:
                image_url = image_endpoint_template.format(cand_uid)
            else:
                image_url = None

        emails.append(email)
        image_urls.append(image_url)

    out = df.copy()
    out['EmployeeEmail'] = emails
    out['imageUrl'] = image_urls
    return out
# ---------------------------------------------------------------------------


# duration_report imports (made robust)
try:
    from duration_report import run_for_date, compute_daily_durations, REGION_CONFIG
except Exception:
    try:
        from duration_report import run_for_date, compute_daily_durations
        REGION_CONFIG = {}
    except Exception:
        # minimal fallback: run_for_date must exist in actual environment; otherwise callers will fail later
        try:
            from duration_report import run_for_date
        except Exception:
            run_for_date = None
        compute_daily_durations = None
        REGION_CONFIG = {}

# try to import config door->zone mapping, but keep fallback
try:
    from config.door_zone import map_door_to_zone as config_map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    config_map_door_to_zone = None
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

# HIST_PATH: try a few likely locations (project config, repository root, absolute path)
CANDIDATE_HISTORY = [
    Path(__file__).parent / "config" / "current_analysis.csv",
    Path(__file__).parent.parent / "config" / "current_analysis.csv",
    Path.cwd() / "current_analysis.csv",
    Path(__file__).parent / "current_analysis.csv"
]
HIST_PATH = None
for p in CANDIDATE_HISTORY:
    if p.exists():
        HIST_PATH = p
        break

if HIST_PATH is None:
    logging.warning("Historical profile file current_analysis.csv not found in candidate locations.")
    HIST_DF = pd.DataFrame()
else:
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)

def _slug_city(city: str) -> str:
    if not city:
        return "pune"
    return str(city).strip().lower().replace(" ", "_")



# ----- small shared helpers: treat empty/placeholder tokens as None -----
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _normalize_id_val(v):
    """
    Normalize an id-like value: strip, convert floats like '320172.0' -> '320172'
    Return None for NaN/empty/placeholder.
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    # strip .0 if integer-like
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s

# GUID / name helpers
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')

def _looks_like_guid(s: object) -> bool:
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        if _looks_like_guid(st):
            return False
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    """
    Create canonical person uid:
      - prefer EmployeeID (normalized) -> 'emp:<id>' only if it is not a GUID
      - else EmployeeIdentity -> 'uid:<val>'
      - else EmployeeName -> hash-based 'name:<shorthash>'
    """
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None

# small helper to extract Card from XML-like strings
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None

# use config_map_door_to_zone if available, else fallback
try:
    _BREAK_ZONES = BREAK_ZONES
    _OUT_OF_OFFICE_ZONE = OUT_OF_OFFICE_ZONE
except Exception:
    _BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    _OUT_OF_OFFICE_ZONE = "Out of office"

def map_door_to_zone(door: object, direction: object = None) -> str:
    try:
        if config_map_door_to_zone is not None:
            return config_map_door_to_zone(door, direction)
    except Exception:
        pass
    try:
        if door is None:
            return None
        s = str(door).strip()
        if not s:
            return None
        s_l = s.lower()
        if direction and isinstance(direction, str):
            d = direction.strip().lower()
            if "out" in d:
                return _OUT_OF_OFFICE_ZONE
            if "in" in d:
                return "Reception Area"
        if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
            return _OUT_OF_OFFICE_ZONE
        return "Working Area"
    except Exception:
        return None

# ----- Config and scenarios -----
VIOLATION_WINDOW_DAYS = 90
RISK_THRESHOLDS = [
    (0.5, "Low"),
    (1.5, "Low Medium"),
    (2.5, "Medium"),
    (4.0, "Medium High"),
    (float("inf"), "High"),
]

def map_score_to_label(score: float) -> (int, str):
    try:
        if score is None:
            score = 0.0
        s = float(score)
    except Exception:
        s = 0.0
    bucket = 1
    label = "Low"
    for i, (threshold, lbl) in enumerate(RISK_THRESHOLDS, start=1):
        if s <= threshold:
            bucket = i
            label = lbl
            break
    return bucket, label

# scenario functions (kept from your improved version)
def scenario_long_gap(row):
    try:
        gap = int(row.get('MaxSwipeGapSeconds') or 0)
        return gap >= int(4.5 * 3600)
    except Exception:
        return False

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = int(row.get('CountSwipes') or 0)
    dur = float(row.get('DurationMinutes') or 0.0)
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = rec.get('TotalSwipes_median', np.nan)
            if pd.notna(median) and median > 0:
                return (cur > 3 * float(median)) and (dur < 60)
    except Exception:
        pass
    try:
        if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
            global_med = HIST_DF['TotalSwipes_median'].median()
            if pd.notna(global_med) and global_med > 0:
                return (cur > 3 * float(global_med)) and (dur < 60)
    except Exception:
        pass
    return (cur > 50) and (dur < 60)

def scenario_high_swipes_benign(row):
    cur = int(row.get('CountSwipes') or 0)
    dur = float(row.get('DurationMinutes') or 0.0)
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = rec.get('TotalSwipes_median', np.nan)
            if pd.notna(median) and median > 0:
                return (cur > 3 * float(median)) and (dur >= 60)
    except Exception:
        pass
    try:
        if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
            global_med = HIST_DF['TotalSwipes_median'].median()
            if pd.notna(global_med) and global_med > 0:
                return (cur > 3 * float(global_med)) and (dur >= 60)
    except Exception:
        pass
    return (cur > 50) and (dur >= 60)

def scenario_behaviour_shift(row, hist_df=None, minutes_threshold=180):
    try:
        if pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None:
            return False
        first_ts = pd.to_datetime(row.get('FirstSwipe'))
        today_minutes = first_ts.hour * 60 + first_ts.minute
        empid = row.get('EmployeeID')
        hist = hist_df if hist_df is not None else (HIST_DF if (HIST_DF is not None and not HIST_DF.empty) else None)
        if hist is None or hist.empty or empid is None:
            return False
        try:
            rec = hist[hist['EmployeeID'] == empid]
            if rec.empty:
                return False
            if 'FirstSwipeMinutes_median' in rec.columns:
                median_min = rec.iloc[0].get('FirstSwipeMinutes_median')
            else:
                median_min = rec.iloc[0].get('AvgFirstSwipeMins_median', None)
            if pd.isna(median_min) or median_min is None:
                return False
            diff = abs(today_minutes - float(median_min))
            return diff >= int(minutes_threshold)
        except Exception:
            return False
    except Exception:
        return False

def scenario_repeated_short_breaks(row):
    try:
        break_count = int(row.get('BreakCount') or 0)
        total_break_mins = float(row.get('TotalBreakMinutes') or 0.0)
        long_break_count = int(row.get('LongBreakCount') or 0)
        short_gap_count = int(row.get('ShortGapCount') or 0)
        if break_count >= 2:
            return True
        if short_gap_count >= 5:
            return True
        if total_break_mins >= 180 and short_gap_count >= 2:
            return True
        return False
    except Exception:
        return False

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map

def scenario_shortstay_longout_repeat(row):
    return bool(row.get('PatternShortLongRepeat', False))

SCENARIOS = [
    ("long_gap_>=4.5h", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap),
    ("high_swipes_benign", scenario_high_swipes_benign),
    ("behaviour_shift", scenario_behaviour_shift),
    ("shortstay_longout_repeat", scenario_shortstay_longout_repeat)
]

# --- improved human-readable scenario explanations (use hours for duration/gaps) ---
def _hrs_from_minutes(mins):
    try:
        m = float(mins or 0.0)
        return round(m / 60.0, 1)
    except Exception:
        return None

def _hrs_from_seconds(sec):
    try:
        s = float(sec or 0.0)
        return round(s / 3600.0, 1)
    except Exception:
        return None

SCENARIO_EXPLANATIONS = {
    "long_gap_>=4.5h": lambda r: (
        (lambda h: f"Long gap between swipes (~{h} h)." if h is not None else "Long gap between swipes.")
        (_hrs_from_seconds(r.get('MaxSwipeGapSeconds')))
    ),
    "short_duration_<4h": lambda r: (
        # if duration is zero but we only saw only_in/only_out, be explicit
        "Only 'IN' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyIn', 0)) == 1 else
        "Only 'OUT' events recorded. No complete IN/OUT pair for this day." if int(r.get('OnlyOut', 0)) == 1 else
        (lambda h: f"Short total presence (~{h} h)." if h is not None else "Short total presence.")(_hrs_from_minutes(r.get('DurationMinutes')))
    ),
    "coffee_badging": lambda r: "Multiple quick swipes in short time.",
    "low_swipe_count_<=2": lambda r: "Very few swipes on day.",
    "single_door": lambda r: "Only a single door used during the day.",
    "only_in": lambda r: "Only 'IN' events recorded.",
    "only_out": lambda r: "Only 'OUT' events recorded.",
    "overtime_>=10h": lambda r: "Overtime detected (>=10 hours).",
    "very_long_duration_>=16h": lambda r: "Very long presence (>=16 hours).",
    "zero_swipes": lambda r: "No swipes recorded on this day.",
    "unusually_high_swipes": lambda r: "Unusually high number of swipes compared to peers/history.",
    "repeated_short_breaks": lambda r: "Many short gaps between swipes.",
    "multiple_location_same_day": lambda r: "Multiple locations/partitions used in same day.",
    "weekend_activity": lambda r: "Activity recorded on weekend day.",
    "repeated_rejection_count": lambda r: "Multiple rejection events recorded.",
    "badge_sharing_suspected": lambda r: "Same card used by multiple users on same day — possible badge sharing.",
    "early_arrival_before_06": lambda r: "First swipe earlier than 06:00.",
    "late_exit_after_22": lambda r: "Last swipe after 22:00.",
    "shift_inconsistency": lambda r: "Duration deviates from historical shift patterns.",
    "trending_decline": lambda r: "Employee shows trending decline in presence.",
    "consecutive_absent_days": lambda r: "Consecutive absent days observed historically.",
    "high_variance_duration": lambda r: "High variance in daily durations historically.",
    "short_duration_on_high_presence_days": lambda r: "Short duration despite normally high presence days.",
    "swipe_overlap": lambda r: "Overlap in swipe times with other persons on same door.",
    "behaviour_shift": lambda r: "Significant change in arrival time compared to historical baseline.",
    "shortstay_longout_repeat": lambda r: "Repeated pattern: short in → long out → short return."
}


def _explain_scenarios_detected(row, detected_list):
    pieces = []
    # derive a human display label consisting of Name and EmployeeID where possible
    try:
        empid = None
        # prefer explicit EmployeeID (non-GUID) from various tokens
        for tok in ('EmployeeID', 'EmployeeID_feat', 'EmployeeID_dur', 'Int1', 'Text12'):
            if tok in row and row.get(tok) not in (None, '', 'nan'):
                val = _normalize_id_val(row.get(tok))
                if val and not _looks_like_guid(val):
                    empid = str(val)
                    break
        # if empid still None, allow non-GUID EmployeeIdentity
        if not empid and row.get('EmployeeIdentity') not in (None, '', 'nan'):
            tmp = _normalize_id_val(row.get('EmployeeIdentity'))
            if tmp and not _looks_like_guid(tmp):
                empid = str(tmp)

        name = None
        try:
            nm = row.get('EmployeeName')
            if nm and _looks_like_name(nm) and not _is_placeholder_str(nm):
                name = str(nm).strip()
            else:
                # fallback: pick first non-guid textual name from common tokens
                for cand in ('EmployeeName', 'EmployeeName_feat', 'EmployeeName_dur', 'ObjectName1'):
                    if cand in row and row.get(cand) not in (None, '', 'nan'):
                        v = row.get(cand)
                        if v and not _looks_like_guid(v) and _looks_like_name(v):
                            name = str(v).strip()
                            break
                if not name:
                    # try to strip person_uid prefixes if present
                    pu = row.get('person_uid')
                    if isinstance(pu, str) and (pu.startswith('emp:') or pu.startswith('uid:') or pu.startswith('name:')):
                        try:
                            stripped = _strip_uid_prefix(pu)
                            if stripped and not _looks_like_guid(stripped):
                                name = str(stripped)
                        except Exception:
                            pass
        except Exception:
            name = None

        # build prefix
        prefix = ""
        if name and empid:
            prefix = f"{name} ({empid}) - "
        elif name:
            prefix = f"{name} - "
        elif empid:
            prefix = f"{empid} - "
        else:
            prefix = ""
    except Exception:
        prefix = ""

    for sc in detected_list:
        sc = sc.strip()
        fn = SCENARIO_EXPLANATIONS.get(sc)
        try:
            if fn:
                pieces.append(fn(row))
            else:
                pieces.append(sc.replace("_", " ").replace(">=", "≥"))
        except Exception:
            pieces.append(sc)
    if not pieces:
        return None
    explanation = " ".join([p if p.endswith('.') else p + '.' for p in pieces])

    # Replace any GUID that accidentally remained inside explanation with the chosen human identifier (without duplicating parentheses)
    try:
        GUID_IN_TEXT_RE = re.compile(r'[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}')
        if prefix and isinstance(explanation, str) and GUID_IN_TEXT_RE.search(explanation):
            # replace GUIDs inside with the prefix label (strip trailing ' - ' from prefix)
            label = prefix.rstrip(' - ')
            explanation = GUID_IN_TEXT_RE.sub(str(label), explanation)
    except Exception:
        pass

    return prefix + explanation


# ---------------- compute_features (robust merged version) ----------------
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)

    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
    else:
        if 'Date' in sw.columns:
            sw['LocaleMessageTime'] = None
            try:
                sw['LocaleMessageTime'] = pd.to_datetime(sw['Date'], errors='coerce')
            except Exception:
                sw['LocaleMessageTime'] = None

    # By default Date comes from LocaleMessageTime (local, human timestamps).
    # However if an AdjustedMessageTime column exists (the Pune 2AM boundary) prefer that
    # for date assignment so trend grouping matches compute_daily_durations().
    if 'AdjustedMessageTime' in sw.columns and sw['AdjustedMessageTime'].notna().any():
        try:
            sw['AdjustedMessageTime'] = pd.to_datetime(sw['AdjustedMessageTime'], errors='coerce')
            # Prefer adjusted date for rows where it exists (this mirrors duration_report logic).
            mask_adj = sw['AdjustedMessageTime'].notna()
            # Ensure LocaleMessageTime parsed for those not adjusted
            sw.loc[~mask_adj, 'Date'] = pd.to_datetime(sw.loc[~mask_adj, 'LocaleMessageTime'], errors='coerce').dt.date
            sw.loc[mask_adj, 'Date']  = sw.loc[mask_adj,  'AdjustedMessageTime'].dt.date
        except Exception:
            # fallback to LocaleMessageTime date
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
    else:
        # normal path
        sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date

    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    try:
        if dir_col and dir_col in sw.columns:
            sw['Direction'] = sw[dir_col]
        if door_col and door_col in sw.columns:
            sw['Door'] = sw[door_col]
        if empid_col and empid_col in sw.columns:
            sw['EmployeeID'] = sw[empid_col]
        if name_col and name_col in sw.columns:
            sw['EmployeeName'] = sw[name_col]
        if card_col and card_col in sw.columns:
            sw['CardNumber'] = sw[card_col]
        if 'LocaleMessageTime' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce').dt.date
        elif 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
    except Exception:
        logging.exception("Normalization of swipe columns failed.")

    # PersonnelType filtering (tolerant) - avoid dropping if column absent
    if 'PersonnelTypeName' in sw.columns:
        sw['PersonnelTypeName'] = sw['PersonnelTypeName'].astype(str).str.strip()
        mask = sw['PersonnelTypeName'].str.lower().str.contains(r'employee|terminated', na=False)
        logging.info("PersonnelTypeName values example: %s", list(sw['PersonnelTypeName'].dropna().unique()[:6]))
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelTypeName filter applied: before=%d after=%d", before, len(sw))
    elif 'PersonnelType' in sw.columns:
        sw['PersonnelType'] = sw['PersonnelType'].astype(str).str.strip()
        mask = sw['PersonnelType'].str.lower().str.contains(r'employee|terminated', na=False)
        before = len(sw)
        sw = sw[mask].copy()
        logging.info("PersonnelType filter applied: before=%d after=%d", before, len(sw))

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # person_uid canonical
    if 'person_uid' not in sw.columns:
        def make_person_uid_local(r):
            empid_val = None
            if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
                empid_val = r.get(empid_col)
            elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
                empid_val = r.get('EmployeeID')
            empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
            name_val = None
            if name_col and name_col in r:
                name_val = r.get(name_col)
            elif 'EmployeeName' in r:
                name_val = r.get('EmployeeName')
            elif 'ObjectName1' in r:
                name_val = r.get('ObjectName1')
            return _canonical_person_uid({
                'EmployeeID': empid_val,
                'EmployeeIdentity': empident_val,
                'EmployeeName': name_val
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
                    'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
                    'EmployeeIdentity'])
    if name_col: sel_cols.add(name_col)
    if empid_col: sel_cols.add(empid_col)
    if card_col: sel_cols.add(card_col)
    if door_col: sel_cols.add(door_col)
    if dir_col: sel_cols.add(dir_col)
    sel_cols = [c for c in sel_cols if c in sw.columns]

    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0

        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

        # card extraction
        card_numbers = []
        if card_col and card_col in g.columns:
            card_numbers = list(pd.unique(g[card_col].dropna()))
        if not card_numbers and 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna()))
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
                    try:
                        vals = list(pd.unique(g[c].dropna()))
                        if vals:
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'xml' in cl:
                    for raw in g[c].dropna().astype(str):
                        extracted = _extract_card_from_xml(raw)
                        if extracted:
                            card_numbers.append(extracted)
        card_numbers = list(dict.fromkeys(card_numbers))
        card_number = None
        for c in card_numbers:
            n = _normalize_id_val(c)
            if n and not _looks_like_guid(n):
                card_number = n
                break

        # stable id/name
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None

        if empid_col and empid_col in g.columns:
            vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        elif 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized

        if (not employee_id) and 'PersonnelType' in g.columns:
            try:
                pvals = g['PersonnelType'].dropna().astype(str)
                if not pvals.empty:
                    p0 = pvals.iloc[0]
                    if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
                        for c in g.columns:
                            if c.lower() == 'text12':
                                vals = g[c].dropna().astype(str).map(lambda x: x.strip())
                                employee_id = _pick_first_non_guid_value(vals)
                                if employee_id:
                                    break
            except Exception:
                pass

        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]

        candidate_name_vals = None
        if name_col and name_col in g.columns:
            candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
        elif 'EmployeeName' in g.columns:
            candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
        elif 'ObjectName1' in g.columns:
            candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

        if candidate_name_vals is not None and not candidate_name_vals.empty:
            employee_name = _pick_first_non_guid_value(candidate_name_vals)
            if employee_name is None:
                for v in candidate_name_vals:
                    if _looks_like_name(v) and not _is_placeholder_str(v):
                        employee_name = str(v).strip()
                        break

        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        first_swipe = None
        last_swipe = None
        if times:
            first_swipe = times[0]
            last_swipe = times[-1]

        # timeline & segments with mapping to zones
        timeline = []
        for _, row in g.sort_values('LocaleMessageTime').iterrows():
            t = row.get('LocaleMessageTime')
            dname = None
            if door_col and door_col in row and pd.notna(row.get(door_col)):
                dname = row.get(door_col)
            elif 'Door' in row and pd.notna(row.get('Door')):
                dname = row.get('Door')
            direction = None
            if dir_col and dir_col in row and pd.notna(row.get(dir_col)):
                direction = row.get(dir_col)
            elif 'Direction' in row and pd.notna(row.get('Direction')):
                direction = row.get('Direction')
            zone = map_door_to_zone(dname, direction)
            timeline.append((t, dname, direction, zone))

        segments = []
        if timeline:
            cur_zone = None
            seg_start = timeline[0][0]
            seg_label = None
            for (t, dname, direction, zone) in timeline:
                if zone in _BREAK_ZONES:
                    lbl = 'break'
                elif zone == _OUT_OF_OFFICE_ZONE:
                    lbl = 'out_of_office'
                else:
                    lbl = 'work'
                if cur_zone is None:
                    cur_zone = zone
                    seg_label = lbl
                    seg_start = t
                else:
                    if lbl != seg_label:
                        segments.append({
                            'label': seg_label,
                            'start': seg_start,
                            'end': t,
                            'start_zone': cur_zone
                        })
                        seg_start = t
                        seg_label = lbl
                        cur_zone = zone
                    else:
                        cur_zone = cur_zone or zone
            if seg_label is not None:
                segments.append({
                    'label': seg_label,
                    'start': seg_start,
                    'end': timeline[-1][0],
                    'start_zone': cur_zone
                })

        break_count = 0
        long_break_count = 0
        total_break_minutes = 0.0

        BREAK_MINUTES_THRESHOLD = 60
        OUT_OFFICE_COUNT_MINUTES = 180
        LONG_BREAK_FLAG_MINUTES = 120

        for i, s in enumerate(segments):
            lbl = s.get('label')
            start = s.get('start')
            end = s.get('end')
            dur_mins = ((end - start).total_seconds() / 60.0) if (start and end) else 0.0
            if lbl == 'break':
                if dur_mins >= BREAK_MINUTES_THRESHOLD:
                    break_count += 1
                    total_break_minutes += dur_mins
                    if dur_mins >= LONG_BREAK_FLAG_MINUTES:
                        long_break_count += 1
            elif lbl == 'out_of_office':
                prev_lbl = segments[i-1]['label'] if i > 0 else None
                next_lbl = segments[i+1]['label'] if i < len(segments)-1 else None
                if prev_lbl == 'work' and next_lbl == 'work' and dur_mins >= OUT_OFFICE_COUNT_MINUTES:
                    break_count += 1
                    total_break_minutes += dur_mins
                    if dur_mins >= LONG_BREAK_FLAG_MINUTES:
                        long_break_count += 1

        pattern_flag = False
        pattern_sequence_readable = None
        try:
            seq = []
            for s in segments:
                dur_mins = (s['end'] - s['start']).total_seconds() / 60.0 if (s['end'] and s['start']) else 0
                seq.append((s['label'], int(round(dur_mins))))
            for i in range(len(seq)-2):
                a = seq[i]
                b = seq[i+1]
                c = seq[i+2]
                if (a[0] == 'work' and a[1] < 60) and \
                   (b[0] in ('out_of_office','break') and b[1] >= LONG_BREAK_FLAG_MINUTES) and \
                   (c[0] == 'work' and c[1] < 60):
                    pattern_flag = True
                    seq_fragment = [a, b, c]
                    pattern_sequence_readable = " -> ".join([f"{lbl} ({mins}m)" for lbl, mins in seq_fragment])
                    break
        except Exception:
            pattern_flag = False
            pattern_sequence_readable = None

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type,
            'FirstSwipe': first_swipe,
            'LastSwipe': last_swipe,
            'BreakCount': int(break_count),
            'LongBreakCount': int(long_break_count),
            'TotalBreakMinutes': float(round(total_break_minutes,1)),
            'PatternShortLongRepeat': bool(pattern_flag),
            'PatternSequenceReadable': pattern_sequence_readable,
            'PatternSequence': None
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])[sel_cols]
    grouped = grouped.apply(agg_swipe_group).reset_index()

    # POST-PROCESS: merge early-morning fragments into previous day (heuristic)
    try:
        grouped['FirstSwipe_dt'] = pd.to_datetime(grouped['FirstSwipe'], errors='coerce')
        grouped['LastSwipe_dt']  = pd.to_datetime(grouped['LastSwipe'],  errors='coerce')
        rows_to_drop = set()
        MERGE_GAP_SECONDS = int(4 * 3600)
        for pid, sub in grouped.sort_values(['person_uid','Date']).groupby('person_uid'):
            prev_idx = None
            for idx, r in sub.reset_index().iterrows():
                real_idx = int(r['index']) if 'index' in r else r.name
                cur_first = pd.to_datetime(grouped.at[real_idx, 'FirstSwipe_dt'])
                if prev_idx is not None:
                    prev_last = pd.to_datetime(grouped.at[prev_idx, 'LastSwipe_dt'])
                    if (not pd.isna(cur_first)) and (not pd.isna(prev_last)):
                        gap = (cur_first - prev_last).total_seconds()
                        if 0 <= gap <= MERGE_GAP_SECONDS and cur_first.time().hour < 2:
                            try:
                                grouped.at[prev_idx, 'CountSwipes'] = int(grouped.at[prev_idx, 'CountSwipes']) + int(grouped.at[real_idx, 'CountSwipes'])
                                grouped.at[prev_idx, 'MaxSwipeGapSeconds'] = max(int(grouped.at[prev_idx, 'MaxSwipeGapSeconds'] or 0), int(grouped.at[real_idx, 'MaxSwipeGapSeconds'] or 0), int(gap))
                                if not pd.isna(grouped.at[real_idx, 'LastSwipe_dt']):
                                    if pd.isna(grouped.at[prev_idx, 'LastSwipe_dt']) or grouped.at[real_idx, 'LastSwipe_dt'] > grouped.at[prev_idx, 'LastSwipe_dt']:
                                        grouped.at[prev_idx, 'LastSwipe_dt'] = grouped.at[real_idx, 'LastSwipe_dt']
                                        grouped.at[prev_idx, 'LastSwipe'] = grouped.at[real_idx, 'LastSwipe']
                                if not grouped.at[prev_idx, 'CardNumber']:
                                    grouped.at[prev_idx, 'CardNumber'] = grouped.at[real_idx, 'CardNumber']
                                grouped.at[prev_idx, 'UniqueDoors'] = int(max(int(grouped.at[prev_idx].get('UniqueDoors') or 0), int(grouped.at[real_idx].get('UniqueDoors') or 0)))
                                grouped.at[prev_idx, 'UniqueLocations'] = int(max(int(grouped.at[prev_idx].get('UniqueLocations') or 0), int(grouped.at[real_idx].get('UniqueLocations') or 0)))
                                rows_to_drop.add(real_idx)
                                continue
                            except Exception:
                                pass
                prev_idx = real_idx
        if rows_to_drop:
            grouped = grouped.drop(index=list(rows_to_drop)).reset_index(drop=True)
    except Exception:
        logging.exception("Failed merge-early-morning fragments (non-fatal).")

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # coalesce duplicated columns (_x/_y) produced by merge
    def _coalesce_merge_columns(df, bases):
        for base in bases:
            x = base + "_x"
            y = base + "_y"
            try:
                has_base = base in df.columns
                base_all_null = False
                if has_base:
                    base_all_null = df[base].isnull().all()
            except Exception:
                has_base = base in df.columns
                base_all_null = True
            if (not has_base) or base_all_null:
                if x in df.columns and y in df.columns:
                    try:
                        df[base] = df[x].combine_first(df[y])
                    except Exception:
                        try:
                            df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
                        except Exception:
                            if x in df.columns:
                                df[base] = df[x]
                            elif y in df.columns:
                                df[base] = df[y]
                elif x in df.columns:
                    df[base] = df[x]
                elif y in df.columns:
                    df[base] = df[y]
    _coalesce_merge_columns(merged, [
        "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
    ])
    drop_cols = [c for c in merged.columns if c.endswith("_x") or c.endswith("_y")]
    if drop_cols:
        try:
            merged.drop(columns=drop_cols, inplace=True)
        except Exception:
            for c in drop_cols:
                if c in merged.columns:
                    try:
                        merged.drop(columns=[c], inplace=True)
                    except Exception:
                        pass

    # ensure columns exist and normalized
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)
    ensure_col(merged, 'BreakCount', 0)
    ensure_col(merged, 'LongBreakCount', 0)
    ensure_col(merged, 'TotalBreakMinutes', 0.0)
    ensure_col(merged, 'PatternShortLongRepeat', False)
    ensure_col(merged, 'PatternSequenceReadable', None)
    ensure_col(merged, 'PatternSequence', None)

    if 'EmployeeName' in merged.columns:
        def choose_best_name(row):
            gname = row.get('EmployeeName')
            dname = None
            for cand in ('EmployeeName', 'employee_name', 'objectname1', 'ObjectName1'):
                if cand in row and row.get(cand) is not None:
                    dname = row.get(cand)
                    break
            if _looks_like_name(gname):
                return str(gname).strip()
            if _looks_like_name(dname):
                return str(dname).strip()
            if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
                return str(gname).strip()
            if dname and not _is_placeholder_str(dname):
                return str(dname).strip()
            return None
        merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
    else:
        if not dur.empty:
            def fill_name_from_dur(row):
                gname = row.get('EmployeeName')
                if _looks_like_name(gname) and not _is_placeholder_str(gname):
                    return gname
                for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
                    if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
                        return row[cand]
                return None
            merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

    def normalize_empid(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            try:
                if '.' in s:
                    f = float(s)
                    if math.isfinite(f) and f.is_integer():
                        return str(int(f))
            except Exception:
                pass
            return s
        except Exception:
            return None

    merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

    def normalize_card(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            return s
        except Exception:
            return None

    merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

    if 'DurationSeconds' not in merged.columns or merged['DurationSeconds'].isnull().all():
        try:
            merged['DurationSeconds'] = (pd.to_datetime(merged['LastSwipe']) - pd.to_datetime(merged['FirstSwipe'])).dt.total_seconds().clip(lower=0).fillna(0)
        except Exception:
            merged['DurationSeconds'] = merged.get('DurationSeconds', 0)

    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)
    merged['BreakCount'] = merged['BreakCount'].fillna(0).astype(int)
    merged['LongBreakCount'] = merged['LongBreakCount'].fillna(0).astype(int)
    merged['TotalBreakMinutes'] = merged['TotalBreakMinutes'].fillna(0.0).astype(float)
    merged['PatternShortLongRepeat'] = merged['PatternShortLongRepeat'].fillna(False).astype(bool)

    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
        if c in merged.columns:
            def _clean_str_val(v):
                if v is None:
                    return None
                try:
                    s = str(v).strip()
                    if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                        return None
                    return s
                except Exception:
                    return None
            merged[c] = merged[c].apply(_clean_str_val)

    if 'EmployeeName' in merged.columns:
        merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

    return merged

# ---------------- SCENARIO WEIGHTS ----------------
WEIGHTS = {
    "long_gap_>=4.5h": 0.3,
    "short_duration_<4h": 1.0,
    "coffee_badging": 1.0,
    "low_swipe_count_<=2": 0.5,
    "single_door": 0.25,
    "only_in": 0.8,
    "only_out": 0.8,
    "overtime_>=10h": 0.2,
    "very_long_duration_>=16h": 1.5,
    "zero_swipes": 0.4,
    "unusually_high_swipes": 1.5,
    "repeated_short_breaks": 0.5,
    "multiple_location_same_day": 0.6,
    "weekend_activity": 0.6,
    "repeated_rejection_count": 0.8,
    "badge_sharing_suspected": 2.0,
    "early_arrival_before_06": 0.4,
    "late_exit_after_22": 0.4,
    "shift_inconsistency": 1.2,
    "trending_decline": 0.7,
    "consecutive_absent_days": 1.2,
    "high_variance_duration": 0.8,
    "short_duration_on_high_presence_days": 1.1,
    "swipe_overlap": 2.0,
    "high_swipes_benign": 0.1,
    "shortstay_longout_repeat": 2.0
}
ANOMALY_THRESHOLD = 1.5

def _read_past_trend_csvs(outdir: str, window_days: int, target_date: date):
    p = Path(outdir)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return pd.DataFrame()
    dfs = []
    cutoff = target_date - timedelta(days=window_days)
    for fp in csvs:
        try:
            df = pd.read_csv(fp, parse_dates=['Date'])
            if 'Date' in df.columns:
                try:
                    df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                except Exception:
                    pass
                # include target_date in the window (cutoff <= Date <= target_date)
                def _date_in_window(d):
                    try:
                        return d is not None and (d >= cutoff and d <= target_date)
                    except Exception:
                        return False
                df = df[df['Date'].apply(_date_in_window)]
            dfs.append(df)
        except Exception:
            try:
                df = pd.read_csv(fp, dtype=str)
                if 'Date' in df.columns:
                    try:
                        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                        # include target_date in the window (cutoff <= Date <= target_date)
                        def _date_in_window(d):
                            try:
                                return d is not None and (d >= cutoff and d <= target_date)
                            except Exception:
                                return False
                        df = df[df['Date'].apply(_date_in_window)]
                    except Exception:
                        pass
                dfs.append(df)
            except Exception:
                continue
    if not dfs:
        return pd.DataFrame()
    try:
        out = pd.concat(dfs, ignore_index=True)
        return out
    except Exception:
        return pd.DataFrame()

def _read_scenario_counts_by_person(outdir: str, window_days: int, target_date: date, scenario_col: str):
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty or scenario_col not in df.columns:
        return {}
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass
    id_cols = [c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in df.columns]
    out = defaultdict(int)
    q = df[df[scenario_col] == True] if df[scenario_col].dtype == bool else df[df[scenario_col].astype(str).str.lower() == 'true']
    for _, r in q.iterrows():
        for col in id_cols:
            try:
                raw = r.get(col)
                if raw in (None, '', float('nan')):
                    continue
                norm = _normalize_id_val(raw)
                if norm:
                    out[str(norm)] += 1
                    stripped = _strip_uid_prefix(str(norm))
                    if stripped != str(norm):
                        out[str(stripped)] += 1
            except Exception:
                continue
        for fallback in ('Int1', 'Text12'):
            if fallback in r and r.get(fallback) not in (None, '', 'nan'):
                try:
                    norm = _normalize_id_val(r.get(fallback))
                    if norm:
                        out[str(norm)] += 1
                except Exception:
                    continue
    return dict(out)

def _compute_weeks_with_threshold(past_df: pd.DataFrame,
                                  person_col: str = 'person_uid',
                                  date_col: str = 'Date',
                                  scenario_col: str = 'short_duration_<4h',
                                  threshold_days: int = 3) -> dict:
    if past_df is None or past_df.empty:
        return {}
    df = past_df.copy()
    if date_col not in df.columns:
        return {}
    try:
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.date
    except Exception:
        pass
    if scenario_col not in df.columns:
        return {}
    try:
        if df[scenario_col].dtype == bool:
            df['__scenario_flag__'] = df[scenario_col].astype(bool)
        else:
            df['__scenario_flag__'] = df[scenario_col].astype(str).str.strip().str.lower().isin({'true', '1', 'yes', 'y', 't'})
    except Exception:
        df['__scenario_flag__'] = df[scenario_col].apply(lambda v: str(v).strip().lower() in ('true','1','yes','y','t') if v is not None else False)
    df = df[df['__scenario_flag__'] == True].copy()
    if df.empty:
        return {}
    if person_col not in df.columns:
        fallback = next((c for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12') if c in past_df.columns), None)
        if fallback is None:
            return {}
        person_col = fallback
    def _week_monday(d):
        try:
            if d is None or (isinstance(d, float) and np.isnan(d)):
                return None
            iso = d.isocalendar()
            return date.fromisocalendar(iso[0], iso[1], 1)
        except Exception:
            return None
    df['__week_monday__'] = df[date_col].apply(_week_monday)
    df = df.dropna(subset=['__week_monday__', person_col])
    if df.empty:
        return {}
    week_counts = (df.groupby([person_col, '__week_monday__'])
                     .size()
                     .reset_index(name='days_flagged'))
    valid_weeks = week_counts[week_counts['days_flagged'] >= int(threshold_days)].copy()
    if valid_weeks.empty:
        return {}
    person_weeks = {}
    for person, grp in valid_weeks.groupby(person_col):
        wlist = sorted(pd.to_datetime(grp['__week_monday__']).dt.date.unique(), reverse=True)
        person_weeks[str(person)] = wlist
    def _consecutive_week_count(week_dates_desc):
        if not week_dates_desc:
            return 0
        count = 1
        prev = week_dates_desc[0]
        for cur in week_dates_desc[1:]:
            try:
                if (prev - cur).days == 7:
                    count += 1
                    prev = cur
                else:
                    break
            except Exception:
                break
        return count
    out = {}
    for pid, weeks in person_weeks.items():
        c = _consecutive_week_count(weeks)
        out[str(pid)] = int(c)
        try:
            stripped = _strip_uid_prefix(str(pid))
            if stripped and stripped != str(pid):
                out[str(stripped)] = int(c)
        except Exception:
            pass
    return out

def _strip_uid_prefix(s):
    try:
        if s is None:
            return s
        st = str(s)
        for p in ('emp:', 'uid:', 'name:'):
            if st.startswith(p):
                return st[len(p):]
        return st
    except Exception:
        return s

def compute_violation_days_map(outdir: str, window_days: int, target_date: date):
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty:
        return {}
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
        except Exception:
            pass
    id_cols = []
    for c in ('person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber'):
        if c in df.columns:
            id_cols.append(c)
    if 'IsFlagged' not in df.columns:
        if 'AnomalyScore' in df.columns:
            df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: float(s) >= ANOMALY_THRESHOLD if not pd.isna(s) else False)
        else:
            df['IsFlagged'] = False
    ident_dates = defaultdict(set)
    try:
        flagged = df[df['IsFlagged'] == True]
        for _, r in flagged.iterrows():
            d = r.get('Date')
            if d is None:
                continue
            for col in id_cols:
                try:
                    raw = r.get(col)
                    if raw is None:
                        continue
                    norm = _normalize_id_val(raw)
                    if norm:
                        ident_dates[str(norm)].add(d)
                        stripped = _strip_uid_prefix(str(norm))
                        if stripped != str(norm):
                            ident_dates[str(stripped)].add(d)
                except Exception:
                    continue
            for fallback in ('Int1', 'Text12'):
                if fallback in r and r.get(fallback) not in (None, '', 'nan'):
                    try:
                        norm = _normalize_id_val(r.get(fallback))
                        if norm:
                            ident_dates[str(norm)].add(d)
                            stripped = _strip_uid_prefix(str(norm))
                            if stripped != str(norm):
                                ident_dates[str(stripped)].add(d)
                    except Exception:
                        continue
    except Exception:
        logging.exception("Error building violation days map from history.")
    out = {k: int(len(v)) for k, v in ident_dates.items()}
    return out



def score_trends_from_durations(combined_df: pd.DataFrame, swipes_df: Optional[pd.DataFrame] = None, outdir: Optional[str] = None, target_date: Optional[date] = None) -> pd.DataFrame:
    """
    Take combined durations DataFrame and optional swipes DataFrame and compute:
      - scenario boolean columns
      - Reasons (semicolon-separated scenario keys)
      - ViolationExplanation (human text)
      - AnomalyScore (weighted sum)
      - IsFlagged (AnomalyScore >= ANOMALY_THRESHOLD)
      - ViolationDaysLast90 (from history)
      - historical bumps, weekly bump, MonitorFlag, etc.
    """
    if combined_df is None or combined_df.empty:
        return pd.DataFrame()

    df = combined_df.copy()
    # Ensure person_uid exists
    if 'person_uid' not in df.columns:
        df['person_uid'] = df.apply(lambda r: _canonical_person_uid(r), axis=1)

    # Ensure key columns exist
    for c in ['FirstSwipe','LastSwipe','CountSwipes','DurationMinutes','MaxSwipeGapSeconds','EmployeeID','CardNumber','person_uid','Date']:
        if c not in df.columns:
            df[c] = None

    # reconcile zero CountSwipes with raw swipes (if provided)
    if swipes_df is not None and not swipes_df.empty and 'person_uid' in swipes_df.columns:
        tsw = swipes_df.copy()
        # ensure LocaleMessageTime parsed
        if 'LocaleMessageTime' in tsw.columns:
            tsw['LocaleMessageTime'] = pd.to_datetime(tsw['LocaleMessageTime'], errors='coerce')
        else:
            for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                if cand in tsw.columns:
                    tsw['LocaleMessageTime'] = pd.to_datetime(tsw[cand], errors='coerce')
                    break
        if 'Date' not in tsw.columns:
            if 'LocaleMessageTime' in tsw.columns:
                tsw['Date'] = tsw['LocaleMessageTime'].dt.date
            else:
                for cand in ('date','Date'):
                    if cand in tsw.columns:
                        try:
                            tsw['Date'] = pd.to_datetime(tsw[cand], errors='coerce').dt.date
                        except Exception:
                            tsw['Date'] = None
                        break

        try:
            grp = tsw.dropna(subset=['person_uid', 'Date']).groupby(['person_uid', 'Date'])
            counts = grp.size().to_dict()
            firsts = grp['LocaleMessageTime'].min().to_dict()
            lasts = grp['LocaleMessageTime'].max().to_dict()
        except Exception:
            counts = {}
            firsts = {}
            lasts = {}

        def _fix_row_by_raw(idx, row):
            key = (row.get('person_uid'), row.get('Date'))
            if key in counts and (int(row.get('CountSwipes') or 0) == 0 or pd.isna(row.get('CountSwipes'))):
                try:
                    c = int(counts.get(key, 0))
                    df.at[idx, 'CountSwipes'] = c
                    f = firsts.get(key)
                    l = lasts.get(key)
                    if pd.notna(f) and (pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None):
                        df.at[idx, 'FirstSwipe'] = pd.to_datetime(f)
                    if pd.notna(l) and (pd.isna(row.get('LastSwipe')) or row.get('LastSwipe') is None):
                        df.at[idx, 'LastSwipe'] = pd.to_datetime(l)
                    try:
                        fs = df.at[idx, 'FirstSwipe']
                        ls = df.at[idx, 'LastSwipe']
                        if pd.notna(fs) and pd.notna(ls):
                            dursec = (pd.to_datetime(ls) - pd.to_datetime(fs)).total_seconds()
                            dursec = max(0, dursec)
                            df.at[idx, 'DurationSeconds'] = float(dursec)
                            df.at[idx, 'DurationMinutes'] = float(dursec / 60.0)
                    except Exception:
                        pass
                except Exception:
                    pass

        for ix, r in df[df['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
            try:
                _fix_row_by_raw(ix, r)
            except Exception:
                logging.debug("Failed to reconcile row %s with raw swipes", ix)

    # Build badge map and swipe overlap maps for higher-severity scenarios
    badge_map = {}
    swipe_overlap_map = {}
    if swipes_df is not None and not swipes_df.empty:
        try:
            tmp = swipes_df[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
            if not tmp.empty:
                grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
                badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}
        except Exception:
            badge_map = {}

        overlap_window_seconds = 2
        if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes_df.columns):
            try:
                tmp2 = swipes_df[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
                if not tmp2.empty:
                    tmp2 = tmp2.sort_values(['Door', 'LocaleMessageTime'])
                    for (d, door), g in tmp2.groupby(['Date', 'Door']):
                        items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                        n = len(items)
                        for i in range(n):
                            t_i, uid_i = items[i]
                            j = i+1
                            while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                                uid_j = items[j][1]
                                if uid_i != uid_j:
                                    swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                                    swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                                j += 1
            except Exception:
                swipe_overlap_map = {}

    # Evaluate scenarios (use weighting to compute anomaly score)
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            df[name] = df.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
        elif name == "swipe_overlap":
            df[name] = df.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map=swipe_overlap_map), axis=1)
        else:
            df[name] = df.apply(lambda r, f=fn: bool(f(r)), axis=1)

    def compute_score(r):
        score = 0.0
        detected = []
        for name, _ in SCENARIOS:
            val = bool(r.get(name))
            w = WEIGHTS.get(name, 0.0)
            if val and w > 0:
                score += float(w)
                detected.append(name)
        return score, detected

    scores = df.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
    df['AnomalyScore'] = scores['AnomalyScore'].astype(float)
    df['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
    df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

    # PresentToday flag, ViolationDays from history, and weekly adjustments
    try:
        df['PresentToday'] = df['CountSwipes'].fillna(0).astype(int) > 0

        # historical scenario counts (for escalation)
        hist_pattern_counts = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today(), 'shortstay_longout_repeat')
        hist_rep_breaks = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today(), 'repeated_short_breaks')
        hist_short_duration = _read_scenario_counts_by_person(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today(), 'short_duration_<4h')

        def get_hist_count_for_row(row, hist_map):
            for k in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12'):
                if k in row and row.get(k) not in (None, '', float('nan')):
                    try:
                        norm = _normalize_id_val(row.get(k))
                        if norm and str(norm) in hist_map:
                            return int(hist_map.get(str(norm), 0))
                        stripped = _strip_uid_prefix(str(norm)) if norm else None
                        if stripped and str(stripped) in hist_map:
                            return int(hist_map.get(str(stripped), 0))
                    except Exception:
                        continue
            return 0

        df['HistPatternShortLongCount90'] = df.apply(lambda r: get_hist_count_for_row(r, hist_pattern_counts), axis=1)
        df['HistRepeatedShortBreakCount90'] = df.apply(lambda r: get_hist_count_for_row(r, hist_rep_breaks), axis=1)
        df['HistShortDurationCount90'] = df.apply(lambda r: get_hist_count_for_row(r, hist_short_duration), axis=1)

        pat_mask = df['HistPatternShortLongCount90'].fillna(0).astype(int) >= 3
        if pat_mask.any():
            df.loc[pat_mask, 'AnomalyScore'] = df.loc[pat_mask, 'AnomalyScore'].astype(float)  # keep value but escalate risk below
            df.loc[pat_mask, 'RiskScore'] = 5
            df.loc[pat_mask, 'RiskLevel'] = 'High'
            df.loc[pat_mask, 'IsFlagged'] = True

        rep_mask = df['HistRepeatedShortBreakCount90'].fillna(0).astype(int) >= 5
        if rep_mask.any():
            df.loc[rep_mask, 'AnomalyScore'] = df.loc[rep_mask, 'AnomalyScore'].astype(float)
            df.loc[rep_mask, 'RiskScore'] = 5
            df.loc[rep_mask, 'RiskLevel'] = 'High'
            df.loc[rep_mask, 'IsFlagged'] = True

        # ViolationDaysLast90
        vmap = compute_violation_days_map(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today())
        def lookup_violation_days(row):
            try:
                candidates = []
                for k in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
                    v = row.get(k)
                    if v not in (None, '', float('nan')):
                        candidates.append(_normalize_id_val(v))
                for c in candidates:
                    if c is None:
                        continue
                    if c in vmap:
                        return int(vmap.get(c, 0))
                    stripped = _strip_uid_prefix(c)
                    if stripped != c and stripped in vmap:
                        return int(vmap.get(stripped, 0))
                return 0
            except Exception:
                return 0
        df['ViolationDaysLast90'] = df.apply(lookup_violation_days, axis=1)

        # Append monitoring note for persons who have past violations and are present today.
        def _append_monitor_note(idx, row):
            try:
                vd = int(row.get('ViolationDaysLast90') or 0)
            except Exception:
                vd = 0
            if vd <= 0:
                return row.get('ViolationExplanation') or row.get('Explanation')
            if not row.get('PresentToday', False):
                return row.get('ViolationExplanation') or row.get('Explanation')
            note = f"Note: Previously flagged {vd} time{'s' if vd!=1 else ''} in the last {VIOLATION_WINDOW_DAYS} days — monitor when present today."
            ex = row.get('ViolationExplanation') or row.get('Explanation') or ''
            if ex and not ex.strip().endswith('.'):
                ex = ex.strip() + '.'
            if note in ex:
                return ex
            return (ex + ' ' + note).strip()
        df['ViolationExplanation'] = df.apply(lambda r: _append_monitor_note(r.name, r), axis=1)

        df['MonitorFlag'] = df.apply(lambda r: (int(r.get('ViolationDaysLast90') or 0) > 0) and bool(r.get('PresentToday')), axis=1)

        # Now compute consecutive-week short-duration runs (post scoring)
        past_df = _read_past_trend_csvs(str(outdir) if outdir else str(OUTDIR), VIOLATION_WINDOW_DAYS, target_date if target_date else date.today())
        week_runs = _compute_weeks_with_threshold(past_df, person_col='person_uid', date_col='Date', scenario_col='short_duration_<4h', threshold_days=3)

        def _get_week_run_for_row(r):
            for k in ('person_uid', 'EmployeeID'):
                if k in r and r.get(k):
                    key = str(r.get(k))
                    if key in week_runs:
                        return int(week_runs[key])
                    stripped = _strip_uid_prefix(key)
                    if stripped in week_runs:
                        return int(week_runs[stripped])
            return 0

        df['ConsecWeeksShort4hrs'] = df.apply(_get_week_run_for_row, axis=1)

        # Apply anomaly score bumps now that AnomalyScore exists
        df['AnomalyScore'] = df['AnomalyScore'].astype(float).fillna(0.0)

        mask1 = df['ConsecWeeksShort4hrs'].fillna(0).astype(int) >= 1
        mask2 = df['ConsecWeeksShort4hrs'].fillna(0).astype(int) >= 2

        if mask1.any():
            df.loc[mask1, 'AnomalyScore'] = df.loc[mask1, 'AnomalyScore'].astype(float) + 0.5
        if mask2.any():
            df.loc[mask2, 'AnomalyScore'] = df.loc[mask2, 'AnomalyScore'].astype(float) + 1.0

        # Recompute IsFlagged and RiskLevel after bumping AnomalyScore
        df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

        def _map_risk_after_bump(r):
            score = r.get('AnomalyScore') or 0.0
            bucket, label = map_score_to_label(score)
            return int(bucket), label
        rs2 = df.apply(lambda r: pd.Series(_map_risk_after_bump(r), index=['RiskScore', 'RiskLevel']), axis=1)
        df['RiskScore'] = rs2['RiskScore']
        df['RiskLevel'] = rs2['RiskLevel']

        # OVERRIDE: force High risk when ViolationDaysLast90 >= 4
        try:
            high_violation_mask = df['ViolationDaysLast90'] >= 4
            if high_violation_mask.any():
                df.loc[high_violation_mask, 'RiskScore'] = 5
                df.loc[high_violation_mask, 'RiskLevel'] = 'High'
        except Exception:
            pass

    except Exception:
        logging.exception("Failed post-scoring weekly-run / monitoring augmentation.")

    # Build textual Reasons and Explanation (if not already)
    def reasons_for_row(r):
        if not bool(r.get('IsFlagged')):
            return None, None
        ds_raw = r.get('DetectedScenarios')
        if ds_raw:
            ds = [s.strip() for s in ds_raw.split(";") if s and s.strip()]
            explanation = _explain_scenarios_detected(r, ds)
            reasons_codes = "; ".join(ds) if ds else None
            return reasons_codes, explanation
        return None, None

    reason_tuples = df.apply(lambda r: pd.Series(reasons_for_row(r), index=['Reasons', 'ViolationExplanation']), axis=1)
    def _sanitize_reason_val(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == "" or _is_placeholder_str(s):
                return None
            return s
        except Exception:
            return None

    df['Reasons'] = reason_tuples['Reasons'].apply(_sanitize_reason_val)
    df['ViolationExplanation'] = reason_tuples['ViolationExplanation'].apply(lambda v: None if _is_placeholder_str(v) else (str(v).strip() if v is not None else None))

    # If flagged but no Reasons, ensure fallback
    def _ensure_reason_for_flagged(row):
        if bool(row.get('IsFlagged')) and (row.get('Reasons') is None or row.get('Reasons') == ''):
            ds = row.get('DetectedScenarios')
            if ds and not _is_placeholder_str(ds):
                parts = [p.strip() for p in re.split(r'[;,\|]', str(ds)) if p and not _is_placeholder_str(p)]
                if parts:
                    return "; ".join(parts)
            if int(row.get('ConsecWeeksShort4hrs') or 0) >= 1:
                return "consecutive_short_weeks"
            if int(row.get('ViolationDaysLast90') or 0) > 0:
                return "historical_monitoring"
            return None
        return row.get('Reasons')

    if 'IsFlagged' in df.columns:
        df['Reasons'] = df.apply(_ensure_reason_for_flagged, axis=1)
    else:
        df['Reasons'] = df['Reasons'].apply(_sanitize_reason_val)

    if 'OverlapWith' not in df.columns:
        def overlap_with_fn(r):
            d = r.get('Date')
            uid = r.get('person_uid')
            if (d, uid) in swipe_overlap_map:
                return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
            return None
        df['OverlapWith'] = df.apply(overlap_with_fn, axis=1)

    # ensure booleans native
    for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
        if col in df.columns:
            df[col] = df[col].astype(bool)

    # return DataFrame
    return df

def run_trend_for_date(target_date: date,
                       regions: Optional[List[str]] = None,
                       outdir: str = None,
                       city: str = "pune",
                       as_dict: bool = False) -> pd.DataFrame:
    """
    Enhanced run_trend_for_date: collects swipes + durations, computes features and scores.
    Mirrors original Pune-focused logic (including 02:00 boundary) for compatibility.
    """
    city_slug = _slug_city(city)
    if regions is None:
        try:
            regions = list(REGION_CONFIG.keys()) if isinstance(REGION_CONFIG, dict) and REGION_CONFIG else []
        except Exception:
            regions = []
    regions = [r.lower() for r in regions if r]
    outdir_path = Path(outdir) if outdir else OUTDIR

    # fetch durations & swipes per region
    if run_for_date is None:
        raise RuntimeError("duration_report.run_for_date is not available in this environment.")
    try:
        results = run_for_date(target_date, regions, str(outdir_path), city)
    except TypeError:
        try:
            results = run_for_date(target_date, regions, str(outdir_path))
        except Exception:
            results = run_for_date(target_date)

    # combine durations and swipes across regions
    dur_list = []
    swipe_list = []
    for rkey, rr in (results or {}).items():
        try:
            dfdur = rr.get('durations')
            if dfdur is not None and not dfdur.empty:
                dfdur = dfdur.copy()
                dfdur['region'] = rkey
                dur_list.append(dfdur)
        except Exception:
            continue
        try:
            dfsw = rr.get('swipes')
            if dfsw is not None and not dfsw.empty:
                dfcopy = dfsw.copy()
                dfcopy['region'] = rkey
                swipe_list.append(dfcopy)
        except Exception:
            continue

    combined = pd.concat(dur_list, ignore_index=True) if dur_list else pd.DataFrame()
    sw_combined = pd.concat(swipe_list, ignore_index=True) if swipe_list else pd.DataFrame()

    # save raw swipes for evidence (full raw) — keep original timestamps in saved raw file
    try:
        if sw_combined is not None and not sw_combined.empty:
            sw_out = Path(outdir_path) / f"swipes_{city_slug}_{target_date.strftime('%Y%m%d')}.csv"
            sw_combined.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception as e:
        logging.warning("Failed to save raw swipes: %s", e)

    # --- Day boundary handling: Pune 02:00 shift (backwards-compatible) ---
    use_pune_2am_boundary = False
    try:
        if city and isinstance(city, str) and 'pun' in city.strip().lower():
            use_pune_2am_boundary = True
        else:
            if os.getenv("PUNE_2AM_BOUNDARY", "0") == "1":
                use_pune_2am_boundary = True
    except Exception:
        use_pune_2am_boundary = False

    sw_for_features = sw_combined.copy() if sw_combined is not None else pd.DataFrame()
    durations_for_features = combined.copy() if combined is not None else pd.DataFrame()

    if use_pune_2am_boundary and (sw_for_features is not None) and (not sw_for_features.empty):
        try:
            if 'LocaleMessageTime' in sw_for_features.columns:
                sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features['LocaleMessageTime'], errors='coerce')
            else:
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                    if cand in sw_for_features.columns:
                        sw_for_features['LocaleMessageTime'] = pd.to_datetime(sw_for_features[cand], errors='coerce')
                        break

            # PRESERVE original times for evidence + later matching
            sw_for_features['OriginalLocaleMessageTime'] = sw_for_features['LocaleMessageTime']

            # shift timestamps backward by 2 hours so the 'date' computed from them moves the day boundary to 02:00
            sw_for_features['LocaleMessageTime'] = sw_for_features['LocaleMessageTime'] - pd.Timedelta(hours=2)

            # recompute durations using shifted swipes so FirstSwipe/LastSwipe/Duration match 02:00-day boundary
            try:
                if callable(compute_daily_durations):
                    durations_for_features = compute_daily_durations(sw_for_features)
                else:
                    durations_for_features = combined.copy() if combined is not None else pd.DataFrame()
            except Exception:
                logging.exception("Failed to recompute durations for Pune 2AM boundary; falling back to original durations.")
                durations_for_features = combined.copy() if combined is not None else pd.DataFrame()

            # For debugging/evidence linking: optionally write shifted-version of raw swipes
            try:
                sw_shifted_out = Path(outdir_path) / f"swipes_{city_slug}_{target_date.strftime('%Y%m%d')}_shifted.csv"
                cols_keep = [c for c in sw_for_features.columns if c in ('person_uid','EmployeeID','CardNumber','LocaleMessageTime','OriginalLocaleMessageTime','Door','Direction','PartitionName2')]
                sw_for_features[cols_keep].to_csv(sw_shifted_out, index=False)
            except Exception:
                logging.debug("Could not write shifted swipes evidence file (non-fatal).")
        except Exception:
            logging.exception("Failed preparing shifted swipes for Pune 2AM logic; using original swipes/durations.")
            sw_for_features = sw_combined.copy() if sw_combined is not None else pd.DataFrame()
            durations_for_features = combined.copy() if combined is not None else pd.DataFrame()

    # compute features using possibly-shifted data (so grouping uses 02:00 boundary for Pune)
    features = compute_features(sw_for_features, durations_for_features)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        if as_dict:
            return {'rows': 0, 'flagged_rows': 0, 'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': 0}
        return pd.DataFrame()

    # Provide an explicit DisplayDate/AdjustedDate for frontend:
    try:
        if 'FirstSwipe' in features.columns:
            features['DisplayDate'] = pd.to_datetime(features['FirstSwipe'], errors='coerce').dt.date
        else:
            features['DisplayDate'] = features['Date']
    except Exception:
        features['DisplayDate'] = features.get('Date', None)

    # If we used a shifted timeline for grouping (Pune), restore FirstSwipe/LastSwipe displayed times by adding 2 hours
    if use_pune_2am_boundary:
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in features.columns:
                try:
                    features[dtcol] = pd.to_datetime(features[dtcol], errors='coerce') + pd.Timedelta(hours=2)
                except Exception:
                    def _add2h_safe(v):
                        try:
                            t = pd.to_datetime(v, errors='coerce')
                            if pd.isna(t):
                                return v
                            return t + pd.Timedelta(hours=2)
                        except Exception:
                            return v
                    features[dtcol] = features[dtcol].apply(_add2h_safe)

    # Merge features with durations (prefer features where present)
    try:
        merged = pd.merge(features, combined, how='left', on=['person_uid', 'Date'], suffixes=('_feat', '_dur'))
    except Exception:
        merged = features

    # Score and enrich merged
    trend_df = score_trends_from_durations(merged, swipes_df=sw_combined, outdir=str(outdir_path), target_date=target_date)

    # write CSV with native types (Pune-compatible filename)
    try:
        write_df = trend_df.copy()
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        write_df = write_df.where(pd.notnull(write_df), None)
        out_csv = Path(outdir_path) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception:
        logging.exception("Failed to write trend CSV")

    # Format DisplayDate (ISO) for output if present
    try:
        if 'DisplayDate' in trend_df.columns:
            trend_df['DisplayDate'] = pd.to_datetime(trend_df['DisplayDate'], errors='coerce').dt.date
            trend_df['DisplayDate'] = trend_df['DisplayDate'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
    except Exception:
        pass

    # Return dict-compatible summary if requested
    if as_dict:
        try:
            rec_df = trend_df.copy()
            for dtcol in ('FirstSwipe', 'LastSwipe'):
                if dtcol in rec_df.columns:
                    rec_df[dtcol] = pd.to_datetime(rec_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
            if 'Date' in rec_df.columns:
                try:
                    rec_df['Date'] = pd.to_datetime(rec_df['Date'], errors='coerce').dt.date
                    rec_df['Date'] = rec_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
                except Exception:
                    pass
            rec_df = rec_df.where(pd.notnull(rec_df), None)
            total_rows = int(len(rec_df))
            flagged_rows = int(rec_df['IsFlagged'].sum()) if 'IsFlagged' in rec_df.columns else 0
            reasons_count = {}
            risk_counts = {}
            try:
                if 'Reasons' in rec_df.columns:
                    for v in rec_df['Reasons'].dropna().astype(str):
                        for part in re.split(r'[;,\|]', v):
                            key = part.strip()
                            if key:
                                reasons_count[key] = reasons_count.get(key, 0) + 1
                if 'RiskLevel' in rec_df.columns:
                    for v in rec_df['RiskLevel'].fillna('').astype(str):
                        if v:
                            risk_counts[v] = risk_counts.get(v, 0) + 1
            except Exception:
                pass
            sample_records = rec_df.head(20).to_dict(orient='records') if not rec_df.empty else []
            return {
                'rows': total_rows,
                'flagged_rows': flagged_rows,
                'aggregated_unique_persons': total_rows,
                'sample': sample_records,
                'reasons_count': reasons_count,
                'risk_counts': risk_counts,
                'files': []
            }
        except Exception:
            logging.exception("Failed to build dict output for run_trend_for_date")
            return {'rows': len(trend_df), 'flagged_rows': int(trend_df['IsFlagged'].sum() if 'IsFlagged' in trend_df.columns else 0),
                    'sample': [], 'reasons_count': {}, 'risk_counts': {}, 'aggregated_unique_persons': len(trend_df)}

    return trend_df



# ---------------- helper wrappers ----------------
def _ensure_date_obj(d):
    if d is None:
        return None
    if isinstance(d, date):
        return d
    if isinstance(d, _datetime):
        return d.date()
    if isinstance(d, str):
        try:
            return _datetime.strptime(d, "%Y-%m-%d").date()
        except Exception:
            try:
                return _datetime.fromisoformat(d).date()
            except Exception:
                raise ValueError(f"Unsupported date string: {d}")
    raise ValueError(f"Unsupported date type: {type(d)}")

def build_monthly_training(start_date=None, end_date=None, outdir: str = None, city: str = 'Pune', as_dict: bool = False):
    od = Path(outdir) if outdir else OUTDIR
    od.mkdir(parents=True, exist_ok=True)
    if start_date is None and end_date is None:
        today = date.today()
        first = date(today.year, today.month, 1)
        last = date(today.year, today.month, calendar.monthrange(today.year, today.month)[1])
    else:
        if start_date is None:
            raise ValueError("start_date must be provided when end_date is provided")
        first = _ensure_date_obj(start_date)
        if end_date is None:
            last = date(first.year, first.month, calendar.monthrange(first.year, first.month)[1])
        else:
            last = _ensure_date_obj(end_date)
    if last < first:
        raise ValueError("end_date must be >= start_date")
    cur = first
    ran = []
    errors = {}
    total_flagged = 0
    total_rows = 0
    while cur <= last:
        try:
            logging.info("build_monthly_training: running for %s (city=%s)", cur.isoformat(), city)
            res = run_trend_for_date(cur, outdir=str(od), city=city, as_dict=as_dict)
            ran.append({'date': cur.isoformat(), 'result': res})
            if isinstance(res, dict):
                total_flagged += int(res.get('flagged_rows', 0) or 0)
                total_rows += int(res.get('rows', 0) or 0)
        except KeyboardInterrupt:
            raise
        except Exception as e:
            logging.exception("build_monthly_training: failed for %s", cur)
            errors[cur.isoformat()] = str(e)
        cur = cur + _timedelta(days=1)
    summary = {
        'start_date': first.isoformat(),
        'end_date': last.isoformat(),
        'dates_attempted': (last - first).days + 1,
        'dates_succeeded': len([r for r in ran if r.get('result') is not None]),
        'dates_failed': len(errors),
        'errors': errors,
        'total_rows': total_rows,
        'total_flagged': total_flagged
    }
    if as_dict:
        return summary
    return ran

def read_90day_cache(outdir: str = None):
    od = Path(outdir) if outdir else OUTDIR
    fp = od / "90day_cache.json"
    if not fp.exists():
        return {}
    try:
        with fp.open("r", encoding="utf8") as fh:
            return json.load(fh)
    except Exception:
        logging.exception("read_90day_cache: failed to read %s", str(fp))
        return {}

if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today, as_dict=False)
    print("Completed; rows:", len(df) if df is not None else 0)










# backend/duration_report.py
from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name (override with environment variable if needed)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
try:
    from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    # fallback — keep behaviour if config file unavailable
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

    def map_door_to_zone(door: object, direction: object = None) -> str:
        """
        Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
        (This is only used if config.door_zone can't be imported.)
        """
        try:
            if door is None:
                return None
            s = str(door).strip()
            if not s:
                return None
            s_l = s.lower()
            # fallback: direction-based inference
            if direction and isinstance(direction, str):
                d = direction.strip().lower()
                if "out" in d:
                    return OUT_OF_OFFICE_ZONE
                if "in" in d:
                    # assume reception/working
                    return "Reception Area"
            # heuristic fallback
            if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
                return OUT_OF_OFFICE_ZONE
            # else treat as working area
            return "Working Area"
        except Exception:
            return None

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "SG.Singapore", "PH.Manila","IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023"
        ],
        "partitions": ["LT.Vilnius","IT.Rome","UK.London","IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition","PA.Panama City","PE.Lima", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        # NOTE: these should match the PartitionName2 values used in DB query WHERE clause.
        # keep as DB-side partition names (example provided earlier used things like 'US.CO.OBS', 'USA/Canada Default', etc).
        "partitions": ["US.CO.OBS", "USA/Canada Default", "US.FL.Miami", "US.NYC"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]  # kept for reference but not used for filtering now
    }
}


# --- Note: Added AdjustedMessageTime into the generic template, date condition templated as {date_condition}
# Also added a hard filter: only PersonnelType = 'Employee' (push filtering to DB).
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND t3.[Name] = 'Employee'
  {date_condition}
  {region_filter}
"""

# Helpers (must exist before build_region_query)
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        # updated: use explicit PartitionName2 list for NAMER (matches the SQL example you provided)
        partitions = rc.get("partitions", [])
        if partitions:
            parts_sql = ", ".join(f"'{p}'" for p in partitions)
            region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
        else:
            # fallback to the old logical_like behaviour (kept for safety)
            likes = rc.get("logical_like", [])
            if likes:
                like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
                region_filter = f"AND ({like_sql})"
            else:
                region_filter = ""
    else:
        region_filter = ""

    # Build date_condition:
    # - For APAC we must fetch LocaleMessageTime rows for target_date AND target_date + 1 day
    #   because AdjustedMessageTime = LocaleMessageTime - 2 hours; some swipes near midnight on next day
    #   belong to the shifted date group.
    if region_key == "apac":
        next_date_str = (target_date + timedelta(days=1)).strftime("%Y-%m-%d")
        date_condition = (
            "AND (CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d1}' "
            "OR CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{d2}')"
            .format(d1=date_str, d2=next_date_str)
        )
    else:
        date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql
    
# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    # use first database in list if present
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
    "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
    "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
    "Direction", "CompanyName", "PrimaryLocation"
]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    # ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Dates parsing
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    try:
        if "AdjustedMessageTime" in df.columns:
            df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
        else:
            df["AdjustedMessageTime"] = pd.NaT
    except Exception:
        df["AdjustedMessageTime"] = pd.to_datetime(df.get("AdjustedMessageTime").astype(str), errors="coerce") if "AdjustedMessageTime" in df.columns else pd.NaT

    # defensive: make text fields strings (avoid object type surprises)
    for tcol in ("Door", "PartitionName2", "PersonnelTypeName", "EmployeeName", "CompanyName", "PrimaryLocation"):
        if tcol in df.columns:
            df[tcol] = df[tcol].fillna("").astype(str)

    # Filter: only Employees (defensive; the SQL template already requests t3.Name = 'Employee')
    try:
        if "PersonnelTypeName" in df.columns:
            df = df[df["PersonnelTypeName"].str.strip().str.lower() == "employee"].copy()
    except Exception:
        # If PersonnelTypeName missing or unexpected, keep df as-is but log
        logging.debug("Could not apply PersonnelTypeName filter for region %s", region_key)

    # maintain person_uid same as compute logic
    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)

    # ----- REGION SPECIFIC NORMALIZATIONS -----
    # APAC: map PartitionName2/Door patterns to friendly names:
    if region_key == "apac" and not df.empty:
        def normalize_apac_partition(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part or ""
            # Door-based mappings
            if "APAC_PI" in d or d.startswith("APAC_PI"):
                return "Taguig City"
            if "APAC_PH" in d or d.startswith("APAC_PH"):
                return "Quezon City"
            # Pune detection (APAC.Default or PUN in door/partition)
            if "PUN" in d or "PUNE" in d or ("APAC.DEFAULT" in p.upper() or p.upper().strip() == "APAC.DEFAULT"):
                return "Pune"
            if "APAC_MY" in d or "MY.KUALA" in p.upper() or "KUALA" in d:
                return "MY.Kuala Lumpur"
            if "IN.HYD" in p.upper() or "HYD" in d:
                return "IN.HYD"
            if "SG.SINGAPORE" in p or "SINGAPORE" in d:
                return "SG.Singapore"
            # fallback to existing PartitionName2
            return part
        df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)

    # NAMER: normalize PartitionName2 and add LogicalLocation per the SQL example you provided
    if region_key == "namer" and not df.empty:
        def namer_partition_and_logical(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()
            normalized = part
            logical = "Other"

            if ("US.CO.HQ" in d) or ("HQ" in d and "HQ" in d[:20]) or ("DENVER" in d) or (p == "US.CO.OBS"):
                normalized = "US.CO.OBS"
                logical = "Denver-HQ"
            elif "AUSTIN" in d or "AUSTIN TX" in d or p == "USA/CANADA DEFAULT":
                normalized = "USA/Canada Default"
                logical = "Austin Texas"
            elif "MIAMI" in d or p == "US.FL.MIAMI":
                normalized = "US.FL.Miami"
                logical = "Miami"
            elif "NYC" in d or "NEW YORK" in d or p == "US.NYC":
                normalized = "US.NYC"
                logical = "New York"
            else:
                # fallback mappings for common PartitionName2 values
                if p == "US.CO.OBS":
                    normalized = "US.CO.OBS"; logical = "Denver-HQ"
                elif p == "USA/CANADA DEFAULT":
                    normalized = "USA/Canada Default"; logical = "Austin Texas"
                elif p == "US.FL.MIAMI":
                    normalized = "US.FL.Miami"; logical = "Miami"
                elif p == "US.NYC":
                    normalized = "US.NYC"; logical = "New York"
                else:
                    normalized = part
                    logical = "Other"
            return pd.Series({"PartitionName2": normalized, "LogicalLocation": logical})

        mapped = df.apply(namer_partition_and_logical, axis=1)
        df["PartitionName2"] = mapped["PartitionName2"].astype(str)
        df["LogicalLocation"] = mapped["LogicalLocation"].astype(str)

    # ensure PartitionName2 column exists as string
    if "PartitionName2" not in df.columns:
        df["PartitionName2"] = ""

    # ensure LogicalLocation exists (maybe empty for non-NAMER rows)
    if "LogicalLocation" not in df.columns:
        df["LogicalLocation"] = ""

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]


# ---------------------------------------------------------------------
# compute_daily_durations
# (restored implementation — used by run_for_date and trend_runner)
# ---------------------------------------------------------------------
# def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "AdjustedMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes if present
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    if "AdjustedMessageTime" in df.columns and df["AdjustedMessageTime"].dtype == object:
        df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")

    # drop exact duplicates (defensive)
    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    try:
        df = df.drop_duplicates(subset=dedupe_cols, keep="first")
    except Exception:
        # if columns not present as expected, fallback to no-dedupe
        pass

    # Date assignment:
    # - For Pune (PartitionName2 == 'APAC.Default') and if AdjustedMessageTime exists, use adjusted date (shifted date)
    # - Otherwise use LocaleMessageTime.date()
    try:
        df["Date"] = df["LocaleMessageTime"].dt.date
        mask = (df.get("PartitionName2") == "APAC.Default") & (pd.notna(df.get("AdjustedMessageTime")))
        if mask.any():
            df.loc[mask, "Date"] = df.loc[mask, "AdjustedMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure a person_uid column (compat with other code)
    df["person_uid"] = df.apply(
        lambda row: row.get("person_uid")
        if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
        else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
        axis=1
    )
    df = df[df["person_uid"].notna()].copy()

    # Group and aggregate
    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", "first"),
            EmployeeName=("EmployeeName", "first"),
            CardNumber=("CardNumber", "first"),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        # fallback groupby implementation
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": first.get("EmployeeID"),
                "EmployeeName": first.get("EmployeeName"),
                "CardNumber": first.get("CardNumber"),
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(
        lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    )

    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    return grouped[out_cols]

def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "AdjustedMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes if present
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    if "AdjustedMessageTime" in df.columns and df["AdjustedMessageTime"].dtype == object:
        df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")

    # drop exact duplicates (defensive)
    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    try:
        df = df.drop_duplicates(subset=dedupe_cols, keep="first")
    except Exception:
        pass

    # Date assignment:
    # - For Pune rows we want to apply the AdjustedMessageTime (2:00 AM shifted boundary).
    # - Detect Pune robustly (PartitionName2 or PrimaryLocation or Door containing 'pun'/'pune').
    try:
        # default: use LocaleMessageTime date
        df["Date"] = df["LocaleMessageTime"].dt.date

        # detect Pune rows
        def _is_pune_row(r):
            try:
                part = str(r.get("PartitionName2") or "").strip().lower()
                ploc = str(r.get("PrimaryLocation") or "").strip().lower()
                door = str(r.get("Door") or "").strip().lower()
                if "pun" in part or "pune" in part:
                    return True
                if "pun" in ploc or "pune" in ploc:
                    return True
                if "pun" in door or "pune" in door:
                    return True
                return False
            except Exception:
                return False

        if "AdjustedMessageTime" in df.columns:
            mask_pune = df.apply(_is_pune_row, axis=1) & df["AdjustedMessageTime"].notna()
            if mask_pune.any():
                df.loc[mask_pune, "Date"] = df.loc[mask_pune, "AdjustedMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure a person_uid column (compat with other code)
    df["person_uid"] = df.apply(
        lambda row: row.get("person_uid")
        if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
        else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
        axis=1
    )
    df = df[df["person_uid"].notna()].copy()

    # Group and aggregate
    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", "first"),
            EmployeeName=("EmployeeName", "first"),
            CardNumber=("CardNumber", "first"),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": first.get("EmployeeID"),
                "EmployeeName": first.get("EmployeeName"),
                "CardNumber": first.get("CardNumber"),
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(
        lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    )

    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    return grouped[out_cols]

# ---------------------------------------------------------------------
# run_for_date (already present in your newer file; kept and wired to compute_daily_durations)
# ---------------------------------------------------------------------
def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    """
    Fetch swipes for each region, compute durations and write CSV files.
    Returns a dict keyed by lower-case region name:
      { 'apac': {'swipes': DataFrame, 'durations': DataFrame}, ... }
    """
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        if not r:
            continue
        rkey = r.lower()
        if rkey not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", rkey, target_date)
        try:
            swipes = fetch_swipes_for_region(rkey, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", rkey)
            swipes = pd.DataFrame()

        # optional city filter (defensive): replicate existing behavior used elsewhere
        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                before = len(swipes)
                swipes = swipes[combined_mask].copy()
                logging.info("City filter '%s' applied for region %s: rows before=%d after=%d", city, rkey, before, len(swipes))
            else:
                logging.warning("City filter requested (%s) but no location columns present in swipes for region %s; skipping city filter", city, rkey)

        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", rkey)
            durations = pd.DataFrame()

        # write outputs for visibility/compatibility with existing pipeline
        try:
            csv_path = outdir_path / f"{rkey}_duration_{target_date.strftime('%Y%m%d')}.csv"
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", rkey)
        try:
            swipes_csv_path = outdir_path / f"{rkey}_swipes_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", rkey)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", rkey, csv_path if 'csv_path' in locals() else '<unknown>', len(durations) if durations is not None else 0)
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", rkey, swipes_csv_path if 'swipes_csv_path' in locals() else '<unknown>', len(swipes) if swipes is not None else 0)

        results[rkey] = {"swipes": swipes, "durations": durations}

    return results


# end of file
















