http://localhost:8002/run?date=2025-10-30


 {
      "AnomalyScore": 1.5,
      "BreakCount": 5,
      "CardNumber": "608470",
      "CompanyName": "WU Srvcs India Private Ltd",
      "CountSwipes": 15,
      "Date": "2025-10-30",
      "DetectedScenarios": "short_duration_<4h; repeated_short_breaks",
      "Duration": "3:00:29",
      "DurationMinutes": 180.48333333333332,
      "DurationSeconds": 10829.0,
      "EmpHistoryPresent": false,
      "EmployeeID": "318542",
      "EmployeeIdentity": "18BDC51C-FA3E-4FAC-A202-69C1017DD727",
      "EmployeeName": "Marothe, Manisha",
      "Explanation": "Marothe, Manisha - Short total presence (<4 hours) \u2014 may indicate short stay or partial day attendance.; Many short gaps between swipes \u2014 repeated short breaks pattern.",
      "FirstDirection": "InDirection",
      "FirstDoor": "APAC_IN_PUN_PODIUM_P-1 TURNSTILE 3-DOOR",
      "FirstSwipe": "2025-10-30T09:53:30",
      "InCount": 9,
      "IsFlagged": true,
      "LastDirection": "OutDirection",
      "LastDoor": "APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2 -OUT DOOR",
      "LastSwipe": "2025-10-30T12:53:59",
      "LongBreakCount": 0,
      "MaxSwipeGapSeconds": 3088,
      "OnlyIn": 0,
      "OnlyOut": 0,
      "OutCount": 6,
      "OverlapWith": null,
      "PartitionName2": "APAC.Default",
      "PatternSequence": null,
      "PatternSequenceReadable": null,
      "PatternShortLongRepeat": false,
      "PersonnelType": "Employee",
      "PersonnelTypeName": "Employee",
      "PrimaryLocation": "Pune - Business Bay",
      "Reasons": "short_duration_<4h; repeated_short_breaks",
      "RejectionCount": 0,
      "RiskLevel": "Low Medium",
      "RiskScore": 2,
      "ShortGapCount": 8,
      "SingleDoor": 0,
      "TotalBreakMinutes": 93.9,
      "UniqueDoors": 6,
      "UniqueLocations": 1,
      "ViolationDaysLast90": 0,
      "badge_sharing_suspected": false,
      "coffee_badging": false,
      "consecutive_absent_days": false,
      "early_arrival_before_06": false,
      "high_variance_duration": false,
      "late_exit_after_22": false,
      "long_gap_>=90min": false,
      "low_swipe_count_<=2": false,
      "multiple_location_same_day": false,
      "only_in": false,
      "only_out": false,
      "overtime_>=10h": false,
      "person_uid": "18BDC51C-FA3E-4FAC-A202-69C1017DD727",
      "repeated_rejection_count": false,
      "repeated_short_breaks": true,
      "shift_inconsistency": false,
      "short_duration_<4h": true,
      "short_duration_on_high_presence_days": false,
      "shortstay_longout_repeat": false,
      "single_door": false,
      "swipe_overlap": false,
      "trending_decline": false,
      "unusually_high_swipes": false,
      "very_long_duration_>=16h": false,
      "weekend_activity": false,
      "zero_swipes": false
    },





Now Explin me Abobve API responce ..
I have review Responce 
   "BreakCount": 5,
Here System Display Breakcount 5 



Marothe, Manisha	318542	608470	2025-10-30	09:53:30	00:00:00	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 3-DOOR	InDirection	Reception Area	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	09:54:20	00:00:50	APAC_IN_PUN_PODIUM_ORANGE_RECEPTION ENTRY-DOOR	InDirection	Orange Zone	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	09:59:16	00:04:56	APAC_IN_PUN_PODIUM_ST3_DOOR 1 (ORANGE)	InDirection	Orange Zone	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	10:11:57	00:12:41	APAC_IN_PUN_PODIUM_ST3_DOOR 1 (ORANGE)	InDirection	Orange Zone	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	10:12:43	00:00:46	APAC_IN_PUN_PODIUM_ORANGE_RECEPTION ENTRY-DOOR	OutDirection	Reception Area	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	10:16:46	00:04:03	APAC_IN_PUN_PODIUM_RED_RECEPTION TO WS ENTRY 1-DOOR NEW	InDirection	Red Zone	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	10:17:05	00:00:19	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	OutDirection	Out of office	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	11:08:33	00:51:28	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	InDirection	Red Zone	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	11:08:53	00:00:20	APAC_IN_PUN_PODIUM_RED_RECEPTION TO WS ENTRY 1-DOOR NEW	OutDirection	Reception Area	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	11:09:14	00:00:21	APAC_IN_PUN_PODIUM_ORANGE_RECEPTION ENTRY-DOOR	InDirection	Orange Zone	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	11:17:38	00:08:24	APAC_IN_PUN_PODIUM_ST3_DOOR 1 (ORANGE)	InDirection	Orange Zone	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	11:59:32	00:41:54	APAC_IN_PUN_PODIUM_ORANGE_RECEPTION ENTRY-DOOR	OutDirection	Reception Area	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	12:36:42	00:37:10	APAC_IN_PUN_PODIUM_ORANGE_RECEPTION ENTRY-DOOR	InDirection	Orange Zone	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	12:53:07	00:16:25	APAC_IN_PUN_PODIUM_ORANGE_RECEPTION ENTRY-DOOR	OutDirection	Reception Area	- (swipes_pune_20251030.csv)
Marothe, Manisha	318542	608470	2025-10-30	12:53:59	00:00:52	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2 -OUT DOOR	OutDirection	Out of office	- (swipes_pune_20251030.csv)


When i check manually there is nor Break Area ...

# zones considered break areas
BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area", "Reception Area"])


This are the break area there is no single Swipe for this Employee so we got Wrong data here need to fix...

For Break if swipe In or Out direction  and Zone == East Outdoor Area", "West Outdoor Area", "Assembly Area", 
and next swipe 
Red Zone Green Zone like ....
then Compute Duration if break duration is greater than 2 Hours then check how many break in a single day Break there are 2 two break with greater than 2 hours Duration then Highlight 
only . otherwise Skip ....


another one is i have review ...





Lets Check 
Another Scenarion need to fix...

Bankar, Pallavi Kailas
ID: 327412

long_gap_>=90min
Long gap between swipes (>= 90 minutes) â€” could indicate long out-of-office break.
repeated_short_breaks
Multiple short breaks within the day.
Pattern detected (short stay -> long out -> short return): ('work', 22)->('out_of_office', 192)->('work', 13)->('out_of_office', 1)
No explanation available.
Available Evidence Files


Lets Understand 

long_gap_>=90min  Increase gap logic like if display only gap is Aroung greater than 5 to 6 hours 


Check this paatern 

Pattern detected (short stay -> long out -> short return): ('work', 22)->('out_of_office', 192)->('work', 13)->('out_of_office', 1)



Bankar, Pallavi Kailas	327412	615675	2025-10-30	10:40:04	00:00:00	APAC_IN_PUN_TOWER B_MAIN RECEPTION DOOR	InDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	10:40:13	00:00:09	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	InDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	10:40:22	00:00:09	APAC_IN_PUN_TOWER B_ST6_GYM SIDE DOOR	OutDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	10:43:10	00:02:48	APAC_IN_PUN_TOWER B_ST6_GYM SIDE DOOR	InDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	11:04:41	00:21:31	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	OutDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	13:49:54	02:45:13	APAC_IN_PUN_TOWER B_ST6_GYM SIDE DOOR	OutDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	13:53:21	00:03:27	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	OutDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	13:54:32	00:01:11	APAC_IN_PUN_TOWER B_MAIN RECEPTION DOOR	OutDirection	Out of office	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	14:17:03	00:22:31	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	InDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	14:29:41	00:12:38	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	OutDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	14:30:18	00:00:37	APAC_IN_PUN_TOWER B_MAIN RECEPTION DOOR	InDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	17:06:06	02:35:48	APAC_IN_PUN_TOWER B_MAIN RECEPTION DOOR	InDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	17:06:18	00:00:12	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	InDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	18:47:01	01:40:43	APAC_IN_PUN_TOWER B_ST6_GYM SIDE DOOR	OutDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	18:55:29	00:08:28	APAC_IN_PUN_TOWER B_ST6_GYM SIDE DOOR	InDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	18:56:00	00:00:31	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	OutDirection	Tower B	- (swipes_pune_20251030.csv)
Bankar, Pallavi Kailas	327412	615675	2025-10-30	18:56:12	00:00:12	APAC_IN_PUN_TOWER B_MAIN RECEPTION DOOR	OutDirection	Out of office	- (swipes_pune_20251030.csv)




Here SCenarion meet 

Bankar, Pallavi Kailas	327412	615675	2025-10-30	13:54:32	00:01:11	APAC_IN_PUN_TOWER B_MAIN RECEPTION DOOR	OutDirection	 Out of office	
Bankar, Pallavi Kailas	327412	615675	2025-10-30	14:17:03	00:22:31	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	InDirection   	Tower B

Scenario detect correct but here duration is 22:31 min so 
Flagg only WHen Duration is greater than 2:30 hours ...


and

  "Explanation": "Marothe, Manisha - Short total presence (<4 hours) \u2014 may indicate short stay or partial day attendance.; Many short gaps between swipes \u2014 repeated short breaks pattern.",
      "FirstDirection": "InDirection",
make Explanation more Natural carefully....
alos we dont understand 
\u2014

Make it very very clear 








# backend/trend_runner.py
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re
import os

# IMPORTANT: duration_report must exist and expose run_for_date(date, regions, outdir, city)
from duration_report import run_for_date
from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE

# HIST_PATH: try a few likely locations (project config, repository root, absolute path)
CANDIDATE_HISTORY = [
    Path(__file__).parent / "config" / "current_analysis.csv",
    Path(__file__).parent.parent / "config" / "current_analysis.csv",
    Path.cwd() / "current_analysis.csv",
    Path(__file__).parent / "current_analysis.csv"
]
HIST_PATH = None
for p in CANDIDATE_HISTORY:
    if p.exists():
        HIST_PATH = p
        break

if HIST_PATH is None:
    logging.warning("Historical profile file current_analysis.csv not found in candidate locations.")
    HIST_DF = pd.DataFrame()
else:
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)

# ----- small shared helpers: treat empty/placeholder tokens as None -----
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', 'â€”', 'â€“', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


def _normalize_id_val(v):
    """
    Normalize an id-like value: strip, convert floats like '320172.0' -> '320172'
    Return None for NaN/empty/placeholder.
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    # strip .0 if integer-like
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s


# prefer to avoid emp:<GUID> person_uids â€” only treat emp: if value looks like a human id (not GUID)
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')

def _looks_like_guid(s: object) -> bool:
    """Return True if s looks like a GUID/UUID string."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    """Heuristic: treat as a plausible human name if it contains letters and not a GUID."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        # reject GUIDs and obviously numeric ids
        if _looks_like_guid(st):
            return False
        # require at least one alphabetic character
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    """Pick the first non-null, non-GUID, non-placeholder value from a pandas Series (as str) or None."""
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    """
    Create canonical person uid:
      - prefer EmployeeID (normalized) -> 'emp:<id>' only if it is not a GUID
      - else EmployeeIdentity -> 'uid:<val>' (GUID allowed)
      - else EmployeeName -> hash-based 'name:<shorthash>'
    """
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        # stable short hash of name
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None


# small helper to extract Card from XML-like strings
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        # fallback: look for CHUID ... Card: pattern or Card: 12345
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


# explicit list of zones considered breaks
BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
OUT_OF_OFFICE_ZONE = "Out of office"

def map_door_to_zone(door: object, direction: object = None) -> str:
    """
    Map a raw Door string (and optionally Direction) to a logical zone.
    - Uses substring matching on lowercased door text and door map.
    - Falls back to PartitionName2 heuristics if door is missing.
    """
    try:
        if door is None:
            return None
        s = str(door).strip()
        if not s:
            return None
        s_l = s.lower()
        # try exact substring matches from map
        # DOOR_ZONE_MAP is expected to be in config.door_zone, but we fallback to keywords
        try:
            from config.door_zone import DOOR_ZONE_MAP
            for k, v in DOOR_ZONE_MAP.items():
                if k in s_l:
                    return v
        except Exception:
            pass
        # fallback: direction-based inference
        if direction and isinstance(direction, str):
            d = direction.strip().lower()
            if "out" in d:
                return OUT_OF_OFFICE_ZONE
            if "in" in d:
                # assume reception/working
                return "Reception Area"
        # heuristic fallback
        if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
            return OUT_OF_OFFICE_ZONE
        # else treat as working area
        return "Working Area"
    except Exception:
        return None

# --- CONFIG for violation window and risk thresholds ---
VIOLATION_WINDOW_DAYS = 90  # look-back window to count violation days (adjustable)
# risk thresholds (numeric ranges) -> labels
RISK_THRESHOLDS = [
    (0.5, "Low"),
    (1.5, "Low Medium"),
    (2.5, "Medium"),
    (4.0, "Medium High"),
    (float("inf"), "High"),
]

def map_score_to_label(score: float) -> (int, str):
    """
    Map a numeric score to RiskScore (1..5) and RiskLevel label.
    Returns (risk_bucket, label)
    """
    try:
        if score is None:
            score = 0.0
        s = float(score)
    except Exception:
        s = 0.0
    bucket = 1
    label = "Low"
    for i, (threshold, lbl) in enumerate(RISK_THRESHOLDS, start=1):
        if s <= threshold:
            bucket = i
            label = lbl
            break
    return bucket, label

# ---------------- SCENARIOS (boolean functions) ----------------
def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
    except Exception:
        pass
    if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    return cur > 50

def scenario_repeated_short_breaks(row):
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map

# NEW scenario: the pattern described by the user
def scenario_shortstay_longout_repeat(row):
    # Uses the feature computed in compute_features: PatternShortLongRepeat
    return bool(row.get('PatternShortLongRepeat', False))


# scenario list (name, fn)
SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap),
    # new pattern scenario
    ("shortstay_longout_repeat", scenario_shortstay_longout_repeat)
]

# --- Human readable explanations per scenario (short and neutral) ---
def _fmt_minutes(seconds):
    try:
        m = int(round((seconds or 0) / 60.0))
        return f"{m} min" if m < 60 else f"{m//60} hr {m%60} min"
    except Exception:
        return None

SCENARIO_EXPLANATIONS = {
    "long_gap_>=90min": lambda r: f"Long gap between swipes (~{_fmt_minutes(r.get('MaxSwipeGapSeconds'))}) â€” may indicate extended out-of-office absence.",
    "short_duration_<4h": lambda r: "Short total presence (<4 hours) â€” may indicate short stay or partial day attendance.",
    "coffee_badging": lambda r: "Multiple quick swipes in short time (possible 'coffee' or proxy badge use).",
    "low_swipe_count_<=2": lambda r: "Very few swipes on day â€” unusually low activity.",
    "single_door": lambda r: "Only a single door used during the day â€” possible badge-sharing or single-entry behavior.",
    "only_in": lambda r: "Only 'IN' events recorded without corresponding 'OUT'.",
    "only_out": lambda r: "Only 'OUT' events recorded without prior 'IN'.",
    "overtime_>=10h": lambda r: "Overtime detected (>=10 hours).",
    "very_long_duration_>=16h": lambda r: "Very long presence (>=16 hours).",
    "zero_swipes": lambda r: "No swipes recorded on this day.",
    "unusually_high_swipes": lambda r: "Unusually high number of swipes compared to peers/history.",
    "repeated_short_breaks": lambda r: "Many short gaps between swipes â€” repeated short breaks pattern.",
    "multiple_location_same_day": lambda r: "Multiple locations/partitions used in same day.",
    "weekend_activity": lambda r: "Activity recorded on weekend day.",
    "repeated_rejection_count": lambda r: "Multiple rejection events recorded.",
    "badge_sharing_suspected": lambda r: "Same card used by multiple users on same day â€” possible badge sharing.",
    "early_arrival_before_06": lambda r: "First swipe earlier than 06:00.",
    "late_exit_after_22": lambda r: f"Last swipe after 22:00 ({(pd.to_datetime(r.get('LastSwipe')).time() if pd.notna(r.get('LastSwipe')) else 'time unknown')}).",
    "shift_inconsistency": lambda r: "Duration deviates from historical shift patterns.",
    "trending_decline": lambda r: "Employee shows trending decline in presence.",
    "consecutive_absent_days": lambda r: "Consecutive absent days observed historically.",
    "high_variance_duration": lambda r: "High variance in daily durations historically.",
    "short_duration_on_high_presence_days": lambda r: "Short duration despite normally high presence days.",
    "swipe_overlap": lambda r: "Overlap in swipe times with other persons on same door â€” suspicious co-located events.",
    "shortstay_longout_repeat": lambda r: "Repeated pattern: short in â†’ long out-of-office â†’ short return â€” may indicate leaving site for extended period between brief visits."
}

def _explain_scenarios_detected(row, detected_list):
    """
    Build a short natural-language explanation using SCENARIO_EXPLANATIONS and numeric context.
    """
    pieces = []
    name = row.get('EmployeeName') or row.get('EmployeeID') or row.get('person_uid') or "Employee"
    prefix = f"{name} - "
    for sc in detected_list:
        sc = sc.strip()
        fn = SCENARIO_EXPLANATIONS.get(sc)
        if sc == "shortstay_longout_repeat":
            # provide pattern readable sequence if available
            seq_readable = row.get('PatternSequenceReadable') or row.get('PatternSequence') or "pattern not available"
            if fn:
                pieces.append(f"{fn(row)} ({seq_readable})")
            else:
                pieces.append(f"Pattern detected: {seq_readable}")
        else:
            try:
                if fn:
                    pieces.append(fn(row))
                else:
                    # generic fallback: humanize code
                    pieces.append(sc.replace("_", " ").replace(">= ", ">= "))
            except Exception:
                pieces.append(sc)
    if not pieces:
        return None
    # join with semicolons to keep it short
    return prefix + "; ".join(pieces)

# --- compute_features (replaced/updated) ---
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    Returns DataFrame per (person_uid, Date) with feature columns and normalized IDs/names.
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # Build lowercase->actual column map for flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}

    # detect time column
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)
    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
        sw['Date'] = sw['LocaleMessageTime'].dt.date
    else:
        if 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
        else:
            sw['Date'] = None

    # find these earlier in compute_features â€” prefer Int1/Text12 for EmployeeID and CHUID/Card for CardNumber
    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    # Filter personnel types: prefer PersonnelTypeName, fallback to PersonnelType
    if 'PersonnelTypeName' in sw.columns:
        sw = sw[sw['PersonnelTypeName'].isin(['Employee', 'Terminated Personnel'])]
    elif 'PersonnelType' in sw.columns:
        sw = sw[sw['PersonnelType'].isin(['Employee', 'Terminated Personnel'])]
    # else keep everything

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # ensure stable person_uid (canonical)
    if 'person_uid' not in sw.columns:
        def make_person_uid_local(r):
            # prefer canonical EmployeeID (normalized, non-GUID) then EmployeeIdentity then EmployeeName
            empid_val = None
            if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
                empid_val = r.get(empid_col)
            elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
                empid_val = r.get('EmployeeID')

            empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
            name_val = None
            if name_col and name_col in r:
                name_val = r.get(name_col)
            elif 'EmployeeName' in r:
                name_val = r.get('EmployeeName')
            elif 'ObjectName1' in r:
                name_val = r.get('ObjectName1')

            return _canonical_person_uid({
                'EmployeeID': empid_val,
                'EmployeeIdentity': empident_val,
                'EmployeeName': name_val
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    # selection columns for aggregation: include discovered columns
    sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
                    'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
                    'EmployeeIdentity'])
    if name_col:
        sel_cols.add(name_col)
    if empid_col:
        sel_cols.add(empid_col)
    if card_col:
        sel_cols.add(card_col)
    if door_col:
        sel_cols.add(door_col)
    if dir_col:
        sel_cols.add(dir_col)
    sel_cols = [c for c in sel_cols if c in sw.columns]

    def agg_swipe_group(g):
        # g is a DataFrame for one person_uid + date
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0

        # Direction counts (default to column names present)
        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

        # pick first non-placeholder, non-guid card number if present (prefer cardnumber/chuid)
        card_numbers = []
        # 1) direct known column
        if card_col and card_col in g.columns:
            card_numbers = list(pd.unique(g[card_col].dropna()))
        # 2) explicit 'CardNumber' output column (from SQL COALESCE)
        if not card_numbers and 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna()))
        # 3) some XML-shred columns may appear as 'value' or other column names
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
                    try:
                        vals = list(pd.unique(g[c].dropna()))
                        if vals:
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        # 4) lastly try to extract from XmlMessage fields
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'xml' in cl or 'xmlmessage' in cl or 'xml_msg' in cl or 'xmlmessage' in cl:
                    for raw in g[c].dropna().astype(str):
                        extracted = _extract_card_from_xml(raw)
                        if extracted:
                            card_numbers.append(extracted)
        # 5) final unique
        card_numbers = list(dict.fromkeys(card_numbers))  # preserve order, unique

        card_number = None
        for c in card_numbers:
            n = _normalize_id_val(c)
            # explicitly reject GUIDs as card numbers
            if n and not _looks_like_guid(n):
                card_number = n
                break

        # stable id/name from the group using discovered columns first
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None

        # Employee ID: prefer Int1/Text12 then EmployeeID; DO NOT use EmployeeIdentity as EmployeeID
        # use _pick_first_non_guid_value to skip GUIDs automatically
        if empid_col and empid_col in g.columns:
            vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                # final trial: numeric normalization (strip .0) but still reject GUIDs
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        elif 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized

        # If still no employee_id and PersonnelType indicates contractor -> prefer Text12 explicitly
        if (not employee_id) and 'PersonnelType' in g.columns:
            try:
                pvals = g['PersonnelType'].dropna().astype(str)
                if not pvals.empty:
                    p0 = pvals.iloc[0]
                    if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
                        # look for text12 explicitly (case-insensitive)
                        for c in g.columns:
                            if c.lower() == 'text12':
                                vals = g[c].dropna().astype(str).map(lambda x: x.strip())
                                employee_id = _pick_first_non_guid_value(vals)
                                if employee_id:
                                    break
            except Exception:
                pass

        # Employee identity (GUID) â€” keep but do not promote to EmployeeID
        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]

        # Employee name: pick non-GUID candidate
        candidate_name_vals = None
        if name_col and name_col in g.columns:
            candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
        elif 'EmployeeName' in g.columns:
            candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
        elif 'ObjectName1' in g.columns:
            candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

        if candidate_name_vals is not None and not candidate_name_vals.empty:
            employee_name = _pick_first_non_guid_value(candidate_name_vals)
            if employee_name is None:
                # accept any value that looks like a name
                for v in candidate_name_vals:
                    if _looks_like_name(v) and not _is_placeholder_str(v):
                        employee_name = str(v).strip()
                        break

        # personnel type
        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        # First/Last swipe times
        first_swipe = None
        last_swipe = None
        if times:
            first_swipe = times[0]
            last_swipe = times[-1]

        # ----------------- NEW: break/out-of-office sequence analysis -----------------
        # Build a timeline of (time, door, direction, zone)
        timeline = []
        for _, row in g.sort_values('LocaleMessageTime').iterrows():
            t = row.get('LocaleMessageTime')
            dname = None
            if door_col and door_col in row and pd.notna(row.get(door_col)):
                dname = row.get(door_col)
            elif 'Door' in row and pd.notna(row.get('Door')):
                dname = row.get('Door')
            direction = None
            if dir_col and dir_col in row and pd.notna(row.get(dir_col)):
                direction = row.get(dir_col)
            elif 'Direction' in row and pd.notna(row.get('Direction')):
                direction = row.get('Direction')
            zone = map_door_to_zone(dname, direction)
            timeline.append((t, dname, direction, zone))

        # compress timeline into segments with labels: 'work', 'break', 'out_of_office'
        segments = []
        if timeline:
            cur_zone = None
            seg_start = timeline[0][0]
            seg_label = None
            for (t, dname, direction, zone) in timeline:
                # determine label
                if zone in BREAK_ZONES:
                    lbl = 'break'
                elif zone == OUT_OF_OFFICE_ZONE:
                    lbl = 'out_of_office'
                else:
                    lbl = 'work'
                if cur_zone is None:
                    cur_zone = zone
                    seg_label = lbl
                    seg_start = t
                else:
                    # if label changes, close previous segment
                    if lbl != seg_label:
                        segments.append({
                            'label': seg_label,
                            'start': seg_start,
                            'end': t,
                            'start_zone': cur_zone
                        })
                        seg_start = t
                        seg_label = lbl
                        cur_zone = zone
                    else:
                        # keep current segment
                        cur_zone = cur_zone or zone
            # close last
            if seg_label is not None:
                segments.append({
                    'label': seg_label,
                    'start': seg_start,
                    'end': timeline[-1][0],
                    'start_zone': cur_zone
                })

        # Compute break metrics
        break_count = 0
        long_break_count = 0
        total_break_minutes = 0.0
        for s in segments:
            if s['label'] in ('break', 'out_of_office'):
                dur_mins = (s['end'] - s['start']).total_seconds() / 60.0 if (s['end'] and s['start']) else 0
                total_break_minutes += dur_mins
                break_count += 1
                if dur_mins >= 60:
                    long_break_count += 1

        # Detect the specific pattern:
        pattern_flag = False
        pattern_sequence_readable = None
        try:
            # create simplified label list with durations
            seq = []
            for s in segments:
                dur_mins = (s['end'] - s['start']).total_seconds() / 60.0 if (s['end'] and s['start']) else 0
                seq.append((s['label'], int(round(dur_mins))))
            # look for the pattern anywhere in sequence
            for i in range(len(seq)-3):
                a = seq[i]     # first work
                b = seq[i+1]   # long out
                c = seq[i+2]   # short work
                d = seq[i+3]   # out / break
                if a[0] == 'work' and a[1] < 60 and \
                   b[0] in ('out_of_office','break') and b[1] >= 180 and \
                   c[0] == 'work' and c[1] < 60:
                    pattern_flag = True
                    # build readable snippet for the 4-segment window
                    seq_fragment = [a, b, c, d]
                    pattern_sequence_readable = " -> ".join([f"{lbl} ({mins}m)" for lbl, mins in seq_fragment])
                    break
        except Exception:
            pattern_flag = False
            pattern_sequence_readable = None

        # ----------------- return aggregated metrics (including new ones) -----------------
        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type,
            'FirstSwipe': first_swipe,
            'LastSwipe': last_swipe,
            # new break features
            'BreakCount': int(break_count),
            'LongBreakCount': int(long_break_count),
            'TotalBreakMinutes': float(round(total_break_minutes,1)),
            'PatternShortLongRepeat': bool(pattern_flag),
            'PatternSequenceReadable': pattern_sequence_readable,
            'PatternSequence': None  # keep old field empty for compatibility
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = grouped.apply(agg_swipe_group).reset_index()

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # --- START PATCH: coalesce duplicate columns produced by merge ---
    def _coalesce_merge_columns(df, bases):
        for base in bases:
            x = base + "_x"
            y = base + "_y"
            try:
                has_base = base in df.columns
                base_all_null = False
                if has_base:
                    base_all_null = df[base].isnull().all()
            except Exception:
                has_base = base in df.columns
                base_all_null = True

            if (not has_base) or base_all_null:
                if x in df.columns and y in df.columns:
                    try:
                        df[base] = df[x].combine_first(df[y])
                    except Exception:
                        try:
                            df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
                        except Exception:
                            if x in df.columns:
                                df[base] = df[x]
                            elif y in df.columns:
                                df[base] = df[y]
                elif x in df.columns:
                    df[base] = df[x]
                elif y in df.columns:
                    df[base] = df[y]
    _coalesce_merge_columns(merged, [
        "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
    ])
    drop_cols = [c for c in merged.columns if c.endswith("_x") or c.endswith("_y")]
    if drop_cols:
        try:
            merged.drop(columns=drop_cols, inplace=True)
        except Exception:
            for c in drop_cols:
                if c in merged.columns:
                    try:
                        merged.drop(columns=[c], inplace=True)
                    except Exception:
                        pass
    # --- END PATCH ---

    # coalesce helpers (ensure column existence)
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)
    ensure_col(merged, 'BreakCount', 0)
    ensure_col(merged, 'LongBreakCount', 0)
    ensure_col(merged, 'TotalBreakMinutes', 0.0)
    ensure_col(merged, 'PatternShortLongRepeat', False)
    ensure_col(merged, 'PatternSequenceReadable', None)
    ensure_col(merged, 'PatternSequence', None)

    # If EmployeeName is missing or a GUID, try to get a better name from durations (durations typically has EmployeeName)
    if 'EmployeeName' in merged.columns:
        def choose_best_name(row):
            gname = row.get('EmployeeName')
            dname = None
            for cand in ('EmployeeName', 'employee_name', 'objectname1', 'ObjectName1'):
                if cand in row and row.get(cand) is not None:
                    dname = row.get(cand)
                    break
            if _looks_like_name(gname):
                return str(gname).strip()
            if _looks_like_name(dname):
                return str(dname).strip()
            if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
                return str(gname).strip()
            if dname and not _is_placeholder_str(dname):
                return str(dname).strip()
            return None
        merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
    else:
        if not dur.empty:
            def fill_name_from_dur(row):
                gname = row.get('EmployeeName')
                if _looks_like_name(gname) and not _is_placeholder_str(gname):
                    return gname
                for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
                    if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
                        return row[cand]
                return None
            merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

    # numeric normalization for EmployeeID: ensure not GUIDs/placeholder, convert floats like '320172.0' -> '320172'
    def normalize_empid(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            try:
                if '.' in s:
                    f = float(s)
                    if math.isfinite(f) and f.is_integer():
                        return str(int(f))
            except Exception:
                pass
            return s
        except Exception:
            return None

    merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

    # normalize card numbers: reject GUIDs and placeholder tokens
    def normalize_card(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            return s
        except Exception:
            return None

    merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

    # numeric normalization
    # If durations DataFrame provided DurationSeconds, use that; else fall back to computed (LastSwipe-FirstSwipe)
    if 'DurationSeconds' not in merged.columns or merged['DurationSeconds'].isnull().all():
        try:
            merged['DurationSeconds'] = (pd.to_datetime(merged['LastSwipe']) - pd.to_datetime(merged['FirstSwipe'])).dt.total_seconds().clip(lower=0).fillna(0)
        except Exception:
            merged['DurationSeconds'] = merged.get('DurationSeconds', 0)

    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)
    merged['BreakCount'] = merged['BreakCount'].fillna(0).astype(int)
    merged['LongBreakCount'] = merged['LongBreakCount'].fillna(0).astype(int)
    merged['TotalBreakMinutes'] = merged['TotalBreakMinutes'].fillna(0.0).astype(float)
    merged['PatternShortLongRepeat'] = merged['PatternShortLongRepeat'].fillna(False).astype(bool)

    # ensure FirstSwipe/LastSwipe are datetimes
    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # EmpHistoryPresent
    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    # normalize string columns for safe downstream use; EmployeeName keep as readable-only
    for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
        if c in merged.columns:
            def _clean_str_val(v):
                if v is None:
                    return None
                try:
                    s = str(v).strip()
                    if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                        return None
                    return s
                except Exception:
                    return None
            merged[c] = merged[c].apply(_clean_str_val)

    # EmployeeName: keep None if empty or GUID/placeholder; otherwise string.
    if 'EmployeeName' in merged.columns:
        merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

    return merged


# ---------------- SCENARIO WEIGHTS (for anomaly scoring) ----------------
WEIGHTS = {
    "long_gap_>=90min": 0.3,
    "short_duration_<4h": 1.0,
    "coffee_badging": 1.0,
    "low_swipe_count_<=2": 0.5,
    "single_door": 0.25,
    "only_in": 0.8,
    "only_out": 0.8,
    "overtime_>=10h": 0.2,
    "very_long_duration_>=16h": 1.5,
    "zero_swipes": 0.4,
    "unusually_high_swipes": 1.5,
    "repeated_short_breaks": 0.5,
    "multiple_location_same_day": 0.6,
    "weekend_activity": 0.6,
    "repeated_rejection_count": 0.8,
    "badge_sharing_suspected": 2.0,
    "early_arrival_before_06": 0.4,
    "late_exit_after_22": 0.4,
    "shift_inconsistency": 1.2,
    "trending_decline": 0.7,
    "consecutive_absent_days": 1.2,
    "high_variance_duration": 0.8,
    "short_duration_on_high_presence_days": 1.1,
    "swipe_overlap": 2.0,
    # weight for new scenario
    "shortstay_longout_repeat": 2.0
}
ANOMALY_THRESHOLD = 1.5


def _read_past_trend_csvs(outdir: str, window_days: int, target_date: date):
    """
    Read existing trend_pune_*.csv in outdir and return a single DataFrame filtered to the
    window (target_date - window_days .. target_date-1).
    """
    p = Path(outdir)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return pd.DataFrame()
    dfs = []
    cutoff = target_date - timedelta(days=window_days)
    for fp in csvs:
        try:
            df = pd.read_csv(fp, parse_dates=['Date'])
            # keep only rows with date in (cutoff .. target_date-1)
            if 'Date' in df.columns:
                try:
                    df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                except Exception:
                    pass
                df = df[df['Date'].apply(lambda d: d is not None and d >= cutoff and d < target_date)]
            dfs.append(df)
        except Exception:
            try:
                df = pd.read_csv(fp, dtype=str)
                if 'Date' in df.columns:
                    try:
                        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
                        df = df[df['Date'].apply(lambda d: d is not None and d >= cutoff and d < target_date)]
                    except Exception:
                        pass
                dfs.append(df)
            except Exception:
                continue
    if not dfs:
        return pd.DataFrame()
    try:
        out = pd.concat(dfs, ignore_index=True)
        return out
    except Exception:
        return pd.DataFrame()


def compute_violation_days_map(outdir: str, window_days: int, target_date: date):
    """
    Return dict: person_uid -> count of days flagged as IsFlagged True in last window_days (excluding target_date).
    """
    df = _read_past_trend_csvs(outdir, window_days, target_date)
    if df is None or df.empty:
        return {}
    # Normalize person_uid column if present, else fallback to EmployeeID
    if 'person_uid' in df.columns:
        key_col = 'person_uid'
    elif 'EmployeeID' in df.columns:
        key_col = 'EmployeeID'
    else:
        return {}
    # consider IsFlagged column
    if 'IsFlagged' not in df.columns:
        # try to infer by AnomalyScore >= threshold
        if 'AnomalyScore' in df.columns:
            df['IsFlagged'] = df['AnomalyScore'].apply(lambda s: float(s) >= ANOMALY_THRESHOLD if not pd.isna(s) else False)
        else:
            df['IsFlagged'] = False
    # count unique dates per person_uid where IsFlagged
    try:
        df_clean = df[[key_col, 'Date', 'IsFlagged']].dropna(subset=[key_col, 'Date'])
        df_clean = df_clean[df_clean['IsFlagged'] == True]
        grouped = df_clean.groupby(key_col)['Date'].nunique().to_dict()
        return grouped
    except Exception:
        return {}


def run_trend_for_date(target_date: date, outdir: str = "./outputs", city='Pune'):
    logging.info("run_trend_for_date: date=%s (city=%s)", target_date, city)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city=city)
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    # save raw swipes for evidence (full raw)
    try:
        if swipes is not None and not swipes.empty:
            sw_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception as e:
        logging.warning("Failed to save raw swipes: %s", e)

    # compute features
    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # ===== START FIX: reconcile zero CountSwipes with raw swipe files =====
    try:
        if swipes is not None and not swipes.empty and 'person_uid' in swipes.columns:
            tsw = swipes.copy()
            if 'LocaleMessageTime' in tsw.columns:
                tsw['LocaleMessageTime'] = pd.to_datetime(tsw['LocaleMessageTime'], errors='coerce')
            else:
                for cand in ('MessageUTC','MessageTime','Timestamp','timestamp'):
                    if cand in tsw.columns:
                        tsw['LocaleMessageTime'] = pd.to_datetime(tsw[cand], errors='coerce')
                        break
            if 'Date' not in tsw.columns:
                if 'LocaleMessageTime' in tsw.columns:
                    tsw['Date'] = tsw['LocaleMessageTime'].dt.date
                else:
                    for cand in ('date','Date'):
                        if cand in tsw.columns:
                            try:
                                tsw['Date'] = pd.to_datetime(tsw[cand], errors='coerce').dt.date
                            except Exception:
                                tsw['Date'] = None
                            break

            try:
                grp = tsw.dropna(subset=['person_uid', 'Date']).groupby(['person_uid', 'Date'])
                counts = grp.size().to_dict()
                firsts = grp['LocaleMessageTime'].min().to_dict()
                lasts = grp['LocaleMessageTime'].max().to_dict()
            except Exception:
                counts = {}
                firsts = {}
                lasts = {}

            def _fix_row_by_raw(idx, row):
                key = (row.get('person_uid'), row.get('Date'))
                if key in counts and (row.get('CountSwipes', 0) == 0 or pd.isna(row.get('CountSwipes'))):
                    try:
                        c = int(counts.get(key, 0))
                        features.at[idx, 'CountSwipes'] = c
                        f = firsts.get(key)
                        l = lasts.get(key)
                        if pd.notna(f) and (pd.isna(row.get('FirstSwipe')) or row.get('FirstSwipe') is None):
                            features.at[idx, 'FirstSwipe'] = pd.to_datetime(f)
                        if pd.notna(l) and (pd.isna(row.get('LastSwipe')) or row.get('LastSwipe') is None):
                            features.at[idx, 'LastSwipe'] = pd.to_datetime(l)
                        try:
                            fs = features.at[idx, 'FirstSwipe']
                            ls = features.at[idx, 'LastSwipe']
                            if pd.notna(fs) and pd.notna(ls):
                                dursec = (pd.to_datetime(ls) - pd.to_datetime(fs)).total_seconds()
                                dursec = max(0, dursec)
                                features.at[idx, 'DurationSeconds'] = float(dursec)
                                features.at[idx, 'DurationMinutes'] = float(dursec / 60.0)
                        except Exception:
                            pass
                    except Exception:
                        pass

            for ix, r in features[features['CountSwipes'].fillna(0).astype(int) == 0].iterrows():
                try:
                    _fix_row_by_raw(ix, r)
                except Exception:
                    logging.debug("Failed to reconcile row %s with raw swipes", ix)
    except Exception:
        logging.exception("Error while reconciling aggregated features with raw swipes (zero-swipe fix).")
    # ===== END FIX =====

    # Build badge map and swipe overlap maps for higher-severity scenarios
    badge_map = {}
    if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}

    swipe_overlap_map = {}
    overlap_window_seconds = 2
    if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
        tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
        if not tmp.empty:
            tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
            for (d, door), g in tmp.groupby(['Date', 'Door']):
                items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                n = len(items)
                for i in range(n):
                    t_i, uid_i = items[i]
                    j = i+1
                    while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                        uid_j = items[j][1]
                        if uid_i != uid_j:
                            swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                            swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                        j += 1

    # Evaluate scenarios (use weighting to compute anomaly score)
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            features[name] = features.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
        elif name == "swipe_overlap":
            features[name] = features.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map), axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    def compute_score(r):
        score = 0.0
        detected = []
        for name, _ in SCENARIOS:
            val = bool(r.get(name))
            w = WEIGHTS.get(name, 0.0)
            if val and w > 0:
                score += float(w)
                detected.append(name)
        return score, detected

    scores = features.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
    features['AnomalyScore'] = scores['AnomalyScore'].astype(float)
    features['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
    features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

    def reasons_for_row(r):
        if not bool(r.get('IsFlagged')):
            return None
        ds_raw = r.get('DetectedScenarios')
        if ds_raw:
            ds = [s.strip() for s in ds_raw.split(";") if s and s.strip()]
            # Build natural explanation sentences for the detected scenarios
            explanation = _explain_scenarios_detected(r, ds)
            # Also produce compact reasons list (code-style) in Reasons for backwards compatibility
            reasons_codes = "; ".join(ds) if ds else None
            return reasons_codes, explanation
        return None, None

    # Apply reasons_for_row to populate Reasons (codes) and Explanation (natural text)
    reason_tuples = features.apply(lambda r: pd.Series(reasons_for_row(r), index=['Reasons', 'Explanation']), axis=1)
    features['Reasons'] = reason_tuples['Reasons']
    features['Explanation'] = reason_tuples['Explanation']

    if 'OverlapWith' not in features.columns:
        def overlap_with_fn(r):
            d = r.get('Date')
            uid = r.get('person_uid')
            if (d, uid) in swipe_overlap_map:
                return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
            return None
        features['OverlapWith'] = features.apply(overlap_with_fn, axis=1)

    # compute ViolationDays in past window (person_uid -> count) using existing trend CSVs in outdir
    try:
        violation_map = compute_violation_days_map(outdir, VIOLATION_WINDOW_DAYS, target_date)
        def map_violation_days(r):
            key = r.get('person_uid')
            if key in violation_map:
                return int(violation_map[key])
            # fallback: try EmployeeID
            eid = r.get('EmployeeID')
            if eid in violation_map:
                return int(violation_map[eid])
            return 0
        features['ViolationDaysLast90'] = features.apply(map_violation_days, axis=1)
    except Exception:
        features['ViolationDaysLast90'] = 0

    # compute RiskScore (bucket) and RiskLevel label
    try:
        def map_risk(r):
            score = r.get('AnomalyScore') or 0.0
            bucket, label = map_score_to_label(score)
            return int(bucket), label
        rs = features.apply(lambda r: pd.Series(map_risk(r), index=['RiskScore', 'RiskLevel']), axis=1)
        features['RiskScore'] = rs['RiskScore']
        features['RiskLevel'] = rs['RiskLevel']
    except Exception:
        features['RiskScore'] = 1
        features['RiskLevel'] = 'Low'

    # Remove suffix columns and fix duplicates
    cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
    if cols_to_drop:
        for c in cols_to_drop:
            base = c[:-2]
            if base in features.columns:
                try:
                    features.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                try:
                    features.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass
    features = features.loc[:, ~features.columns.duplicated()]

    # ensure booleans are native Python (avoid numpy.bool_)
    for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
        if col in features.columns:
            features[col] = features[col].astype(bool)

    # write CSV with native types
    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    try:
        write_df = features.copy()
        # FirstSwipe/LastSwipe -> ISO strings
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        # Date -> ISO date
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        write_df = write_df.where(pd.notnull(write_df), None)
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception as e:
        logging.exception("Failed to write trend CSV: %s", e)

    return features


# ---------------- training dataset builder (restored) ----------------
def build_monthly_training(end_date: date = None, months: int = 3, min_unique_employees: int = 1000,
                           outdir: str = "./outputs", city: str = "Pune"):
    if end_date is None:
        end_date = datetime.now().date()
    logging.info("build_monthly_training: end_date=%s months=%d min_unique=%d", end_date, months, min_unique_employees)
    outdir = Path(outdir)
    month_windows = []
    cur = end_date.replace(day=1)
    for _ in range(months):
        start = cur
        next_month = (cur.replace(day=28) + timedelta(days=4)).replace(day=1)
        last = next_month - timedelta(days=1)
        month_windows.append((start, last))
        cur = (start - timedelta(days=1)).replace(day=1)

    person_month_rows = []
    unique_persons = set()

    for start, last in month_windows:
        d = start
        month_dfs = []
        while d <= last:
            csv_path = outdir / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
            if csv_path.exists():
                try:
                    df = pd.read_csv(csv_path)
                    month_dfs.append(df)
                except Exception:
                    try:
                        df = pd.read_csv(csv_path, dtype=str)
                        month_dfs.append(df)
                    except Exception as e:
                        logging.warning("Failed reading %s: %s", csv_path, e)
            else:
                # generate the daily trend if missing
                logging.info("Monthly builder: trend CSV missing for %s â€” generating by running run_trend_for_date", d.isoformat())
                try:
                    run_trend_for_date(d, outdir=str(outdir), city=city)
                    # attempt to read after generating
                    if csv_path.exists():
                        try:
                            df = pd.read_csv(csv_path)
                            month_dfs.append(df)
                        except Exception:
                            try:
                                df = pd.read_csv(csv_path, dtype=str)
                                month_dfs.append(df)
                            except Exception as e:
                                logging.warning("Failed reading %s after generation: %s", csv_path, e)
                except Exception as e:
                    logging.warning("Failed to generate trend for %s: %s", d, e)
            d = d + timedelta(days=1)

        if not month_dfs:
            logging.info("No daily trend CSVs found for month %s - %s", start.isoformat(), last.isoformat())
            continue

        month_df = pd.concat(month_dfs, ignore_index=True)
        # ensure person_uid exists
        if 'person_uid' not in month_df.columns:
            def make_person_uid(row):
                parts = []
                for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                    v = row.get(c)
                    if pd.notna(v) and str(v).strip():
                        parts.append(str(v).strip())
                return "|".join(parts) if parts else None
            month_df['person_uid'] = month_df.apply(make_person_uid, axis=1)

        # convert boolean columns to int for aggregation if necessary
        for name, _ in SCENARIOS:
            if name in month_df.columns:
                month_df[name] = month_df[name].astype(int)

        agg_funcs = {
            'CountSwipes': ['median', 'mean', 'sum'],
            'DurationMinutes': ['median', 'mean', 'sum'],
            'MaxSwipeGapSeconds': ['max', 'median'],
            'ShortGapCount': ['sum'],
            'UniqueDoors': ['median'],
            'UniqueLocations': ['median'],
            'RejectionCount': ['sum']
        }
        scenario_cols = [name for name,_ in SCENARIOS if name in month_df.columns]
        group_cols = ['person_uid']
        grp = month_df.groupby(group_cols)

        for person, g in grp:
            row = {}
            row['person_uid'] = person
            row['EmployeeID'] = next((v for v in g.get('EmployeeID', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['EmployeeName'] = next((v for v in g.get('EmployeeName', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['MonthStart'] = start.isoformat()
            row['MonthEnd'] = last.isoformat()
            for col, funcs in agg_funcs.items():
                if col in g.columns:
                    for f in funcs:
                        key = f"{col}_{f}"
                        try:
                            val = getattr(g[col], f)()
                            row[key] = float(val) if pd.notna(val) else None
                        except Exception:
                            row[key] = None
                else:
                    for f in funcs:
                        row[f"{col}_{f}"] = None
            for s in scenario_cols:
                row[f"{s}_days"] = int(g[s].sum())
                row[f"{s}_label"] = int(g[s].sum() > 0)
            row['days_present'] = int(g.shape[0])
            person_month_rows.append(row)
            unique_persons.add(person)

        if len(unique_persons) >= min_unique_employees:
            logging.info("Reached min unique employees=%d, stopping aggregation early", min_unique_employees)
            break

    if not person_month_rows:
        logging.warning("No person-month rows created (no data).")
        return None

    training_df = pd.DataFrame(person_month_rows)
    train_out = outdir / "training_person_month.csv"
    training_df.to_csv(train_out, index=False)
    logging.info("Saved training CSV to %s (rows=%d unique_persons=%d)", train_out, len(training_df), len(unique_persons))
    return train_out


if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today)
    print("Completed; rows:", len(df) if df is not None else 0)














# ml_training.py
"""
Train one binary classifier per scenario using the training CSV produced by trend_runner.build_monthly_training.
Usage:
    python ml_training.py --input outputs/training_person_month.csv --models_dir models/
Outputs:
    models/<scenario>.joblib
Requirements:
    scikit-learn, joblib, pandas, numpy
"""
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)

try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report
    import joblib
except Exception as e:
    logging.error("Required ML packages missing: %s", e)
    logging.error("Install scikit-learn and joblib: pip install scikit-learn joblib")
    raise

DEFAULT_FEATURE_COLS = [
    'CountSwipes_median', 'CountSwipes_mean', 'CountSwipes_sum',
    'DurationMinutes_median', 'DurationMinutes_mean', 'DurationMinutes_sum',
    'MaxSwipeGapSeconds_max', 'MaxSwipeGapSeconds_median',
    'ShortGapCount_sum', 'UniqueDoors_median', 'UniqueLocations_median', 'RejectionCount_sum',
    'days_present'
]

def auto_detect_scenarios(df):
    scenario_labels = [c for c in df.columns if c.endswith('_label')]
    scenarios = [c[:-6] for c in scenario_labels]
    return scenarios

def prepare_features(df, features=None):
    if features is None:
        features = DEFAULT_FEATURE_COLS
    # ensure columns exist, fill missing with 0/median
    X = df.copy()
    for f in features:
        if f not in X.columns:
            X[f] = 0.0
    X = X[features].fillna(0.0)
    return X

def train_one(df, scenario, features):
    label_col = f"{scenario}_label"
    if label_col not in df.columns:
        logging.warning("Label %s not in dataframe, skipping", label_col)
        return None
    y = df[label_col].astype(int)
    if y.sum() == 0:
        logging.warning("No positive examples for %s; skipping model training", scenario)
        return None
    X = prepare_features(df, features)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    logging.info("Classification report for %s:\n%s", scenario, classification_report(y_test, y_pred, zero_division=0))
    return clf

def main(input_csv: Path, models_dir: Path, feature_cols=None):
    if not input_csv.exists():
        raise FileNotFoundError(f"input CSV not found: {input_csv}")
    df = pd.read_csv(input_csv)
    scenarios = auto_detect_scenarios(df)
    if not scenarios:
        logging.error("No scenarios ( *_label ) columns found in %s", input_csv)
        return
    models_dir.mkdir(parents=True, exist_ok=True)
    for s in scenarios:
        logging.info("Training for scenario: %s", s)
        clf = train_one(df, s, feature_cols)
        if clf is not None:
            outp = models_dir / f"{s}.joblib"
            joblib.dump({"model": clf, "features": (feature_cols or DEFAULT_FEATURE_COLS)}, outp)
            logging.info("Saved model to %s", outp)
        else:
            logging.info("Skipped training for %s", s)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="training CSV (person-month) created by /train endpoint")
    parser.add_argument("--models_dir", default="models", help="folder to save models")
    parser.add_argument("--features", default=None, help="comma separated feature columns (optional)")
    args = parser.parse_args()

    input_csv = Path(args.input)
    models_dir = Path(args.models_dir)
    feature_cols = args.features.split(",") if args.features else None

    main(input_csv, models_dir, feature_cols)












# backend/app.py
from flask import Flask, jsonify, request, send_from_directory
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re
import io
import base64
import os
from typing import Optional, List, Dict, Any
from duration_report import REGION_CONFIG


from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR
from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE



# --- Use REGION_CONFIG servers to talk to ACVSCore (no separate ACVSCORE_DB_CONFIG) ---
# ODBC driver variable is already defined earlier: ODBC_DRIVER

_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20

def _get_acvscore_conn():
    """
    Try to connect to ACVSCore by reusing credentials from REGION_CONFIG.
    Loops through REGION_CONFIG entries and attempts:
      1) SQL auth (UID/PWD) to database "ACVSCore" using region server + credentials
      2) If SQL auth fails on that server, try Trusted_Connection (Windows auth) as a fallback
    If that fails, optionally attempt ACVSCORE_DB_CONFIG if defined (safe: checked via globals()).
    Returns first successful pyodbc connection or None.
    Implements a short backoff after recent failure to reduce log noise.
    """
    try:
        import pyodbc
    except Exception:
        logging.exception("pyodbc not installed; ACVSCore lookups unavailable.")
        return None

    # basic backoff: skip attempts if we just failed recently
    from datetime import datetime
    now = datetime.now().timestamp()
    last = _acvscore_backoff.get("ts")
    if last and _acvscore_backoff.get("failed") and (now - last) < _ACVSCORE_BACKOFF_SECONDS:
        logging.debug("Skipping ACVSCore connection attempt (backoff active).")
        return None

    # iterate region servers (use the same credentials defined in REGION_CONFIG)
    tried = []
    for region_key, rc in (REGION_CONFIG or {}).items():
        server = rc.get("server")
        user = rc.get("user")
        pwd = rc.get("password")
        if not server:
            continue

        # Try SQL auth first if credentials present
        if user and pwd:
            tried.append(f"{region_key}@{server}(sql)")
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={server};DATABASE=ACVSCore;UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;"
            )
            try:
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (sql auth).", server, region_key)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("SQL auth to %s failed: %s", server, e)

        # Try Trusted Connection fallback on same server
        tried.append(f"{region_key}@{server}(trusted)")
        conn_str_trusted = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={server};DATABASE=ACVSCore;Trusted_Connection=yes;"
            "TrustServerCertificate=Yes;"
        )
        try:
            conn = pyodbc.connect(conn_str_trusted, autocommit=True, timeout=5)
            logging.info("Connected to ACVSCore on server %s using REGION_CONFIG[%s] (trusted connection).", server, region_key)
            _acvscore_backoff["ts"] = None
            _acvscore_backoff["failed"] = False
            return conn
        except Exception as e:
            logging.debug("Trusted connection to %s failed: %s", server, e)
            continue

    # Fallback: if a global ACVSCORE_DB_CONFIG exists, try it (safe check)
    try:
        if 'ACVSCORE_DB_CONFIG' in globals() and isinstance(globals().get('ACVSCORE_DB_CONFIG'), dict):
            cfg = globals().get('ACVSCORE_DB_CONFIG')
            server = cfg.get('server')
            user = cfg.get('user')
            pwd = cfg.get('password')
            database = cfg.get('database', 'ACVSCore')
            tried.append(f"ACVSCORE_DB_CONFIG@{server}")
            try:
                conn_str = (
                    f"DRIVER={{{ODBC_DRIVER}}};"
                    f"SERVER={server};DATABASE={database};UID={user};PWD={pwd};"
                    "TrustServerCertificate=Yes;"
                )
                conn = pyodbc.connect(conn_str, autocommit=True, timeout=5)
                logging.info("Connected to ACVSCore using ACVSCORE_DB_CONFIG (%s).", server)
                _acvscore_backoff["ts"] = None
                _acvscore_backoff["failed"] = False
                return conn
            except Exception as e:
                logging.debug("ACVSCORE_DB_CONFIG connection failed: %s", e)
    except Exception:
        # defensive: do not propagate any errors from fallback logic
        logging.debug("ACVSCORE_DB_CONFIG fallback not available or failed.")

    # record failure to backoff
    _acvscore_backoff["ts"] = now
    _acvscore_backoff["failed"] = True
    logging.error("Failed to connect to ACVSCore using REGION_CONFIG servers. Tried: %s", tried)
    return None


# ODBC driver (keep existing env-based driver)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data


def get_personnel_info(candidate_identifier: object) -> Dict[str, Any]:
    """
    Try to resolve personnel record using a flexible lookup.
    Returns dict with keys: ObjectID (may be None), GUID, Name, EmailAddress, ManagerEmail
    If resolution fails returns empty dict.
    """
    out: Dict[str, Any] = {}
    if candidate_identifier is None:
        return out
    conn = _get_acvscore_conn()
    if conn is None:
        return out

    try:
        cur = conn.cursor()
        # We'll attempt to match on multiple columns: ObjectID, GUID, Int1, Text12, Name
        # Use parameters (all strings) â€” SQL will handle mismatched types.
        sql = """
            SELECT TOP 1 ObjectID, GUID, Name, EmailAddress, ManagerEmail
            FROM ACVSCore.Access.Personnel
            WHERE
              (CAST(ObjectID AS NVARCHAR(200)) = ?)
              OR (GUID = ?)
              OR (CAST(Int1 AS NVARCHAR(200)) = ?)
              OR (Text12 = ?)
              OR (Name = ?)
            ORDER BY ObjectID DESC
        """
        cand = str(candidate_identifier).strip()
        params = (cand, cand, cand, cand, cand)
        cur.execute(sql, params)
        row = cur.fetchone()
        if row:
            # columns: ObjectID, GUID, Name, EmailAddress, ManagerEmail
            try:
                out['ObjectID'] = row[0]
                out['GUID'] = row[1]
                out['Name'] = row[2]
                out['EmailAddress'] = row[3]
                out['ManagerEmail'] = row[4]
            except Exception:
                # defensive assignment by index
                out = {
                    'ObjectID': row[0] if len(row) > 0 else None,
                    'GUID': row[1] if len(row) > 1 else None,
                    'Name': row[2] if len(row) > 2 else None,
                    'EmailAddress': row[3] if len(row) > 3 else None,
                    'ManagerEmail': row[4] if len(row) > 4 else None
                }
    except Exception:
        logging.exception("Failed personnel lookup for candidate: %s", candidate_identifier)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    return out

def get_person_image_bytes(parent_id) -> Optional[bytes]:
    """
    Query ACVSCore.Access.Images for top image where ParentId = parent_id and return raw bytes.
    Returns None if not found or on error.
    """
    conn = _get_acvscore_conn()
    if conn is None:
        return None
    try:
        cur = conn.cursor()
        sql = """
            SELECT TOP 1 AI.Image
            FROM ACVSCore.Access.Images AI
            WHERE AI.ParentId = ?
              AND DATALENGTH(AI.Image) > 0
            ORDER BY AI.ObjectID DESC
        """
        cur.execute(sql, (str(parent_id),))
        row = cur.fetchone()
        if row and row[0] is not None:
            # pyodbc returns buffer/bytearray for varbinary; convert to bytes
            try:
                b = bytes(row[0])
                return b
            except Exception:
                # sometimes row[0] is already bytes-like
                return row[0]
    except Exception:
        logging.exception("Failed to fetch image for ParentId=%s", parent_id)
    finally:
        try:
            cur.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass
    return None

# ---------- New route to serve employee image ----------
# We'll import send_file later where used; define route after app created.

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)


from flask import send_file
try:
    # optional import; used for styling
    from openpyxl import load_workbook
    from openpyxl.styles import Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False



def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        # convert floats NaN handled above
        return x
    try:
        # fallback to string
        return str(x)
    except Exception:
        return None


_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False


# Helper: format seconds to HH:MM:SS
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return None
        # guard against floats and NaN
        s = int(float(seconds))
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return None


# Placeholder tokens (keep consistent with trend_runner expectations)
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', 'â€”', 'â€“', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


def _resolve_field_from_record(record: dict, candidate_tokens: list):
    """
    Search a single row `record` (dict) for likely columns listed in candidate_tokens.
    Return first non-placeholder value found (converted to Python scalar), else None.
    """
    if record is None:
        return None

    # 1) exact key matches (case-sensitive & common casing)
    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)

    # 2) case-insensitive contains match
    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)

    # 3) xml / value parsing fallback for CardNumber
    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue

    # 4) final fallback: first non-placeholder value
    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)

    return None


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Clean a dataframe for JSON output (convert NaN -> None, pandas types -> native, format datetimes).
    """
    if df is None or df.empty:
        return []
    df = df.copy()

    # remove duplicate suffix columns
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    # Date normalization
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    # Datetime columns to ISO strings
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # Replace NaN/inf -> None
    df = df.where(pd.notnull(df), None)

    # Convert records to safe Python types
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        # Typical fields
        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        # ----- Schema-aware fallback resolution -----
        if not emp_id:
            emp_tokens = ['Int1', 'Text12', 'EmployeeID', 'empid', 'id']
            resolved_emp = _resolve_field_from_record(r, emp_tokens)
            if resolved_emp is not None:
                try:
                    s = str(resolved_emp).strip()
                    # remove trailing .0 for floats
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    if _looks_like_guid(s):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = s
                        emp_id = s
                except Exception:
                    if _looks_like_guid(resolved_emp):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = resolved_emp
                        emp_id = resolved_emp

        # Prefer Credential.CardNumber / CHUID / Card as CardNumber when missing â€” reject GUIDs/placeholders
        if out.get('CardNumber') in (None, '', 'nan'):
            card_tokens = ['CardNumber', 'CHUID', 'Card', 'card_no', 'cardnum']
            resolved_card = _resolve_field_from_record(r, card_tokens)
            if resolved_card is not None:
                try:
                    cs = str(resolved_card).strip()
                    if _looks_like_guid(cs) or _is_placeholder_str(cs):
                        out['CardNumber'] = None
                    else:
                        out['CardNumber'] = cs
                except Exception:
                    out['CardNumber'] = None

        # final safety: ensure EmployeeID/CardNumber are not GUID-like tokens
        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None
        if 'CardNumber' in out and isinstance(out['CardNumber'], str) and _looks_like_guid(out['CardNumber']):
            out['CardNumber'] = None

        # If EmployeeName empty or looks like a GUID, prefer EmployeeID (human id) over GUIDs
        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            if emp_id not in (None, '', 'nan') and not _looks_like_guid(emp_id):
                out['EmployeeName'] = str(emp_id)
            else:
                out['EmployeeName'] = None

        cleaned.append(out)
    return cleaned


@app.route('/')
def root():
    return "Trend Analysis API â€” Pune test"


@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.json or {}

    date_str = params.get('date')
    start_str = params.get('start')
    end_str = params.get('end')

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            # default: include yesterday and today so previous-day evidence gets generated by default
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    combined_rows = []
    files = []
    samples = []

    for d in dates:
        try:
            df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        if df is None or df.empty:
            continue

        # Ensure IsFlagged exists; Reasons only when flagged
        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        # Prefer sample of flagged rows (makes QA easier); else generic sample
        flagged = df[df['IsFlagged'] == True]
        sample_df = flagged.head(10) if not flagged.empty else df.head(10)
        samples.extend(_clean_sample_df(sample_df, max_rows=10))

        combined_rows.append(df)

    combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()

    # Determine best identifier column to count unique persons (priority)
    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in combined_df.columns), None)

    def _norm_id_val(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        # convert floats like "320172.0" -> "320172"
        try:
            if '.' in s:
                f = float(s)
                if math.isfinite(f) and f.is_integer():
                    s = str(int(f))
        except Exception:
            pass
        return s

    if id_col is None or combined_df.empty:
        analysis_count = int(len(combined_df))
        flagged_count = int(combined_df['IsFlagged'].sum()) if 'IsFlagged' in combined_df.columns else 0
    else:
        # if Int1 exists in combined_df, prefer it when normalizing ids
        ids_series = combined_df[id_col].apply(_norm_id_val)
        unique_ids = set([x for x in ids_series.unique() if x])
        analysis_count = int(len(unique_ids))

        # flagged unique persons (using IsFlagged)
        if 'IsFlagged' in combined_df.columns:
            flagged_series = combined_df.loc[combined_df['IsFlagged'] == True, id_col].apply(_norm_id_val)
            flagged_unique = set([x for x in flagged_series.unique() if x])
            flagged_count = int(len(flagged_unique))
        else:
            flagged_count = 0

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "files": files,
        # legacy totals
        "aggregated_rows_total": int(len(combined_df)),
        # new semantics (distinct persons)
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "sample": samples[:20]
    }
    return jsonify(resp)


@app.route('/latest', methods=['GET'])
def latest_results():
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    sample = _clean_sample_df(df, max_rows=5)
    return jsonify({
        "file": latest.name,
        "rows": int(len(df)),
        "sample": sample
    })



@app.route('/record', methods=['GET'])
def get_record():
    """
    /record?employee_id=... or /record?person_uid=...
    Returns matching aggregated trend rows and filtered raw swipe rows (only for flagged persons).
    Updated: read ALL trend CSVs (not just the latest), so previous days are included.
    Also add Zone and SwipeGap (seconds) to raw_swipes returned as evidence.

    New behaviour:
      - By default, raw_swipes are returned only for aggregated rows where IsFlagged == True
      - Set include_unflagged=1 (or true/yes) to also fetch evidence for unflagged aggregated rows.
      - If an aggregated row has no Date/FirstSwipe/LastSwipe, we fall back to scanning all swipes files.
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    # Read all CSVs (concat) so /record will search previous days too
    df_list = []
    for fp in csvs:
        try:
            # removed deprecated infer_datetime_format argument
            tmp = pd.read_csv(fp, parse_dates=['FirstSwipe','LastSwipe'])
        except Exception:
            try:
                tmp = pd.read_csv(fp, dtype=str)
            except Exception:
                continue
        df_list.append(tmp)
    if not df_list:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200
    df = pd.concat(df_list, ignore_index=True)

    if q is None:
        cleaned = _clean_sample_df(df, max_rows=10)
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    q_str = str(q).strip()

    def normalize_series(s):
        if s is None:
            return pd.Series([''] * len(df))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if math.isfinite(fv) and fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    found_mask = pd.Series(False, index=df.index)

    if 'EmployeeID' in df.columns:
        emp_series = normalize_series(df['EmployeeID'])
        found_mask = found_mask | (emp_series == q_str)

    if 'person_uid' in df.columns:
        uid_series = normalize_series(df['person_uid'])
        found_mask = found_mask | (uid_series == q_str)

    # also check Int1 (Personnel.Int1) if present in CSV
    if 'Int1' in df.columns and not found_mask.any():
        int1_series = normalize_series(df['Int1'])
        found_mask = found_mask | (int1_series == q_str)

    if not found_mask.any():
        # try numeric equivalence
        try:
            q_numeric = float(q_str)
            if 'EmployeeID' in df.columns:
                emp_numeric = pd.to_numeric(df['EmployeeID'], errors='coerce')
                found_mask = found_mask | (emp_numeric == q_numeric)
            if 'Int1' in df.columns and not found_mask.any():
                int_numeric = pd.to_numeric(df['Int1'], errors='coerce')
                found_mask = found_mask | (int_numeric == q_numeric)
        except Exception:
            pass

    matched = df[found_mask]
    if matched.empty:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    cleaned_matched = _clean_sample_df(matched, max_rows=len(matched))


    # --- ENRICHMENT START ---
    # cleaned_matched is a list of dicts
    # Try to map each cleaned row to a personnel record and attach email/manager/image info
    try:
        # Build a quick index over matched DataFrame to find best lookup candidate for each cleaned row
        matched_indexed = matched.reset_index(drop=True)
        for idx_c, cleaned in enumerate(cleaned_matched):
            # Try to find a matching row in matched DataFrame by person_uid -> EmployeeID -> EmployeeName
            candidate_row = None
            try:
                if cleaned.get('person_uid'):
                    if 'person_uid' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('person_uid', '').astype(str) == str(cleaned['person_uid'])]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                if candidate_row is None and cleaned.get('EmployeeID'):
                    if 'EmployeeID' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('EmployeeID', '').astype(str) == str(cleaned['EmployeeID'])]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                if candidate_row is None and cleaned.get('EmployeeName'):
                    # names exact match (defensive)
                    if 'EmployeeName' in matched_indexed.columns:
                        mr = matched_indexed[matched_indexed.get('EmployeeName', '').astype(str).str.strip().fillna('') == str(cleaned['EmployeeName']).strip()]
                        if not mr.empty:
                            candidate_row = mr.iloc[0].to_dict()
                # fallback: take first matched row if nothing else
                if candidate_row is None and len(matched_indexed) > 0:
                    candidate_row = matched_indexed.iloc[0].to_dict()
            except Exception:
                candidate_row = None

            # Determine the best candidate identifier to query Personnel
            lookup_candidates = []
            if candidate_row:
                for k in ('EmployeeObjID', 'EmployeeObjId', 'EmployeeIdentity', 'ObjectID', 'GUID', 'EmployeeID', 'Int1', 'Text12', 'EmployeeName'):
                    if k in candidate_row and candidate_row.get(k) not in (None, '', 'nan'):
                        lookup_candidates.append(candidate_row.get(k))
            # also include cleaned fields
            for k in ('EmployeeID', 'person_uid', 'EmployeeName'):
                if cleaned.get(k) not in (None, '', 'nan'):
                    lookup_candidates.append(cleaned.get(k))

            # try each lookup candidate until we find personnel info
            personnel_info = {}
            for cand in lookup_candidates:
                if cand is None:
                    continue
                try:
                    info = get_personnel_info(cand)
                    if info and (info.get('ObjectID') is not None or info.get('EmailAddress') is not None or info.get('ManagerEmail') is not None):
                        personnel_info = info
                        break
                except Exception:
                    continue

            # attach if found
            if personnel_info:
                # safe assignments
                cleaned['EmployeeObjID'] = personnel_info.get('ObjectID')
                cleaned['EmployeeEmail'] = personnel_info.get('EmailAddress')
                cleaned['ManagerEmail'] = personnel_info.get('ManagerEmail')
                # HasImage and imageUrl
                if personnel_info.get('ObjectID') is not None:
                    cleaned['imageUrl'] = f"/employee/{personnel_info.get('ObjectID')}/image"
                    # quick check if image exists (non-blocking: try fetch bytes but ignore failure)
                    try:
                        b = get_person_image_bytes(personnel_info.get('ObjectID'))
                        cleaned['HasImage'] = True if b else False
                    except Exception:
                        cleaned['HasImage'] = False
                else:
                    cleaned['imageUrl'] = None
                    cleaned['HasImage'] = False
    except Exception:
        logging.exception("Failed to enrich aggregated rows with personnel/email/image info")
    # --- ENRICHMENT END ---


    # Resolve raw swipe file names by Date (collect all dates present in matched rows)
    raw_files = set()
    raw_swipes_out = []

    # Helper to add and dedupe swipe rows
    seen_swipe_keys = set()
    def _append_swipe(out_row, source_name):
        # create a dedupe key (date,time,door,direction,card)
        key = (
            out_row.get('Date') or '',
            out_row.get('Time') or '',
            (out_row.get('Door') or '').strip(),
            (out_row.get('Direction') or '').strip(),
            (out_row.get('CardNumber') or out_row.get('Card') or '').strip()
        )
        if key in seen_swipe_keys:
            return
        seen_swipe_keys.add(key)
        out_row['_source'] = source_name
        raw_swipes_out.append(out_row)

    # iterate matched aggregated rows and search raw swipe files
    for idx, agg_row in matched.iterrows():
        person_uid = agg_row.get('person_uid') if 'person_uid' in agg_row else None
        empid = agg_row.get('EmployeeID') if 'EmployeeID' in agg_row else None
        if (not empid) and 'Int1' in agg_row:
            empid = agg_row.get('Int1')
        card = agg_row.get('CardNumber') if 'CardNumber' in agg_row else None

        # build dates_for_row from Date / FirstSwipe / LastSwipe
        dates_for_row = set()
        if 'Date' in agg_row and pd.notna(agg_row['Date']):
            try:
                d = pd.to_datetime(agg_row['Date']).date()
                dates_for_row.add(d.isoformat())
            except Exception:
                pass
        for col in ('FirstSwipe','LastSwipe'):
            if col in agg_row and pd.notna(agg_row[col]):
                try:
                    d = pd.to_datetime(agg_row[col]).date()
                    dates_for_row.add(d.isoformat())
                except Exception:
                    pass

        # If aggregated row is not flagged and include_unflagged is False, skip fetching raw evidence
        is_flagged = bool(agg_row.get('IsFlagged', False))
        if (not is_flagged) and (not include_unflagged):
            continue

        # If no dates found, fallback to scanning all swipes files (so we don't miss evidence)
        dates_to_scan = dates_for_row or {None}

        for d in dates_to_scan:
            try:
                if d is None:
                    # scan all swipes files
                    candidates = list(Path(DEFAULT_OUTDIR).glob(f"swipes_*.csv"))
                else:
                    dd = d[:10]  # 'YYYY-MM-DD'
                    target = dd.replace('-', '')
                    candidates = list(Path(DEFAULT_OUTDIR).glob(f"swipes_*_{target}.csv"))

                if not candidates:
                    continue

                for fp in candidates:
                    raw_name = fp.name
                    raw_files.add(raw_name)
                    try:
                        # removed deprecated infer_datetime_format argument
                        raw_df = pd.read_csv(fp, parse_dates=['LocaleMessageTime'])
                    except Exception:
                        try:
                            raw_df = pd.read_csv(fp, dtype=str)
                        except Exception:
                            continue

                    cols_lower = {c.lower(): c for c in raw_df.columns}
                    tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
                    emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                    name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
                    card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
                    door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
                    dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
                    note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None

                    # build filter mask
                    mask = pd.Series(False, index=raw_df.index)
                    if person_uid is not None and 'person_uid' in raw_df.columns:
                        mask = mask | (raw_df['person_uid'].astype(str).str.strip() == str(person_uid).strip())
                    if emp_col:
                        if empid is not None:
                            try:
                                cmp_val = str(empid).strip()
                                if '.' in cmp_val:
                                    fv = float(cmp_val)
                                    if math.isfinite(fv) and fv.is_integer():
                                        cmp_val = str(int(fv))
                            except Exception:
                                cmp_val = str(empid).strip()
                            mask = mask | (raw_df[emp_col].astype(str).str.strip() == cmp_val)
                    if card_col and card is not None:
                        mask = mask | (raw_df[card_col].astype(str).str.strip() == str(card).strip())

                    if (not mask.any()) and name_col and 'EmployeeName' in agg_row and pd.notna(agg_row.get('EmployeeName')):
                        mask = mask | (raw_df[name_col].astype(str).str.strip() == str(agg_row.get('EmployeeName')).strip())

                    # filter by date if possible
                    if d is not None and tcol and tcol in raw_df.columns:
                        try:
                            raw_df[tcol] = pd.to_datetime(raw_df[tcol], errors='coerce')
                            mask = mask & (raw_df[tcol].dt.date == pd.to_datetime(d).date())
                        except Exception:
                            pass

                    filtered = raw_df[mask].copy()
                    if filtered.empty:
                        # xml value fallback for embedded card values
                        if card is not None:
                            for ccol in raw_df.columns:
                                cl = ccol.lower()
                                if 'xml' in cl or 'msg' in cl or 'value' == cl:
                                    try:
                                        vals = raw_df[ccol].dropna().astype(str)
                                        match_mask = vals.apply(lambda x: (_extract_card_from_xml_text(x) == str(card).strip()))
                                        if match_mask.any():
                                            idxs = match_mask.index[match_mask]
                                            filtered = raw_df.loc[idxs].copy()
                                            break
                                    except Exception:
                                        continue
                        if filtered.empty:
                            continue

                    # enrich filtered rows and append to output (deduped)
                    try:
                        if tcol and tcol in filtered.columns:
                            filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                        else:
                            if 'localemessagetime' in filtered.columns:
                                filtered['localemessagetime'] = pd.to_datetime(filtered['localemessagetime'], errors='coerce')
                                tcol = 'localemessagetime'
                    except Exception:
                        pass

                    if tcol and tcol in filtered.columns:
                        filtered = filtered.sort_values(by=tcol)
                        filtered['_prev_ts'] = filtered[tcol].shift(1)
                        try:
                            filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                        except Exception:
                            filtered['_swipe_gap_seconds'] = 0.0
                    else:
                        filtered['_swipe_gap_seconds'] = 0.0

                    try:
                        if door_col and door_col in filtered.columns:
                            if dir_col and dir_col in filtered.columns:
                                filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                            else:
                                filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
                        else:
                            if 'PartitionName2' in filtered.columns:
                                filtered['_zone'] = filtered['PartitionName2'].fillna('').astype(str).apply(lambda x: x if x else None)
                            else:
                                filtered['_zone'] = None
                    except Exception:
                        filtered['_zone'] = None

                    # normalize and append
                    for _, r in filtered.iterrows():
                        out = {}
                        out['EmployeeName'] = _to_python_scalar(r.get(name_col)) if name_col and name_col in raw_df.columns else _to_python_scalar(agg_row.get('EmployeeName') or agg_row.get('person_uid'))

                        # EmployeeID
                        emp_val = None
                        if 'int1' in cols_lower and cols_lower.get('int1') in raw_df.columns:
                            emp_val = _to_python_scalar(r.get(cols_lower.get('int1')))
                        elif emp_col and emp_col in raw_df.columns:
                            emp_val = _to_python_scalar(r.get(emp_col))
                        else:
                            possible_emp = None
                            for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                                if cand.lower() in cols_lower:
                                    possible_emp = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                                    if possible_emp not in (None, '', 'nan'):
                                        break
                            emp_val = possible_emp if possible_emp not in (None, '', 'nan') else _to_python_scalar(agg_row.get('EmployeeID'))

                        if emp_val is not None:
                            try:
                                s = str(emp_val).strip()
                                if '.' in s:
                                    f = float(s)
                                    if math.isfinite(f) and f.is_integer():
                                        s = str(int(f))
                                if _looks_like_guid(s) or _is_placeholder_str(s):
                                    emp_val = None
                                else:
                                    emp_val = s
                            except Exception:
                                if _looks_like_guid(emp_val):
                                    emp_val = None
                        out['EmployeeID'] = emp_val

                        # CardNumber
                        card_val = None
                        if 'cardnumber' in cols_lower and cols_lower.get('cardnumber') in raw_df.columns:
                            card_val = _to_python_scalar(r.get(cols_lower.get('cardnumber')))
                        elif card_col and card_col in raw_df.columns:
                            card_val = _to_python_scalar(r.get(card_col))
                        else:
                            possible_card = None
                            for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                                if cand.lower() in cols_lower:
                                    possible_card = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                                    if possible_card not in (None, '', 'nan'):
                                        break
                            if possible_card in (None, '', 'nan'):
                                for ccc in raw_df.columns:
                                    cl = ccc.lower()
                                    if 'xml' in cl or 'msg' in cl or 'value' == cl:
                                        try:
                                            txt = r.get(ccc)
                                            extracted = _extract_card_from_xml_text(str(txt)) if txt is not None else None
                                            if extracted:
                                                possible_card = extracted
                                                break
                                        except Exception:
                                            continue
                            card_val = possible_card if possible_card not in (None, '', 'nan') else _to_python_scalar(agg_row.get('CardNumber'))

                        if card_val is not None:
                            try:
                                cs = str(card_val).strip()
                                if _looks_like_guid(cs) or _is_placeholder_str(cs):
                                    card_val = None
                                else:
                                    card_val = cs
                            except Exception:
                                card_val = None
                        out['CardNumber'] = card_val

                        # timestamp -> Date/Time
                        if tcol and tcol in raw_df.columns:
                            ts = r.get(tcol)
                            try:
                                ts_py = pd.to_datetime(ts)
                                out['Date'] = ts_py.date().isoformat()
                                out['Time'] = ts_py.time().isoformat()
                            except Exception:
                                txt = str(r.get(tcol))
                                out['Date'] = txt[:10]
                                out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                        else:
                            out['Date'] = d if d is not None else None
                            out['Time'] = None

                        out['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
                        out['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in r else None
                        out['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None

                        try:
                            out['Zone'] = _to_python_scalar(r.get('_zone')) if '_zone' in r else map_door_to_zone(out['Door'], out['Direction'])
                        except Exception:
                            out['Zone'] = None
                        try:
                            gap = r.get('_swipe_gap_seconds') if '_swipe_gap_seconds' in r else None
                            out['SwipeGapSeconds'] = float(gap) if gap is not None else None
                            out['SwipeGap'] = format_seconds_to_hms(out['SwipeGapSeconds'])
                        except Exception:
                            out['SwipeGapSeconds'] = None
                            out['SwipeGap'] = None

                        _append_swipe(out, raw_name)
            except Exception as e:
                logging.exception("Error processing raw swipe file for date %s: %s", d, e)
                continue

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": sorted(list(raw_files)),
        "raw_swipes": raw_swipes_out
    }), 200



@app.route('/record/export', methods=['GET'])
def export_record_excel():
    """
    /record/export?employee_id=...&date=YYYY-MM-DD  OR /record/export?person_uid=...&date=YYYY-MM-DD
    Produces an Excel file (xlsx) filtered for the requested employee and date (if provided).
    Two sheets:
      - "Details â€” Evidence": EmployeeName, EmployeeID, Door, Direction, Zone, Date, LocaleMessageTime, SwipeGapSeconds, PartitionName2, _source_file
      - "Swipe timeline": Employee Name, Employee ID, Card, Date, Time, SwipeGapSeconds, Door, Direction, Zone, Note
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    date_str = request.args.get('date')  # optional 'YYYY-MM-DD' (single date)
    if not q:
        return jsonify({"error":"employee_id or person_uid is required"}), 400

    # Determine list of raw swipe files to scan. If date provided, restrict to that date only.
    files_to_scan = []
    p = Path(DEFAULT_OUTDIR)
    if date_str:
        try:
            dd = pd.to_datetime(date_str).date()
            target = dd.strftime("%Y%m%d")
            candidates = list(p.glob(f"swipes_*_{target}.csv"))
            files_to_scan = candidates
        except Exception:
            return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400
    else:
        # scan all swipes files (most recent first)
        files_to_scan = sorted(p.glob("swipes_*.csv"), reverse=True)

    if not files_to_scan:
        return jsonify({"error":"no raw swipe files found for requested date / outputs"}), 404

    all_rows = []
    for fp in files_to_scan:
        try:
            # removed deprecated infer_datetime_format argument
            raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
        except Exception:
            try:
                raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

        # normalize column names
        cols_lower = {c.lower(): c for c in raw_df.columns}
        # pick possible columns
        tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
        emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
        name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
        card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
        door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
        dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
        note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None
        person_uid_col = cols_lower.get('person_uid')

        # build mask matching requested q: try person_uid, emp_col, card, name
        mask = pd.Series(False, index=raw_df.index)
        if person_uid_col and person_uid_col in raw_df.columns:
            mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == str(q).strip())
        if emp_col and emp_col in raw_df.columns:
            mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(q).strip())
        # try matching numeric equivalence
        if not mask.any() and emp_col and emp_col in raw_df.columns:
            try:
                q_numeric = float(q)
                emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                mask = mask | (emp_numeric == q_numeric)
            except Exception:
                pass
        # also try name match if nothing matched
        if not mask.any() and name_col and name_col in raw_df.columns:
            mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == str(q).strip().lower())

        if not mask.any():
            # nothing to include from this file
            continue

        filtered = raw_df[mask].copy()
        if filtered.empty:
            continue

        # ensure timestamp col exists and parsed
        if tcol and tcol in filtered.columns:
            try:
                filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
            except Exception:
                pass

        # compute swipe gaps (seconds)
        if tcol and tcol in filtered.columns:
            filtered = filtered.sort_values(by=tcol)
            filtered['_prev_ts'] = filtered[tcol].shift(1)
            try:
                filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
            except Exception:
                filtered['_swipe_gap_seconds'] = 0.0
        else:
            filtered['_swipe_gap_seconds'] = 0.0

        # compute zone per row (use door+direction if available)
        try:
            if door_col and door_col in filtered.columns:
                if dir_col and dir_col in filtered.columns:
                    filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                else:
                    filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
            else:
                filtered['_zone'] = filtered.get('PartitionName2', None)
        except Exception:
            filtered['_zone'] = None

        # normalize columns into common shape and append rows
        for _, r in filtered.iterrows():
            row = {}
            row['EmployeeName'] = _to_python_scalar(r.get(name_col)) if (name_col and name_col in filtered.columns) else None
            # EmployeeID attempts: Int1/Text12/EmployeeID
            emp_val = None
            if emp_col and emp_col in filtered.columns:
                emp_val = _to_python_scalar(r.get(emp_col))
            else:
                # fallbacks
                for cand in ('int1','text12','employeeid','employee_identity','employeeidentity'):
                    if cand in cols_lower and cols_lower[cand] in filtered.columns:
                        emp_val = _to_python_scalar(r.get(cols_lower[cand]))
                        if emp_val:
                            break
            row['EmployeeID'] = emp_val
            row['Card'] = _to_python_scalar(r.get(card_col)) if (card_col and card_col in filtered.columns) else None

            # Date and Time
            if tcol and tcol in filtered.columns:
                ts = r.get(tcol)
                try:
                    ts_py = pd.to_datetime(ts)
                    row['Date'] = ts_py.date().isoformat()
                    row['Time'] = ts_py.time().isoformat()
                    row['LocaleMessageTime'] = ts_py.isoformat()
                except Exception:
                    txt = str(r.get(tcol))
                    row['Date'] = txt[:10]
                    row['Time'] = txt[11:19] if len(txt) >= 19 else None
                    row['LocaleMessageTime'] = txt
            else:
                row['Date'] = None
                row['Time'] = None
                row['LocaleMessageTime'] = None

            row['SwipeGapSeconds'] = float(r.get('_swipe_gap_seconds')) if '_swipe_gap_seconds' in r else 0.0
            row['SwipeGap'] = format_seconds_to_hms(row['SwipeGapSeconds'])

            row['Door'] = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else None
            row['Direction'] = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else None
            row['Note'] = _to_python_scalar(r.get(note_col)) if (note_col and note_col in filtered.columns) else None

            # Zone (computed above)
            try:
                zone_val = r.get('_zone') if '_zone' in r else None
                if zone_val is None:
                    # fallback from door/direction
                    zone_val = map_door_to_zone(row['Door'], row['Direction'])
                row['Zone'] = _to_python_scalar(zone_val)
            except Exception:
                row['Zone'] = None

            row['PartitionName2'] = _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None
            row['_source_file'] = fp.name
            all_rows.append(row)

    if not all_rows:
        return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

    df_out = pd.DataFrame(all_rows)

    # Build two sheets as requested (with exact requested column order)
    details_cols = ['EmployeeName','EmployeeID','Door','Direction','Zone','Date','LocaleMessageTime','SwipeGapSeconds','PartitionName2','_source_file']
    timeline_cols = ['EmployeeName','EmployeeID','Card','Date','Time','SwipeGapSeconds','Door','Direction','Zone','Note','_source_file']

    details_df = df_out[[c for c in details_cols if c in df_out.columns]].copy()
    timeline_df = df_out[[c for c in timeline_cols if c in df_out.columns]].copy()

    # Create excel in-memory
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            details_df.to_excel(writer, sheet_name='Details â€” Evidence', index=False)
            timeline_df.to_excel(writer, sheet_name='Swipe timeline', index=False)
            writer.save()
            output.seek(0)
    except Exception as e:
        logging.exception("Failed to create Excel: %s", e)
        return jsonify({"error":"failed to create excel"}), 500

    # If openpyxl available, apply formatting (bold header, center align, borders)
    if OPENPYXL_AVAILABLE:
        try:
            wb = load_workbook(output)
            thin = Side(border_style="thin", color="000000")
            thick = Side(border_style="medium", color="000000")
            for ws in wb.worksheets:
                # header styling
                header = ws[1]
                for cell in header:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
                # data rows: center & thin border
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                        cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
                # autosize columns (best-effort)
                for col in ws.columns:
                    max_len = 0
                    col_letter = col[0].column_letter
                    for cell in col:
                        try:
                            v = str(cell.value) if cell.value is not None else ""
                        except Exception:
                            v = ""
                        if len(v) > max_len:
                            max_len = len(v)
                    # limit column width
                    width = min(max(10, max_len + 2), 50)
                    ws.column_dimensions[col_letter].width = width
            # write back to bytes
            out2 = io.BytesIO()
            wb.save(out2)
            out2.seek(0)
            return send_file(out2, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        except Exception:
            logging.exception("Excel styling failed, returning raw file")
            output.seek(0)
            return send_file(output, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    else:
        # fallback: return raw excel binary without styling
        output.seek(0)
        return send_file(output, as_attachment=True,
                         download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                         mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")


@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    """
    Serve raw swipe CSVs from outputs/ (filename should be the file name only).
    """
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    # send file
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)


@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500


# Serve employee image route (defined after app)
@app.route('/employee/<empid>/image', methods=['GET'])
def serve_employee_image(empid):
    """
    Serve the portrait for a personnel record where ParentId = empid (ACVSCore.Images).
    Uses the query you provided:
      SELECT AI.Image AS ImageBuffer FROM ACVSCore.Access.Images AI WHERE AI.ParentId = @id
    If found, returns binary with best-effort content-type detection.
    """
    if empid is None:
        return jsonify({"error": "employee id required"}), 400

    try:
        img_bytes = get_person_image_bytes(empid)
        if not img_bytes:
            return jsonify({"error": "no image found"}), 404

        # try to detect jpeg/png
        header = img_bytes[:8]
        content_type = 'application/octet-stream'
        if len(header) >= 2 and header[0] == 0xFF and header[1] == 0xD8:
            content_type = 'image/jpeg'
        elif header.startswith(b'\x89PNG\r\n\x1a\n'):
            content_type = 'image/png'
        # send as file-like
        bio = io.BytesIO(img_bytes)
        bio.seek(0)
        return send_file(bio, mimetype=content_type)
    except Exception:
        logging.exception("Error serving image for employee %s", empid)
        return jsonify({"error": "failed to serve image"}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)




