Check Below Console Error as welll Api Responce carefully and Fix the issue carefully and Give me Fully Updated file caredully


(.venv) PS C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics> python -m uvicorn app:app --host 0.0.0.0 --port 8000
INFO:     Started server process [32084]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:49673 - "GET /ccure/stream HTTP/1.1" 200 OK
INFO:     127.0.0.1:52307 - "GET /ccure/stream HTTP/1.1" 200 OK
[compute_daily_attendance] no swipes for 2025-09-02
2025-09-02 15:13:47,536 INFO data_compare_service_v2: [presence_fetch] fallback broad DB query returned 0 rows for 2025-09-01 -> 2025-09-05
2025-09-02 15:13:47,541 INFO data_compare_service_v2: [presence_fetch] DB-derived presence found for 0/1132 employees
2025-09-02 15:13:47,542 INFO data_compare_service_v2: [presence_fetch] DB coverage low (0/1132) - trying region occupancy history fallback
2025-09-02 15:13:55,560 ERROR data_compare_service_v2: [region_history] failed to fetch/parse http://10.199.22.57:3008/api/occupancy/history
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connection.py", line 565, in getresponse
    httplib_response = super().getresponse()
  File "C:\Program Files\Python313\Lib\http\client.py", line 1430, in getresponse
    response.begin()
    ~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 292, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\socket.py", line 719, in readinto
    return self._sock.recv_into(b)
           ~~~~~~~~~~~~~~~~~~~~^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\util.py", line 39, in reraise
    raise value
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 536, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 367, in _raise_timeout
    raise ReadTimeoutError(
        self, url, f"Read timed out. (read timeout={timeout_value})"
    ) from err
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service_v2.py", line 189, in _fetch_presence_from_region_histories
    resp = requests.get(url, timeout=8)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 713, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=8)
2025-09-02 15:14:03,582 ERROR data_compare_service_v2: [region_history] failed to fetch/parse http://10.199.22.57:3006/api/occupancy/history
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connection.py", line 565, in getresponse
    httplib_response = super().getresponse()
  File "C:\Program Files\Python313\Lib\http\client.py", line 1430, in getresponse
    response.begin()
    ~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 292, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\socket.py", line 719, in readinto
    return self._sock.recv_into(b)
           ~~~~~~~~~~~~~~~~~~~~^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\util.py", line 39, in reraise
    raise value
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 536, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 367, in _raise_timeout
    raise ReadTimeoutError(
        self, url, f"Read timed out. (read timeout={timeout_value})"
    ) from err
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service_v2.py", line 189, in _fetch_presence_from_region_histories
    resp = requests.get(url, timeout=8)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 713, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='10.199.22.57', port=3006): Read timed out. (read timeout=8)
2025-09-02 15:14:11,594 ERROR data_compare_service_v2: [region_history] failed to fetch/parse http://10.199.22.57:3007/api/occupancy/history
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connection.py", line 565, in getresponse
    httplib_response = super().getresponse()
  File "C:\Program Files\Python313\Lib\http\client.py", line 1430, in getresponse
    response.begin()
    ~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 292, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\socket.py", line 719, in readinto
    return self._sock.recv_into(b)
           ~~~~~~~~~~~~~~~~~~~~^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\util.py", line 39, in reraise
    raise value
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 536, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 367, in _raise_timeout
    raise ReadTimeoutError(
        self, url, f"Read timed out. (read timeout={timeout_value})"
    ) from err
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='10.199.22.57', port=3007): Read timed out. (read timeout=8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service_v2.py", line 189, in _fetch_presence_from_region_histories
    resp = requests.get(url, timeout=8)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 713, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='10.199.22.57', port=3007): Read timed out. (read timeout=8)
2025-09-02 15:14:19,621 ERROR data_compare_service_v2: [region_history] failed to fetch/parse http://10.199.22.57:4000/api/occupancy/history
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connection.py", line 565, in getresponse
    httplib_response = super().getresponse()
  File "C:\Program Files\Python313\Lib\http\client.py", line 1430, in getresponse
    response.begin()
    ~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 292, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\socket.py", line 719, in readinto
    return self._sock.recv_into(b)
           ~~~~~~~~~~~~~~~~~~~~^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\util.py", line 39, in reraise
    raise value
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 536, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 367, in _raise_timeout
    raise ReadTimeoutError(
        self, url, f"Read timed out. (read timeout={timeout_value})"
    ) from err
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service_v2.py", line 189, in _fetch_presence_from_region_histories
    resp = requests.get(url, timeout=8)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 713, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=8)
2025-09-02 15:14:19,629 INFO data_compare_service_v2: [region_history] scanned 0 detail rows across 4 endpoints; matched 0 presence entries
2025-09-02 15:14:19,632 INFO data_compare_service_v2: [presence_fetch] final presence coverage: 0/1132 employees have at least one positive day
INFO:     127.0.0.1:64772 - "GET /ccure/compare_v2?region_filter=APAC&location_city=Pune HTTP/1.1" 200 OK
2025-09-02 15:14:25,635 WARNING region_clients: [region_clients] attempt 1/2 failed for http://10.199.22.57:4000/api/occupancy/history: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=20)       
INFO:     127.0.0.1:61949 - "GET /ccure/verify?raw=true HTTP/1.1" 200 OK
2025-09-02 15:15:08,519 INFO data_compare_service_v2: [presence_fetch] fallback broad DB query returned 0 rows for 2025-09-01 -> 2025-09-05
2025-09-02 15:15:08,525 INFO data_compare_service_v2: [presence_fetch] DB-derived presence found for 0/1132 employees
2025-09-02 15:15:08,525 INFO data_compare_service_v2: [presence_fetch] DB coverage low (0/1132) - trying region occupancy history fallback
2025-09-02 15:15:16,557 ERROR data_compare_service_v2: [region_history] failed to fetch/parse http://10.199.22.57:3008/api/occupancy/history
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connection.py", line 565, in getresponse
    httplib_response = super().getresponse()
  File "C:\Program Files\Python313\Lib\http\client.py", line 1430, in getresponse
    response.begin()
    ~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 292, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\socket.py", line 719, in readinto
    return self._sock.recv_into(b)
           ~~~~~~~~~~~~~~~~~~~~^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\util.py", line 39, in reraise
    raise value
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 536, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 367, in _raise_timeout
    raise ReadTimeoutError(
        self, url, f"Read timed out. (read timeout={timeout_value})"
    ) from err
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service_v2.py", line 189, in _fetch_presence_from_region_histories
    resp = requests.get(url, timeout=8)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 713, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='10.199.22.57', port=3008): Read timed out. (read timeout=8)
2025-09-02 15:15:30,821 ERROR data_compare_service_v2: [region_history] failed to fetch/parse http://10.199.22.57:3007/api/occupancy/history
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connection.py", line 565, in getresponse
    httplib_response = super().getresponse()
  File "C:\Program Files\Python313\Lib\http\client.py", line 1430, in getresponse
    response.begin()
    ~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 292, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\socket.py", line 719, in readinto
    return self._sock.recv_into(b)
           ~~~~~~~~~~~~~~~~~~~~^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\util.py", line 39, in reraise
    raise value
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 536, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 367, in _raise_timeout
    raise ReadTimeoutError(
        self, url, f"Read timed out. (read timeout={timeout_value})"
    ) from err
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='10.199.22.57', port=3007): Read timed out. (read timeout=8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service_v2.py", line 189, in _fetch_presence_from_region_histories
    resp = requests.get(url, timeout=8)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 713, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='10.199.22.57', port=3007): Read timed out. (read timeout=8)
2025-09-02 15:15:38,867 ERROR data_compare_service_v2: [region_history] failed to fetch/parse http://10.199.22.57:4000/api/occupancy/history
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connection.py", line 565, in getresponse
    httplib_response = super().getresponse()
  File "C:\Program Files\Python313\Lib\http\client.py", line 1430, in getresponse
    response.begin()
    ~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\Python313\Lib\http\client.py", line 292, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "C:\Program Files\Python313\Lib\socket.py", line 719, in readinto
    return self._sock.recv_into(b)
           ~~~~~~~~~~~~~~~~~~~~^^^
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\util\util.py", line 39, in reraise
    raise value
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 536, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\urllib3\connectionpool.py", line 367, in _raise_timeout
    raise ReadTimeoutError(
        self, url, f"Read timed out. (read timeout={timeout_value})"
    ) from err
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service_v2.py", line 189, in _fetch_presence_from_region_histories
    resp = requests.get(url, timeout=8)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\.venv\Lib\site-packages\requests\adapters.py", line 713, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='10.199.22.57', port=4000): Read timed out. (read timeout=8)
2025-09-02 15:15:38,877 INFO data_compare_service_v2: [region_history] scanned 6176 detail rows across 4 endpoints; matched 0 presence entries
2025-09-02 15:15:38,885 INFO data_compare_service_v2: [presence_fetch] final presence coverage: 0/1132 employees have at least one positive day
INFO:     127.0.0.1:58120 - "GET /ccure/compare_v2?region_filter=APAC&location_city=Pune HTTP/1.1" 200 OK




http://localhost:8000/ccure/compare_v2?region_filter=APAC&location_city=Pune


{
  "mode": "full",
  "stats_detail": "ActiveProfiles",
  "summary": {
    "filters": {
      "region": "APAC",
      "location_city": "Pune",
      "location_state": null,
      "location_description": null,
      "week_monday": "2025-09-01",
      "week_friday": "2025-09-05"
    },
    "counts": {
      "total_active_in_sheet": 1132,
      "today_headcount_from_summary": 0,
      "today_headcount_pct_vs_sheet": 0,
      "on_leave_count_in_sheet": 15,
      "employee_type_counts": {
        "regular": 1132
      }
    },
    "regular_attendance_summary": {
      "regular_total": 1132,
      "present_5_day_count": 0,
      "present_3_or_more_count": 0,
      "present_less_than_3_count": 1132,
      "present_only_1_day_count": 0
    }
  },
  "details": {
    "present_5_days": [],
    "present_3_or_more_days": [],
    "defaulters_less_than_3_days": [
      {
        "employee_id": "319473",
        "full_name": "., Anushka",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "324002",
        "full_name": "., Diwakar",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "323879",
        "full_name": "., Vikas",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "320818",
        "full_name": "ANAND, NUPUR",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "317710",
        "full_name": "Abdul Hafeez, Sameera Begum",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "323050",
        "full_name": "Abey, Jinsy",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "311840",
        "full_name": "Abhale, Satish",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "323873",
        "full_name": "Abhishek, Mahesh",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "312751",
        "full_name": "Ablankar, Sujit",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "327185",
        "full_name": "Adda, Rohith Kumar",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "325046",
        "full_name": "Adik, Dhananjay",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "326419",
        "full_name": "Adoni, Sureshkumar",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "309779",
        "full_name": "Agarwal, Aseem",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "327099",
        "full_name": "Agarwal, Mihir",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "323880",
        "full_name": "Agarwal, Shivam",
        "days_present": 0,
        "on_leave": false
      },
      {
        "employee_id": "327034",
        "full_name": "Aggarwal, Ketan",
        "days_present": 0,
        "on_leave": false
      },









# C:\Users\W0024618\Desktop\global-page\backend\attendance-analytics\data_compare_service_v2.py
"""
New comparison service (v2) - improved presence fetching using:
  - AttendanceSummary DB (chunked IN queries + broad range)
  - Fallback: occupancy history endpoints (region services) to derive presence by EmployeeID/CardNumber
  - Respects filters (region/location/partition) when possible
"""
import sys
import re
import uuid
import logging
from pathlib import Path
from datetime import date, datetime, timedelta
from typing import Optional, Dict, Any, List

import pandas as pd

# HTTP client for region histories
try:
    import requests
except Exception:
    requests = None

# DB imports (same as your project)
from db import SessionLocal
from models import AttendanceSummary, ActiveEmployee

# Use settings if present to find DATA_DIR / OUTPUT_DIR and REGION_HISTORY_URLS
try:
    from settings import DATA_DIR as SETTINGS_DATA_DIR, OUTPUT_DIR as SETTINGS_OUTPUT_DIR, REGION_HISTORY_URLS as SETTINGS_REGION_HISTORY_URLS
    DATA_DIR = Path(SETTINGS_DATA_DIR)
    OUTPUT_DIR = Path(SETTINGS_OUTPUT_DIR)
    REGION_HISTORY_URLS = SETTINGS_REGION_HISTORY_URLS
except Exception:
    DATA_DIR = Path(__file__).resolve().parent / "data"
    OUTPUT_DIR = Path(__file__).resolve().parent / "output"
    # sensible defaults based on examples you provided; can be overridden via settings.py
    REGION_HISTORY_URLS = [
        "http://10.199.22.57:3008/api/occupancy/history",  # APAC (example)
        "http://10.199.22.57:3006/api/occupancy/history",  # NAMER
        "http://10.199.22.57:3007/api/occupancy/history",  # EMEA
        "http://10.199.22.57:4000/api/occupancy/history"   # LACA
    ]

DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger("data_compare_service_v2")
logger.setLevel(logging.INFO)
if not logger.handlers:
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s"))
    logger.addHandler(ch)


# ----------------------------
# Helpers
# ----------------------------
def _find_active_employee_file():
    for ext in (".xlsx", ".xls", ".csv"):
        p = DATA_DIR / f"active_employee{ext}"
        if p.exists():
            return p
    # try case-insensitive search
    for p in DATA_DIR.iterdir():
        if p.is_file() and "active_employee" in p.name.lower():
            return p
    return None

def _normalize_key(k):
    if k is None:
        return None
    return str(k).strip()

def _safe_int(v, default=0):
    try:
        if v is None or v == "":
            return default
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return default

def _week_monday_and_friday(ref_date: Optional[date] = None):
    if ref_date is None:
        ref_date = date.today()
    # ISO weekday: Monday=1
    iso = ref_date.isoweekday()
    monday = ref_date - timedelta(days=(iso - 1))
    friday = monday + timedelta(days=4)
    return monday, friday

def _maybe_mark_on_leave(status_str: Optional[str]) -> bool:
    if not status_str:
        return False
    s = str(status_str).strip().lower()
    for tok in ("leave", "vacation", "on leave", "holiday", "sabbatical", "furlough", "loa"):
        if tok in s:
            return True
    return False

# ----------------------------
# Core loader: active employees (sheet)
# ----------------------------
def load_active_employees_dataframe() -> pd.DataFrame:
    """
    Loads canonical active_employee spreadsheet into a normalized DataFrame.
    """
    src = _find_active_employee_file()
    if not src:
        raise FileNotFoundError(f"Active employee canonical file not found in {DATA_DIR}")
    ext = src.suffix.lower()
    if ext in (".xls", ".xlsx"):
        df = pd.read_excel(src, sheet_name=0, dtype=str)
    else:
        df = pd.read_csv(src, dtype=str)

    # normalize column names (strip)
    df.columns = [c.strip() for c in df.columns]

    def _first_present(row, candidates):
        for c in candidates:
            if c in row and pd.notna(row[c]) and str(row[c]).strip() != "":
                return row[c]
        return None

    rows = []
    for _, row in df.iterrows():
        emp_id = _first_present(row, ['Employee ID','EmployeeID','Employee Id','EmpID','Emp Id'])
        full_name = _first_present(row, ['Full Name','FullName','EmpName','Name']) or (
            f"{row.get('First Name','') or ''} {row.get('Last Name','') or ''}".strip()
        )
        location_city = _first_present(row, ['Location City','Location City ' ,'Location','City','LocationCity'])
        location_desc = _first_present(row, ['Location Description','Location Description ','LocationDescription'])
        location_state = _first_present(row, ['Location State / Province','Location State / Province ','Location State','State','Province'])
        region_code = _first_present(row, ['Region Code','Region','RegionCode'])
        current_status = _first_present(row, ['Current Status','Status','Employee Status','Employee_Status'])
        employee_type = _first_present(row, ['Employee Type','Type','Time Type'])
        rows.append({
            "employee_id": _normalize_key(emp_id),
            "full_name": _normalize_key(full_name),
            "location_city": _normalize_key(location_city),
            "location_desc": _normalize_key(location_desc),
            "location_state": _normalize_key(location_state),
            "region_code": (str(region_code).strip() if region_code is not None else None),
            "current_status": _normalize_key(current_status),
            "employee_type": _normalize_key(employee_type),
            "raw_row": row.to_dict()
        })
    ndf = pd.DataFrame(rows)
    for col in ("employee_id","full_name","location_city","location_desc","location_state","region_code","current_status","employee_type","raw_row"):
        if col not in ndf:
            ndf[col] = None
    ndf = ndf[ndf["employee_id"].notna() & (ndf["employee_id"].str.strip() != "")]
    ndf.reset_index(drop=True, inplace=True)
    return ndf

# ----------------------------
# Region occupancy history fallback
# ----------------------------
def _fetch_presence_from_region_histories(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None) -> Dict[str, Dict[date,int]]:
    """
    Query configured region occupancy history endpoints for the date range and build presence map.
    Returns mapping { sheet_employee_id: { date_obj: 0/1, ... }, ... }

    partition_filter: if provided (e.g. "Pune"), only consider occupancy detail rows that match the partition name.
    """
    presence = {eid: {} for eid in employee_ids}
    if requests is None:
        logger.warning("[region_history] requests not available, skipping region history fallback")
        # fill zeros for range - caller expects zeros filled later
        return presence

    # Build set and normalized forms for matching
    orig_ids = [str(e).strip() for e in employee_ids]
    norm_set = set([s for s in orig_ids if s])
    digits_map = {e: re.sub(r'\D+', '', e) for e in orig_ids}

    day = start_date
    all_hits = 0
    scanned_details = 0
    for url in REGION_HISTORY_URLS:
        try:
            # call occupancy/history endpoint (no params in sample). If the endpoint accepts query params to restrict dates,
            # you can add them here (e.g. ?from=yyyy-mm-dd&to=yyyy-mm-dd). For now, we fetch and filter locally.
            logger.debug("[region_history] fetching %s", url)
            resp = requests.get(url, timeout=8)
            if resp.status_code != 200:
                logger.debug("[region_history] non-200 from %s: %s", url, resp.status_code)
                continue
            try:
                payload = resp.json()
            except Exception:
                logger.exception("[region_history] invalid json from %s", url)
                continue

            details = payload.get("details") or []
            scanned_details += len(details)

            for d in details:
                try:
                    # Extract date (use LocaleMessageTime if available, else SwipeDate)
                    ts = d.get("LocaleMessageTime") or d.get("SwipeDate") or d.get("SwipeTime")
                    if not ts:
                        continue
                    try:
                        # parse ISO-like timestamps
                        t = datetime.fromisoformat(ts.replace("Z", "+00:00")) if "T" in ts else datetime.fromisoformat(ts)
                    except Exception:
                        # fallback parse
                        try:
                            t = datetime.strptime(ts[:19], "%Y-%m-%dT%H:%M:%S")
                        except Exception:
                            continue
                    dt = t.date()
                    if dt < start_date or dt > end_date:
                        continue

                    # apply partition filter if requested
                    partition_value = (d.get("PartitionNameFriendly") or d.get("PartitionName2") or d.get("PartitionName") or d.get("PrimaryLocation") or "")
                    if partition_filter:
                        if not partition_value:
                            continue
                        if partition_filter.strip().lower() not in str(partition_value).strip().lower():
                            continue

                    scanned_key = None
                    # prefer EmployeeID if present
                    raw_emp = d.get("EmployeeID") or d.get("PersonID") or None
                    if raw_emp and str(raw_emp).strip() != "":
                        db_key = str(raw_emp).strip()
                        if db_key in norm_set:
                            scanned_key = db_key
                        else:
                            # digits-only
                            dd = re.sub(r'\D+', '', db_key)
                            if dd:
                                cand = dd.lstrip('0') or dd
                                # match against orig ids or digits_map
                                for orig in orig_ids:
                                    if orig == cand or digits_map.get(orig) == dd or orig.lstrip('0') == db_key:
                                        scanned_key = orig
                                        break
                    # If still not matched, try CardNumber -> digits match
                    if scanned_key is None:
                        card = d.get("CardNumber") or d.get("BadgeNumber") or d.get("Card")
                        if card:
                            cd = re.sub(r'\D+', '', str(card))
                            if cd:
                                cand = cd.lstrip('0') or cd
                                # see if any orig id digits match
                                for orig in orig_ids:
                                    od = digits_map.get(orig, "")
                                    if od and (od == cd or od.lstrip('0') == cand or orig == cand):
                                        scanned_key = orig
                                        break

                    if scanned_key:
                        all_hits += 1
                        presence.setdefault(scanned_key, {})
                        presence[scanned_key][dt] = 1
                except Exception:
                    continue

        except Exception:
            logger.exception("[region_history] failed to fetch/parse %s", url)
            continue

    logger.info("[region_history] scanned %d detail rows across %d endpoints; matched %d presence entries",
                scanned_details, len(REGION_HISTORY_URLS), all_hits)

    # ensure entries exist for each date in range (caller will also fill zeros)
    cur = start_date
    while cur <= end_date:
        for eid in employee_ids:
            presence.setdefault(eid, {})
            if cur not in presence[eid]:
                presence[eid][cur] = 0
        cur = cur + timedelta(days=1)

    return presence

# ----------------------------
# Attendance summary queries (combined approach)
# ----------------------------
def _fetch_presence_for_employees(employee_ids: List[str], start_date: date, end_date: date, partition_filter: Optional[str] = None) -> Dict[str, Dict[date,int]]:
    """
    Fetch presence for employee_ids using:
      1) chunked DB IN queries (fast)
      2) fallback: broad DB query and Python normalization
      3) fallback: region occupancy history endpoints if DB couldn't provide matches
    Returns mapping { sheet_employee_id: { date_obj: 0/1, ... }, ... }
    """
    if not employee_ids:
        return {}

    orig_ids = [str(e).strip() for e in employee_ids]
    norm_id_set = set([s for s in orig_ids if s])
    result = {eid: {} for eid in orig_ids}

    rows = []
    chunk_size = 500
    try:
        with SessionLocal() as db:
            # chunked IN queries first
            for i in range(0, len(orig_ids), chunk_size):
                chunk = orig_ids[i:i+chunk_size]
                try:
                    q = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date,
                        AttendanceSummary.employee_id.in_(chunk)
                    )
                    rows_chunk = q.all()
                    if rows_chunk:
                        rows.extend(rows_chunk)
                except Exception:
                    logger.exception("chunked query failed for _fetch_presence_for_employees (continuing)")
                    continue

            # if no rows found at all, do broad query
            if not rows:
                try:
                    rows = db.query(AttendanceSummary).filter(
                        AttendanceSummary.date >= start_date,
                        AttendanceSummary.date <= end_date
                    ).all()
                    logger.info("[presence_fetch] fallback broad DB query returned %d rows for %s -> %s", len(rows), start_date, end_date)
                except Exception:
                    logger.exception("fallback broad DB query failed in _fetch_presence_for_employees")
                    rows = []
    except Exception:
        logger.exception("DB session error in _fetch_presence_for_employees")
        rows = []

    # Map DB rows to employee ids using normalization heuristics
    for r in rows:
        try:
            raw_eid = r.employee_id
            if raw_eid is None:
                continue
            db_key = str(raw_eid).strip()
            match_key = None

            # direct match
            if db_key in norm_id_set:
                match_key = db_key
            else:
                # digits-only match
                digits = re.sub(r'\D+', '', db_key)
                if digits:
                    cand = digits.lstrip('0') or digits
                    if cand in norm_id_set:
                        match_key = cand

                # reverse comparisons (strip leading zeros of orig ids)
                if match_key is None:
                    for o in orig_ids:
                        if o == db_key or o.lstrip('0') == db_key or db_key.lstrip('0') == o:
                            match_key = o
                            break

            if not match_key:
                continue

            d = r.date
            present = 0
            try:
                present = int(r.presence_count or 0)
            except Exception:
                present = 1 if (r.presence_count and str(r.presence_count).strip() != "0") else 0

            result.setdefault(match_key, {})
            prev = result[match_key].get(d, 0)
            result[match_key][d] = 1 if (prev == 1 or present > 0) else 0
        except Exception:
            continue

    # Fill missing dates with zeros for now
    cur = start_date
    while cur <= end_date:
        for eid in orig_ids:
            result.setdefault(eid, {})
            if cur not in result[eid]:
                result[eid][cur] = 0
        cur = cur + timedelta(days=1)

    # Count how many employees have any positive presence from DB
    db_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] DB-derived presence found for %d/%d employees", db_positive, len(orig_ids))

    # If DB provided no positives (or very few), use region history fallback to improve coverage
    if db_positive == 0 or db_positive < max(10, int(0.1 * len(orig_ids))):
        try:
            logger.info("[presence_fetch] DB coverage low (%d/%d) - trying region occupancy history fallback", db_positive, len(orig_ids))
            region_presence = _fetch_presence_from_region_histories(orig_ids, start_date, end_date, partition_filter=partition_filter)
            # merge region_presence into result (any positive overwrites zero)
            for eid in orig_ids:
                rp = region_presence.get(eid, {})
                for d, v in rp.items():
                    if v and result.setdefault(eid, {}).get(d, 0) == 0:
                        result[eid][d] = 1
        except Exception:
            logger.exception("region history fallback failed in _fetch_presence_for_employees")

    # Final logging: how many employees have any presence
    final_positive = sum(1 for eid in orig_ids if any(v == 1 for v in result.get(eid, {}).values()))
    logger.info("[presence_fetch] final presence coverage: %d/%d employees have at least one positive day", final_positive, len(orig_ids))

    # ensure each employee has entries for every date (already filled), return
    return result

# ----------------------------
# Main comparison function
# ----------------------------
def compare_ccure_vs_sheets(
    mode: str = "full",
    stats_detail: str = "ActiveProfiles",
    limit_list: int = 200,
    export: bool = False,
    # extra optional filters (if present, will be applied)
    region_filter: Optional[str] = None,        # e.g. "APAC"
    location_city: Optional[str] = None,       # e.g. "Pune"
    location_state: Optional[str] = None,
    location_description: Optional[str] = None,
    week_ref_date: Optional[str] = None        # "YYYY-MM-DD" - week to evaluate (Mon-Fri)
) -> Dict[str, Any]:
    """
    Main compare function - returns structured summary + details.
    """
    try:
        df = load_active_employees_dataframe()
    except Exception as e:
        logger.exception("Failed to load active employees")
        return {"error": f"active sheet load failed: {e}"}

    # normalize filters
    rf = region_filter.strip().lower() if region_filter else None
    lc = location_city.strip().lower() if location_city else None
    ls = location_state.strip().lower() if location_state else None
    ld = location_description.strip().lower() if location_description else None

    # apply filters
    sel = df.copy()
    if rf:
        sel = sel[sel["region_code"].fillna("").str.strip().str.lower() == rf]
    if lc:
        sel = sel[sel["location_city"].fillna("").str.strip().str.lower() == lc]
    if ls:
        sel = sel[sel["location_state"].fillna("").str.strip().str.lower() == ls]
    if ld:
        sel = sel[sel["location_desc"].fillna("").str.strip().str.lower() == ld]

    total_active = len(sel)
    employee_ids = sel["employee_id"].astype(str).str.strip().tolist()

    # week calculation
    today = date.today()
    monday, friday = _week_monday_and_friday(date.fromisoformat(week_ref_date)) if week_ref_date else _week_monday_and_friday(today)

    # fetch presence map (DB first, then region history fallback if needed)
    presence_map = _fetch_presence_for_employees(employee_ids, monday, friday, partition_filter=location_city)

    # compute today count (today within Mon-Fri window or not)
    today_count = 0
    for eid in employee_ids:
        pm = presence_map.get(eid, {})
        if today in pm:
            if pm[today] > 0:
                today_count += 1
        else:
            # fallback direct DB single date check (robust normalized attempts)
            try:
                with SessionLocal() as db:
                    row = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == eid, AttendanceSummary.date == today).first()
                    if row and getattr(row, "presence_count", 0) > 0:
                        today_count += 1
                        continue
                    # try digits normalization
                    digits = re.sub(r'\D+', '', eid)
                    if digits:
                        cand = digits.lstrip('0') or digits
                        row2 = db.query(AttendanceSummary).filter(AttendanceSummary.employee_id == cand, AttendanceSummary.date == today).first()
                        if row2 and getattr(row2, "presence_count", 0) > 0:
                            today_count += 1
            except Exception:
                continue

    today_pct = round((today_count / float(total_active)) * 100.0, 2) if total_active > 0 else None

    # leave and type counts
    sel["on_leave"] = sel["current_status"].apply(lambda x: _maybe_mark_on_leave(x))
    leave_count = int(sel["on_leave"].sum())
    sel["employee_type_norm"] = sel["employee_type"].fillna("").str.strip().str.lower()
    type_counts = sel["employee_type_norm"].value_counts().to_dict()

    # regular employees
    regular_df = sel[sel["employee_type_norm"].str.contains("regular", na=False)]
    regular_ids = regular_df["employee_id"].astype(str).str.strip().tolist()

    regular_presence = {}
    for eid in regular_ids:
        week_map = presence_map.get(eid, {})
        days_present = sum(1 for d, v in week_map.items() if v and (monday <= d <= friday))
        days_present = int(days_present)
        regular_presence[eid] = {
            "days_present": days_present,
            "on_leave": bool(sel[sel["employee_id"] == eid]["on_leave"].any()),
            "full_name": sel[sel["employee_id"] == eid]["full_name"].iloc[0] if not sel[sel["employee_id"] == eid].empty else None
        }

    present_5_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 5)
    present_3_or_more_count = sum(1 for v in regular_presence.values() if v["days_present"] >= 3)
    present_less_than_3_count = sum(1 for v in regular_presence.values() if v["days_present"] < 3)
    present_only_1_count = sum(1 for v in regular_presence.values() if v["days_present"] == 1)

    present_5_list = []
    present_3_list = []
    defaulters_list = []

    for eid, info in regular_presence.items():
        entry = {
            "employee_id": eid,
            "full_name": info["full_name"],
            "days_present": info["days_present"],
            "on_leave": info["on_leave"]
        }
        if info["days_present"] >= 5:
            present_5_list.append(entry)
        if info["days_present"] >= 3:
            present_3_list.append(entry)
        if info["days_present"] < 3:
            defaulters_list.append(entry)

    present_5_list = sorted(present_5_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    present_3_list = sorted(present_3_list, key=lambda x: (-x["days_present"], x["full_name"] or ""))[:limit_list]
    defaulters_list = sorted(defaulters_list, key=lambda x: (x["days_present"], x["on_leave"], x["full_name"] or ""))[:limit_list]

    summary = {
        "filters": {
            "region": region_filter,
            "location_city": location_city,
            "location_state": location_state,
            "location_description": location_description,
            "week_monday": monday.isoformat(),
            "week_friday": friday.isoformat()
        },
        "counts": {
            "total_active_in_sheet": total_active,
            "today_headcount_from_summary": today_count,
            "today_headcount_pct_vs_sheet": today_pct,
            "on_leave_count_in_sheet": leave_count,
            "employee_type_counts": type_counts
        },
        "regular_attendance_summary": {
            "regular_total": len(regular_ids),
            "present_5_day_count": present_5_count,
            "present_3_or_more_count": present_3_or_more_count,
            "present_less_than_3_count": present_less_than_3_count,
            "present_only_1_day_count": present_only_1_count
        }
    }

    details = {
        "present_5_days": present_5_list,
        "present_3_or_more_days": present_3_list,
        "defaulters_less_than_3_days": defaulters_list
    }

    report_path = None
    if export:
        try:
            report_name = f"attendance_compare_{(region_filter or 'all')}_{(location_city or 'all')}_{uuid.uuid4().hex[:8]}.xlsx"
            report_file = OUTPUT_DIR / report_name
            with pd.ExcelWriter(report_file, engine="openpyxl") as writer:
                sel_df_for_export = sel.copy()
                sel_df_for_export["raw_row_str"] = sel_df_for_export["raw_row"].apply(lambda r: str(r) if not pd.isna(r) else "")
                sel_df_for_export.to_excel(writer, sheet_name="active_sheet_selection", index=False)
                pd.DataFrame([summary["counts"]]).to_excel(writer, sheet_name="summary_counts", index=False)
                pd.DataFrame([summary["regular_attendance_summary"]]).to_excel(writer, sheet_name="regular_summary", index=False)
                pd.DataFrame(details["present_5_days"]).to_excel(writer, sheet_name="present_5_days", index=False)
                pd.DataFrame(details["present_3_or_more_days"]).to_excel(writer, sheet_name="present_3_plus", index=False)
                pd.DataFrame(details["defaulters_less_than_3_days"]).to_excel(writer, sheet_name="defaulters_lt3", index=False)
            report_path = str(report_file.name)
        except Exception:
            logger.exception("Failed to write export report")
            report_path = None

    out = {
        "mode": mode,
        "stats_detail": stats_detail,
        "summary": summary,
        "details": details
    }
    if report_path:
        out["report_path"] = report_path
    return out


if __name__ == "__main__":
    # quick smoke test (adjust filters as needed)
    res = compare_ccure_vs_sheets(region_filter="APAC", location_city="Pune", export=False, limit_list=50)
    import json as _json
    print(_json.dumps(res, indent=2, default=str))




