    # --- apply optional location filter for ALL aggregates (previously only recent used it) ---
    df_agg = df
    if location and str(location).strip() and str(location).strip().lower() != "all locations":
        try:
            df_agg = df[df["PartitionName2"].astype(str) == str(location)]
        except Exception:
            LOG.exception("Failed to apply location filter to df for aggregates; falling back to full df")
            df_agg = df

    # compute aggregates safely using df_agg so API honors the 'location' query param
    try:
        by_loc = df_agg.groupby("PartitionName2", dropna=False).size().reset_index(name="count").sort_values("count", ascending=False)
    except Exception:
        by_loc = pd.DataFrame(columns=["PartitionName2", "count"])

    try:
        by_emp = df_agg.groupby("person_uid", dropna=False).size().reset_index(name="count").sort_values("count", ascending=False)
    except Exception:
        by_emp = pd.DataFrame(columns=["person_uid", "count"])

    # Build person_map from df (keeps original behavior - still built from full df so names available)
    person_map = {}
    try:
        for _, r in df.iterrows():
            pu = r.get("person_uid")
            if pu is None or (isinstance(pu, float) and pd.isna(pu)):
                continue
            key = str(pu)
            entry = person_map.get(key, {"EmployeeID": "", "EmployeeName": ""})
            eid = r.get("EmployeeID") or ""
            ename = r.get("EmployeeName") or ""
            if eid and not entry["EmployeeID"]:
                entry["EmployeeID"] = str(eid)
            if ename and not entry["EmployeeName"]:
                entry["EmployeeName"] = str(ename)
            person_map[key] = entry
    except Exception:
        LOG.exception("Failed to build person_map; proceeding without enriched names")
        person_map = {}

    # Enrich by_emp with EmployeeID and EmployeeName (if present)
    try:
        if not by_emp.empty:
            by_emp["EmployeeID"] = by_emp["person_uid"].apply(lambda x: person_map.get(str(x), {}).get("EmployeeID", ""))
            by_emp["EmployeeName"] = by_emp["person_uid"].apply(lambda x: person_map.get(str(x), {}).get("EmployeeName", ""))
    except Exception:
        LOG.exception("Failed to enrich by_emp with names")

    # MONTH-WISE aggregates (using df_agg so location filter applies if present)
    try:
        month_totals_df = (df_agg.groupby(["Month", "MonthName"], dropna=False)
                           .size()
                           .reset_index(name="count")
                           .sort_values(["Month"]))
    except Exception:
        month_totals_df = pd.DataFrame(columns=["Month", "MonthName", "count"])

    try:
        month_by_location_df = (df_agg.groupby(["Month", "MonthName", "PartitionName2"], dropna=False)
                                 .size()
                                 .reset_index(name="count")
                                 .sort_values(["Month", "count"], ascending=[True, False]))
    except Exception:
        month_by_location_df = pd.DataFrame(columns=["Month", "MonthName", "PartitionName2", "count"])

    try:
        month_by_employee_df = (df_agg.groupby(["Month", "MonthName", "person_uid"], dropna=False)
                                 .size()
                                 .reset_index(name="count")
                                 .sort_values(["Month", "count"], ascending=[True, False]))
        if not month_by_employee_df.empty:
            month_by_employee_df["EmployeeID"] = month_by_employee_df["person_uid"].apply(lambda x: person_map.get(str(x), {}).get("EmployeeID",""))
            month_by_employee_df["EmployeeName"] = month_by_employee_df["person_uid"].apply(lambda x: person_map.get(str(x), {}).get("EmployeeName",""))
    except Exception:
        month_by_employee_df = pd.DataFrame(columns=["Month", "MonthName", "person_uid", "count", "EmployeeID", "EmployeeName"])

    # total_rejections should reflect df_agg (i.e., location filter applied). Month-specific totals are returned in month_totals.
    total_rejections_value = int(len(df_agg))











    resp = {
        "total_rejections": total_rejections_value,
        "by_location": by_loc_records,
        ...
    }












      // Core fetch â€” silent update pattern:
      async function fetchOnce() {
        if (inFlightRef.current) return;
        inFlightRef.current = true;
        setLoading(true);

        try {
          const params = new URLSearchParams();
          if (year) params.set('year', String(year));
          if (location && location !== 'All locations') params.set('location', location);
          const url = API_ENDPOINT + '?' + params.toString();

          const r = await fetch(url, { cache: 'no-store' });
          if (!r.ok) {
            const txt = await r.text().catch(()=>'');
            throw new Error('API error: ' + r.status + ' ' + txt);
          }
          const js = await r.json();

          // parse API response
          const total_rejections = (js.total_rejections !== undefined) ? js.total_rejections
            : (js.total !== undefined ? js.total : (js.totalRejections || 0));

          const rawByLoc = js.by_location || js.byLocation || js.location_counts || [];
          const normalizedByLoc = Array.isArray(rawByLoc) ? rawByLoc.map(function(x) {
            var k = x.PartitionName2 || x.partition || x.location || x.name || x.PartitionName || '';
            var c = x.count !== undefined ? x.count : (x.value !== undefined ? x.value : 0);
            return { name: String(k || '').trim(), count: Number(c || 0) };
          }) : [];

          const rawByEmp = js.by_employee || js.byEmployee || js.employee_counts || [];
          const normalizedByEmp = Array.isArray(rawByEmp) ? rawByEmp.map(function(x) {
            var id = x.person_uid || x.EmployeeID || x.employee_id || x.id || x.person || '';
            var c = x.count !== undefined ? x.count : (x.value !== undefined ? x.value : 0);
            var empIdCol = x.EmployeeID || x.EmployeeId || x.employee_id || '';
            var empNameCol = x.EmployeeName || x.Employee || x.EmployeeName || '';
            return { id: String(id || '').trim(), count: Number(c || 0), EmployeeID: empIdCol || '', EmployeeName: empNameCol || '' };
          }) : [];

          const rawRecent = js.recent || js.rows || js.events || js.recent_rows || [];

          // month-wise
          const rawMonthTotals = js.month_totals || js.monthTotals || [];
          const normalizedMonthTotals = Array.isArray(rawMonthTotals) ? rawMonthTotals.map(function(x) {
            return {
              Month: Number(x.Month || x.month || 0),
              MonthName: x.MonthName || x.monthName || x.monthname || "",
              count: Number(x.count || x.value || 0)
            };
          }) : [];

          const rawMonthByLoc = js.month_by_location || js.monthByLocation || [];
          const normalizedMonthByLoc = Array.isArray(rawMonthByLoc) ? rawMonthByLoc.map(function(x) {
            return {
              Month: Number(x.Month || x.month || 0),
              MonthName: x.MonthName || x.monthName || x.monthname || "",
              PartitionName2: x.PartitionName2 || x.Partition || x.location || "",
              count: Number(x.count || x.value || 0)
            };
          }) : [];

          const rawMonthByEmp = js.month_by_employee || js.monthByEmployee || [];
          const normalizedMonthByEmp = Array.isArray(rawMonthByEmp) ? rawMonthByEmp.map(function(x) {
            return {
              Month: Number(x.Month || x.month || 0),
              MonthName: x.MonthName || x.monthName || x.monthname || "",
              person_uid: x.person_uid || x.EmployeeID || x.employee_id || x.id || "",
              count: Number(x.count || x.value || 0),
              EmployeeID: x.EmployeeID || x.employee_id || '',
              EmployeeName: x.EmployeeName || ''
            };
          }) : [];

          // parse person_map (if provided)
          const rawPersonMap = js.person_map || js.personMap || {};
          const normalizedPersonMap = {};
          Object.keys(rawPersonMap || {}).forEach(k => {
            const v = rawPersonMap[k] || {};
            normalizedPersonMap[String(k)] = {
              EmployeeID: v.EmployeeID || v.employeeId || v.EmployeeId || '',
              EmployeeName: v.EmployeeName || v.EmployeeName || v.name || ''
            };
          });

          // --- derive summary numbers, but respect selectedMonth (if user selected a month) ---
          let derivedTotal = Number(total_rejections || 0);
          let derivedUnique = (new Set((normalizedByEmp || []).map(e => e.id).filter(Boolean))).size;
          let derivedRepeaters = (normalizedByEmp || []).filter(e => e.count > 1).length;

          if (selectedMonth && Number(selectedMonth) !== 0) {
            const m = Number(selectedMonth);
            // total for selected month: prefer month_by_location (if location set) OR month_totals (if no location)
            if (location && location !== 'All locations') {
              // sum month_by_location rows for that month & location (server returns month_by_location filtered by location already)
              const rowsForMonthLoc = (normalizedMonthByLoc || []).filter(r => Number(r.Month) === m && String(r.PartitionName2) === String(location));
              derivedTotal = rowsForMonthLoc.reduce((s, r) => s + Number(r.count || 0), 0);
            } else {
              const rec = (normalizedMonthTotals || []).find(r => Number(r.Month) === m);
              derivedTotal = rec ? Number(rec.count || 0) : 0;
            }

            // unique & repeaters for selected month: use month_by_employee (server returns month_by_employee filtered by location if the API was called with location)
            const empRowsForMonth = (normalizedMonthByEmp || []).filter(r => Number(r.Month) === m);
            const empSet = new Set(empRowsForMonth.map(r => String(r.person_uid || '').trim()).filter(Boolean));
            derivedUnique = empSet.size;
            derivedRepeaters = empRowsForMonth.filter(r => Number(r.count || 0) > 1).length;
          }

          // Now update React state with derived (month-aware) values
          setTotal(Number(derivedTotal || 0));
          setByLocation(normalizedByLoc);
          setByEmployee(normalizedByEmp);
          setRecent(Array.isArray(rawRecent) ? rawRecent : []);
          setUniqueEmployees(Number(derivedUnique || 0));
          setRepeatersCount(Number(derivedRepeaters || 0));

          setMonthTotals(normalizedMonthTotals);
          setMonthByLocation(normalizedMonthByLoc);
          setMonthByEmployee(normalizedMonthByEmp);

          setPersonMap(normalizedPersonMap);

          // update location chart using buildLocationChartData (this picks monthByLocation when month selected)
          if (locChartRef.current) {
            const locChartData = buildLocationChartData(normalizedByLoc, normalizedMonthByLoc, selectedMonth, location);
            const cfg = {
              type: locChartData.type === 'bar' ? 'bar' : 'line',
              data: {
                labels: locChartData.labels,
                datasets: [{
                  label: 'PIN rejections',
                  data: locChartData.data,
                  borderColor: '#2563eb',
                  backgroundColor: locChartData.type === 'bar' ? 'rgba(37,99,235,0.5)' : 'rgba(37,99,235,0.2)',
                  fill: locChartData.type !== 'bar',
                  tension: 0.3,
                  pointBackgroundColor: '#2563eb',
                  pointRadius: 5,
                  pointHoverRadius: 7,
                  borderWidth: 2
                }]
              },
              options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: { legend: { display: false } },
                scales: { x: { type: 'category' }, y: { beginAtZero: true, ticks: { precision: 0 } } }
              }
            };
            updateOrCreateChart(locChartInst, locChartRef.current, cfg);
          }

          // Month totals chart (unchanged)
          if (monthChartRef.current) {
            var monthArr = Array.from({length:12}, function(_,i) {
              var m = i + 1;
              var rec = null;
              for (var ri = 0; ri < normalizedMonthTotals.length; ri++) {
                var cand = normalizedMonthTotals[ri];
                if (Number(cand.Month) === m) { rec = cand; break; }
              }
              return rec && rec.count ? Number(rec.count) : 0;
            });
            updateOrCreateChart(monthChartInst, monthChartRef.current, {
              type: 'line',
              data: { labels: MONTH_NAMES, datasets: [{ label: 'Monthly PIN rejections', data: monthArr, fill:false, tension:0.3 }] },
              options: { responsive:true, maintainAspectRatio:false, plugins:{legend:{display:false}}, scales:{ y:{beginAtZero:true} } }
            });
          }

          setLastFetch(new Date().toISOString());

        } catch (err) {
          console.error("Fetch EMEA PIN failed", err);
        } finally {
          setLoading(false);
          inFlightRef.current = false;
        }


























# backend/api_server.py
from pathlib import Path
from datetime import datetime, date
import time
import traceback
import sys
import importlib
import logging
from typing import Any

# ----- ensure project root on sys.path (do this before other imports) -----
HERE = Path(__file__).resolve()
PROJECT_ROOT = HERE.parent.parent  # <project-root> (one level above backend/)
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# now safe to import FastAPI etc.
from fastapi import FastAPI, Query, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import pandas as pd

LOG = logging.getLogger("emea.api")
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

# ----- try to import backend.emea_pin in a robust way (clear error if fails) -----
ep = None
try:
    ep = importlib.import_module("backend.emea_pin")
    LOG.info("Imported backend.emea_pin via package import")
except Exception as first_exc:
    LOG.warning("Package import backend.emea_pin failed: %s", first_exc)
    try:
        ep = importlib.import_module("emea_pin")
        LOG.info("Imported emea_pin as top-level module")
    except Exception as second_exc:
        tb = traceback.format_exc()
        LOG.error("Failed to import emea_pin module.\nPackage import error: %s\nFallback error: %s\nTrace:\n%s",
                  first_exc, second_exc, tb)
        raise RuntimeError(f"Failed to import emea_pin module (see server logs). Primary error: {first_exc}") from second_exc

# create app
app = FastAPI(title="EMEA PIN API (wrapper)")

# permissive CORS for local/dev
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "OPTIONS", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

# cache for DataFrame
CACHE = {"ts": 0.0, "year": None, "df": None}
CACHE_TTL = 5  # seconds

def _load_fallback_csv(project_root: Path):
    fallback = project_root / "out" / "emea_pin_raw.csv"
    if fallback.exists():
        LOG.info("Loading fallback CSV: %s", fallback)
        try:
            df = pd.read_csv(fallback, parse_dates=["LocaleMessageTime"], keep_default_na=False)
            # sanitize DateOnly if present: ensure string
            if "LocaleMessageTime" in df.columns:
                df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
            if "DateOnly" in df.columns:
                # ensure DateOnly is string
                df["DateOnly"] = df["DateOnly"].astype(str)
            return df
        except Exception as e:
            LOG.exception("Failed to parse fallback CSV: %s", e)
            raise
    raise FileNotFoundError(str(fallback))

# --- sanitizer helpers to make response JSON-safe ---
def _serialize_value(v: Any) -> Any:
    """Convert common non-JSON types to JSON-safe primitives."""
    if v is None:
        return None
    if isinstance(v, (str, bool, int, float)):
        return v
    if isinstance(v, (datetime, date)):
        return v.isoformat()
    try:
        import pandas as pd
        if isinstance(v, pd.Timestamp):
            if pd.isna(v):
                return None
            return v.isoformat()
        if isinstance(v, pd.Timedelta):
            return str(v)
        if v is pd.NaT:
            return None
    except Exception:
        pass
    try:
        import numpy as _np
        if isinstance(v, _np.generic):
            return v.item()
        if isinstance(v, _np.datetime64):
            try:
                ts = pd.to_datetime(v)
                return ts.isoformat()
            except Exception:
                return str(v)
    except Exception:
        pass
    try:
        return str(v)
    except Exception:
        return None

def _sanitize(obj: Any) -> Any:
    """Recursively walk `obj` and convert values to JSON-safe types."""
    if isinstance(obj, dict):
        return {str(k): _sanitize(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)):
        return [_sanitize(v) for v in obj]
    if isinstance(obj, pd.DataFrame):
        try:
            return _sanitize(obj.fillna("").to_dict(orient="records"))
        except Exception:
            return []
    if isinstance(obj, pd.Series):
        try:
            return _sanitize(obj.fillna("").to_list())
        except Exception:
            return []
    return _serialize_value(obj)

@app.get("/api/emea_pin_live")
def emea_pin_live(year: int | None = None, location: str | None = Query(None)):
    target_year = int(year or datetime.now().year)
    now = time.time()

    # refresh cache as needed
    if CACHE["df"] is None or CACHE["year"] != target_year or (now - CACHE["ts"]) > CACHE_TTL:
        try:
            df = ep.fetch_pin_rows_for_year(target_year)
            LOG.info("Fetched df from emea_pin (rows=%d)", len(df) if getattr(df, "shape", None) is not None else -1)
        except Exception as e:
            LOG.warning("fetch_pin_rows_for_year failed: %s", e)
            try:
                df = _load_fallback_csv(PROJECT_ROOT)
            except Exception as e2:
                tb = traceback.format_exc()
                LOG.error("Failed both live fetch and fallback CSV: %s\n%s", e2, tb)
                raise HTTPException(status_code=500, detail=f"Failed to fetch PIN rows: {e}\nFallback error: {e2}\n{tb}")

        if df is None:
            df = pd.DataFrame()
        CACHE.update({"ts": now, "year": target_year, "df": df})
    else:
        df = CACHE["df"]

    # defensive column existence
    for col in ["LocaleMessageTime", "PartitionName2", "person_uid", "EmployeeName", "EmployeeID", "CardNumber", "DateOnly", "Month", "MonthName"]:
        if col not in df.columns:
            if col == "Month":
                df[col] = pd.NA
            else:
                df[col] = pd.NA

    # ensure LocaleMessageTime parsed
    if "LocaleMessageTime" in df.columns:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    # ensure Month/MonthName derived (in case they weren't present)
    if "Month" not in df.columns or df["Month"].isnull().all():
        df["Month"] = df["LocaleMessageTime"].dt.month.fillna(0).astype("Int64")
    if "MonthName" not in df.columns or df["MonthName"].isnull().all():
        df["MonthName"] = df["LocaleMessageTime"].dt.strftime("%b").fillna("")

    # optional location filter for recent rows (kept internal)
    df_for_recent = df
    if location and location.strip() and location.strip().lower() != "all locations":
        try:
            df_for_recent = df_for_recent[df_for_recent["PartitionName2"].astype(str) == location]
        except Exception:
            LOG.exception("Location filtering failed; using unfiltered df_for_recent")
            df_for_recent = df

    # compute aggregates safely
    try:
        by_loc = df.groupby("PartitionName2", dropna=False).size().reset_index(name="count").sort_values("count", ascending=False)
    except Exception:
        by_loc = pd.DataFrame(columns=["PartitionName2", "count"])
    try:
        # Basic employee counts (group by canonical person_uid)
        by_emp = df.groupby("person_uid", dropna=False).size().reset_index(name="count").sort_values("count", ascending=False)
    except Exception:
        by_emp = pd.DataFrame(columns=["person_uid", "count"])

    # Build person_map (person_uid -> EmployeeID, EmployeeName) from full df
    person_map = {}
    try:
        for _, r in df.iterrows():
            pu = r.get("person_uid")
            if pu is None or (isinstance(pu, float) and pd.isna(pu)):
                continue
            key = str(pu)
            entry = person_map.get(key, {"EmployeeID": "", "EmployeeName": ""})
            eid = r.get("EmployeeID") or ""
            ename = r.get("EmployeeName") or ""
            if eid and not entry["EmployeeID"]:
                entry["EmployeeID"] = str(eid)
            if ename and not entry["EmployeeName"]:
                entry["EmployeeName"] = str(ename)
            person_map[key] = entry
    except Exception:
        LOG.exception("Failed to build person_map; proceeding without enriched names")
        person_map = {}

    # Enrich by_emp with EmployeeID and EmployeeName (if present)
    try:
        if not by_emp.empty:
            by_emp["EmployeeID"] = by_emp["person_uid"].apply(lambda x: person_map.get(str(x), {}).get("EmployeeID", ""))
            by_emp["EmployeeName"] = by_emp["person_uid"].apply(lambda x: person_map.get(str(x), {}).get("EmployeeName", ""))
    except Exception:
        LOG.exception("Failed to enrich by_emp with names")

    # MONTH-WISE aggregates (location & employee)
    try:
        month_totals_df = (df.groupby(["Month", "MonthName"], dropna=False)
                           .size()
                           .reset_index(name="count")
                           .sort_values(["Month"]))
    except Exception:
        month_totals_df = pd.DataFrame(columns=["Month", "MonthName", "count"])

    try:
        month_by_location_df = (df.groupby(["Month", "MonthName", "PartitionName2"], dropna=False)
                                 .size()
                                 .reset_index(name="count")
                                 .sort_values(["Month", "count"], ascending=[True, False]))
    except Exception:
        month_by_location_df = pd.DataFrame(columns=["Month", "MonthName", "PartitionName2", "count"])

    try:
        month_by_employee_df = (df.groupby(["Month", "MonthName", "person_uid"], dropna=False)
                                 .size()
                                 .reset_index(name="count")
                                 .sort_values(["Month", "count"], ascending=[True, False]))
        # attach employee id / name to monthly employee counts
        if not month_by_employee_df.empty:
            month_by_employee_df["EmployeeID"] = month_by_employee_df["person_uid"].apply(lambda x: person_map.get(str(x), {}).get("EmployeeID",""))
            month_by_employee_df["EmployeeName"] = month_by_employee_df["person_uid"].apply(lambda x: person_map.get(str(x), {}).get("EmployeeName",""))
    except Exception:
        month_by_employee_df = pd.DataFrame(columns=["Month", "MonthName", "person_uid", "count", "EmployeeID", "EmployeeName"])

    # convert to records (strings / primitives) and sanitize recursively
    by_loc_records = _sanitize(by_loc.fillna("").to_dict(orient="records"))
    by_emp_records = _sanitize(by_emp.fillna("").to_dict(orient="records"))
    recent = df_for_recent.copy()
    if "LocaleMessageTime" in recent.columns:
        try:
            recent = recent.sort_values("LocaleMessageTime", ascending=False)
        except Exception:
            LOG.debug("Could not sort recent by LocaleMessageTime")
    recent = recent.head(500)
    recent_records = _sanitize(recent.fillna("").to_dict(orient="records"))

    month_totals_records = _sanitize(month_totals_df.fillna("").to_dict(orient="records"))
    month_by_location_records = _sanitize(month_by_location_df.fillna("").to_dict(orient="records"))
    month_by_employee_records = _sanitize(month_by_employee_df.fillna("").to_dict(orient="records"))

    # Ensure person_map values are primitive types (strings)
    safe_person_map = {}
    for k, v in person_map.items():
        safe_person_map[str(k)] = {
            "EmployeeID": _serialize_value(v.get("EmployeeID","")),
            "EmployeeName": _serialize_value(v.get("EmployeeName",""))
        }

    resp = {
        "total_rejections": int(len(df)),
        "by_location": by_loc_records,
        "by_employee": by_emp_records,
        "recent": recent_records,
        # month-wise additions
        "month_totals": month_totals_records,
        "month_by_location": month_by_location_records,
        "month_by_employee": month_by_employee_records,
        # helper map for frontend name lookups
        "person_map": safe_person_map,
    }

    safe_resp = _sanitize(resp)
    return JSONResponse(safe_resp)











#C:\Users\W0024618\Desktop\Trend Analysis\backend\emea_pin.py
"""
EMEA PIN violation analysis.

Usage:
    python -m backend.emea_pin --year 2024 --outdir ./out
"""
from __future__ import annotations

import argparse
import logging
from datetime import date
from pathlib import Path
from typing import List
import sys

import pandas as pd

# Robust import for duration_report (works when module loaded as package or as plain module)
try:
    from backend import duration_report as dr
except Exception:
    try:
        import duration_report as dr
    except Exception as e:
        raise RuntimeError(f"Could not import duration_report (tried 'backend.duration_report' and 'duration_report'): {e}") from e

# local alias (may be None if duration_report couldn't supply it)
pyodbc = getattr(dr, "pyodbc", None)

LOG = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")


SQL_DB_TEMPLATE = r"""
SELECT
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.ObjectName1 AS EmployeeName,
    t1.PartitionName2 AS PartitionName2,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t5rej.value AS Rejection_Type,
    CASE WHEN t3.Name IN ('Contractor','Terminated Contractor') THEN t2.Text12 ELSE CAST(t2.Int1 AS NVARCHAR) END AS EmployeeID,
    t3.Name AS PersonnelType,
    t1.ObjectName2 AS DoorName,
    t1.XmlGUID
FROM [{db}].dbo.ACVSUJournalLog t1
LEFT JOIN ACVSCore.Access.Personnel t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml ON t1.XmlGUID = t_xml.GUID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred sc ON t1.XmlGUID = sc.GUID AND sc.Name IN ('Card','CHUID')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5rej ON t1.XmlGUID = t5rej.GUID AND t5rej.Name = 'RejectCode'
WHERE
    t1.MessageType = 'CardRejected'
    AND t5rej.value = 'PIN'
    AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) BETWEEN '{start}' AND '{end}'
"""

def _get_emea_databases() -> List[str]:
    rc = dr.REGION_CONFIG.get("emea")
    if not rc:
        raise RuntimeError("No 'emea' region config found in duration_report.py")
    candidates = dr._get_candidate_databases(rc)
    valid = dr._filter_existing_databases(rc, candidates)
    return valid or candidates

def _connect_to_db(server: str, database: str, user: str, password: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc not available; install pyodbc to run queries.")
    conn_str = (
        f"DRIVER={{{dr.ODBC_DRIVER}}};"
        f"SERVER={server};DATABASE={database};UID={user};PWD={password};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_pin_rows_for_year(target_year: int) -> pd.DataFrame:
    rc = dr.REGION_CONFIG["emea"]
    start = date(target_year, 1, 1).strftime("%Y-%m-%d")
    end = date(target_year, 12, 31).strftime("%Y-%m-%d")
    dbs = _get_emea_databases()
    LOG.info("Querying EMEA databases: %s", dbs)

    frames = []
    for db in dbs:
        sql = SQL_DB_TEMPLATE.format(db=db, start=start, end=end)
        LOG.info("Running query against %s.%s (rows may be large)", rc["server"], db)
        conn = None
        try:
            conn = _connect_to_db(rc["server"], db, rc["user"], rc["password"])
            # pandas will pull the results into a DataFrame
            df = pd.read_sql(sql, conn)
            if not df.empty:
                frames.append(df)
        except Exception:
            LOG.exception("Failed to fetch from database %s", db)
        finally:
            try:
                if conn is not None:
                    conn.close()
            except Exception:
                pass

    if not frames:
        return pd.DataFrame(columns=[
            "LocaleMessageTime", "EmployeeName", "PartitionName2", "CardNumber",
            "Rejection_Type", "EmployeeID", "PersonnelType", "DoorName", "XmlGUID"
        ])
    out = pd.concat(frames, ignore_index=True)
    # normalize types
    out["LocaleMessageTime"] = pd.to_datetime(out["LocaleMessageTime"], errors="coerce")

    # IMPORTANT CHANGE: store DateOnly AS STRING (ISO) to avoid python date objects
    out["DateOnly"] = out["LocaleMessageTime"].dt.strftime("%Y-%m-%d")

    # Add Month and MonthName for month-wise analysis
    out["Month"] = out["LocaleMessageTime"].dt.month.fillna(0).astype("Int64")  # pandas nullable int
    out["MonthName"] = out["LocaleMessageTime"].dt.strftime("%b").fillna("")

    # ensure columns exist
    for c in ["EmployeeID", "EmployeeName", "CardNumber", "PartitionName2"]:
        if c not in out.columns:
            out[c] = None
    # restrict to PersonnelType == 'Employee' (defensive)
    if "PersonnelType" in out.columns:
        out = out[out["PersonnelType"].fillna("").str.strip().str.lower() == "employee"].copy()
    # canonical person uid using helper from duration_report
    def make_person_uid(row):
        try:
            return dr._canonical_person_uid_from_row(row)
        except Exception:
            # fallback: prefer EmployeeID -> CardNumber -> EmployeeName
            for cand in ("EmployeeID", "CardNumber", "EmployeeName"):
                v = row.get(cand)
                if v is not None and str(v).strip():
                    return str(v).strip()
            return None
    if not out.empty:
        out["person_uid"] = out.apply(make_person_uid, axis=1)
    return out

def analyze_pin_df(df: pd.DataFrame, outdir: Path) -> dict:
    outdir.mkdir(parents=True, exist_ok=True)
    results = {}
    total = len(df)
    results["total_rejections"] = int(total)
    LOG.info("Total PIN rejections in dataset: %d", total)

    # ensure month columns exist and are in friendly form
    if "Month" not in df.columns:
        df["Month"] = pd.to_datetime(df.get("LocaleMessageTime", None), errors="coerce").dt.month.fillna(0).astype(int)
    if "MonthName" not in df.columns:
        df["MonthName"] = pd.to_datetime(df.get("LocaleMessageTime", None), errors="coerce").dt.strftime("%b").fillna("")

    # by location (PartitionName2)
    by_loc = df.groupby("PartitionName2", dropna=False).size().reset_index(name="count").sort_values("count", ascending=False)
    results["by_location"] = by_loc
    by_loc.to_csv(outdir / "emea_pin_by_location.csv", index=False)

    # by employee (person_uid) - top offenders
    by_emp = df.groupby("person_uid", dropna=False).size().reset_index(name="count").sort_values("count", ascending=False)
    results["by_employee"] = by_emp
    by_emp.to_csv(outdir / "emea_pin_by_employee.csv", index=False)

    # repeaters: employees with count > 1
    repeaters = by_emp[by_emp["count"] > 1].copy()
    results["repeaters"] = repeaters
    repeaters.to_csv(outdir / "emea_pin_repeaters.csv", index=False)

    # MONTH-WISE: total by month
    month_totals = (df.groupby(["Month", "MonthName"], dropna=False)
                    .size()
                    .reset_index(name="count")
                    .sort_values(["Month"]))
    results["month_totals"] = month_totals
    month_totals.to_csv(outdir / "emea_pin_month_totals.csv", index=False)

    # MONTH-WISE: location-wise counts per month
    month_loc = (df.groupby(["Month", "MonthName", "PartitionName2"], dropna=False)
                 .size()
                 .reset_index(name="count")
                 .sort_values(["Month", "count"], ascending=[True, False]))
    results["month_by_location"] = month_loc
    month_loc.to_csv(outdir / "emea_pin_month_by_location.csv", index=False)

    # MONTH-WISE: employee-wise counts per month
    month_emp = (df.groupby(["Month", "MonthName", "person_uid"], dropna=False)
                 .size()
                 .reset_index(name="count")
                 .sort_values(["Month", "count"], ascending=[True, False]))
    results["month_by_employee"] = month_emp
    month_emp.to_csv(outdir / "emea_pin_month_by_employee.csv", index=False)

    # summary stats
    results["unique_employees"] = int(by_emp["person_uid"].nunique()) if "person_uid" in by_emp.columns else int(by_emp.shape[0])
    results["repeat_count_total"] = int(repeaters["count"].sum()) if not repeaters.empty else 0
    results["num_repeaters"] = int(len(repeaters))
    LOG.info("Unique employees with PIN rejects: %s", results["unique_employees"])
    LOG.info("Number of repeaters (count>1): %s, total repeat events: %s", results["num_repeaters"], results["repeat_count_total"])

    # Save full raw dataframe for audit (DateOnly as string avoids JSON issues)
    df.to_csv(outdir / "emea_pin_raw.csv", index=False)

    return results

def main():
    parser = argparse.ArgumentParser(description="EMEA PIN rejection analysis (yearly).")
    parser.add_argument("--year", type=int, required=False, default=date.today().year, help="Year to analyze (e.g. 2024)")
    parser.add_argument("--outdir", required=False, default="./out", help="Output directory for CSVs")
    args = parser.parse_args()

    target_year = int(args.year)
    outdir = Path(args.outdir)

    LOG.info("Starting EMEA PIN analysis for year %d", target_year)
    df = fetch_pin_rows_for_year(target_year)
    LOG.info("Rows fetched: %d", len(df))
    results = analyze_pin_df(df, outdir)
    # print short summary
    print("==== EMEA PIN analysis summary ====")
    print(f"Year: {target_year}")
    print(f"Total PIN rejections: {results.get('total_rejections', 0)}")
    print(f"Unique employees with rejects: {results.get('unique_employees', 0)}")
    print(f"Repeaters (employees with >1 reject): {results.get('num_repeaters', 0)}  (repeat events total: {results.get('repeat_count_total', 0)})")
    print(f"CSV outputs saved to: {outdir.resolve()}")

if __name__ == "__main__":
    main()






