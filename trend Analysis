C:\Users\326131\OneDrive - Western Union\Desktop\Trend_backend\app.py

'''
from flask import Flask, jsonify
from db import fetch_live_swipe
from logic import flag_employee
import pandas as pd
import numpy as np
from datetime import datetime

app = Flask(__name__)

def make_json_safe(obj):
    if isinstance(obj,dict):
        return {k: make_json_safe(v) for k, v in obj.items()}
    elif isinstance(obj,list):
        return [make_json_safe(item) for item in obj]
    elif isinstance(obj, (pd.Timestamp,datetime)):
        return obj.isoformat()
    elif pd.isna(obj) or obj is pd.NaT:
        return None
    elif isinstance(obj,(np.integer,np.floating)):
        return obj.item()
    else:
        return obj


@app.route('/')
def root():
    return "Employee alert API is running"

@app.route('/check', methods=['GET'])
def check_employee():
    try:
        swipe_data = fetch_live_swipe()
        print(f"Received swipe_data type: {type(swipe_data)}")
        print(f"Swipe_data shape: {swipe_data.shape if isinstance(swipe_data, pd.DataFrame) else 'Not a DataFrame'}")
        print(f"Swipe_data columns: {swipe_data.columns.tolist() if isinstance(swipe_data, pd.DataFrame) else 'No columns'}")
        
        if swipe_data is None or swipe_data.empty:
            print("No swipe data found or DataFrame is empty")
            return jsonify({'error': 'No swipe data found', 'rows_fetched': 0}), 404

        # Log first few rows for debugging
        print("First 3 rows of swipe_data:\n", swipe_data.head(3).to_dict(orient='records'))

        # Process all rows and collect results
        results = []
        flagged_count=0
        for index, row in swipe_data.iterrows():
            row_dict = row.apply(lambda x: x[0] if isinstance(x, list) else x).to_dict()
            
            for field in ['InTime','OutTime','SwipeDate']:
                try:
                    if pd.notnull(row_dict.get(field)):
                        row_dict[field]=pd.to_datetime(row_dict[field])
                    else:
                        row_dict[field]=pd.NaT
                except Exception as dt_err:
                    print(f"Warning: could not parse datetime for {field} : {dt_err}")
                    row_dict[field] = pd.NaT      

            print(f"Processing row {index}: EmployeeID={row_dict.get('EmployeeID')}")
            flag, reasons = flag_employee(row_dict)
            if flag:
                flagged_count+=1
            
            # convert row dict to JSON-Serializable
            serializable_row_dict = make_json_safe(row_dict) 
                        
            results.append({
                'EmployeeID': serializable_row_dict.get('EmployeeID'),
                'Flagged': flag,
                'Reasons': reasons,
                'LiveSwipeData': serializable_row_dict
            })

        response = {
            'results': results,
            'total_records': len(results),
            'flagged_employees':flagged_count,
            'columns': swipe_data.columns.tolist()
        }
        print(f"Returning {len(results)} records")
        return jsonify(response)

    except Exception as e:
        print(f"Error in check_employee: {str(e)}")
        return jsonify({'error': str(e), 'rows_fetched': 0}), 500

if __name__ == '__main__':
    app.run(debug=True)
    '''
from flask import Flask, jsonify, request
from db import fetch_live_swipe
from logic import flag_employee
import pandas as pd
import numpy as np
from datetime import datetime

app = Flask(__name__)

def make_json_safe(obj):
    if isinstance(obj, dict):
        return {k: make_json_safe(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [make_json_safe(item) for item in obj]
    elif isinstance(obj, (pd.Timestamp, datetime)):
        return obj.isoformat()
    elif pd.isna(obj) or obj is pd.NaT:
        return None
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    else:
        return obj

@app.route('/')
def root():
    return "Employee alert API is running"

@app.route('/check', methods=['GET'])
def check_employee():
    try:
        swipe_data = fetch_live_swipe()
        if swipe_data is None or swipe_data.empty:
            return jsonify({'error': 'No swipe data found', 'rows_fetched': 0}), 404

        # ========== NEW: Filter by selected date ==========
        date_str = request.args.get("date", None)
        selected_date = None

        if date_str:
            try:
                selected_date = pd.to_datetime(date_str).normalize()
                swipe_data['SwipeDate'] = pd.to_datetime(swipe_data['SwipeDate'])
                swipe_data = swipe_data[swipe_data['SwipeDate'].dt.normalize() == selected_date]
            except Exception as e:
                return jsonify({'error': f"Invalid date format: {e}"}), 400

        if swipe_data.empty:
            return jsonify({'error': 'No swipe data found for the given date', 'rows_fetched': 0}), 404

        # ========== PROCESS EACH EMPLOYEE ==========
        results = []
        flagged_count = 0

        for index, row in swipe_data.iterrows():
            row_dict = row.apply(lambda x: x[0] if isinstance(x, list) else x).to_dict()
            
            for field in ['InTime', 'OutTime', 'SwipeDate']:
                try:
                    if pd.notnull(row_dict.get(field)):
                        row_dict[field] = pd.to_datetime(row_dict[field])
                    else:
                        row_dict[field] = pd.NaT
                except Exception as dt_err:
                    print(f"Warning: could not parse datetime for {field} : {dt_err}")
                    row_dict[field] = pd.NaT

            print(f"Processing row {index}: EmployeeID={row_dict.get('EmployeeID')}")
            flag, reasons = flag_employee(row_dict)
            if flag:
                flagged_count += 1

            serializable_row_dict = make_json_safe(row_dict)

            results.append({
                'EmployeeID': serializable_row_dict.get('EmployeeID'),
                'Flagged': flag,
                'Reasons': reasons,
                'LiveSwipeData': serializable_row_dict
            })

        response = {
            'results': results,
            'selected_date': selected_date.strftime("%Y-%m-%d") if selected_date else "All Dates",
            'total_records': len(results),
            'flagged_employees': flagged_count,
            'columns': swipe_data.columns.tolist()
        }

        return jsonify(response)

    except Exception as e:
        print(f"Error in check_employee: {str(e)}")
        return jsonify({'error': str(e), 'rows_fetched': 0}), 500


# ========== NEW ENDPOINT: RETURN ALL UNIQUE DATES ==========
@app.route('/available-dates', methods=['GET'])
def get_available_dates():
    try:
        swipe_data = fetch_live_swipe()
        if swipe_data is None or swipe_data.empty or 'SwipeDate' not in swipe_data.columns:
            return jsonify({'error': 'No swipe data available'}), 404

        swipe_data['SwipeDate'] = pd.to_datetime(swipe_data['SwipeDate'], errors='coerce')
        today=pd.Timestamp.today().normalize()
        available_dates = swipe_data['SwipeDate'].dropna()
        available_dates = available_dates[available_dates.dt.normalize() <today]
        
        unique_dates= sorted(available_dates.dt.date.unique(),reverse=True)

        return jsonify({'available_dates': [d.isoformat() for d in unique_dates]})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


if __name__ == '__main__':
    app.run(debug=True)








C:\Users\326131\OneDrive - Western Union\Desktop\Trend_backend\config.py


# config.py the details for connecting with the server

DB_CONFIG = {
    'driver':'OBDC Driver 17 for SQL Server',
    'server':'SRVWUPNQ0986V',
    'database':'ACVSUJournal_00010028',
    'username':'GSOC_Test',
    'password':'Westernuniongsoc@2025'
}







C:\Users\326131\OneDrive - Western Union\Desktop\Trend_backend\db.py

# db.py , we are connecting and fetching data from the database here

from sqlalchemy import create_engine
import urllib
import pyodbc
import pandas as pd
from config import DB_CONFIG

def fetch_live_swipe():
    print("step 1: entering the function")
    try:
        connection_string=(
            f"DRIVER={{ODBC Driver 17 for SQL Server}};"
            f"SERVER={DB_CONFIG['server']};"
            f"DATABASE={DB_CONFIG['database']};"
            f"UID={DB_CONFIG['username']};"
            f"PWD={DB_CONFIG['password']};"
        )
        connection_uri= f"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(connection_string)}"
        engine=create_engine(connection_uri)
        print("connection successful")
        query = """
WITH MainDoorSwipes AS (
        SELECT
            t1.ObjectName1 AS EmployeeName,
			t3.Name AS PersonnelType,
            CASE 
            WHEN t3.Name IN ('Contractor','Terminated Contractor') THEN t2.Text12
            ELSE CAST(t2.Int1 AS NVARCHAR)
        END                                                                             AS EmployeeID,
        DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS LocaleMessageTime,
        x.Value AS Direction,
        t1.ObjectName2 AS DoorName,
        -- Compute WorkDayKey based on 2 AM cut
        CASE
            WHEN CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS TIME) >= '02:00:00'
                THEN CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS DATE)
            ELSE DATEADD(DAY, -1, CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS DATE))
        END AS WorkDayKey
        FROM
        [ACVSUJournal_00010028].[dbo].[ACVSUJournalLog] AS t1
        INNER JOIN [ACVSCore].[Access].[Personnel] AS t2
        ON t1.ObjectIdentity1 = t2.GUID
        INNER JOIN [ACVSCore].[Access].[PersonnelType] AS t3
        ON t2.PersonnelTypeID = t3.ObjectID
        INNER JOIN [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxmlShred] AS x
        ON t1.XMLGuid = x.GUID
        WHERE
        t1.MessageType = 'CardAdmitted'
        AND t1.PartitionName2 = 'APAC.Default'
        AND t3.Name = 'Employee'
        AND DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) >= '2025-06-01'
        AND t1.ObjectName2 IN (
            -- your main door list here
            'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B',
            'APAC_IN_PUN_PODIUM_MAIN PODIUM RIGHT ENTRY-DOOR NEW',
            'APAC_IN_PUN_PODIUM_MAIN PODIUM LEFT ENTRY-DOOR NEW',
            'APAC_IN_PUN_TOWER B_MAIN RECEPTION DOOR',
            'APAC_IN_PUN_TOWER B_LIFT LOBBY DOOR',
            'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW',
            'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 1-DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2-DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 3-DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4-DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2 -OUT DOOR',
            'APAC_IN_PUN-PODIUM_P-1 TURNSTILE 3 -OUT DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 1-OUT DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4 -OUT DOOR',
            'APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED)',
            'APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)'
        )
        AND x.Value IN ('InDirection', 'OutDirection')
        ),
        RankedSwipes AS (
        SELECT
        *,
        LEAD(LocaleMessageTime) OVER (PARTITION BY EmployeeID, WorkDayKey ORDER BY LocaleMessageTime) AS NextSwipeTime,
        LEAD(Direction) OVER (PARTITION BY EmployeeID, WorkDayKey ORDER BY LocaleMessageTime) AS NextDirection,
        LEAD(DoorName) OVER (PARTITION BY EmployeeID ORDER BY LocaleMessageTime) AS NextDoor
        FROM
        MainDoorSwipes
        ),
        FlaggedBreaks AS (
        SELECT
        EmployeeName,
        EmployeeID,
		PersonnelType,
        DoorName AS OutDoor,
        LocaleMessageTime AS OutTime,
		NextDoor AS Indoor,
        NextSwipeTime AS InTime, 
		CONVERT(date, LocaleMessageTime)         AS SwipeDate,
        DATEDIFF(MINUTE, LocaleMessageTime, NextSwipeTime) AS MinutesDiff
        FROM
        RankedSwipes
        WHERE
        Direction = 'OutDirection'
        AND NextDirection = 'InDirection'
        AND DATEDIFF(MINUTE, LocaleMessageTime, NextSwipeTime) >= 120
        ),
        CombinedQuery AS (
        SELECT 
        DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])                 AS LocaleMessageTime,
        t1.ObjectName1                                                                  AS EmployeeName,
        CASE
            WHEN t3.Name IN ('Contractor','Terminated Contractor') THEN t2.Text12
			WHEN t3.Name IN ('Property Management', 'Visitor', 'Temp Badge') THEN t2.Text9
            ELSE CAST(t2.Int1 AS NVARCHAR)											END AS EmployeeID,
        t3.Name                                                                         AS PersonnelType,
        t1.PartitionName2                                                               AS Location,
        t5_card.CardNumber,
        t5_admit.Value                                                                  AS AdmitCode,
        t5_dir.Value                                                                    AS Direction,
        t1.ObjectName2                                                                  AS Door,
        t5_rej.Value                                                                    AS Rejection_Type,
        t1.XmlGUID
        FROM [ACVSUJournal_00010028].[dbo].[ACVSUJournalLog] AS t1
        LEFT JOIN [ACVSCore].[Access].[Personnel]     AS t2 ON t1.ObjectIdentity1 = t2.GUID
        LEFT JOIN [ACVSCore].[Access].[PersonnelType] AS t3 ON t2.PersonnelTypeId   = t3.ObjectID

        -- AdmitCode shred
        LEFT JOIN [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxmlShred] AS t5_admit
        ON t1.XmlGUID = t5_admit.GUID AND t5_admit.Name = 'AdmitCode'

        -- Direction shred
        LEFT JOIN [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxmlShred] AS t5_dir
        ON t1.XmlGUID = t5_dir.GUID AND t5_dir.Name = 'Direction'

        -- RejectCode shred
        LEFT JOIN [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxmlShred] AS t5_rej
        ON t1.XmlGUID = t5_rej.GUID AND t5_rej.Name = 'RejectCode'

        -- Pull entire XML and shredded “Card” fallback
        LEFT JOIN [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxml]      AS t_xml
        ON t1.XmlGUID = t_xml.GUID

        LEFT JOIN (
        SELECT GUID,[Value] FROM [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxmlShred]
        WHERE [Name] IN ('Card','CHUID')
        ) AS SCard ON t1.XmlGUID = SCard.GUID

        OUTER APPLY (
        SELECT COALESCE(
            TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
            TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
            SCard.[Value]
        ) AS CardNumber
        ) AS t5_card

        WHERE 
        t1.MessageType   IN ('CardAdmitted','CardRejected')
        AND t1.PartitionName2 = 'APAC.Default'
        AND CONVERT(date,
                DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])
            ) >= '2025-05-01'
        ),


--------------------------------------------------------------------------------
-- 2) Swipe counts per day
DailySwipes AS (
    SELECT
        EmployeeID,
        EmployeeName,
		PersonnelType,
        CONVERT(date, LocaleMessageTime)         AS SwipeDate,
        COUNT(DISTINCT LocaleMessageTime)        AS TotalSwipes
    FROM CombinedQuery
    WHERE PersonnelType IS NOT NULL
    AND PersonnelType NOT IN ('Visitor','Temp Badge')
    GROUP BY EmployeeID, EmployeeName,PersonnelType, CONVERT(date, LocaleMessageTime)
),


--------------------------------------------------------------------------------
-- 3) Only In / Only Out
DirectionSummary AS (
    SELECT
        EmployeeID,
        CONVERT(date, LocaleMessageTime)         AS SwipeDate,
        MAX(CASE WHEN Direction = 'InDirection'  THEN 1 ELSE 0 END)  AS InSwipe,
        MAX(CASE WHEN Direction = 'OutDirection' THEN 1 ELSE 0 END)  AS OutSwipe
    FROM CombinedQuery
    GROUP BY EmployeeID, CONVERT(date, LocaleMessageTime)
),


--------------------------------------------------------------------------------
-- 4) Single-door usage
SingleDoor AS (
    SELECT
        EmployeeID,
        CONVERT(date, LocaleMessageTime)         AS SwipeDate,
        COUNT(DISTINCT Door)                     AS DoorCount
    FROM CombinedQuery
    GROUP BY EmployeeID, CONVERT(date, LocaleMessageTime)
    HAVING COUNT(DISTINCT Door) = 1
),

--------------------------------------------------------------------------------
-- 5) Shift duration (PH/LT/CR locations break into shifts)
SwipeSequence AS (
    SELECT
        EmployeeID,
        EmployeeName,
        PersonnelType,
        Location,
        LocaleMessageTime,
        LAG(LocaleMessageTime) OVER (PARTITION BY EmployeeID ORDER BY LocaleMessageTime) AS PrevTime
    FROM CombinedQuery
),
ShiftIdentification AS (
    SELECT
        EmployeeID,
        EmployeeName,
        PersonnelType,
        Location,
        LocaleMessageTime,
        CASE
        WHEN PrevTime IS NULL OR DATEDIFF(MINUTE, PrevTime, LocaleMessageTime) > 360 THEN 1
        ELSE 0
        END AS IsNewShift
    FROM SwipeSequence
),
ShiftGroups AS (
    SELECT
        EmployeeID,
        EmployeeName,
        PersonnelType,
        Location,
        LocaleMessageTime,
        SUM(IsNewShift) OVER (PARTITION BY EmployeeID ORDER BY LocaleMessageTime) AS ShiftID
    FROM ShiftIdentification
),
ShiftBoundaries AS (
    SELECT
        EmployeeID,
        EmployeeName,
        PersonnelType,
        Location,
        ShiftID,
        MIN(LocaleMessageTime)                     AS ShiftStart,
        MAX(LocaleMessageTime)                     AS ShiftEnd,
        DATEDIFF(MINUTE, MIN(LocaleMessageTime), MAX(LocaleMessageTime)) AS DurationMinutes
    FROM ShiftGroups
    WHERE Location IN ('PH.Manila','LT.Vilnius','CR.Costa Rica Partition')
    GROUP BY EmployeeID,EmployeeName,PersonnelType,Location,ShiftID
),
DailyDuration AS (
    SELECT
        EmployeeID,
        EmployeeName,
        PersonnelType,
        Location,
        CONVERT(date, LocaleMessageTime)           AS AttendanceDate,
        MIN(LocaleMessageTime)                     AS ShiftStart,
        MAX(LocaleMessageTime)                     AS ShiftEnd,
        DATEDIFF(MINUTE, MIN(LocaleMessageTime), MAX(LocaleMessageTime)) AS DurationMinutes
    FROM CombinedQuery
    WHERE Location NOT IN ('PH.Manila','LT.Vilnius','CR.Costa Rica Partition')
    GROUP BY EmployeeID,EmployeeName,PersonnelType,Location,CONVERT(date, LocaleMessageTime)
),
AllShifts AS (
    SELECT EmployeeID, EmployeeName, PersonnelType, Location,
        CONVERT(date,ShiftStart) AS AttendanceDate, ShiftStart, ShiftEnd, DurationMinutes
    FROM ShiftBoundaries
UNION ALL
    SELECT EmployeeID, EmployeeName, PersonnelType, Location,
        AttendanceDate, FirstSwipeTime = ShiftStart, LastSwipeTime = ShiftEnd, DurationMinutes
    FROM DailyDuration
),

FinalMerged AS(
	SELECT 
		d.EmployeeID,
		d.EmployeeName,
		d.PersonnelType,
		d.SwipeDate,
		d.TotalSwipes,
		CASE WHEN ds.InSwipe  = 1 AND ds.OutSwipe = 0 THEN 1 ELSE 0 END AS OnlyIn,
		CASE WHEN ds.OutSwipe = 1 AND ds.InSwipe  = 0 THEN 1 ELSE 0 END AS OnlyOut,
		CASE WHEN sd.DoorCount = 1             THEN 1 ELSE 0 END AS SingleDoor,
		sh.DurationMinutes,
		fb.OutTime,
		fb.OutDoor,
		fb.InTime,
		fb.Indoor,
		fb.MinutesDiff
		FROM DailySwipes AS d
		LEFT JOIN DirectionSummary   AS ds ON ds.EmployeeID = d.EmployeeID AND ds.SwipeDate = d.SwipeDate
		LEFT JOIN SingleDoor         AS sd ON sd.EmployeeID = d.EmployeeID AND sd.SwipeDate = d.SwipeDate
		LEFT JOIN AllShifts          AS sh ON sh.EmployeeID = d.EmployeeID AND sh.AttendanceDate = d.SwipeDate
		LEFT JOIN FlaggedBreaks      AS fb ON fb.EmployeeID = d.EmployeeID AND fb.SwipeDate = sh.AttendanceDate 
),
-- Add this after FinalMerged CTE
WeeklyStatus AS (
    SELECT
        ds.EmployeeID,
        ds.EmployeeName,
		ds.PersonnelType,
        DATEADD(
            DAY,
            1 - DATEPART(WEEKDAY, ds.SwipeDate),  -- Week starts on Sunday
            ds.SwipeDate
        ) AS WeekOf,
        COUNT(DISTINCT ds.SwipeDate) AS DaysPresentInWeek,
        SUM(CASE WHEN ds.OnlyIn = 0 AND ds.OnlyOut = 0 THEN 1 ELSE 0 END) AS DaysWithoutDefault,
        CASE
            WHEN SUM(CASE WHEN ds.OnlyIn = 0 AND ds.OnlyOut = 0 THEN 1 ELSE 0 END) < 3 THEN 'Yes'
            ELSE 'No'
        END AS Defaulter
    FROM FinalMerged ds
    GROUP BY
        ds.EmployeeID,
        ds.EmployeeName,
		ds.PersonnelType,
        DATEADD(
            DAY,
            1 - DATEPART(WEEKDAY, ds.SwipeDate),
            ds.SwipeDate
        )
)

-- Final SELECT with WeeklyStatus join
SELECT
    fm.EmployeeID,
    fm.EmployeeName,
	fm.PersonnelType,
    fm.SwipeDate,
    fm.TotalSwipes,
    fm.OnlyIn,
    fm.OnlyOut,
    fm.SingleDoor,
    fm.DurationMinutes,
    fm.OutTime,
    fm.OutDoor,
    fm.InTime,
    fm.Indoor,
    fm.MinutesDiff,
    ws.WeekOf,
    ws.DaysPresentInWeek,
    ws.Defaulter
FROM FinalMerged fm
LEFT JOIN WeeklyStatus ws
ON fm.EmployeeID = ws.EmployeeID
AND fm.SwipeDate BETWEEN ws.WeekOf AND DATEADD(DAY, 6, ws.WeekOf)
ORDER BY fm.SwipeDate DESC; 
        """
        print(" step 3: query prepared")
        print("Query preview: \n",query[:300], "...")
        df=pd.read_sql(query,engine)
        print("step 4: query executed")
        print(f"total fetched rows :", df.shape[0])
        print("columns : ",df.columns.tolist())
        if df.empty:
            print("warning : query ran but return 0 rows")    
        else:
            print("preview of the result:",df.head(3))    
        return df

    except Exception as e:
        print("Connection error" ,e)
        













C:\Users\326131\OneDrive - Western Union\Desktop\Trend_backend\logic.py



'''
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Load weekly trend analysis CSV
employee_profile = pd.read_csv("current_analysis.csv")

# ========================
# 1. FILTER OUT INCOMPLETE WEEK (1-Jun-25)
# ========================
employee_profile = employee_profile[employee_profile['WeekOf'] != '01-Jun-25']

# ========================
# 2. WEEKLY TO DAILY CONVERSION
# ========================
# We'll convert weekly metrics to average daily values for comparison with live swipe data.
def weekly_to_daily(row):
    days = row['DaysPresentInWeek'] if row['DaysPresentInWeek'] > 0 else 1  # Avoid division by zero
    return pd.Series({
        'DailySwipesAvg': row['TotalSwipes'] / days,
        'DailyDurationAvg': row['AvgDurationMins'],  # already a daily average in some datasets
        'DailyCoffeeBadgingAvg': row['CoffeeBadgingCount'] / days,
        'DailySingleDoorAvg': row['SingleDoorEntries'] / days,
        'DailyOnlyInAvg': row['OnlyInCount'] / days,
        'DailyOnlyOutAvg': row['OnlyOutCount'] / days
    })

# Apply conversion row-wise and join back to the original dataframe
daily_metrics = employee_profile.apply(weekly_to_daily, axis=1)
employee_profile = pd.concat([employee_profile[['EmployeeID', 'EmployeeName', 'WeekOf']], daily_metrics], axis=1)

# ========================
# 3. FLAGGING LOGIC
# ========================
def flag_employee(row_dict):
    in_time=row_dict.get('InTime')
    out_time=row_dict.get('OutTime')
    if pd.notnull(in_time):
        in_time.hour
    else:
        in_time= None
    if pd.notnull(out_time):
        out_time.hour
    else:
        out_time= None

                            
    employee_id = row_dict.get('EmployeeID')
    personnel_type = row_dict.get('PersonnelType')  # Ensure this exists in your live swipe data

    print(f"\n\n--- Checking EmployeeID: {employee_id} ---")
    print("Live row:", row_dict)

    emp_hist = employee_profile[employee_profile['EmployeeID'] == employee_id]
    if emp_hist.empty:
        return False, ["No historical trend data"]

    # Initialize flags
    reasons = []
    if pd.notnull(row_dict.get('InTime')) and pd.notnull(row_dict.get('OutTime')):
        reasons.append("Coffee badging count detected(both intime and outtime present")
    
    if row_dict.get("OnlyIn",0) == 1:
        reasons.append("Onlyin entry detected")
    if row_dict.get("OnlyOut",0) == 1:
        reasons.append("OnlyOut entry detected")
    if row_dict.get("SingleDoor",0) == 1:
        reasons.append("SingleDoor entry detected")   

    # ============ 3A. DEFUALTER LOGIC (ONLY for Employees) ============
    if personnel_type == "Employee":
        is_defaulter = row_dict.get("Defaulter","No")  # Already calculated in live data
        if str(is_defaulter).strip().lower()=="yes":
            reasons.append("Flagged as Defaulter by company policy")

    # ============ 3B. DAILY BEHAVIOR ANOMALY ============
    # Compare current values to historical daily averages using z-score (robust)
    for metric, daily_col in [
        ('TotalSwipes', 'DailySwipesAvg'),
        ('DurationMinutes', 'DailyDurationAvg')
        #('CoffeeBadgingCount', 'DailyCoffeeBadgingAvg')
    ]:
        try:
            live_val = row_dict.get(metric)
            if live_val is None or pd.isna(live_val):
                reasons.append(f"{metric} missing or null in live data")
            hist_median = emp_hist[daily_col].median()
            hist_mad = (emp_hist[daily_col] - hist_median).abs().median() + 1e-9

            z = (live_val - hist_median) / hist_mad
            print(f"Z-score for {metric}: {z:.2f}")

            if abs(z) > 4:  # Adjust threshold as needed
                reasons.append(f"Abnormal {metric} detected (Z = {z:.2f})")
        except Exception as e:
            reasons.append(f"Error analyzing {metric}: {str(e)}")

    # ============ 3C. SHORT DURATION FLAG (e.g., coffee badging)
    try:
        duration = row_dict.get('DurationMinutes', 0)
        days_present = row_dict.get('DaysPresentInWeek', 0)
        if days_present < 3 and duration < 480:
            reasons.append("Duration < 8 hours on limited office days")
    except Exception as e:
        reasons.append(f"Error checking duration logic: {str(e)}")

    return len(reasons) > 0, reasons
'''
import pandas as pd
import numpy as np
from datetime import datetime

# Load employee historical profile
employee_profile = pd.read_csv("current_analysis.csv")

# ========================
# FLAGGING LOGIC
# ========================
def flag_employee(row_dict):
    employee_id = row_dict.get('EmployeeID')
    personnel_type = row_dict.get('PersonnelType')
    days_present = max(row_dict.get('DaysPresentInWeek', 1), 1)  # Prevent divide-by-zero

    print(f"\n--- Checking EmployeeID: {employee_id} ---")
    print("Live row:", row_dict)

    # Filter past data for the same employee
    emp_hist = employee_profile[employee_profile['EmployeeID'] == employee_id]
    if emp_hist.empty:
        return False, ["No historical trend data found"]

    reasons = []

    # ========== 1. Coffee Badging Check ==========
    if pd.notnull(row_dict.get('InTime')) and pd.notnull(row_dict.get('OutTime')):
        reasons.append("Coffee badging pattern detected (both InTime and OutTime present)")

    # ========== 2. Swipe Type Flags ==========
    if row_dict.get("OnlyIn", 0) == 1:
        reasons.append("OnlyIn entry detected")
    if row_dict.get("OnlyOut", 0) == 1:
        reasons.append("OnlyOut entry detected")
    if row_dict.get("SingleDoor", 0) == 1:
        reasons.append("SingleDoor entry detected")

    # ========== 3. Company Defaulter Policy ==========
    if personnel_type == "Employee":
        is_defaulter = row_dict.get("Defaulter", "No")
        if str(is_defaulter).strip().lower() == "yes":
            reasons.append("Flagged as Defaulter by company policy")

    # ========== 4. Behavior Anomaly Based on Median ± Std ==========
    metric_column_map = {
        'DurationMinutes': ('AvgDurationMins_median', 'AvgDurationMins_std'),
        'TotalSwipes': ('TotalSwipes_median', 'TotalSwipes_std')
    }

    for metric, (median_col, std_col) in metric_column_map.items():
        try:
            live_val = row_dict.get(metric)
            if live_val is None or pd.isna(live_val):
                reasons.append(f"{metric} missing or null in live data")
                continue

            median_val = emp_hist[median_col].values[0] #/ days_present
            std_val = emp_hist[std_col].values[0] #/ days_present
            buffer_multiplier = 2.5

            lower_bound = median_val - buffer_multiplier * std_val
            upper_bound = median_val + buffer_multiplier * std_val

            if live_val < lower_bound or live_val > upper_bound:
                reasons.append(
                    f"Abnormal {metric}: current={live_val}, expected ∈ [{lower_bound:.1f}, {upper_bound:.1f}] "
                    f"(median={median_val:.1f} ± {buffer_multiplier}×std={std_val:.1f})"
                )

        except Exception as e:
            reasons.append(f"Error analyzing {metric}: {str(e)}")

    # ========== 5. Short Duration on Few Office Days ==========
    try:
        duration = row_dict.get('DurationMinutes', 0)
        if days_present < 3 and duration < 480:
            reasons.append("Duration < 8 hours on limited office days")
    except Exception as e:
        reasons.append(f"Error checking duration logic: {str(e)}")

    return len(reasons) > 0, reasons
