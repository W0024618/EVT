# ----------------- START OF FILE: trend_runner.py (UPDATED) -----------------
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re

# IMPORTANT: duration_report must exist and expose run_for_date(date, regions, outdir, city)
from duration_report import run_for_date

# historical profile (optional)
HIST_PATH = Path(__file__).parent / "current_analysis.csv"
if HIST_PATH.exists():
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()
else:
    logging.warning("Historical profile file current_analysis.csv not found; history-based scenarios will fallback.")
    HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)


# ----- small shared helpers: treat empty/placeholder tokens as None -----
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


def _normalize_id_val(v):
    """
    Normalize an id-like value: strip, convert floats like '320172.0' -> '320172'
    Return None for NaN/empty/placeholder.
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    # strip .0 if integer-like
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s


# prefer to avoid emp:<GUID> person_uids — only treat emp: if value looks like a human id (not GUID)
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')

def _looks_like_guid(s: object) -> bool:
    """Return True if s looks like a GUID/UUID string."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    """Heuristic: treat as a plausible human name if it contains letters and not a GUID."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        # reject GUIDs and obviously numeric ids
        if _looks_like_guid(st):
            return False
        # require at least one alphabetic character
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    """Pick the first non-null, non-GUID, non-placeholder value from a pandas Series (as str) or None."""
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    """
    Create canonical person uid:
      - prefer EmployeeID (normalized) -> 'emp:<id>' only if it is not a GUID
      - else EmployeeIdentity -> 'uid:<val>' (GUID allowed)
      - else EmployeeName -> hash-based 'name:<shorthash>'
    """
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        # stable short hash of name
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None


# small helper to extract Card from XML-like strings
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        # fallback: look for CHUID ... Card: pattern or Card: 12345
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


# ---------------- NEW: door -> zone mapping helpers ----------------
# This is a compact, extendable map. Add exact keys or substrings as needed.
# Keys lower-cased, matching uses "if key in door_lower".
DOOR_ZONE_MAP = {
    # explicit substrings -> zone
    "east outdoor": "East Outdoor Area",
    "west outdoor": "West Outdoor Area",
    "assembly area": "Assembly Area",
    "out of office": "Out of office",
    "turnstile": "Reception Area",
    "turnstile 4 -out": "Out of office",
    "main lift lobby": "Reception Area",
    "reception entry": "Reception Area",
    "fire exit": "East Outdoor Area",
    "green reception": "Reception Area",
    "yellow reception": "Reception Area",
    "podium_p-1 turnstile": "Reception Area",  # example
    "podium_p-1 turnstile 4 -out door": "Out of office",
    "podium_yellow_fire exit": "East Outdoor Area",
    "podium_yellow_main lift lobby": "Yellow Zone",
    "podium_red_main lift lobby entry": "Red Zone",
    "podium_green_main lift lobby": "Green Zone",
    "2ndflr_liftlobby to reception": "2nd Floor, Pune",
    "2ndflr_reception to workstation": "2nd Floor, Pune",
    # generic patterns:
    "reception": "Reception Area",
    "lobby": "Reception Area",
    "outdoor": "East Outdoor Area",
    "outside": "East Outdoor Area",
    "main lift": "Reception Area",
    "ws entry": "Workstation Area",
    "workstation": "Workstation Area",
    "podium": "Podium",
    "tower b": "Tower B",
    "idF": "Restricted Area",
    "mdf": "Restricted Area",
    "kitchenette": "West Outdoor Area",
    "fire exit": "East Outdoor Area",
}

# explicit list of zones considered breaks
BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
OUT_OF_OFFICE_ZONE = "Out of office"

def map_door_to_zone(door: object, direction: object = None) -> str:
    """
    Map a raw Door string (and optionally Direction) to a logical zone.
    - Uses substring matching on lowercased door text and door map.
    - Falls back to PartitionName2 heuristics if door is missing.
    """
    try:
        if door is None:
            return None
        s = str(door).strip()
        if not s:
            return None
        s_l = s.lower()
        # try exact substring matches from map
        for k, v in DOOR_ZONE_MAP.items():
            if k in s_l:
                # if mapping says out-of-office for certain OUT direction tokens, prefer that
                return v
        # fallback: direction-based inference
        if direction and isinstance(direction, str):
            d = direction.strip().lower()
            if "out" in d:
                return OUT_OF_OFFICE_ZONE
            if "in" in d:
                # assume reception/working
                return "Reception Area"
        # heuristic fallback
        if "out" in s_l or "exit" in s_l or "turnstile" in s_l and "out" in s_l:
            return OUT_OF_OFFICE_ZONE
        # else treat as working area
        return "Working Area"
    except Exception:
        return None

# --- compute_features (replaced/updated) ---
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    Returns DataFrame per (person_uid, Date) with feature columns and normalized IDs/names.
    This implementation follows the duration_report column conventions and avoids
    treating GUIDs or placeholder tokens as EmployeeName/EmployeeID/CardNumber.
    It also computes break segments and the special pattern you requested.
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # Build lowercase->actual column map for flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}

    # detect time column
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)
    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
        sw['Date'] = sw['LocaleMessageTime'].dt.date
    else:
        if 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
        else:
            sw['Date'] = None

    # find these earlier in compute_features — prefer Int1/Text12 for EmployeeID and CHUID/Card for CardNumber
    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    # Filter personnel types: prefer PersonnelTypeName, fallback to PersonnelType
    if 'PersonnelTypeName' in sw.columns:
        sw = sw[sw['PersonnelTypeName'].isin(['Employee', 'Terminated Personnel'])]
    elif 'PersonnelType' in sw.columns:
        sw = sw[sw['PersonnelType'].isin(['Employee', 'Terminated Personnel'])]
    # else keep everything

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # ensure stable person_uid (canonical)
    if 'person_uid' not in sw.columns:
        def make_person_uid_local(r):
            # prefer canonical EmployeeID (normalized, non-GUID) then EmployeeIdentity then EmployeeName
            empid_val = None
            if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
                empid_val = r.get(empid_col)
            elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
                empid_val = r.get('EmployeeID')

            empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
            name_val = None
            if name_col and name_col in r:
                name_val = r.get(name_col)
            elif 'EmployeeName' in r:
                name_val = r.get('EmployeeName')
            elif 'ObjectName1' in r:
                name_val = r.get('ObjectName1')

            return _canonical_person_uid({
                'EmployeeID': empid_val,
                'EmployeeIdentity': empident_val,
                'EmployeeName': name_val
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    # selection columns for aggregation: include discovered columns
    sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
                    'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
                    'EmployeeIdentity'])
    if name_col:
        sel_cols.add(name_col)
    if empid_col:
        sel_cols.add(empid_col)
    if card_col:
        sel_cols.add(card_col)
    if door_col:
        sel_cols.add(door_col)
    if dir_col:
        sel_cols.add(dir_col)
    sel_cols = [c for c in sel_cols if c in sw.columns]

    def agg_swipe_group(g):
        # g is a DataFrame for one person_uid + date
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0

        # Direction counts (default to column names present)
        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

        # pick first non-placeholder, non-guid card number if present (prefer cardnumber/chuid)
        card_numbers = []
        # 1) direct known column
        if card_col and card_col in g.columns:
            card_numbers = list(pd.unique(g[card_col].dropna()))
        # 2) explicit 'CardNumber' output column (from SQL COALESCE)
        if not card_numbers and 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna()))
        # 3) some XML-shred columns may appear as 'value' or other column names
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
                    try:
                        vals = list(pd.unique(g[c].dropna()))
                        if vals:
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        # 4) lastly try to extract from XmlMessage fields
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'xml' in cl or 'xmlmessage' in cl or 'xml_msg' in cl or 'xmlmessage' in cl:
                    for raw in g[c].dropna().astype(str):
                        extracted = _extract_card_from_xml(raw)
                        if extracted:
                            card_numbers.append(extracted)
        # 5) final unique
        card_numbers = list(dict.fromkeys(card_numbers))  # preserve order, unique

        card_number = None
        for c in card_numbers:
            n = _normalize_id_val(c)
            # explicitly reject GUIDs as card numbers
            if n and not _looks_like_guid(n):
                card_number = n
                break

        # stable id/name from the group using discovered columns first
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None

        # Employee ID: prefer Int1/Text12 then EmployeeID; DO NOT use EmployeeIdentity as EmployeeID
        # use _pick_first_non_guid_value to skip GUIDs automatically
        if empid_col and empid_col in g.columns:
            vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                # final trial: numeric normalization (strip .0) but still reject GUIDs
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        elif 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized

        # If still no employee_id and PersonnelType indicates contractor -> prefer Text12 explicitly
        if (not employee_id) and 'PersonnelType' in g.columns:
            try:
                pvals = g['PersonnelType'].dropna().astype(str)
                if not pvals.empty:
                    p0 = pvals.iloc[0]
                    if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
                        # look for text12 explicitly (case-insensitive)
                        for c in g.columns:
                            if c.lower() == 'text12':
                                vals = g[c].dropna().astype(str).map(lambda x: x.strip())
                                employee_id = _pick_first_non_guid_value(vals)
                                if employee_id:
                                    break
            except Exception:
                pass

        # Employee identity (GUID) — keep but do not promote to EmployeeID
        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]

        # Employee name: pick non-GUID candidate
        candidate_name_vals = None
        if name_col and name_col in g.columns:
            candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
        elif 'EmployeeName' in g.columns:
            candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
        elif 'ObjectName1' in g.columns:
            candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

        if candidate_name_vals is not None and not candidate_name_vals.empty:
            employee_name = _pick_first_non_guid_value(candidate_name_vals)
            if employee_name is None:
                # accept any value that looks like a name
                for v in candidate_name_vals:
                    if _looks_like_name(v) and not _is_placeholder_str(v):
                        employee_name = str(v).strip()
                        break

        # personnel type
        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        # First/Last swipe times
        first_swipe = None
        last_swipe = None
        if times:
            first_swipe = times[0]
            last_swipe = times[-1]

        # ----------------- NEW: break/out-of-office sequence analysis -----------------
        # Build a timeline of (time, door, direction, zone)
        timeline = []
        for _, row in g.sort_values('LocaleMessageTime').iterrows():
            t = row.get('LocaleMessageTime')
            dname = None
            if door_col and door_col in row and pd.notna(row.get(door_col)):
                dname = row.get(door_col)
            elif 'Door' in row and pd.notna(row.get('Door')):
                dname = row.get('Door')
            direction = None
            if dir_col and dir_col in row and pd.notna(row.get(dir_col)):
                direction = row.get(dir_col)
            elif 'Direction' in row and pd.notna(row.get('Direction')):
                direction = row.get('Direction')
            zone = map_door_to_zone(dname, direction)
            timeline.append((t, dname, direction, zone))

        # compress timeline into segments with labels: 'work', 'break', 'out_of_office'
        segments = []
        if timeline:
            cur_zone = None
            seg_start = timeline[0][0]
            seg_label = None
            for (t, dname, direction, zone) in timeline:
                # determine label
                if zone in BREAK_ZONES:
                    lbl = 'break'
                elif zone == OUT_OF_OFFICE_ZONE:
                    lbl = 'out_of_office'
                else:
                    lbl = 'work'
                if cur_zone is None:
                    cur_zone = zone
                    seg_label = lbl
                    seg_start = t
                else:
                    # if label changes, close previous segment
                    if lbl != seg_label:
                        segments.append({
                            'label': seg_label,
                            'start': seg_start,
                            'end': t,
                            'start_zone': cur_zone
                        })
                        seg_start = t
                        seg_label = lbl
                        cur_zone = zone
                    else:
                        # keep current segment
                        cur_zone = cur_zone or zone
            # close last
            if seg_label is not None:
                segments.append({
                    'label': seg_label,
                    'start': seg_start,
                    'end': timeline[-1][0],
                    'start_zone': cur_zone
                })

        # Compute break metrics
        break_count = 0
        long_break_count = 0
        total_break_minutes = 0.0
        for s in segments:
            if s['label'] in ('break', 'out_of_office'):
                dur_mins = (s['end'] - s['start']).total_seconds() / 60.0 if (s['end'] and s['start']) else 0
                total_break_minutes += dur_mins
                break_count += 1
                if dur_mins >= 60:
                    long_break_count += 1

        # Detect the specific pattern:
        # work (short <60) -> out_of_office (long >=180) -> work (short <60) -> out_* (any)
        pattern_flag = False
        pattern_sequence = None
        try:
            # create simplified label list with durations
            seq = []
            for s in segments:
                dur_mins = (s['end'] - s['start']).total_seconds() / 60.0 if (s['end'] and s['start']) else 0
                seq.append((s['label'], round(dur_mins)))
            # look for the pattern anywhere in sequence
            for i in range(len(seq)-3):
                a = seq[i]     # first work
                b = seq[i+1]   # long out
                c = seq[i+2]   # short work
                d = seq[i+3]   # out / break
                if a[0] == 'work' and a[1] < 60 and \
                   b[0] in ('out_of_office','break') and b[1] >= 180 and \
                   c[0] == 'work' and c[1] < 60 and \
                   d[0] in ('out_of_office','break','work'):
                    pattern_flag = True
                    pattern_sequence = f"{a}->{b}->{c}->{d}"
                    break
        except Exception:
            pattern_flag = False
            pattern_sequence = None

        # ----------------- return aggregated metrics (including new ones) -----------------
        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type,
            'FirstSwipe': first_swipe,
            'LastSwipe': last_swipe,
            # new break features
            'BreakCount': int(break_count),
            'LongBreakCount': int(long_break_count),
            'TotalBreakMinutes': float(round(total_break_minutes,1)),
            'PatternShortLongRepeat': bool(pattern_flag),
            'PatternSequence': pattern_sequence
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = grouped.apply(agg_swipe_group).reset_index()

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # --- START PATCH: coalesce duplicate columns produced by merge ---
    def _coalesce_merge_columns(df, bases):
        for base in bases:
            x = base + "_x"
            y = base + "_y"
            try:
                has_base = base in df.columns
                base_all_null = False
                if has_base:
                    base_all_null = df[base].isnull().all()
            except Exception:
                has_base = base in df.columns
                base_all_null = True

            if (not has_base) or base_all_null:
                if x in df.columns and y in df.columns:
                    try:
                        df[base] = df[x].combine_first(df[y])
                    except Exception:
                        try:
                            df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
                        except Exception:
                            if x in df.columns:
                                df[base] = df[x]
                            elif y in df.columns:
                                df[base] = df[y]
                elif x in df.columns:
                    df[base] = df[x]
                elif y in df.columns:
                    df[base] = df[y]
    _coalesce_merge_columns(merged, [
        "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
    ])
    drop_cols = [c for c in merged.columns if c.endswith("_x") or c.endswith("_y")]
    if drop_cols:
        try:
            merged.drop(columns=drop_cols, inplace=True)
        except Exception:
            for c in drop_cols:
                if c in merged.columns:
                    try:
                        merged.drop(columns=[c], inplace=True)
                    except Exception:
                        pass
    # --- END PATCH ---

    # coalesce helpers (ensure column existence)
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)
    ensure_col(merged, 'BreakCount', 0)
    ensure_col(merged, 'LongBreakCount', 0)
    ensure_col(merged, 'TotalBreakMinutes', 0.0)
    ensure_col(merged, 'PatternShortLongRepeat', False)
    ensure_col(merged, 'PatternSequence', None)

    # If EmployeeName is missing or a GUID, try to get a better name from durations (durations typically has EmployeeName)
    if 'EmployeeName' in merged.columns:
        def choose_best_name(row):
            gname = row.get('EmployeeName')
            dname = None
            for cand in ('EmployeeName', 'employee_name', 'objectname1', 'ObjectName1'):
                if cand in row and row.get(cand) is not None:
                    dname = row.get(cand)
                    break
            if _looks_like_name(gname):
                return str(gname).strip()
            if _looks_like_name(dname):
                return str(dname).strip()
            if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
                return str(gname).strip()
            if dname and not _is_placeholder_str(dname):
                return str(dname).strip()
            return None
        merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
    else:
        if not dur.empty:
            def fill_name_from_dur(row):
                gname = row.get('EmployeeName')
                if _looks_like_name(gname) and not _is_placeholder_str(gname):
                    return gname
                for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
                    if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
                        return row[cand]
                return None
            merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

    # numeric normalization for EmployeeID: ensure not GUIDs/placeholder, convert floats like '320172.0' -> '320172'
    def normalize_empid(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            try:
                if '.' in s:
                    f = float(s)
                    if math.isfinite(f) and f.is_integer():
                        return str(int(f))
            except Exception:
                pass
            return s
        except Exception:
            return None

    merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

    # normalize card numbers: reject GUIDs and placeholder tokens
    def normalize_card(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            return s
        except Exception:
            return None

    merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

    # numeric normalization
    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)
    merged['BreakCount'] = merged['BreakCount'].fillna(0).astype(int)
    merged['LongBreakCount'] = merged['LongBreakCount'].fillna(0).astype(int)
    merged['TotalBreakMinutes'] = merged['TotalBreakMinutes'].fillna(0.0).astype(float)
    merged['PatternShortLongRepeat'] = merged['PatternShortLongRepeat'].fillna(False).astype(bool)

    # ensure FirstSwipe/LastSwipe are datetimes
    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # EmpHistoryPresent
    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    # normalize string columns for safe downstream use; EmployeeName keep as readable-only
    for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
        if c in merged.columns:
            def _clean_str_val(v):
                if v is None:
                    return None
                try:
                    s = str(v).strip()
                    if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                        return None
                    return s
                except Exception:
                    return None
            merged[c] = merged[c].apply(_clean_str_val)

    # EmployeeName: keep None if empty or GUID/placeholder; otherwise string.
    if 'EmployeeName' in merged.columns:
        merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

    return merged


# ---------------- SCENARIOS (boolean functions) ----------------
def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
    except Exception:
        pass
    if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    return cur > 50

def scenario_repeated_short_breaks(row):
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map

# NEW scenario: the pattern described by the user
def scenario_shortstay_longout_repeat(row):
    # Uses the feature computed in compute_features: PatternShortLongRepeat
    return bool(row.get('PatternShortLongRepeat', False))


# scenario list (name, fn)
SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap),
    # new pattern scenario
    ("shortstay_longout_repeat", scenario_shortstay_longout_repeat)
]


def run_trend_for_date(target_date: date, outdir: str = "./outputs", city='Pune'):
    logging.info("run_trend_for_date: date=%s (city=%s)", target_date, city)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city=city)
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    # save raw swipes for evidence (full raw)
    try:
        if swipes is not None and not swipes.empty:
            sw_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception as e:
        logging.warning("Failed to save raw swipes: %s", e)

    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # Build badge map and swipe overlap maps for higher-severity scenarios
    badge_map = {}
    if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}

    swipe_overlap_map = {}
    overlap_window_seconds = 2
    if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
        tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
        if not tmp.empty:
            tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
            for (d, door), g in tmp.groupby(['Date', 'Door']):
                items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                n = len(items)
                for i in range(n):
                    t_i, uid_i = items[i]
                    j = i+1
                    while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                        uid_j = items[j][1]
                        if uid_i != uid_j:
                            swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                            swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                        j += 1

    # Evaluate scenarios (use weighting to compute anomaly score)
    WEIGHTS = {
        "long_gap_>=90min": 0.3,
        "short_duration_<4h": 1.0,
        "coffee_badging": 1.0,
        "low_swipe_count_<=2": 0.5,
        "single_door": 0.25,
        "only_in": 0.8,
        "only_out": 0.8,
        "overtime_>=10h": 0.2,
        "very_long_duration_>=16h": 1.5,
        "zero_swipes": 0.4,
        "unusually_high_swipes": 1.5,
        "repeated_short_breaks": 0.5,
        "multiple_location_same_day": 0.6,
        "weekend_activity": 0.6,
        "repeated_rejection_count": 0.8,
        "badge_sharing_suspected": 2.0,
        "early_arrival_before_06": 0.4,
        "late_exit_after_22": 0.4,
        "shift_inconsistency": 1.2,
        "trending_decline": 0.7,
        "consecutive_absent_days": 1.2,
        "high_variance_duration": 0.8,
        "short_duration_on_high_presence_days": 1.1,
        "swipe_overlap": 2.0,
        # weight for new scenario
        "shortstay_longout_repeat": 2.0
    }
    ANOMALY_THRESHOLD = 1.5

    # evaluate scenarios and compute score
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            features[name] = features.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
        elif name == "swipe_overlap":
            features[name] = features.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map), axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    def compute_score(r):
        score = 0.0
        detected = []
        for name, _ in SCENARIOS:
            val = bool(r.get(name))
            w = WEIGHTS.get(name, 0.0)
            if val and w > 0:
                score += float(w)
                detected.append(name)
        return score, detected

    scores = features.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
    features['AnomalyScore'] = scores['AnomalyScore'].astype(float)
    features['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
    features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

    def reasons_for_row(r):
        if not bool(r.get('IsFlagged')):
            return None
        ds = r.get('DetectedScenarios')
        if ds:
            # prefer human explanation for the new pattern
            reasons = []
            for sc in (ds.split(";") if isinstance(ds, str) else []):
                sc = sc.strip()
                if sc == "shortstay_longout_repeat":
                    # explain with computed pattern sequence and break durations
                    seq = r.get('PatternSequence') or "sequence_not_available"
                    reasons.append(f"Pattern detected (short stay -> long out -> short return): {seq}")
                else:
                    reasons.append(sc)
            return "; ".join(reasons) if reasons else ds
        return None
    features['Reasons'] = features.apply(reasons_for_row, axis=1)

    if 'OverlapWith' not in features.columns:
        def overlap_with_fn(r):
            d = r.get('Date')
            uid = r.get('person_uid')
            if (d, uid) in swipe_overlap_map:
                return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
            return None
        features['OverlapWith'] = features.apply(overlap_with_fn, axis=1)

    # Remove suffix columns and fix duplicates
    cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
    if cols_to_drop:
        for c in cols_to_drop:
            base = c[:-2]
            if base in features.columns:
                try:
                    features.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                try:
                    features.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass
    features = features.loc[:, ~features.columns.duplicated()]

    # ensure booleans are native Python (avoid numpy.bool_)
    for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
        if col in features.columns:
            features[col] = features[col].astype(bool)

    # write CSV with native types
    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    try:
        write_df = features.copy()
        # FirstSwipe/LastSwipe -> ISO strings
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        # Date -> ISO date
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        write_df = write_df.where(pd.notnull(write_df), None)
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception as e:
        logging.exception("Failed to write trend CSV: %s", e)

    return features


# ---------------- training dataset builder (restored) ----------------
def build_monthly_training(end_date: date = None, months: int = 3, min_unique_employees: int = 1000,
                           outdir: str = "./outputs", city: str = "Pune"):
    if end_date is None:
        end_date = datetime.now().date()
    logging.info("build_monthly_training: end_date=%s months=%d min_unique=%d", end_date, months, min_unique_employees)
    outdir = Path(outdir)
    month_windows = []
    cur = end_date.replace(day=1)
    for _ in range(months):
        start = cur
        next_month = (cur.replace(day=28) + timedelta(days=4)).replace(day=1)
        last = next_month - timedelta(days=1)
        month_windows.append((start, last))
        cur = (start - timedelta(days=1)).replace(day=1)

    person_month_rows = []
    unique_persons = set()

    for start, last in month_windows:
        d = start
        month_dfs = []
        while d <= last:
            csv_path = outdir / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
            if csv_path.exists():
                try:
                    df = pd.read_csv(csv_path)
                    month_dfs.append(df)
                except Exception:
                    try:
                        df = pd.read_csv(csv_path, dtype=str)
                        month_dfs.append(df)
                    except Exception as e:
                        logging.warning("Failed reading %s: %s", csv_path, e)
            d = d + timedelta(days=1)

        if not month_dfs:
            logging.info("No daily trend CSVs found for month %s - %s", start.isoformat(), last.isoformat())
            continue

        month_df = pd.concat(month_dfs, ignore_index=True)
        # ensure person_uid exists
        if 'person_uid' not in month_df.columns:
            def make_person_uid(row):
                parts = []
                for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                    v = row.get(c)
                    if pd.notna(v) and str(v).strip():
                        parts.append(str(v).strip())
                return "|".join(parts) if parts else None
            month_df['person_uid'] = month_df.apply(make_person_uid, axis=1)

        # convert boolean columns to int for aggregation if necessary
        for name, _ in SCENARIOS:
            if name in month_df.columns:
                month_df[name] = month_df[name].astype(int)

        agg_funcs = {
            'CountSwipes': ['median', 'mean', 'sum'],
            'DurationMinutes': ['median', 'mean', 'sum'],
            'MaxSwipeGapSeconds': ['max', 'median'],
            'ShortGapCount': ['sum'],
            'UniqueDoors': ['median'],
            'UniqueLocations': ['median'],
            'RejectionCount': ['sum']
        }
        scenario_cols = [name for name,_ in SCENARIOS if name in month_df.columns]
        group_cols = ['person_uid']
        grp = month_df.groupby(group_cols)

        for person, g in grp:
            row = {}
            row['person_uid'] = person
            row['EmployeeID'] = next((v for v in g.get('EmployeeID', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['EmployeeName'] = next((v for v in g.get('EmployeeName', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['MonthStart'] = start.isoformat()
            row['MonthEnd'] = last.isoformat()
            for col, funcs in agg_funcs.items():
                if col in g.columns:
                    for f in funcs:
                        key = f"{col}_{f}"
                        try:
                            val = getattr(g[col], f)()
                            row[key] = float(val) if pd.notna(val) else None
                        except Exception:
                            row[key] = None
                else:
                    for f in funcs:
                        row[f"{col}_{f}"] = None
            for s in scenario_cols:
                row[f"{s}_days"] = int(g[s].sum())
                row[f"{s}_label"] = int(g[s].sum() > 0)
            row['days_present'] = int(g.shape[0])
            person_month_rows.append(row)
            unique_persons.add(person)

        if len(unique_persons) >= min_unique_employees:
            logging.info("Reached min unique employees=%d, stopping aggregation early", min_unique_employees)
            break

    if not person_month_rows:
        logging.warning("No person-month rows created (no data).")
        return None

    training_df = pd.DataFrame(person_month_rows)
    train_out = outdir / "training_person_month.csv"
    training_df.to_csv(train_out, index=False)
    logging.info("Saved training CSV to %s (rows=%d unique_persons=%d)", train_out, len(training_df), len(unique_persons))
    return train_out


if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today)
    print("Completed; rows:", len(df) if df is not None else 0)
# ----------------- END OF FILE: trend_runner.py (UPDATED) -----------------























Now Today we will start Building each Scenarion Carefully and train model ...

initially will share you some Scenarion details ....

Message Type	Date	Time	Message Text	IN/OUT	Object 1 Name	Object 2 Name	Partition
Card Admitted	09 July 2025	7:28:10 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	09 July 2025	7:29:18 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	09 July 2025	7:29:26 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	09 July 2025	6:13:22 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74 [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74	APAC.Default
Card Admitted	09 July 2025	6:13:41 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (IN) ([Unused]) entering area Pune 2nd floor Out Area.	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	09 July 2025	7:57:31 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (OUT) ([Unused]) entering area Pune 2nd Floor IN Area.	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	10 July 2025	7:19:51 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	10 July 2025	7:20:09 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW) [APAC.Default]' (OUT).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)	APAC.Default
Card Admitted	10 July 2025	7:22:05 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED) [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED)	APAC.Default
Card Admitted	10 July 2025	7:22:25 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	10 July 2025	6:09:37 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74 [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74	APAC.Default
Card Admitted	10 July 2025	6:09:43 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (IN) ([Unused]) entering area Pune 2nd floor Out Area.	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	10 July 2025	6:30:40 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (OUT) ([Unused]) entering area Pune 2nd Floor IN Area.	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	10 July 2025	6:48:48 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74 [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74	APAC.Default
Card Admitted	10 July 2025	6:48:52 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (IN) ([Unused]) entering area Pune 2nd floor Out Area.	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	10 July 2025	6:49:01 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (OUT) ([Unused]) entering area Pune 2nd Floor IN Area.	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	18 July 2025	6:48:04 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	18 July 2025	6:49:20 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED) [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED)	APAC.Default
Card Admitted	18 July 2025	6:49:41 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	18 July 2025	6:49:50 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	18 July 2025	4:47:04 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	18 July 2025	4:49:17 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR [APAC.Default]' (OUT) entering area Pune Ofice Outside Area ( Podium Floor).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	18 July 2025	5:01:30 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_RECEPTION TO WS ENTRY 1-DOOR NEW [APAC.Default]' (IN) ([Unused]) entering area Pune Office Inside Area ( Workstation Area).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_RECEPTION TO WS ENTRY 1-DOOR NEW	APAC.Default
Card Admitted	18 July 2025	5:04:21 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74 [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74	APAC.Default
Card Admitted	18 July 2025	5:04:28 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (IN) ([Unused]) entering area Pune 2nd floor Out Area.	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	18 July 2025	6:21:49 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (OUT) ([Unused]) entering area Pune 2nd Floor IN Area.	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	21 July 2025	6:42:50 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	21 July 2025	6:43:50 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	21 July 2025	5:26:49 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74 [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74	APAC.Default
Card Admitted	21 July 2025	5:27:01 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74 [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74	APAC.Default
Card Admitted	21 July 2025	5:27:05 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (IN) ([Unused]) entering area Pune 2nd floor Out Area.	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	21 July 2025	5:27:39 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (OUT) ([Unused]) entering area Pune 2nd Floor IN Area.	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	21 July 2025	5:28:58 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74 [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74	APAC.Default
Card Admitted	21 July 2025	5:29:04 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (IN) ([Unused]) entering area Pune 2nd floor Out Area.	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	21 July 2025	6:13:22 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (OUT) ([Unused]) entering area Pune 2nd Floor IN Area.	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	28 July 2025	6:19:14 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	28 July 2025	6:20:14 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	28 July 2025	6:20:31 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	28 July 2025	4:29:43 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74 [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74	APAC.Default
Card Admitted	28 July 2025	4:29:49 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (IN) ([Unused]) entering area Pune 2nd floor Out Area.	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	28 July 2025	5:19:36 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (OUT) ([Unused]) entering area Pune 2nd Floor IN Area.	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	04 August 2025	6:11:26 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	04 August 2025	2:12:34 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74 [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74	APAC.Default
Card Admitted	04 August 2025	2:12:38 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (IN) ([Unused]) entering area Pune 2nd floor Out Area.	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	04 August 2025	4:29:19 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (OUT) ([Unused]) entering area Pune 2nd Floor IN Area.	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	04 August 2025	5:20:07 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74 [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR_10:05:74	APAC.Default
Card Admitted	04 August 2025	5:20:12 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (IN) ([Unused]) entering area Pune 2nd floor Out Area.	IN	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	04 August 2025	5:44:19 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B [APAC.Default]' (OUT) ([Unused]) entering area Pune 2nd Floor IN Area.	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B	APAC.Default
Card Admitted	12 August 2025	7:09:05 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	12 August 2025	7:09:59 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	12 August 2025	7:11:58 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	12 August 2025	7:25:28 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (OUT).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	12 August 2025	7:26:16 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	13 August 2025	6:43:02 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	13 August 2025	6:43:19 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW) [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)	APAC.Default
Card Admitted	13 August 2025	6:44:07 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	21 October 2025	4:25:29 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4-DOOR [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4-DOOR	APAC.Default
Card Admitted	21 October 2025	4:26:08 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR [APAC.Default]' (IN) ([Unused]) entering area Pune Office Inside Area ( Workstation Area).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	21 October 2025	5:08:07 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR [APAC.Default]' (OUT) ([Unused]) entering area Pune Ofice Outside Area ( Podium Floor).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	21 October 2025	5:08:55 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4 -OUT DOOR [APAC.Default]' (OUT).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4 -OUT DOOR	APAC.Default
Card Admitted	22 October 2025	7:40:58 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	22 October 2025	7:41:12 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	22 October 2025	4:19:24 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4-DOOR [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4-DOOR	APAC.Default
Card Admitted	22 October 2025	4:20:03 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR [APAC.Default]' (IN) ([Unused]) entering area Pune Office Inside Area ( Workstation Area).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	22 October 2025	5:08:19 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR [APAC.Default]' (OUT) ([Unused]) entering area Pune Ofice Outside Area ( Podium Floor).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	22 October 2025	5:09:02 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4 -OUT DOOR [APAC.Default]' (OUT).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4 -OUT DOOR	APAC.Default
Card Admitted	23 October 2025	7:25:46 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	23 October 2025	7:26:03 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED) [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED)	APAC.Default
Card Admitted	23 October 2025	7:27:48 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED) [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED)	APAC.Default
Card Admitted	23 October 2025	7:28:03 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR [APAC.Default]' (OUT) ([Unused]).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR	APAC.Default
Card Admitted	23 October 2025	4:24:28 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	23 October 2025	4:24:52 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR [APAC.Default]' (OUT) entering area Pune Ofice Outside Area ( Podium Floor).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	23 October 2025	4:25:18 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR [APAC.Default]' (IN) ([Unused]) entering area Pune Office Inside Area ( Workstation Area).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	23 October 2025	4:27:19 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR [APAC.Default]' (OUT) ([Unused]) entering area Pune Ofice Outside Area ( Podium Floor).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	23 October 2025	4:41:35 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR [APAC.Default]' (IN) ([Unused]) entering area Pune Office Inside Area ( Workstation Area).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	23 October 2025	4:46:40 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR [APAC.Default]' (OUT) ([Unused]) entering area Pune Ofice Outside Area ( Podium Floor).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	23 October 2025	4:48:08 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4 -OUT DOOR [APAC.Default]' (OUT).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4 -OUT DOOR	APAC.Default
Card Admitted	27 October 2025	6:51:21 AM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	27 October 2025	3:03:23 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (IN) ([Unused]).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default
Card Admitted	27 October 2025	3:03:43 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR [APAC.Default]' (OUT) entering area Pune Ofice Outside Area ( Podium Floor).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	27 October 2025	3:12:16 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR [APAC.Default]' (IN) ([Unused]) entering area Pune Office Inside Area ( Workstation Area).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR	APAC.Default
Card Admitted	27 October 2025	4:07:20 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_FIRE EXIT 1-DOOR NEW [APAC.Default]' (OUT).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_FIRE EXIT 1-DOOR NEW	APAC.Default
Card Admitted	27 October 2025	4:31:55 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_FIRE EXIT 1-DOOR NEW [APAC.Default]' (IN).	IN	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_FIRE EXIT 1-DOOR NEW	APAC.Default
Card Admitted	27 October 2025	4:32:42 PM	Admitted 'Sai golla, Jagadeesh [Global]' (Card: 615509)   at 'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW [APAC.Default]' (OUT).	OUT	Sai golla, Jagadeesh	APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW	APAC.Default





Lets Check above Swipe details ..




Date	Morning IN	Morning OUT	Duration (HH:MM)	Evening IN	Evening OUT	Duration (HH:MM)
09 July 2025	7:28:10 AM	7:29:26 AM	0:01	6:13:22 PM	7:57:31 PM	1:44
10 July 2025	7:19:51 AM	7:22:25 AM	0:02	6:09:37 PM	6:49:01 PM	0:39
18 July 2025	6:48:04 AM	6:49:50 AM	0:01	4:47:04 PM	6:21:49 PM	1:34
21 July 2025	6:42:50 AM	6:43:50 AM	0:01	5:26:49 PM	6:13:22 PM	0:46
28 July 2025	6:19:14 AM	6:20:31 AM	0:01	4:29:43 PM	5:19:36 PM	0:49
04 August 2025	6:11:26 AM	-	0:00	2:12:34 PM	5:44:19 PM	3:31
12 August 2025	7:09:05 AM	7:09:59 AM	0:00	7:11:58 PM	7:26:16 PM	0:14
13 August 2025	6:43:02 AM	6:44:07 AM	0:01	-	-	0:00
21 October 2025	-	-	0:00	4:25:29 PM	5:08:55 PM	0:43
22 October 2025	7:40:58 AM	7:41:12 AM	0:00	4:19:24 PM	5:09:02 PM	0:49
23 October 2025	7:25:46 AM	7:28:03 AM	0:02	4:24:28 PM	4:48:08 PM	0:23
27 October 2025	6:51:21 AM	-	0:00	3:03:23 PM	4:32:42 PM	1:29


and this is Duration of tht EMployee ..
SO we need to find out that type of people ...
WHo are come initailly in Office ...
then He stay office for less than hour he went went out of office for more than 3 hr and again he cam office 
and stay less than hr and went out of office 

We want to add this SCnearion...

alos we have door details 
When EMployee Swipe badge In or out which Zone Floor , or Out of office 

all details so in our project create one door file So we can 
add break details , also Out of office details ...




const doorZoneMap = {


  // Podium / Red
  "APAC_IN_PUN_PODIUM_RED_IDF ROOM-02-RESTRICTED DOOR___InDirection":                 "Red Zone",
  "APAC_IN_PUN_PODIUM_ST2 DOOR 1 (RED)___InDirection":                               "Red Zone",
  "APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR___InDirection":                 "Red Zone",
  "APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR___OutDirection":                "Out of office",
  "APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED)___InDirection":                                "Red Zone",
  "APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED)___OutDirection":                               "Red Zone",
  "APAC_IN_PUN_PODIUM_RED_RECEPTION TO WS ENTRY 1-DOOR NEW___InDirection":             "Red Zone",
  "APAC_IN_PUN_PODIUM_RED_RECEPTION TO WS ENTRY 1-DOOR NEW___OutDirection":            "Reception Area",
  "APAC_IN_PUN_PODIUM_RED_RECREATION AREA FIRE EXIT 1-DOOR NEW___InDirection":         "Red Zone",
  "APAC_IN_PUN_PODIUM_RED_RECREATION AREA FIRE EXIT 1-DOOR NEW___OutDirection":        "East Outdoor Area",

  // Podium / Yellow
  "APAC_IN_PUN_PODIUM_ST2 DOOR 2 (YELLOW)___InDirection":                              "Yellow Zone",
  "APAC_IN_PUN_PODIUM_YELLOW_MDF RESTRICTED DOOR___InDirection":                       "Yellow Zone",
  "APAC_IN_PUN_PODIUM_YELLOW_IT STORE ROOM-DOOR RESTRICTED DOOR___InDirection":        "Yellow Zone",
  "APAC_IN_PUN_PODIUM_YELLOW_REPRO STORE-DOOR RESTRICTED DOOR___InDirection":           "Yellow Zone",
  "APAC_IN_PUN_PODIUM_YELLOW_CONTROL PANEL ROOM-DOOR RESTRICTED DOOR___InDirection":   "Yellow Zone",
  "APAC_IN_PUN_PODIUM_YELLOW_PREACTION ROOM-DOOR RESTRICTED DOOR___InDirection":        "Yellow Zone",
  "APAC_IN_PUN_PODIUM_YELLOW_TESTING LAB-DOOR RESTRICTED DOOR___InDirection":           "Yellow Zone",
  "APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR___InDirection":                       "Yellow Zone",
  "APAC_IN_PUN_PODIUM_YELLOW_RECEPTION ENTRY-DOOR___OutDirection":                      "Reception Area",
  "APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW___InDirection":                   "Yellow Zone",
  "APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW___OutDirection":                  "Out of office",
  "APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)___InDirection":                              "Yellow Zone",
  "APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)___OutDirection":                             "Yellow Zone",
  "APAC_IN_PUN_PODIUM_YELLOW_FIRE EXIT 1-DOOR NEW___InDirection":                       "Yellow Zone",
  "APAC_IN_PUN_PODIUM_YELLOW_FIRE EXIT 1-DOOR NEW___OutDirection":                      "East Outdoor Area",

  // Podium / Green
  "APAC_IN_PUN_PODIUM_GREEN-_IDF ROOM 1-RESTRICTED DOOR___InDirection":                "Green Zone",
  "APAC_IN_PUN_PODIUM_GREEN_UPS ENTRY 1-DOOR RESTRICTED DOOR___InDirection":            "Green Zone",
  "APAC_IN_PUN_PODIUM_GREEN_UPS ENTRY 2-DOOR RESTRICTED DOOR___InDirection":            "Green Zone",
  "APAC_IN_PUN_PODIUM_GREEN_LOCKER HR STORE 3-DOOR RESTRICTED DOOR___InDirection":      "Green Zone",
  "APAC_IN_PUN_PODIUM_ST4 DOOR 2 (GREEN)___InDirection":                                "Green Zone",
   "APAC_IN_PUN_PODIUM_ST4 DOOR 2 (GREEN)___OutDirection":                               "Green Zone",
  "APAC_IN_PUN_PODIUM_GREEN-MAIN LIFT LOBBY-DOOR___InDirection":                        "Green Zone",
  "APAC_IN_PUN_PODIUM_GREEN-MAIN LIFT LOBBY-DOOR___OutDirection":                       "East Outdoor Area",
  "APAC_IN_PUN_PODIUM_ST3 DOOR 2 (GREEN)___InDirection":                                "Green Zone",
  "APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR___InDirection":                        "Green Zone",
  "APAC_IN_PUN_PODIUM_GREEN_RECEPTION ENTRY-DOOR___OutDirection":                       "Reception Area",

  // Podium / Orange
  "APAC_IN_PUN_PODIUM_ST4 DOOR 1 (ORANGE)___InDirection":                               "Orange Zone",
  "APAC_IN_PUN_PODIUM_ST4 DOOR 1 (ORANGE)___OutDirection":                               "Orange Zone",
  "APAC_IN_PUN_PODIUM_ORANGE_RECEPTION ENTRY-DOOR___InDirection":                      "Orange Zone",
  "APAC_IN_PUN_PODIUM_ORANGE_RECEPTION ENTRY-DOOR___OutDirection":                     "Reception Area",
  "APAC_IN_PUN_PODIUM_ST3_DOOR 1 (ORANGE)___InDirection":                              "Orange Zone",

  "APAC_IN_PUN_PODIUM_ORANGE_MAIN LIFT LOBBY-DOOR___InDirection":                       "Orange Zone",
  "APAC_IN_PUN_PODIUM_ORANGE_MAIN LIFT LOBBY-DOOR___OutDirection":                      "Green Zone",
  "APAC_IN_PUN_PODIUM_ORANGE-IDF ROOM 3-RESTRICTED DOOR___InDirection":                "Orange Zone",
  "APAC_IN_PUN_PODIUM_ORANGE_KITCHENETTE FIRE EXIT-DOOR NEW___InDirection":            "Orange Zone",
  "APAC_IN_PUN_PODIUM_ORANGE_KITCHENETTE FIRE EXIT-DOOR NEW___OutDirection":           "West Outdoor Area",

  // Podium / GSOC door
  "APAC_IN_PUN_PODIUM_GSOC DOOR RESTRICTED DOOR___InDirection":                         "Yellow Zone",

  // Podium / Main Right & Left Entry
  "APAC_IN_PUN_PODIUM_MAIN PODIUM RIGHT ENTRY-DOOR NEW___InDirection":                  "Reception Area",
  "APAC_IN_PUN_PODIUM_MAIN PODIUM RIGHT ENTRY-DOOR NEW___OutDirection":                 "Assembly Area",
  "APAC_IN_PUN_PODIUM_MAIN PODIUM LEFT ENTRY-DOOR NEW___InDirection":                   "Reception Area",
  "APAC_IN_PUN_PODIUM_MAIN PODIUM LEFT ENTRY-DOOR NEW___OutDirection":                  "Assembly Area",

  // Podium / Turnstiles
  "APAC_IN_PUN_PODIUM_P-1 TURNSTILE 1-DOOR___InDirection":                               "Reception Area",
  "APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2-DOOR___InDirection":                               "Reception Area",
  "APAC_IN_PUN_PODIUM_P-1 TURNSTILE 3-DOOR___InDirection":                               "Reception Area",
  "APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4-DOOR___InDirection":                               "Reception Area",
  "APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2 -OUT DOOR___OutDirection":                         "Out of office",
  "APAC_IN_PUN-PODIUM_P-1 TURNSTILE 3 -OUT DOOR___OutDirection":                         "Out of office",
  "APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4 -OUT DOOR___OutDirection":                         "Out of office",
  "APAC_IN_PUN_PODIUM_P-1 TURNSTILE 1-OUT DOOR___OutDirection":                          "Out of office",

  // 2nd-Floor / IDF + UPS/ELEC + Reception→Workstation + LiftLobby→Reception
  "APAC_IN_PUN_2NDFLR_IDF ROOM_10:05:86 RESTRICTED DOOR___InDirection":                  "2nd Floor, Pune",
  "APAC_IN_PUN_2NDFLR_UPS/ELEC ROOM RESTRICTED DOOR___InDirection":                      "2nd Floor, Pune",
  "APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR___InDirection":                       "2nd Floor, Pune",
  "APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR___OutDirection":                      "Out of office",
  "APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR___InDirection":                   "2nd Floor, Pune",
  "APAC_IN_PUN_2NDFLR_LIFTLOBBY TO RECEPTION EMTRY DOOR___OutDirection":                  "2nd Floor, Pune",

  // Tower B
  "APAC_IN_PUN_TOWER B_MAIN RECEPTION DOOR___InDirection":                               "Tower B",
  "APAC_IN_PUN_TOWER B_MAIN RECEPTION DOOR___OutDirection":                              "Out of office",
  "APAC_IN_PUN_TOWER B_LIFT LOBBY DOOR___InDirection":                                   "Tower B",
  "APAC_IN_PUN_TOWER B_LIFT LOBBY DOOR___OutDirection":                                  "Out of office",
  "APAC_IN_PUN_TOWER B_ST6_GYM SIDE DOOR___InDirection":                                 "Tower B",
  "APAC_IN_PUN_TOWER B_ST6_GYM SIDE DOOR___OutDirection":                                "Tower B",
  "APAC_IN_PUN_TOWER B_ST6_WKS SIDE DOOR___InDirection":                                 "Tower B",
  "APAC_IN_PUN_TOWER B_ST6_WKS SIDE DOOR___OutDirection":                                "Tower B",
  "APAC_IN_PUN_TOWER B_ST5_KAPIL DEV DOOR___InDirection":                                "Tower B",
  "APAC_IN_PUN_TOWER B_ST5_KAPIL DEV DOOR___OutDirection":                               "Tower B",
  "APAC_IN_PUN_TOWER B_ST5_WKS SIDE DOOR___InDirection":                                 "Tower B",
  "APAC_IN_PUN_TOWER B_ST5_WKS SIDE DOOR___OutDirection":                                "Tower B",
  "APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR___InDirection":                               "Tower B",
  "APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR___OutDirection":                              "Tower B",
  "APAC_IN_PUN_TOWER B_RECEPTION RIGHT DOOR___InDirection":                              "Tower B",
  "APAC_IN_PUN_TOWER B_RECEPTION RIGHT DOOR___OutDirection":                             "Tower B",
  "APAC_IN_PUN_TOWER B_IBMS ROOM___InDirection":                                         "Tower B",
  "APAC_IN_PUN_TOWER B_UPS ROOM___InDirection":                                          "Tower B",
  "APAC_IN_PUN_TOWER B_MDF ROOM___InDirection":                                          "Tower B",
  "APAC_IN_PUN_TOWER B_PAC ROOM___InDirection":                                          "Tower B",
  "APAC_IN_PUN_TOWER B_IT STORE ROOM___InDirection":                                    "Tower B",
  "APAC_IN_PUN_TOWER B_GYM ROOM___InDirection":                                          "Tower B GYM",
  "APAC_IN_PUN_TOWER B_GYM ROOM___OutDirection":                                         "Tower B GYM",
  "APAC_IN_PUN_TOWER B_SITE OPS STORE___InDirection":                    "Tower B",
  "APAC_IN_PUN_TOWER B_INNOVATION CENTER___InDirection" :   "Tower B",
  "APAC_IN_PUN_TOWER B_INNOVATION CENTER___OutDirection":   "Tower B",

   "APAC_IN_PUN_TOWER B_MOBILE LAB___InDirection":    "Tower B",
   "APAC_IN_PUN_TOWER B_MOBILE LAB___OutDirection":    "Tower B",
  "APAC_IN_PUN_TOWER B_CEC DOOR___InDirection":           "Tower B",
  "APAC_IN_PUN_TOWER B_CEC DOOR___OutDirection":          "Tower B",
};

module.exports = doorZoneMap;





Check above door details 
When swipe came InDirection or OutDirection i have mention Zone details ..


our break details is ..
When Employee Swipe InDirection or OutDirection and Zone is
East Outdoor Area
West Oudoor Area 
Assembly Area 

these 3 are our break Area ..
this is easy to identify how many time EMployee Spend time here ....

if break is less than 1 hour its fine 
Suppose Employee take break 3 time and each break  time is greater than 1 hr 
then we need to flag ..

Dont flag only single break ...

remening Zone , are our Working Area...



2) For out of office i have identify door details ...
Whne for Same day Employee came office and saty in working Area for x -time again he went out of office then 
after certain time he came office again and stay office less time then Flag this EMployee..


3)Find out Swipe gap for EMployee .
Only IN swipe for day ..
Only Out for day..
Single door swipe...



Referv below Query for 

Swipe Count 

WITH AllSwipes AS (
    SELECT
        t1.ObjectName1 AS EmployeeName,
        CASE
            WHEN t2.Int1 = 0 THEN t2.Text12
            ELSE CAST(t2.Int1 AS NVARCHAR)
        END AS EmployeeID,
        DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS LocaleMessageTime,
        x.Value AS Direction,
        t1.ObjectName2 AS DoorName,
        CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS DATE) AS SwipeDate
    FROM
        [ACVSUJournal_00010030].[dbo].[ACVSUJournalLog] AS t1
    INNER JOIN [ACVSCore].[Access].[Personnel] AS t2
        ON t1.ObjectIdentity1 = t2.GUID
    INNER JOIN [ACVSCore].[Access].[PersonnelType] AS t3
        ON t2.PersonnelTypeID = t3.ObjectID
    INNER JOIN [ACVSUJournal_00010030].[dbo].[ACVSUJournalLogxmlShred] AS x
        ON t1.XMLGuid = x.GUID
    WHERE
 t1.MessageType = 'CardAdmitted'
     AND t2.Text5 IN ('Pune','Pune - Business Bay')
        AND t1.PartitionName2 = 'APAC.Default'
        AND t3.Name = 'Employee'
        AND x.Value IN ('InDirection', 'OutDirection')
        AND CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS DATE) BETWEEN '2025-10-01' AND '2025-10-09'
),

SwipeCounts AS (
    SELECT
        EmployeeID,
        EmployeeName,
        SwipeDate,
        COUNT(*) AS SwipeCount
    FROM
        AllSwipes
    GROUP BY
        EmployeeID, EmployeeName, SwipeDate
)

-- Final Output
SELECT
    EmployeeID,
    EmployeeName,
    SwipeDate,
    SwipeCount,
 COUNT(*) OVER (PARTITION BY EmployeeID) AS RepeatCount

FROM
    SwipeCounts
WHERE
    SwipeCount between 2 and 5
ORDER BY
    RepeatCount DESC, EmployeeName;





refer QUery for Swipe Gap + Duration..


WITH AllSwipes AS (
    SELECT
        t1.ObjectName1 AS EmployeeName,
        CASE
            WHEN t2.Int1 = 0 THEN t2.Text12
            ELSE CAST(t2.Int1 AS NVARCHAR)
        END AS EmployeeID,
        DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS LocaleMessageTime,
        x.Value AS Direction,
        CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS DATE) AS SwipeDate
    FROM
        [ACVSUJournal_00010030].[dbo].[ACVSUJournalLog] AS t1
    INNER JOIN [ACVSCore].[Access].[Personnel] AS t2
        ON t1.ObjectIdentity1 = t2.GUID
    INNER JOIN [ACVSCore].[Access].[PersonnelType] AS t3
        ON t2.PersonnelTypeID = t3.ObjectID
    INNER JOIN [ACVSUJournal_00010030].[dbo].[ACVSUJournalLogxmlShred] AS x
        ON t1.XMLGuid = x.GUID
    WHERE
        t1.MessageType = 'CardAdmitted'
        AND t2.Text5 IN ('Pune','Pune - Business Bay')
        AND t1.PartitionName2 = 'APAC.Default'
        AND t3.Name = 'Employee'
        AND x.Value IN ('InDirection', 'OutDirection')
        AND CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS DATE) BETWEEN '2025-10-27' AND '2025-10-27'
),

SwipeGap AS (
    SELECT
        EmployeeID,
        EmployeeName,
        SwipeDate,
        LocaleMessageTime AS CurrentSwipe,
        LAG(LocaleMessageTime) OVER (PARTITION BY EmployeeID, SwipeDate ORDER BY LocaleMessageTime) AS PrevSwipe,
        DATEDIFF(HOUR, LAG(LocaleMessageTime) OVER (PARTITION BY EmployeeID, SwipeDate ORDER BY LocaleMessageTime), LocaleMessageTime) AS GapHours
    FROM AllSwipes
),

GapReport AS (
    SELECT
        EmployeeID,
        EmployeeName,
        SwipeDate,
        PrevSwipe,
        CurrentSwipe,
        GapHours
    FROM SwipeGap
    WHERE GapHours > 6
),

DayDuration AS (
    SELECT
        EmployeeID,
        SwipeDate,
        MIN(LocaleMessageTime) AS FirstSwipe,
        MAX(LocaleMessageTime) AS LastSwipe,
        -- Duration in HH:MM format
        RIGHT('0' + CAST(DATEDIFF(MINUTE, MIN(LocaleMessageTime), MAX(LocaleMessageTime)) / 60 AS VARCHAR), 2)
        + ':' +
        RIGHT('0' + CAST(DATEDIFF(MINUTE, MIN(LocaleMessageTime), MAX(LocaleMessageTime)) % 60 AS VARCHAR), 2) AS DurationHHMM
    FROM AllSwipes
    GROUP BY EmployeeID, SwipeDate
)

SELECT
    g.EmployeeID,
    g.EmployeeName,
    g.SwipeDate,
    g.PrevSwipe,
    g.CurrentSwipe,
    g.GapHours,
    COUNT(*) OVER (PARTITION BY g.EmployeeID) AS RepeatCount,
    d.DurationHHMM
FROM GapReport g
INNER JOIN DayDuration d
    ON g.EmployeeID = d.EmployeeID AND g.SwipeDate = d.SwipeDate
ORDER BY RepeatCount DESC, g.EmployeeName, g.SwipeDate;









also if system flag any Employee need to Explain details Description...


Why EMployee Flagg....for this SCenarion train model deeply make Anomely for each EMployee and and Explain me step by step 
Check below file carefully and line by line and share me fully updated file so i can easily swap file each other......






# ----------------- START OF FILE: app.py -----------------
from flask import Flask, jsonify, request, send_from_directory
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re

from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)


def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        # convert floats NaN handled above
        return x
    try:
        # fallback to string
        return str(x)
    except Exception:
        return None


_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False


# Placeholder tokens (keep consistent with trend_runner expectations)
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


def _resolve_field_from_record(record: dict, candidate_tokens: list):
    """
    Search a single row `record` (dict) for likely columns listed in candidate_tokens.
    Return first non-placeholder value found (converted to Python scalar), else None.

    This helper is used to prefer the schema columns you specified:
      - EmployeeID: Int1 (ACVSCore.Access.Personnel.Int1)
      - CardNumber: CardNumber (ACVSCore.Access.Credential.CardNumber)
    but it will also look for common alternates (Text12, CHUID, Card, etc.).
    """
    if record is None:
        return None

    # 1) exact key matches (case-sensitive & common casing)
    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)

    # 2) case-insensitive contains match
    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)

    # 3) xml / value parsing fallback for CardNumber
    # if candidate_tokens refers to Card-like tokens, try parse xml-like fields
    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue

    # 4) final fallback: first non-placeholder value (avoid GUID-like values only if caller expects human id)
    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)

    return None


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Clean a dataframe for JSON output (convert NaN -> None, pandas types -> native, format datetimes).
    Also ensure EmployeeName fallback uses EmployeeID first and avoid showing GUIDs as names.

    Enhanced to use schema hints:
      - Prefer Int1 for EmployeeID (Personnel.Int1).
      - Prefer CardNumber (Credential.CardNumber) for CardNumber.
      - Explicitly reject GUID-like values for EmployeeID/CardNumber.
    """
    if df is None or df.empty:
        return []
    df = df.copy()

    # remove duplicate suffix columns
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    # Date normalization
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    # Datetime columns to ISO strings
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # Replace NaN/inf -> None
    df = df.where(pd.notnull(df), None)

    # Convert records to safe Python types
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        # Typical fields
        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        # ----- Schema-aware fallback resolution -----
        # Prefer Personnel.Int1 as EmployeeID when missing (do NOT use EmployeeIdentity/person_uid)
        if not emp_id:
            emp_tokens = ['Int1', 'Text12', 'EmployeeID', 'empid', 'id']
            resolved_emp = _resolve_field_from_record(r, emp_tokens)
            if resolved_emp is not None:
                try:
                    s = str(resolved_emp).strip()
                    # remove trailing .0 for floats
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    # If the resolved value looks like a GUID, do NOT use it as EmployeeID
                    if _looks_like_guid(s):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = s
                        emp_id = s
                except Exception:
                    if _looks_like_guid(resolved_emp):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = resolved_emp
                        emp_id = resolved_emp

        # Prefer Credential.CardNumber / CHUID / Card as CardNumber when missing — reject GUIDs/placeholders
        if out.get('CardNumber') in (None, '', 'nan'):
            card_tokens = ['CardNumber', 'CHUID', 'Card', 'card_no', 'cardnum']
            resolved_card = _resolve_field_from_record(r, card_tokens)
            if resolved_card is not None:
                try:
                    cs = str(resolved_card).strip()
                    if _looks_like_guid(cs) or _is_placeholder_str(cs):
                        out['CardNumber'] = None
                    else:
                        out['CardNumber'] = cs
                except Exception:
                    out['CardNumber'] = None

        # final safety: ensure EmployeeID/CardNumber are not GUID-like tokens
        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None
        if 'CardNumber' in out and isinstance(out['CardNumber'], str) and _looks_like_guid(out['CardNumber']):
            out['CardNumber'] = None

        # If EmployeeName empty or looks like a GUID, prefer EmployeeID (human id) over GUIDs
        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            if emp_id not in (None, '', 'nan') and not _looks_like_guid(emp_id):
                out['EmployeeName'] = str(emp_id)
            else:
                out['EmployeeName'] = None

        cleaned.append(out)
    return cleaned


@app.route('/')
def root():
    return "Trend Analysis API — Pune test"


@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.json or {}

    date_str = params.get('date')
    start_str = params.get('start')
    end_str = params.get('end')

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            dates = [datetime.now().date()]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    combined_rows = []
    files = []
    samples = []

    for d in dates:
        try:
            df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        if df is None or df.empty:
            continue

        # Ensure IsFlagged exists; Reasons only when flagged
        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        # Prefer sample of flagged rows (makes QA easier); else generic sample
        flagged = df[df['IsFlagged'] == True]
        sample_df = flagged.head(10) if not flagged.empty else df.head(10)
        samples.extend(_clean_sample_df(sample_df, max_rows=10))

        combined_rows.append(df)

    combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()

    # Determine best identifier column to count unique persons (priority)
    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in combined_df.columns), None)

    def _norm_id_val(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        # convert floats like "320172.0" -> "320172"
        try:
            if '.' in s:
                f = float(s)
                if math.isfinite(f) and f.is_integer():
                    s = str(int(f))
        except Exception:
            pass
        return s

    if id_col is None or combined_df.empty:
        analysis_count = int(len(combined_df))
        flagged_count = int(combined_df['IsFlagged'].sum()) if 'IsFlagged' in combined_df.columns else 0
    else:
        # if Int1 exists in combined_df, prefer it when normalizing ids
        ids_series = combined_df[id_col].apply(_norm_id_val)
        unique_ids = set([x for x in ids_series.unique() if x])
        analysis_count = int(len(unique_ids))

        # flagged unique persons (using IsFlagged)
        if 'IsFlagged' in combined_df.columns:
            flagged_series = combined_df.loc[combined_df['IsFlagged'] == True, id_col].apply(_norm_id_val)
            flagged_unique = set([x for x in flagged_series.unique() if x])
            flagged_count = int(len(flagged_unique))
        else:
            flagged_count = 0

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "files": files,
        # legacy totals
        "aggregated_rows_total": int(len(combined_df)),
        # new semantics (distinct persons)
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "sample": samples[:20]
    }
    return jsonify(resp)


@app.route('/latest', methods=['GET'])
def latest_results():
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    sample = _clean_sample_df(df, max_rows=5)
    return jsonify({
        "file": latest.name,
        "rows": int(len(df)),
        "sample": sample
    })


@app.route('/record', methods=['GET'])
def get_record():
    """
    /record?employee_id=... or /record?person_uid=...
    Returns matching aggregated trend rows and filtered raw swipe rows (only for flagged persons).
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200
    try:
        latest = csvs[0]
        try:
            df = pd.read_csv(latest, parse_dates=['FirstSwipe','LastSwipe'], infer_datetime_format=True)
        except Exception:
            df = pd.read_csv(latest, dtype=str)
    except Exception:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    if q is None:
        cleaned = _clean_sample_df(df, max_rows=10)
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    q_str = str(q).strip()

    def normalize_series(s):
        if s is None:
            return pd.Series([None]*len(df))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if math.isfinite(fv) and fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    found_mask = pd.Series(False, index=df.index)

    if 'EmployeeID' in df.columns:
        emp_series = normalize_series(df['EmployeeID'])
        found_mask = found_mask | (emp_series == q_str)

    if 'person_uid' in df.columns:
        uid_series = normalize_series(df['person_uid'])
        found_mask = found_mask | (uid_series == q_str)

    # also check Int1 (Personnel.Int1) if present in CSV
    if 'Int1' in df.columns and not found_mask.any():
        int1_series = normalize_series(df['Int1'])
        found_mask = found_mask | (int1_series == q_str)

    if not found_mask.any():
        # try numeric equivalence
        try:
            q_numeric = float(q_str)
            if 'EmployeeID' in df.columns:
                emp_numeric = pd.to_numeric(df['EmployeeID'], errors='coerce')
                found_mask = found_mask | (emp_numeric == q_numeric)
            if 'Int1' in df.columns and not found_mask.any():
                int_numeric = pd.to_numeric(df['Int1'], errors='coerce')
                found_mask = found_mask | (int_numeric == q_numeric)
        except Exception:
            pass

    matched = df[found_mask]
    if matched.empty:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    cleaned_matched = _clean_sample_df(matched, max_rows=len(matched))

    # Resolve raw swipe file names by Date
    raw_files = set()
    date_vals = set()
    if 'Date' in matched.columns:
        try:
            dates_parsed = pd.to_datetime(matched['Date'], errors='coerce').dropna().dt.date.unique()
            for d in dates_parsed:
                date_vals.add(str(d.isoformat()))
        except Exception:
            pass

    if not date_vals:
        for col in ('FirstSwipe', 'LastSwipe'):
            if col in matched.columns:
                try:
                    vals = pd.to_datetime(matched[col], errors='coerce').dropna().dt.date.unique()
                    for d in vals:
                        date_vals.add(str(d.isoformat()))
                except Exception:
                    pass

    # build filtered raw swipe rows ONLY for flagged persons
    raw_swipes_out = []
    # pick first matched row to decide identifiers to filter raw swipes
    for idx, agg_row in matched.iterrows():
        # gather the identifying values
        person_uid = agg_row.get('person_uid') if 'person_uid' in agg_row else None
        empid = agg_row.get('EmployeeID') if 'EmployeeID' in agg_row else None
        # also check Int1 in aggregated row if present
        if (not empid) and 'Int1' in agg_row:
            empid = agg_row.get('Int1')
        card = agg_row.get('CardNumber') if 'CardNumber' in agg_row else None

        # determine date string(s) to look for raw files using Date / FirstSwipe / LastSwipe
        dates_for_row = set()
        if 'Date' in agg_row and pd.notna(agg_row['Date']):
            try:
                d = pd.to_datetime(agg_row['Date']).date()
                dates_for_row.add(d.isoformat())
            except Exception:
                pass
        for col in ('FirstSwipe','LastSwipe'):
            if col in agg_row and pd.notna(agg_row[col]):
                try:
                    d = pd.to_datetime(agg_row[col]).date()
                    dates_for_row.add(d.isoformat())
                except Exception:
                    pass

        # only fetch raw swipe evidence when this aggregated row is flagged
        is_flagged = bool(agg_row.get('IsFlagged', False))
        if not is_flagged:
            continue

        for d in dates_for_row:
            try:
                dd = d[:10]
                raw_name = f"swipes_pune_{dd.replace('-','')}.csv"
                fp = DEFAULT_OUTDIR / raw_name
                if not fp.exists():
                    continue
                raw_files.add(raw_name)
                # read the raw swipe CSV (try to parse datetimes if possible)
                try:
                    raw_df = pd.read_csv(fp, parse_dates=['LocaleMessageTime'], infer_datetime_format=True)
                except Exception:
                    try:
                        raw_df = pd.read_csv(fp, dtype=str)
                    except Exception:
                        continue

                # normalize columns names to lowercase for searching
                cols_lower = {c.lower(): c for c in raw_df.columns}

                # candidate column names (schema-aware)
                tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
                # prefer Int1 / EmployeeID / EmployeeIdentity
                emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
                # prefer CardNumber (Credential.CardNumber) - include 'value' fallback
                card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
                door_col = cols_lower.get('door') or cols_lower.get('doorname') or None
                dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or None
                note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None

                # build filter mask: match person_uid OR empid OR card (whichever present)
                mask = pd.Series(False, index=raw_df.index)
                if person_uid is not None and 'person_uid' in raw_df.columns:
                    mask = mask | (raw_df['person_uid'].astype(str).str.strip() == str(person_uid).strip())
                if emp_col:
                    if empid is not None:
                        # compare after stripping (normalize numeric float .0)
                        try:
                            cmp_val = str(empid).strip()
                            # if empid looks numeric with .0 convert
                            if '.' in cmp_val:
                                fv = float(cmp_val)
                                if math.isfinite(fv) and fv.is_integer():
                                    cmp_val = str(int(fv))
                        except Exception:
                            cmp_val = str(empid).strip()
                        mask = mask | (raw_df[emp_col].astype(str).str.strip() == cmp_val)
                if card_col and card is not None:
                    mask = mask | (raw_df[card_col].astype(str).str.strip() == str(card).strip())

                # Also attempt to match by EmployeeName if agg has name and no other match
                if (not mask.any()) and name_col and 'EmployeeName' in agg_row and pd.notna(agg_row.get('EmployeeName')):
                    mask = mask | (raw_df[name_col].astype(str).str.strip() == str(agg_row.get('EmployeeName')).strip())

                # Also filter by date: ensure the timestamp's date equals dd
                if tcol and tcol in raw_df.columns:
                    try:
                        raw_df[tcol] = pd.to_datetime(raw_df[tcol], errors='coerce')
                        mask = mask & (raw_df[tcol].dt.date == pd.to_datetime(dd).date())
                    except Exception:
                        pass

                filtered = raw_df[mask]
                if filtered.empty:
                    # additional attempt: try matching where card is embedded inside xml/value fields
                    # e.g., if aggregated card exists but not matched by direct column, search xml columns
                    if card is not None:
                        for c in raw_df.columns:
                            cl = c.lower()
                            if 'xml' in cl or 'msg' in cl or 'value' == cl:
                                try:
                                    # extract card from each cell and compare
                                    vals = raw_df[c].dropna().astype(str)
                                    match_mask = vals.apply(lambda x: (_extract_card_from_xml_text(x) == str(card).strip()))
                                    if match_mask.any():
                                        filtered = raw_df.loc[match_mask.index[match_mask]]
                                        break
                                except Exception:
                                    continue
                    if filtered.empty:
                        continue

                # Convert filtered rows into the standardized output structure requested by frontend
                for _, r in filtered.iterrows():
                    out = {}
                    # EmployeeName
                    if name_col and name_col in raw_df.columns:
                        out['EmployeeName'] = _to_python_scalar(r.get(name_col))
                    else:
                        out['EmployeeName'] = _to_python_scalar(agg_row.get('EmployeeName') or agg_row.get('person_uid'))

                    # EmployeeID: prefer Int1 in raw file, then EmployeeID, then fallback to aggregated value
                    emp_val = None
                    if 'int1' in cols_lower and cols_lower.get('int1') in raw_df.columns:
                        emp_val = _to_python_scalar(r.get(cols_lower.get('int1')))
                    elif emp_col and emp_col in raw_df.columns:
                        emp_val = _to_python_scalar(r.get(emp_col))
                    else:
                        # fallback: try other likely tokens in the raw row
                        possible_emp = None
                        for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                            if cand.lower() in cols_lower:
                                possible_emp = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                                if possible_emp not in (None, '', 'nan'):
                                    break
                        emp_val = possible_emp if possible_emp not in (None, '', 'nan') else _to_python_scalar(agg_row.get('EmployeeID'))

                    # Normalize employee value and reject GUIDs
                    if emp_val is not None:
                        try:
                            s = str(emp_val).strip()
                            if '.' in s:
                                f = float(s)
                                if math.isfinite(f) and f.is_integer():
                                    s = str(int(f))
                            if _looks_like_guid(s) or _is_placeholder_str(s):
                                emp_val = None
                            else:
                                emp_val = s
                        except Exception:
                            if _looks_like_guid(emp_val):
                                emp_val = None
                    out['EmployeeID'] = emp_val

                    # CardNumber: prefer CardNumber in raw file, then CHUID/Card, value, then aggregated value (and xml extraction)
                    card_val = None
                    if 'cardnumber' in cols_lower and cols_lower.get('cardnumber') in raw_df.columns:
                        card_val = _to_python_scalar(r.get(cols_lower.get('cardnumber')))
                    elif card_col and card_col in raw_df.columns:
                        card_val = _to_python_scalar(r.get(card_col))
                    else:
                        # fallback: value or xml extraction
                        possible_card = None
                        for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                            if cand.lower() in cols_lower:
                                possible_card = _to_python_scalar(r.get(cols_lower[cand.lower()]))
                                if possible_card not in (None, '', 'nan'):
                                    break
                        # if possible_card still None or looks like xml, try extracting
                        if possible_card in (None, '', 'nan'):
                            for c in raw_df.columns:
                                cl = c.lower()
                                if 'xml' in cl or 'msg' in cl or 'value' == cl:
                                    try:
                                        txt = r.get(c)
                                        extracted = _extract_card_from_xml_text(str(txt)) if txt is not None else None
                                        if extracted:
                                            possible_card = extracted
                                            break
                                    except Exception:
                                        continue
                        card_val = possible_card if possible_card not in (None, '', 'nan') else _to_python_scalar(agg_row.get('CardNumber'))

                    # Normalize card and reject GUIDs/placeholders
                    if card_val is not None:
                        try:
                            cs = str(card_val).strip()
                            if _looks_like_guid(cs) or _is_placeholder_str(cs):
                                card_val = None
                            else:
                                card_val = cs
                        except Exception:
                            card_val = None
                    out['CardNumber'] = card_val

                    # Date and Time
                    if tcol and tcol in raw_df.columns:
                        ts = r.get(tcol)
                        try:
                            ts_py = pd.to_datetime(ts)
                            out['Date'] = ts_py.date().isoformat()
                            out['Time'] = ts_py.time().isoformat()
                        except Exception:
                            txt = str(r.get(tcol))
                            out['Date'] = txt[:10]
                            out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                    else:
                        out['Date'] = d
                        out['Time'] = None

                    # Door
                    if door_col and door_col in raw_df.columns:
                        out['Door'] = _to_python_scalar(r.get(door_col))
                    else:
                        out['Door'] = None

                    # Direction
                    if dir_col and dir_col in raw_df.columns:
                        out['Direction'] = _to_python_scalar(r.get(dir_col))
                    else:
                        out['Direction'] = _to_python_scalar(r.get('Direction')) if 'Direction' in r else None

                    # Note (rejection / source)
                    if note_col and note_col in raw_df.columns:
                        out['Note'] = _to_python_scalar(r.get(note_col))
                    else:
                        out['Note'] = None

                    out['_source'] = raw_name
                    raw_swipes_out.append(out)
            except Exception as e:
                logging.exception("Error processing raw swipe file for date %s: %s", d, e)
                continue

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": sorted(list(raw_files)),
        "raw_swipes": raw_swipes_out
    }), 200


@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    """
    Serve raw swipe CSVs from outputs/ (filename should be the file name only).
    """
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    # send file
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)


@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)
# ----------------- END OF FILE: app.py -----------------















# backend/duration_report.py
from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name (override with environment variable if needed)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur", "IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023"
        ],
        "partitions": ["LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["Denver", "Austin Texas", "Miami", "New York"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}



GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    -- prefer contractor text12 else Int1 (match original logic)
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    -- expose raw XML & shredded value so downstream code can attempt robust extraction
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    -- CardNumber: try CHUID/Card inside XmlMessage, fallback to shredded value, then text12/int1
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
"""
# Helpers
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql
    
# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    # use first database in list if present
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
    "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
    "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "MessageType",
    "Direction", "CompanyName", "PrimaryLocation"
]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    for c in cols:
        if c not in df.columns:
            df[c] = None

    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # maintain person_uid same as compute logic
    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]

# compute durations (unchanged largely)
def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    if df["LocaleMessageTime"].dtype == object:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    df = df.drop_duplicates(subset=dedupe_cols, keep="first")

    df["Date"] = df["LocaleMessageTime"].dt.date

    df["person_uid"] = df.apply(
        lambda row: row["person_uid"]
        if pd.notna(row.get("person_uid")) and str(row.get("person_uid")).strip() != ""
        else "|".join([str(row.get(c)).strip() for c in ["EmployeeID", "CardNumber", "EmployeeName"] if pd.notna(row.get(c)) and str(row.get(c)).strip() != ""]),
        axis=1
    )
    df = df[df["person_uid"].notna()].copy()

    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", "first"),
            EmployeeName=("EmployeeName", "first"),
            CardNumber=("CardNumber", "first"),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": first.get("EmployeeID"),
                "EmployeeName": first.get("EmployeeName"),
                "CardNumber": first.get("CardNumber"),
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(
        lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    )

    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    return grouped[out_cols]

# runner helper: run_for_date
def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        try:
            swipes = fetch_swipes_for_region(r, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", r)
            swipes = pd.DataFrame()

        # optional city filter
        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                swipes = swipes[combined_mask].copy()

        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", r)
            durations = pd.DataFrame()

        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
        try:
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", r)
        try:
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", r)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
        results[r] = {"swipes": swipes, "durations": durations}

    return results










# backend/logic.py
import pandas as pd
import numpy as np
import logging
from pathlib import Path

PROFILE_PATH = Path(__file__).parent / "current_analysis.csv"
if PROFILE_PATH.exists():
    try:
        employee_profile = pd.read_csv(PROFILE_PATH)
        logging.info("logic.py: loaded historical profile (%d rows)", len(employee_profile))
    except Exception as e:
        logging.warning("logic.py: failed to read current_analysis.csv: %s", e)
        employee_profile = pd.DataFrame()
else:
    logging.warning("logic.py: current_analysis.csv not found; history-based checks will be limited.")
    employee_profile = pd.DataFrame()


def _safe_get_emp_hist(employee_id):
    if employee_profile.empty or employee_id is None or (isinstance(employee_id, float) and np.isnan(employee_id)):
        return pd.DataFrame()
    # normalize employee_id comparison as string/int
    try:
        empid_str = str(employee_id).strip()
    except Exception:
        empid_str = employee_id
    if 'EmployeeID' in employee_profile.columns:
        # try numeric match first
        try:
            # if both numeric/int equal
            emp_numeric = None
            try:
                emp_numeric = float(empid_str)
            except Exception:
                emp_numeric = None
            if emp_numeric is not None and not np.isnan(emp_numeric):
                # try numeric comparison when employee_profile EmployeeID numeric-like
                try:
                    profile_nums = pd.to_numeric(employee_profile['EmployeeID'], errors='coerce')
                    mask = (profile_nums == emp_numeric)
                    res = employee_profile[mask]
                    if not res.empty:
                        return res
                except Exception:
                    pass
        except Exception:
            pass
        # fallback to string match
        try:
            return employee_profile[employee_profile['EmployeeID'].astype(str).str.strip() == empid_str]
        except Exception:
            return pd.DataFrame()
    return pd.DataFrame()


def flag_employee(row_dict):
    """
    Return (is_flagged: bool, reasons: list[str])
    This is an extra per-employee heuristic (uses historical profile if available).
    It will return (False, []) when there is no historical data; do not return stray reasons.
    """
    employee_id = row_dict.get('EmployeeID')
    personnel_type = row_dict.get('PersonnelType') or row_dict.get('PersonnelTypeName')
    try:
        days_present = int(row_dict.get('DaysPresentInWeek') or 0)
    except Exception:
        days_present = 0

    logging.info("flag_employee: checking EmployeeID=%s", employee_id)

    emp_hist = _safe_get_emp_hist(employee_id)
    if emp_hist.empty:
        # do not fabricate a reason when history missing — caller can still combine other signals
        return False, []

    reasons = []

    # example checks
    try:
        if row_dict.get('InTime') and row_dict.get('OutTime'):
            reasons.append("Has both InTime and OutTime (possible short cycles)")
    except Exception:
        pass

    if int(row_dict.get("OnlyIn", 0) or 0) == 1:
        reasons.append("OnlyIn entry detected")
    if int(row_dict.get("OnlyOut", 0) or 0) == 1:
        reasons.append("OnlyOut entry detected")
    if int(row_dict.get("SingleDoor", 0) or 0) == 1:
        reasons.append("SingleDoor usage detected")

    if personnel_type and str(personnel_type).strip().lower() == "employee":
        is_defaulter = row_dict.get("Defaulter", "No")
        if str(is_defaulter).strip().lower() == "yes":
            reasons.append("Flagged as Defaulter by company policy")

    # historical median/std checks for numeric metrics
    metric_column_map = {
        'DurationMinutes': ('AvgDurationMins_median', 'AvgDurationMins_std'),
        'TotalSwipes': ('TotalSwipes_median', 'TotalSwipes_std')
    }

    for metric, (median_col, std_col) in metric_column_map.items():
        try:
            live_val = row_dict.get(metric)
            if live_val is None or (isinstance(live_val, float) and np.isnan(live_val)):
                # don't automatically add reason; only when metric is required
                continue
            if median_col in emp_hist.columns and std_col in emp_hist.columns:
                median_val = emp_hist.iloc[0].get(median_col, np.nan)
                std_val = emp_hist.iloc[0].get(std_col, np.nan)
                if pd.notna(median_val) and pd.notna(std_val) and std_val >= 0:
                    try:
                        median_val = float(median_val)
                        std_val = float(std_val)
                        buffer = 2.5 * std_val
                        if float(live_val) < median_val - buffer or float(live_val) > median_val + buffer:
                            reasons.append(f"Abnormal {metric}: {live_val} outside expected [{median_val-buffer:.1f}, {median_val+buffer:.1f}]")
                    except Exception:
                        pass
        except Exception as e:
            logging.debug("flag_employee: error evaluating metric %s: %s", metric, e)

    try:
        duration = float(row_dict.get('DurationMinutes') or 0)
        if days_present < 3 and duration < 480:
            reasons.append("Duration < 8 hours on limited office days")
    except Exception:
        pass

    return (len(reasons) > 0), reasons










# ml_training.py
"""
Train one binary classifier per scenario using the training CSV produced by trend_runner.build_monthly_training.
Usage:
    python ml_training.py --input outputs/training_person_month.csv --models_dir models/
Outputs:
    models/<scenario>.joblib
Requirements:
    scikit-learn, joblib, pandas, numpy
"""
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)

try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report
    import joblib
except Exception as e:
    logging.error("Required ML packages missing: %s", e)
    logging.error("Install scikit-learn and joblib: pip install scikit-learn joblib")
    raise

DEFAULT_FEATURE_COLS = [
    'CountSwipes_median', 'CountSwipes_mean', 'CountSwipes_sum',
    'DurationMinutes_median', 'DurationMinutes_mean', 'DurationMinutes_sum',
    'MaxSwipeGapSeconds_max', 'MaxSwipeGapSeconds_median',
    'ShortGapCount_sum', 'UniqueDoors_median', 'UniqueLocations_median', 'RejectionCount_sum',
    'days_present'
]

def auto_detect_scenarios(df):
    scenario_labels = [c for c in df.columns if c.endswith('_label')]
    scenarios = [c[:-6] for c in scenario_labels]
    return scenarios

def prepare_features(df, features=None):
    if features is None:
        features = DEFAULT_FEATURE_COLS
    # ensure columns exist, fill missing with 0/median
    X = df.copy()
    for f in features:
        if f not in X.columns:
            X[f] = 0.0
    X = X[features].fillna(0.0)
    return X

def train_one(df, scenario, features):
    label_col = f"{scenario}_label"
    if label_col not in df.columns:
        logging.warning("Label %s not in dataframe, skipping", label_col)
        return None
    y = df[label_col].astype(int)
    if y.sum() == 0:
        logging.warning("No positive examples for %s; skipping model training", scenario)
        return None
    X = prepare_features(df, features)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    logging.info("Classification report for %s:\n%s", scenario, classification_report(y_test, y_pred, zero_division=0))
    return clf

def main(input_csv: Path, models_dir: Path, feature_cols=None):
    if not input_csv.exists():
        raise FileNotFoundError(f"input CSV not found: {input_csv}")
    df = pd.read_csv(input_csv)
    scenarios = auto_detect_scenarios(df)
    if not scenarios:
        logging.error("No scenarios ( *_label ) columns found in %s", input_csv)
        return
    models_dir.mkdir(parents=True, exist_ok=True)
    for s in scenarios:
        logging.info("Training for scenario: %s", s)
        clf = train_one(df, s, feature_cols)
        if clf is not None:
            outp = models_dir / f"{s}.joblib"
            joblib.dump({"model": clf, "features": (feature_cols or DEFAULT_FEATURE_COLS)}, outp)
            logging.info("Saved model to %s", outp)
        else:
            logging.info("Skipped training for %s", s)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="training CSV (person-month) created by /train endpoint")
    parser.add_argument("--models_dir", default="models", help="folder to save models")
    parser.add_argument("--features", default=None, help="comma separated feature columns (optional)")
    args = parser.parse_args()

    input_csv = Path(args.input)
    models_dir = Path(args.models_dir)
    feature_cols = args.features.split(",") if args.features else None

    main(input_csv, models_dir, feature_cols)









# ----------------- START OF FILE: trend_runner.py -----------------
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re

# IMPORTANT: duration_report must exist and expose run_for_date(date, regions, outdir, city)
from duration_report import run_for_date

# historical profile (optional)
HIST_PATH = Path(__file__).parent / "current_analysis.csv"
if HIST_PATH.exists():
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()
else:
    logging.warning("Historical profile file current_analysis.csv not found; history-based scenarios will fallback.")
    HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)


# ----- small shared helpers: treat empty/placeholder tokens as None -----
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


def _normalize_id_val(v):
    """
    Normalize an id-like value: strip, convert floats like '320172.0' -> '320172'
    Return None for NaN/empty/placeholder.
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    # strip .0 if integer-like
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s


# prefer to avoid emp:<GUID> person_uids — only treat emp: if value looks like a human id (not GUID)
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')

def _looks_like_guid(s: object) -> bool:
    """Return True if s looks like a GUID/UUID string."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    """Heuristic: treat as a plausible human name if it contains letters and not a GUID."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        # reject GUIDs and obviously numeric ids
        if _looks_like_guid(st):
            return False
        # require at least one alphabetic character
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    """Pick the first non-null, non-GUID, non-placeholder value from a pandas Series (as str) or None."""
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    """
    Create canonical person uid:
      - prefer EmployeeID (normalized) -> 'emp:<id>' only if it is not a GUID
      - else EmployeeIdentity -> 'uid:<val>' (GUID allowed)
      - else EmployeeName -> hash-based 'name:<shorthash>'
    """
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        # stable short hash of name
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None


# small helper to extract Card from XML-like strings
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        # fallback: look for CHUID ... Card: pattern or Card: 12345
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


# --- compute_features (replaced/updated) ---
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    Returns DataFrame per (person_uid, Date) with feature columns and normalized IDs/names.
    This implementation follows the duration_report column conventions and avoids
    treating GUIDs or placeholder tokens as EmployeeName/EmployeeID/CardNumber.
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # Build lowercase->actual column map for flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}

    # detect time column
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)
    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
        sw['Date'] = sw['LocaleMessageTime'].dt.date
    else:
        if 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
        else:
            sw['Date'] = None

    # find these earlier in compute_features — prefer Int1/Text12 for EmployeeID and CHUID/Card for CardNumber
    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    # Filter personnel types: prefer PersonnelTypeName, fallback to PersonnelType
    if 'PersonnelTypeName' in sw.columns:
        sw = sw[sw['PersonnelTypeName'].isin(['Employee', 'Terminated Personnel'])]
    elif 'PersonnelType' in sw.columns:
        sw = sw[sw['PersonnelType'].isin(['Employee', 'Terminated Personnel'])]
    # else keep everything

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # ensure stable person_uid (canonical)
    if 'person_uid' not in sw.columns:
        def make_person_uid_local(r):
            # prefer canonical EmployeeID (normalized, non-GUID) then EmployeeIdentity then EmployeeName
            empid_val = None
            if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
                empid_val = r.get(empid_col)
            elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
                empid_val = r.get('EmployeeID')

            empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
            name_val = None
            if name_col and name_col in r:
                name_val = r.get(name_col)
            elif 'EmployeeName' in r:
                name_val = r.get('EmployeeName')
            elif 'ObjectName1' in r:
                name_val = r.get('ObjectName1')

            return _canonical_person_uid({
                'EmployeeID': empid_val,
                'EmployeeIdentity': empident_val,
                'EmployeeName': name_val
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    # selection columns for aggregation: include discovered columns
    sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
                    'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
                    'EmployeeIdentity'])
    if name_col:
        sel_cols.add(name_col)
    if empid_col:
        sel_cols.add(empid_col)
    if card_col:
        sel_cols.add(card_col)
    if door_col:
        sel_cols.add(door_col)
    if dir_col:
        sel_cols.add(dir_col)
    sel_cols = [c for c in sel_cols if c in sw.columns]

    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0

        # Direction counts (default to column names present)
        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

        # pick first non-placeholder, non-guid card number if present (prefer cardnumber/chuid)
        card_numbers = []
        # 1) direct known column
        if card_col and card_col in g.columns:
            card_numbers = list(pd.unique(g[card_col].dropna()))
        # 2) explicit 'CardNumber' output column (from SQL COALESCE)
        if not card_numbers and 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna()))
        # 3) some XML-shred columns may appear as 'value' or other column names
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
                    try:
                        vals = list(pd.unique(g[c].dropna()))
                        if vals:
                            # extend but prefer first real-looking
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        # 4) lastly try to extract from XmlMessage fields
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'xml' in cl or 'xmlmessage' in cl or 'xml_msg' in cl or 'xmlmessage' in cl:
                    for raw in g[c].dropna().astype(str):
                        extracted = _extract_card_from_xml(raw)
                        if extracted:
                            card_numbers.append(extracted)
        # 5) final unique
        card_numbers = list(dict.fromkeys(card_numbers))  # preserve order, unique

        card_number = None
        for c in card_numbers:
            n = _normalize_id_val(c)
            # explicitly reject GUIDs as card numbers
            if n and not _looks_like_guid(n):
                card_number = n
                break

        # stable id/name from the group using discovered columns first
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None

        # Employee ID: prefer Int1/Text12 then EmployeeID; DO NOT use EmployeeIdentity as EmployeeID
        # use _pick_first_non_guid_value to skip GUIDs automatically
        if empid_col and empid_col in g.columns:
            vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                # final trial: numeric normalization (strip .0) but still reject GUIDs
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        elif 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized

        # If still no employee_id and PersonnelType indicates contractor -> prefer Text12 explicitly
        if (not employee_id) and 'PersonnelType' in g.columns:
            try:
                pvals = g['PersonnelType'].dropna().astype(str)
                if not pvals.empty:
                    p0 = pvals.iloc[0]
                    if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
                        # look for text12 explicitly (case-insensitive)
                        for c in g.columns:
                            if c.lower() == 'text12':
                                vals = g[c].dropna().astype(str).map(lambda x: x.strip())
                                employee_id = _pick_first_non_guid_value(vals)
                                if employee_id:
                                    break
            except Exception:
                pass

        # Employee identity (GUID) — keep but do not promote to EmployeeID
        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]

        # Employee name: pick non-GUID candidate
        candidate_name_vals = None
        if name_col and name_col in g.columns:
            candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
        elif 'EmployeeName' in g.columns:
            candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
        elif 'ObjectName1' in g.columns:
            candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

        if candidate_name_vals is not None and not candidate_name_vals.empty:
            employee_name = _pick_first_non_guid_value(candidate_name_vals)
            if employee_name is None:
                # accept any value that looks like a name
                for v in candidate_name_vals:
                    if _looks_like_name(v) and not _is_placeholder_str(v):
                        employee_name = str(v).strip()
                        break

        # personnel type
        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        # First/Last swipe times
        first_swipe = None
        last_swipe = None
        if times:
            first_swipe = times[0]
            last_swipe = times[-1]

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type,
            'FirstSwipe': first_swipe,
            'LastSwipe': last_swipe
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = grouped.apply(agg_swipe_group).reset_index()

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # --- START PATCH: coalesce duplicate columns produced by merge ---
    # When merging grouped + durations we may get *_x / *_y columns.
    # Coalesce them into canonical column names (prefer _x then _y), then remove suffixes.
    def _coalesce_merge_columns(df, bases):
        for base in bases:
            x = base + "_x"
            y = base + "_y"
            # If canonical exists and has non-null values, keep it; else build from x/y
            try:
                has_base = base in df.columns
                base_all_null = False
                if has_base:
                    base_all_null = df[base].isnull().all()
            except Exception:
                has_base = base in df.columns
                base_all_null = True

            if (not has_base) or base_all_null:
                if x in df.columns and y in df.columns:
                    try:
                        df[base] = df[x].combine_first(df[y])
                    except Exception:
                        # fallback: prefer x then y by simple fillna
                        try:
                            df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
                        except Exception:
                            # last resort - assign x or y raw
                            if x in df.columns:
                                df[base] = df[x]
                            elif y in df.columns:
                                df[base] = df[y]
                elif x in df.columns:
                    df[base] = df[x]
                elif y in df.columns:
                    df[base] = df[y]
    # coalesce common id/name/card columns we expect
    _coalesce_merge_columns(merged, [
        "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
    ])
    # Drop any remaining suffix columns
    drop_cols = [c for c in merged.columns if c.endswith("_x") or c.endswith("_y")]
    if drop_cols:
        try:
            merged.drop(columns=drop_cols, inplace=True)
        except Exception:
            for c in drop_cols:
                if c in merged.columns:
                    try:
                        merged.drop(columns=[c], inplace=True)
                    except Exception:
                        pass
    # --- END PATCH ---

    # coalesce helpers (ensure column existence)
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)

    # If EmployeeName is missing or a GUID, try to get a better name from durations (durations typically has EmployeeName)
    # After coalescing we no longer expect _x/_y suffixes; prefer a valid human name from available columns.
    if 'EmployeeName' in merged.columns:
        def choose_best_name(row):
            gname = row.get('EmployeeName')
            # try also duration-side name tokens that may have different column names
            dname = None
            for cand in ('EmployeeName', 'employee_name', 'objectname1', 'ObjectName1'):
                if cand in row and row.get(cand) is not None:
                    dname = row.get(cand)
                    break
            if _looks_like_name(gname):
                return str(gname).strip()
            if _looks_like_name(dname):
                return str(dname).strip()
            # if grouped name exists but is GUID-like, prefer duration's name if available
            if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
                return str(gname).strip()
            if dname and not _is_placeholder_str(dname):
                return str(dname).strip()
            return None
        merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
    else:
        # fallback: try to use 'EmployeeName' from durations if our EmployeeName is missing/invalid
        if not dur.empty:
            def fill_name_from_dur(row):
                gname = row.get('EmployeeName')
                if _looks_like_name(gname) and not _is_placeholder_str(gname):
                    return gname
                # try find matching row in durations (person_uid + date) - already merged, so duration name may be in other columns
                for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
                    if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
                        return row[cand]
                return None
            merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

    # numeric normalization for EmployeeID: ensure not GUIDs/placeholder, convert floats like '320172.0' -> '320172'
    def normalize_empid(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            # strip .0 integer floats
            try:
                if '.' in s:
                    f = float(s)
                    if math.isfinite(f) and f.is_integer():
                        return str(int(f))
            except Exception:
                pass
            return s
        except Exception:
            return None

    merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

    # normalize card numbers: reject GUIDs and placeholder tokens
    def normalize_card(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            return s
        except Exception:
            return None

    merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

    # numeric normalization
    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)

    # ensure FirstSwipe/LastSwipe are datetimes
    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # EmpHistoryPresent
    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    # normalize string columns for safe downstream use; EmployeeName keep as readable-only
    for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
        if c in merged.columns:
            def _clean_str_val(v):
                if v is None:
                    return None
                try:
                    s = str(v).strip()
                    if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                        return None
                    return s
                except Exception:
                    return None
            merged[c] = merged[c].apply(_clean_str_val)

    # EmployeeName: keep None if empty or GUID/placeholder; otherwise string.
    if 'EmployeeName' in merged.columns:
        merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

    return merged


# ---------------- SCENARIOS (boolean functions) ----------------
def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
    except Exception:
        pass
    if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    return cur > 50

def scenario_repeated_short_breaks(row):
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map


# scenario list (name, fn)
SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap)
]


def run_trend_for_date(target_date: date, outdir: str = "./outputs", city='Pune'):
    logging.info("run_trend_for_date: date=%s (city=%s)", target_date, city)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city=city)
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    # save raw swipes for evidence (full raw)
    try:
        if swipes is not None and not swipes.empty:
            sw_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception as e:
        logging.warning("Failed to save raw swipes: %s", e)

    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # Build badge map and swipe overlap maps for higher-severity scenarios
    badge_map = {}
    if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}

    swipe_overlap_map = {}
    overlap_window_seconds = 2
    if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
        tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
        if not tmp.empty:
            tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
            for (d, door), g in tmp.groupby(['Date', 'Door']):
                items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                n = len(items)
                for i in range(n):
                    t_i, uid_i = items[i]
                    j = i+1
                    while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                        uid_j = items[j][1]
                        if uid_i != uid_j:
                            swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                            swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                        j += 1

    # Evaluate scenarios (use weighting to compute anomaly score)
    WEIGHTS = {
        "long_gap_>=90min": 0.3,
        "short_duration_<4h": 1.0,
        "coffee_badging": 1.0,
        "low_swipe_count_<=2": 0.5,
        "single_door": 0.25,
        "only_in": 0.8,
        "only_out": 0.8,
        "overtime_>=10h": 0.2,
        "very_long_duration_>=16h": 1.5,
        "zero_swipes": 0.4,
        "unusually_high_swipes": 1.5,
        "repeated_short_breaks": 0.5,
        "multiple_location_same_day": 0.6,
        "weekend_activity": 0.6,
        "repeated_rejection_count": 0.8,
        "badge_sharing_suspected": 2.0,
        "early_arrival_before_06": 0.4,
        "late_exit_after_22": 0.4,
        "shift_inconsistency": 1.2,
        "trending_decline": 0.7,
        "consecutive_absent_days": 1.2,
        "high_variance_duration": 0.8,
        "short_duration_on_high_presence_days": 1.1,
        "swipe_overlap": 2.0
    }
    ANOMALY_THRESHOLD = 1.5

    # evaluate scenarios and compute score
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            features[name] = features.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
        elif name == "swipe_overlap":
            features[name] = features.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map), axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    def compute_score(r):
        score = 0.0
        detected = []
        for name, _ in SCENARIOS:
            val = bool(r.get(name))
            w = WEIGHTS.get(name, 0.0)
            if val and w > 0:
                score += float(w)
                detected.append(name)
        return score, detected

    scores = features.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
    features['AnomalyScore'] = scores['AnomalyScore'].astype(float)
    features['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
    features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

    def reasons_for_row(r):
        if not bool(r.get('IsFlagged')):
            return None
        ds = r.get('DetectedScenarios')
        if ds:
            return ds
        return None
    features['Reasons'] = features.apply(reasons_for_row, axis=1)

    if 'OverlapWith' not in features.columns:
        def overlap_with_fn(r):
            d = r.get('Date')
            uid = r.get('person_uid')
            if (d, uid) in swipe_overlap_map:
                return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
            return None
        features['OverlapWith'] = features.apply(overlap_with_fn, axis=1)

    # Remove suffix columns and fix duplicates
    cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
    if cols_to_drop:
        for c in cols_to_drop:
            base = c[:-2]
            if base in features.columns:
                try:
                    features.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                try:
                    features.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass
    features = features.loc[:, ~features.columns.duplicated()]

    # ensure booleans are native Python (avoid numpy.bool_)
    for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
        if col in features.columns:
            features[col] = features[col].astype(bool)

    # write CSV with native types
    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    try:
        write_df = features.copy()
        # FirstSwipe/LastSwipe -> ISO strings
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        # Date -> ISO date
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        write_df = write_df.where(pd.notnull(write_df), None)
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception as e:
        logging.exception("Failed to write trend CSV: %s", e)

    return features


# ---------------- training dataset builder (restored) ----------------
def build_monthly_training(end_date: date = None, months: int = 3, min_unique_employees: int = 1000,
                           outdir: str = "./outputs", city: str = "Pune"):
    if end_date is None:
        end_date = datetime.now().date()
    logging.info("build_monthly_training: end_date=%s months=%d min_unique=%d", end_date, months, min_unique_employees)
    outdir = Path(outdir)
    month_windows = []
    cur = end_date.replace(day=1)
    for _ in range(months):
        start = cur
        next_month = (cur.replace(day=28) + timedelta(days=4)).replace(day=1)
        last = next_month - timedelta(days=1)
        month_windows.append((start, last))
        cur = (start - timedelta(days=1)).replace(day=1)

    person_month_rows = []
    unique_persons = set()

    for start, last in month_windows:
        d = start
        month_dfs = []
        while d <= last:
            csv_path = outdir / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
            if csv_path.exists():
                try:
                    df = pd.read_csv(csv_path)
                    month_dfs.append(df)
                except Exception:
                    try:
                        df = pd.read_csv(csv_path, dtype=str)
                        month_dfs.append(df)
                    except Exception as e:
                        logging.warning("Failed reading %s: %s", csv_path, e)
            d = d + timedelta(days=1)

        if not month_dfs:
            logging.info("No daily trend CSVs found for month %s - %s", start.isoformat(), last.isoformat())
            continue

        month_df = pd.concat(month_dfs, ignore_index=True)
        # ensure person_uid exists
        if 'person_uid' not in month_df.columns:
            def make_person_uid(row):
                parts = []
                for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                    v = row.get(c)
                    if pd.notna(v) and str(v).strip():
                        parts.append(str(v).strip())
                return "|".join(parts) if parts else None
            month_df['person_uid'] = month_df.apply(make_person_uid, axis=1)

        # convert boolean columns to int for aggregation if necessary
        for name, _ in SCENARIOS:
            if name in month_df.columns:
                month_df[name] = month_df[name].astype(int)

        agg_funcs = {
            'CountSwipes': ['median', 'mean', 'sum'],
            'DurationMinutes': ['median', 'mean', 'sum'],
            'MaxSwipeGapSeconds': ['max', 'median'],
            'ShortGapCount': ['sum'],
            'UniqueDoors': ['median'],
            'UniqueLocations': ['median'],
            'RejectionCount': ['sum']
        }
        scenario_cols = [name for name,_ in SCENARIOS if name in month_df.columns]
        group_cols = ['person_uid']
        grp = month_df.groupby(group_cols)

        for person, g in grp:
            row = {}
            row['person_uid'] = person
            row['EmployeeID'] = next((v for v in g.get('EmployeeID', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['EmployeeName'] = next((v for v in g.get('EmployeeName', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['MonthStart'] = start.isoformat()
            row['MonthEnd'] = last.isoformat()
            for col, funcs in agg_funcs.items():
                if col in g.columns:
                    for f in funcs:
                        key = f"{col}_{f}"
                        try:
                            val = getattr(g[col], f)()
                            row[key] = float(val) if pd.notna(val) else None
                        except Exception:
                            row[key] = None
                else:
                    for f in funcs:
                        row[f"{col}_{f}"] = None
            for s in scenario_cols:
                row[f"{s}_days"] = int(g[s].sum())
                row[f"{s}_label"] = int(g[s].sum() > 0)
            row['days_present'] = int(g.shape[0])
            person_month_rows.append(row)
            unique_persons.add(person)

        if len(unique_persons) >= min_unique_employees:
            logging.info("Reached min unique employees=%d, stopping aggregation early", min_unique_employees)
            break

    if not person_month_rows:
        logging.warning("No person-month rows created (no data).")
        return None

    training_df = pd.DataFrame(person_month_rows)
    train_out = outdir / "training_person_month.csv"
    training_df.to_csv(train_out, index=False)
    logging.info("Saved training CSV to %s (rows=%d unique_persons=%d)", train_out, len(training_df), len(unique_persons))
    return train_out


if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today)
    print("Completed; rows:", len(df) if df is not None else 0)
# ----------------- END OF FILE: trend_runner.py -----------------





