For Pune Keep here Only EMployee Where Personnel Type is Employee or Peronnel type Terminated Personnel 
Only keep this 
Exclude other like Contractor or any 

Also I want to train model for Each Scenarion

Currently We Got API responce like 
581
Rows analysed
580
Flagged rows

SO i want Build Logic basis on 

Make Logic like 1000 EMployees data Analysed (Unique 1000) People over 20 People Flagged on Diffrent Diffrent Condtion 
so fetch data for Month wise to Current and train model ...


WITH AllSwipes AS (
    SELECT
        t1.ObjectName1 AS EmployeeName,
        CASE
            WHEN t2.Int1 = 0 THEN t2.Text12
            ELSE CAST(t2.Int1 AS NVARCHAR)
        END AS EmployeeID,
        DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS LocaleMessageTime,
        x.Value AS Direction,
        CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS DATE) AS SwipeDate
    FROM
        [ACVSUJournal_00010030].[dbo].[ACVSUJournalLog] AS t1
    INNER JOIN [ACVSCore].[Access].[Personnel] AS t2
        ON t1.ObjectIdentity1 = t2.GUID
    INNER JOIN [ACVSCore].[Access].[PersonnelType] AS t3
        ON t2.PersonnelTypeID = t3.ObjectID
    INNER JOIN [ACVSUJournal_00010030].[dbo].[ACVSUJournalLogxmlShred] AS x
        ON t1.XMLGuid = x.GUID
    WHERE
        t1.MessageType = 'CardAdmitted'
        AND t2.Text5 IN ('Pune','Pune - Business Bay')
        AND t1.PartitionName2 = 'APAC.Default'
        AND t3.Name = 'Employee'
        AND x.Value IN ('InDirection', 'OutDirection')
        AND CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS DATE) BETWEEN '2025-10-27' AND '2025-10-27'
),

SwipeGap AS (
    SELECT
        EmployeeID,
        EmployeeName,
        SwipeDate,
        LocaleMessageTime AS CurrentSwipe,
        LAG(LocaleMessageTime) OVER (PARTITION BY EmployeeID, SwipeDate ORDER BY LocaleMessageTime) AS PrevSwipe,
        DATEDIFF(HOUR, LAG(LocaleMessageTime) OVER (PARTITION BY EmployeeID, SwipeDate ORDER BY LocaleMessageTime), LocaleMessageTime) AS GapHours
    FROM AllSwipes
),

GapReport AS (
    SELECT
        EmployeeID,
        EmployeeName,
        SwipeDate,
        PrevSwipe,
        CurrentSwipe,
        GapHours
    FROM SwipeGap
    WHERE GapHours > 6
),

DayDuration AS (
    SELECT
        EmployeeID,
        SwipeDate,
        MIN(LocaleMessageTime) AS FirstSwipe,
        MAX(LocaleMessageTime) AS LastSwipe,
        -- Duration in HH:MM format
        RIGHT('0' + CAST(DATEDIFF(MINUTE, MIN(LocaleMessageTime), MAX(LocaleMessageTime)) / 60 AS VARCHAR), 2)
        + ':' +
        RIGHT('0' + CAST(DATEDIFF(MINUTE, MIN(LocaleMessageTime), MAX(LocaleMessageTime)) % 60 AS VARCHAR), 2) AS DurationHHMM
    FROM AllSwipes
    GROUP BY EmployeeID, SwipeDate
)

SELECT
    g.EmployeeID,
    g.EmployeeName,
    g.SwipeDate,
    g.PrevSwipe,
    g.CurrentSwipe,
    g.GapHours,
    COUNT(*) OVER (PARTITION BY g.EmployeeID) AS RepeatCount,
    d.DurationHHMM
FROM GapReport g
INNER JOIN DayDuration d
    ON g.EmployeeID = d.EmployeeID AND g.SwipeDate = d.SwipeDate
ORDER BY RepeatCount DESC, g.EmployeeName, g.SwipeDate;



Refer above Query Carefully and and Logic this also
Employee gaps between 2 swipes + duration
Here build logic like 
When Employee Swipe their badhe InDirection he came Office within he went out of office again after 5 to 6 hr later he came office we want to find that Employee 
List also...
also in Frontend We dont need JSON Reponce we need Details section also 
If Someone is Highlight due to Swipe gap 
then iN details display Swipe Records ..
How We analyse data Explaination here why system Highlight their name and Evidanace also .....


C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py

from datetime import date, datetime, time
from pathlib import Path
import pandas as pd
import numpy as np
import logging

# IMPORTANT: import duration_report as a top-level module (file in same folder).
from duration_report import run_for_date

# try to load historical profile (current_analysis.csv) if present
HIST_PATH = Path(__file__).parent / "current_analysis.csv"
if HIST_PATH.exists():
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()
else:
    logging.warning("Historical profile file current_analysis.csv not found; history-based scenarios will fallback.")
    HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)


def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()
    sw['LocaleMessageTime'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce')
    sw['Date'] = sw['LocaleMessageTime'].dt.date

    # ensure person_uid exists (same logic as duration_report)
    if 'person_uid' not in sw.columns:
        def make_person_uid(row):
            parts = []
            for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                v = row.get(c)
                if pd.notna(v) and str(v).strip():
                    parts.append(str(v).strip())
            return "|".join(parts) if parts else None
        sw['person_uid'] = sw.apply(make_person_uid, axis=1)

    # Choose the columns present in swipes for aggregation
    sel_cols = [c for c in [
        'LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
        'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType'
    ] if c in sw.columns]

    # aggregator
    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist())
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0
        in_count = int((g.get('Direction') == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g.get('Direction') == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0
        card_numbers = list(pd.unique(g['CardNumber'].dropna())) if 'CardNumber' in g.columns else []
        card_number = card_numbers[0] if card_numbers else None

        # pick name/id/personnel from any available column (prefer the explicit ones)
        employee_id = None
        employee_name = None
        personnel_type = None
        if 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna()
            employee_id = vals.iloc[0] if not vals.empty else None
        # prefer EmployeeName then ObjectName1
        if 'EmployeeName' in g.columns:
            vals = g['EmployeeName'].dropna()
            if not vals.empty:
                employee_name = vals.iloc[0]
        if employee_name is None and 'ObjectName1' in g.columns:
            vals = g['ObjectName1'].dropna()
            if not vals.empty:
                employee_name = vals.iloc[0]
        if 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            personnel_type = vals.iloc[0] if not vals.empty else None

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type
        })

    # run groupby using selected columns (prevents pandas future warning)
    gb = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = gb.apply(agg_swipe_group).reset_index()

    # normalize durations side
    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date']).dt.date

    # merge features with durations
    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # Coalesce function that consolidates _x/_y into base if needed
    def coalesce_field(df: pd.DataFrame, base: str, fallback_order: list = None, default=None):
        """
        Ensure df[base] exists. Priority:
         - if base exists, keep it
         - else try base_x, base_y
         - else if fallback_order provided, attempt those columns in order
         - else set default
        """
        if base in df.columns:
            return
        candidates = [base + '_x', base + '_y']
        if fallback_order:
            candidates = candidates + fallback_order
        for c in candidates:
            if c in df.columns:
                df[base] = df[c]
                return
        df[base] = default

    # coalesce common fields (including EmployeeName variants)
    coalesce_field(merged, 'CountSwipes', default=0)
    coalesce_field(merged, 'DurationSeconds', default=0)
    coalesce_field(merged, 'FirstSwipe', default=pd.NaT)
    coalesce_field(merged, 'LastSwipe', default=pd.NaT)
    coalesce_field(merged, 'CardNumber', default=None)
    coalesce_field(merged, 'EmployeeID', default=None, fallback_order=['ObjectName1'])
    # EmployeeName: prefer EmployeeName, then ObjectName1, then EmployeeName_x/_y, then EmployeeName_y
    coalesce_field(merged, 'EmployeeName', fallback_order=['ObjectName1', 'EmployeeName_x', 'EmployeeName_y'], default=None)
    coalesce_field(merged, 'PersonnelType', default=None)

    # Now ensure types & defaults
    merged['DurationSeconds'] = merged.get('DurationSeconds', 0).fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged.get('MaxSwipeGapSeconds', 0).fillna(0).astype(int)
    merged['ShortGapCount'] = merged.get('ShortGapCount', 0).fillna(0).astype(int)
    merged['RejectionCount'] = merged.get('RejectionCount', 0).fillna(0).astype(int)
    merged['UniqueLocations'] = merged.get('UniqueLocations', 0).fillna(0).astype(int)
    merged['UniqueDoors'] = merged.get('UniqueDoors', 0).fillna(0).astype(int)

    # ensure FirstSwipe/LastSwipe exist and convert to datetime
    for col in ['FirstSwipe', 'LastSwipe']:
        if col not in merged.columns:
            merged[col] = pd.NaT
        else:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')

    # boolean flags
    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # historical presence map
    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: x in hist_map if pd.notna(x) else False)

    return merged


# ----------------- Scenarios -----------------
def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return (row.get('CountSwipes') or 0) <= 2 and (row.get('CountSwipes') or 0) > 0

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        try:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
        except Exception:
            pass
    if 'TotalSwipes_median' in HIST_DF.columns and not HIST_DF.empty:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    return cur > 50

def scenario_repeated_short_breaks(row):
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, full_df=None):
    return False  # evaluated later using badge_map

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row):
    # boolean version - actual membership will be applied in run_trend_for_date
    return False


SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap)
]


def run_trend_for_date(target_date: date, outdir: str = "./outputs"):
    logging.info("run_trend_for_date: date=%s (Pune test)", target_date)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city='Pune')
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # Build badge map (Date, CardNumber) => distinct person count (badge-sharing)
    badge_map = {}
    if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            badge_map = {(row.Date, row.CardNumber): row.distinct_users for row in grouped_card.itertuples(index=False)}

    # Build swipe-overlap details: map (Date, person_uid) -> set(other_person_uid)
    swipe_overlap_map = {}
    overlap_window_seconds = 2  # adjustable window for "simultaneous" swipes
    if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
        tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna(subset=['Door', 'LocaleMessageTime', 'person_uid'])
        if not tmp.empty:
            tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
            for (d, door), g in tmp.groupby(['Date', 'Door']):
                # list of tuples (time, uid)
                items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                n = len(items)
                j = 0
                for i in range(n):
                    t_i, uid_i = items[i]
                    j = max(j, i+1)
                    while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                        uid_j = items[j][1]
                        if uid_i != uid_j:
                            swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                            swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                        j += 1

    # Apply scenarios — special handling for badge_sharing and swipe_overlap
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            def badge_fn(r):
                card = r.get('CardNumber')
                d = r.get('Date')
                if pd.isna(card) or card is None:
                    return False
                return badge_map.get((d, card), 0) > 1
            features[name] = features.apply(badge_fn, axis=1)
        elif name == "swipe_overlap":
            # boolean flag
            def overlap_flag_fn(r):
                d = r.get('Date')
                uid = r.get('person_uid')
                if pd.isna(uid) or d is None:
                    return False
                return (d, uid) in swipe_overlap_map
            features[name] = features.apply(overlap_flag_fn, axis=1)
            # create readable OverlapWith column listing other uids
            def overlap_with_fn(r):
                d = r.get('Date')
                uid = r.get('person_uid')
                if pd.isna(uid) or d is None:
                    return None
                others = swipe_overlap_map.get((d, uid), None)
                if not others:
                    return None
                # join sorted uids
                try:
                    return ";".join(sorted(str(o) for o in others))
                except Exception:
                    return ";".join(str(o) for o in others)
            features['OverlapWith'] = features.apply(overlap_with_fn, axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    # Create reason text
    def reasons_for_row(r):
        reasons = [name for name, _ in SCENARIOS if bool(r.get(name))]
        return "; ".join(reasons) if reasons else None

    features['Reasons'] = features.apply(reasons_for_row, axis=1)

    # Coalesce employee name once more if merge produced weird suffixes
    # prefer EmployeeName > ObjectName1 > EmployeeName_x/_y
    for candidate in ('EmployeeName', 'ObjectName1', 'EmployeeName_x', 'EmployeeName_y'):
        if candidate in features.columns and 'EmployeeName' not in features.columns:
            features.rename(columns={candidate: 'EmployeeName'}, inplace=True)

    # --- cleanup duplicate suffixed columns before saving ---
    cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
    if cols_to_drop:
        for c in cols_to_drop:
            base = c[:-2]
            if base in features.columns:
                try:
                    features.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                try:
                    features.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass
    features = features.loc[:, ~features.columns.duplicated()]

    # Ensure boolean columns are standard Python booleans (helps JSON output)
    for col in [name for name, _ in SCENARIOS]:
        if col in features.columns:
            features[col] = features[col].astype(bool)

    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    features.to_csv(out_csv, index=False)
    logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(features))
    return features


if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today)
    print("Completed; rows:", len(df))







# backend/logic.py
import pandas as pd
import numpy as np
import logging
from pathlib import Path

PROFILE_PATH = Path(__file__).parent / "current_analysis.csv"
if PROFILE_PATH.exists():
    try:
        employee_profile = pd.read_csv(PROFILE_PATH)
        logging.info("logic.py: loaded historical profile (%d rows)", len(employee_profile))
    except Exception as e:
        logging.warning("logic.py: failed to read current_analysis.csv: %s", e)
        employee_profile = pd.DataFrame()
else:
    logging.warning("logic.py: current_analysis.csv not found; history-based checks will be limited.")
    employee_profile = pd.DataFrame()

def _safe_get_emp_hist(employee_id):
    if employee_profile.empty or pd.isna(employee_id):
        return pd.DataFrame()
    return employee_profile[employee_profile['EmployeeID'] == employee_id]

def flag_employee(row_dict):
    employee_id = row_dict.get('EmployeeID')
    personnel_type = row_dict.get('PersonnelType') or row_dict.get('PersonnelTypeName')
    days_present = int(row_dict.get('DaysPresentInWeek') or 0)

    logging.info("flag_employee: checking EmployeeID=%s", employee_id)

    emp_hist = _safe_get_emp_hist(employee_id)
    if emp_hist.empty:
        return False, ["No historical trend data found"]

    reasons = []

    if pd.notnull(row_dict.get('InTime')) and pd.notnull(row_dict.get('OutTime')):
        reasons.append("Coffee badging pattern detected (both InTime and OutTime present)")

    if int(row_dict.get("OnlyIn", 0) or 0) == 1:
        reasons.append("OnlyIn entry detected")
    if int(row_dict.get("OnlyOut", 0) or 0) == 1:
        reasons.append("OnlyOut entry detected")
    if int(row_dict.get("SingleDoor", 0) or 0) == 1:
        reasons.append("SingleDoor entry detected")

    if personnel_type == "Employee":
        is_defaulter = row_dict.get("Defaulter", "No")
        if str(is_defaulter).strip().lower() == "yes":
            reasons.append("Flagged as Defaulter by company policy")

    metric_column_map = {
        'DurationMinutes': ('AvgDurationMins_median', 'AvgDurationMins_std'),
        'TotalSwipes': ('TotalSwipes_median', 'TotalSwipes_std')
    }

    for metric, (median_col, std_col) in metric_column_map.items():
        try:
            live_val = row_dict.get(metric)
            if live_val is None or (isinstance(live_val, float) and np.isnan(live_val)):
                reasons.append(f"{metric} missing or null in live data")
                continue
            if median_col in emp_hist.columns and std_col in emp_hist.columns:
                median_val = float(emp_hist.iloc[0].get(median_col, np.nan))
                std_val = float(emp_hist.iloc[0].get(std_col, np.nan))
                if pd.notna(median_val) and pd.notna(std_val):
                    buffer = 2.5 * std_val
                    if live_val < median_val - buffer or live_val > median_val + buffer:
                        reasons.append(f"Abnormal {metric}: {live_val} outside expected [{median_val-buffer:.1f}, {median_val+buffer:.1f}]")
        except Exception as e:
            reasons.append(f"Error analyzing {metric}: {e}")

    try:
        duration = float(row_dict.get('DurationMinutes') or 0)
        if days_present < 3 and duration < 480:
            reasons.append("Duration < 8 hours on limited office days")
    except Exception as e:
        reasons.append(f"Error checking duration logic: {e}")

    return (len(reasons) > 0), reasons






C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py

from flask import Flask, jsonify, request
from datetime import datetime, timedelta
from pathlib import Path
import logging
import pandas as pd
import numpy as np

# Try to enable CORS if available; otherwise continue without it.
try:
    from flask_cors import CORS
    _HAS_CORS = True
except Exception:
    CORS = None
    _HAS_CORS = False

# import runner (local)
from trend_runner import run_trend_for_date

app = Flask(__name__)
if _HAS_CORS:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS. Install Flask-Cors to enable cross-origin access from browser.")

logging.basicConfig(level=logging.INFO)

# Resolve output directory relative to this file
BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Return a clean JSON-serializable sample.  - drop any _x/_y duplicates and convert datetimes.
    Important: replace NaN / NaT with None and convert numpy scalars to native Python types so
    Flask/json can produce valid JSON.
    """
    if df is None or df.empty:
        return []

    # work on a copy
    df = df.copy()

    # drop _x/_y columns (prefer canonical base if present, otherwise rename _x/_y -> base)
    cols_to_suffix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    if cols_to_suffix:
        for c in cols_to_suffix:
            base = c[:-2]
            if base in df.columns:
                # base exists, drop the suffixed column
                try:
                    df.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                # rename suffixed column to base (prefer _x over _y if both present will be handled by dropping later)
                try:
                    df.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass

    # drop any duplicate-named columns preserving first occurrence
    df = df.loc[:, ~df.columns.duplicated()]

    # convert known datetime columns to ISO strings (or None)
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                # isoformat without timezone (consistent readable format)
                df[dtcol] = df[dtcol].dt.strftime('%Y-%m-%dT%H:%M:%S')
                # above returns NaN for NaT; will be converted to None below
            except Exception:
                # fall back to string conversion then nullify invalid
                try:
                    df[dtcol] = df[dtcol].astype(str)
                except Exception:
                    pass

    # ensure all pandas / numpy NaN/NaT are replaced with Python None
    df = df.where(pd.notnull(df), None)

    # convert numpy scalar types to native Python types (e.g., numpy.int64 -> int)
    # applymap is fine for small sample sizes
    def _to_python_scalar(x):
        # numpy scalar
        if isinstance(x, np.generic):
            try:
                return x.item()
            except Exception:
                return x
        # pandas Timestamp already converted to str above; but if any left, convert to iso string
        if isinstance(x, pd.Timestamp):
            try:
                return x.to_pydatetime().isoformat()
            except Exception:
                return str(x)
        return x

    df = df.head(max_rows).applymap(_to_python_scalar)

    # to_dict now produces JSON-serializable Python primitives (None instead of NaN)
    return df.to_dict(orient='records')


@app.route('/')
def root():
    return "Trend Analysis API — Pune test"


@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    """
    /run?date=YYYY-MM-DD                 -> single date
    /run?start=YYYY-MM-DD&end=YYYY-MM-DD -> date range inclusive
    POST json: { "date": "..."} or { "start": "...", "end":"..." }
    Returns summary + sample rows + list of generated files.
    """
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.json or {}

    # parse date(s)
    date_str = params.get('date')
    start_str = params.get('start')
    end_str = params.get('end')

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            # default: today
            dates = [datetime.now().date()]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    combined_rows = []
    files = []
    total_rows = 0
    total_flagged = 0
    samples = []

    for d in dates:
        try:
            df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
        files.append(csv_path.name if csv_path.exists() else None)

        if df is None or df.empty:
            continue

        # ensure Reasons column exists
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        # count
        total_rows += len(df)
        total_flagged += int(df['Reasons'].notna().sum())

        # sample top flagged first, else top rows
        flagged = df[df['Reasons'].notna()]
        sample_df = flagged.head(10) if not flagged.empty else df.head(10)
        samples.extend(_clean_sample_df(sample_df, max_rows=10))

        # keep combined rows if user wants full (we won't return full by default)
        combined_rows.append(df)

    # combine for convenience if needed
    combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()

    # produce response
    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "files": [f for f in files if f],
        "rows": int(total_rows),
        "flagged_rows": int(total_flagged),
        "sample": samples[:20]  # limit sample size
    }
    return jsonify(resp)


@app.route('/latest', methods=['GET'])
def latest_results():
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    sample = _clean_sample_df(df, max_rows=5)
    return jsonify({
        "file": latest.name,
        "rows": int(len(df)),
        "sample": sample
    })


@app.route('/record', methods=['GET'])
def get_record():
    """
    /record?employee_id=... or /record?person_uid=...
    Returns matching rows from latest output (cleaned).
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({'error':'no outputs'}), 404
    try:
        df = pd.read_csv(csvs[0], parse_dates=['FirstSwipe','LastSwipe'], infer_datetime_format=True)
    except Exception:
        df = pd.read_csv(csvs[0])
    # clean suffixes and types
    df_clean = pd.DataFrame(_clean_sample_df(df, max_rows=len(df)))
    if q is None:
        return jsonify(df_clean.head(10).to_dict(orient='records'))
    mask = (df_clean.get('EmployeeID', '').astype(str) == str(q)) | (df_clean.get('person_uid', '').astype(str) == str(q))
    rows = df_clean[mask]
    if rows.empty:
        return jsonify({'error':'not found'}), 404
    return jsonify(rows.to_dict(orient='records'))


if __name__ == "__main__":
    # bind 0.0.0.0 so it is reachable on LAN (if firewall / network allows)
    app.run(host="0.0.0.0", port=8002, debug=True)







