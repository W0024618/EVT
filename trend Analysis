refer all Below Logic carefully and initially we need to mnake trend analysis only for Pune 
This is Our Test Envirnoment ...
Connect all database , Fetch all data just test for Pune 



check Duration Logic also refer below file carefully then will go stepp by step 




# duration_report.py
"""
Duration report module (standalone).

- Fetches swipe logs from ACVSUJournal DBs via pyodbc.
- Computes daily durations per person (sessionization logic retained).
- Writes CSVs and returns in-memory pandas DataFrames for programmatic use.

Save this as duration_report.py and import from your app.
"""
from __future__ import annotations

import argparse
import logging
import os
import re
import warnings
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name (override with environment variable if needed)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Put your region configuration here
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "database": "ACVSUJournal_00010030",
        "last_n_databases": 7,
        "partitions": [
            "APAC.Default", "JP.Tokyo", "PH.Manila", "MY.Kuala Lumpur","IN.HYD"
        ]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "database": "ACVSUJournal_00011029",
        "last_n_databases": 7,
        "partitions": [
            "LT.Vilnius", "AUT.Vienna", "IE.DUblin", "DU.Abu Dhab", "ES.Madrid",
            "IT.Rome", "MA.Casablanca", "RU.Moscow", "UK.London"
        ]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "database": "ACVSUJournal_00010030",
        "last_n_databases": 7,
        "partitions": [
            "AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition", "MX.Mexico City",
            "PA.Panama City", "PE.Lima"
        ]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "database": "ACVSUJournal_00010030",
        "last_n_databases": 7,
        "partitions": ["Denver" ,"Austin Texas" ,"Miami","New York"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
INNER JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
INNER JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
"""

# ----- helpers for DB discovery & SQL building -----
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", [])
        like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
        region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# ----- DB connection & fetch -----
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={rc['database']};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    """
    Returns DataFrame with columns:
    EmployeeName, Door, EmployeeID, CardNumber, PersonnelTypeName, EmployeeIdentity,
    PartitionName2, LocaleMessageTime, MessageType, Direction, CompanyName, PrimaryLocation
    """
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "CardNumber", "PersonnelTypeName", "EmployeeIdentity",
        "PartitionName2", "LocaleMessageTime", "MessageType", "Direction", "CompanyName", "PrimaryLocation"
    ]
    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = get_connection(region_key)
    try:
        # suppress the pandas UserWarning about DBAPI2 objects vs SQLAlchemy connectables to reduce log spam
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    finally:
        try:
            conn.close()
        except Exception:
            pass

    for c in cols:
        if c not in df.columns:
            df[c] = None

    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    return df[cols]

# ----- compute durations -----
def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    df = swipes_df.copy()
    expected = ["EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
                "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction"]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    if df["LocaleMessageTime"].dtype == object:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")

    # remove obvious duplicates
    dedupe_cols = ["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber", "Door"]
    df = df.drop_duplicates(subset=dedupe_cols, keep="first")

    df["Date"] = df["LocaleMessageTime"].dt.date

    def make_person_uid(row):
        eid = row.get("EmployeeIdentity")
        if pd.notna(eid) and str(eid).strip() != "":
            return str(eid).strip()
        pieces = [
            (str(row.get("EmployeeID")) if row.get("EmployeeID") is not None else "").strip(),
            (str(row.get("CardNumber")) if row.get("CardNumber") is not None else "").strip(),
            (str(row.get("EmployeeName")) if row.get("EmployeeName") is not None else "").strip()
        ]
        joined = "|".join([p for p in pieces if p])
        return joined or None

    df["person_uid"] = df.apply(make_person_uid, axis=1)
    df = df[df["person_uid"].notna()].copy()

    # Rewritten aggregation using groupby.agg for speed and to avoid FutureWarning.
    try:
        df = df.sort_values("LocaleMessageTime")
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", "first"),
            EmployeeName=("EmployeeName", "first"),
            CardNumber=("CardNumber", "first"),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        # fallback to safer (but slower) apply-based aggregation
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            first_dir = first.get("Direction")
            last_dir = last.get("Direction")
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": first.get("EmployeeID"),
                "EmployeeName": first.get("EmployeeName"),
                "CardNumber": first.get("CardNumber"),
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first_dir,
                "LastDirection": last_dir
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    grouped["DurationSeconds"] = (grouped["LastSwipe"] - grouped["FirstSwipe"]).dt.total_seconds().clip(lower=0)
    grouped["Duration"] = grouped["DurationSeconds"].apply(
        lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) and s >= 0 else None
    )

    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    return grouped[out_cols]

# ----- categorization helper -----
def categorize_seconds(s: Optional[int]) -> str:
    """
    Category labels (used when secs present > 0):
      - "0-30m"      -> 0 .. 1800 (inclusive)
      - "30m-2h"     -> 1801 .. 7200
      - "2h-6h"      -> 7201 .. 21600
      - "6h-8h"      -> 21601 .. 28800
      - "8h+"        -> >= 28800
    """
    try:
        if s is None or s <= 0:
            return "0-30m"
        s = int(s)
        if s <= 1800:
            return "0-30m"
        if s <= 7200:
            return "30m-2h"
        if s <= 21600:
            return "2h-6h"
        if s < 28800:
            return "6h-8h"
        return "8h+"
    except Exception:
        return "0-30m"

# ----- main runner (per date) -----
def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None) -> Dict[str, Any]:
    """
    Compute durations + swipes for provided regions on target_date.
    Returns results dict: { region_key: {"swipes": DataFrame, "durations": DataFrame}, ... }
    Also writes CSVs to outdir: <region>_duration_YYYYMMDD.csv and <region>_swipes_YYYYMMDD.csv
    """
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    results: Dict[str, Any] = {}
    for r in regions:
        r = r.lower()
        if r not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", r, target_date)
        try:
            swipes = fetch_swipes_for_region(r, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", r)
            swipes = pd.DataFrame()

        # optional city filter
        if city and not swipes.empty:
            city_l = str(city).strip().lower()
            mask_parts = []
            for col in ("PartitionName2", "PrimaryLocation", "Door", "EmployeeName"):
                if col in swipes.columns:
                    mask_parts.append(swipes[col].fillna("").astype(str).str.lower().str.contains(city_l, na=False))
            if mask_parts:
                combined_mask = mask_parts[0]
                for m in mask_parts[1:]:
                    combined_mask = combined_mask | m
                swipes = swipes[combined_mask].copy()
            else:
                logging.debug("City filter provided but no matching columns to filter on for region %s", r)

        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", r)
            durations = pd.DataFrame()

        csv_path = outdir_path / f"{r}_duration_{target_date.strftime('%Y%m%d')}.csv"
        swipes_csv_path = outdir_path / f"{r}_swipes_{target_date.strftime('%Y%m%d')}.csv"
        try:
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", r)
        try:
            swipes.to_csv(swipes_csv_path, index=False)
        except Exception:
            logging.exception("Failed writing swipes CSV for %s", r)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", r, csv_path, len(durations))
        logging.info("Wrote swipes CSV for %s to %s (rows=%d)", r, swipes_csv_path, len(swipes))
        results[r] = {"swipes": swipes, "durations": durations}

    return results

# ----- CLI -----
def parse_args():
    p = argparse.ArgumentParser(description="Generate daily duration reports from ACVSUJournal swipe logs.")
    p.add_argument("--date", "-d", help="Target date (YYYY-MM-DD). Defaults to today in Asia/Kolkata", default=None)
    p.add_argument("--regions", "-r", help="Comma-separated regions (apac,emea,laca,namer). Default: all",
                   default="apac,emea,laca,namer")
    p.add_argument("--outdir", "-o", help="Output directory for CSVs", default="./duration_reports")
    p.add_argument("--city", help="Optional city/location filter (e.g. Pune)", default=None)
    return p.parse_args()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    args = parse_args()

    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    else:
        tz = ZoneInfo("Asia/Kolkata")
        target_date = datetime.now(tz).date()

    regions = [x.strip().lower() for x in args.regions.split(",") if x.strip()]
    outdir = args.outdir

    logging.info("Generating duration reports for date %s and regions: %s", target_date, regions)
    results = run_for_date(target_date, regions, outdir, city=args.city)

    for r, obj in results.items():
        dur = obj.get("durations")
        logging.info("Region %s: %d persons with computed durations", r, len(dur) if dur is not None else 0)
    logging.info("Completed. CSVs are in %s", Path(outdir).absolute())


Check database Details 

const regions = {
  apac: {
    user: 'GSOC_Test',
    password: 'Westernccure@2025',
    server: 'SRVWUPNQ0986V',
    // primary DB first; add previous/other ACVSUJournal DB names to this array as needed
    databases: ['ACVSUJournal_00010030','ACVSUJournal_00010029','ACVSUJournal_00010028','ACVSUJournal_00010027','ACVSUJournal_00010026','ACVSUJournal_00010025'],
    ...commonOpts
  },
  emea: {
    user: 'GSOC_Test',
    password: 'Westernccure@2025',
    server: 'SRVWUFRA0986V',
    databases: ['ACVSUJournal_00011029','ACVSUJournal_00011028','ACVSUJournal_00011027','ACVSUJournal_00011026','ACVSUJournal_00011025','ACVSUJournal_00011024','ACVSUJournal_00011023'],
    ...commonOpts
  },
  laca: {
    user: 'GSOC_Test',
    password: 'Westernccure@2025',
    server: 'SRVWUSJO0986V',
    databases: ['ACVSUJournal_00010030','ACVSUJournal_00010029','ACVSUJournal_00010028','ACVSUJournal_00010027','ACVSUJournal_00010026','ACVSUJournal_00010025'],
    ...commonOpts
  },
  namer: {
    user: 'GSOC_Test',
    password: 'Westernccure@2025',
    server: 'SRVWUDEN0891V',
    databases: ['ACVSUJournal_00010030','ACVSUJournal_00010029','ACVSUJournal_00010028','ACVSUJournal_00010027','ACVSUJournal_00010026','ACVSUJournal_00010025'],
    ...commonOpts
  }
};






refer Logic Raw report logic also .....
This is very very helpful for Making Query...



export async function rawReportStreamToResponse(res, region, { startDate, endDate, location, admitFilter = 'all', employees = null }) {
  if (!region) throw new Error('region required');
  if (!startDate || !endDate) throw new Error('startDate and endDate required');

  // small helper: normalize incoming date to YYYY-MM-DD (string) or throw
  function toISODateOnly(input) {
    if (!input) return null;
    const d = (input instanceof Date) ? input : new Date(input);
    if (isNaN(d.getTime())) throw new Error(`Invalid date input: ${input}`);
    const yyyy = d.getFullYear();
    const mm = String(d.getMonth() + 1).padStart(2, '0');
    const dd = String(d.getDate()).padStart(2, '0');
    return `${yyyy}-${mm}-${dd}`; // SQL Date-friendly
  }

  console.log(`rawReportStreamToResponse called: region=${region} startDate=${startDate} endDate=${endDate} location=${location} admitFilter=${admitFilter} employees=${employees}`);

  // normalize dates (this ensures SQL receives unambiguous dates)
  let startDateIso, endDateIso;
  try {
    startDateIso = toISODateOnly(startDate);
    endDateIso = toISODateOnly(endDate);
  } catch (err) {
    console.error('Date normalization error', err);
    throw err;
  }

  const pool = await getPool(region);

  // Best-effort: bump pool-level request timeout if possible (avoids the 300000ms default)
  try {
    if (pool && pool.config && pool.config.options) {
      pool.config.options.requestTimeout = 30 * 60 * 1000; // 30 minutes
      console.log('Adjusted pool.config.options.requestTimeout to 30 minutes');
    }
  } catch (e) {
    console.warn('Could not adjust pool.config.options.requestTimeout', e && e.message ? e.message : e);
  }

  const request = pool.request();

  // Per-request timeout: disable/extend (streaming queries often run long)
  request.timeout = 0; // 0 = no timeout on request object

  const locationParam = (location && String(location).trim()) ? String(location).trim() : null;
  const employeesParam = (employees && String(employees).trim()) ? String(employees).trim() : null;

  // Pass normalized ISO dates to SQL — pass as JS Date objects to sql.Date to avoid driver/string conversion issues
  // create Date objects at UTC midnight for clarity
  const startDateObj = new Date(`${startDateIso}T00:00:00.000Z`);
  const endDateObj   = new Date(`${endDateIso}T00:00:00.000Z`);

  request.input('location', sql.NVarChar(200), locationParam);
  request.input('startDate', sql.Date, startDateObj);
  request.input('endDate', sql.Date, endDateObj);
  request.input('admitFilter', sql.NVarChar(20), String(admitFilter || 'all'));
  request.input('regionKey', sql.NVarChar(50), String(region || ''));
  request.input('employees', sql.NVarChar(sql.MAX), employeesParam);

  // Build the CTEs for region so we use AllLogs / AllShred / AllXml which UNION across configured DBs
  const regionCTEs = buildRegionCTEs(region);


  const query = `
  WITH ${regionCTEs},
  CombinedQuery AS(
    SELECT 
       DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS LocaleMessageTime,
       t1.ObjectName1,
       t1.PartitionName2       AS PartitionName2,
       t5_card.CardNumber,
       t5_admit.value          AS AdmitCode,
       t5_dir.value            AS Direction,
       t1.ObjectName2          AS Door,
       t5_rej.value            AS Rejection_Type,
       CASE WHEN t3.Name IN ('Contractor','Terminated Contractor')
            THEN t2.Text12
            ELSE CAST(t2.Int1 AS NVARCHAR)
       END                       AS EmployeeID,
       t3.Name                  AS PersonnelType,
       t1.MessageType,
       t1.XmlGUID,
       CASE
         WHEN (UPPER(ISNULL(@regionKey,'')) = 'NAMER' OR t1.PartitionName2 LIKE 'US.%' OR t1.PartitionName2 LIKE 'USA%')
         THEN
           CASE
             WHEN t1.ObjectName2 LIKE '%HQ%' THEN 'Denver-HQ'
             WHEN t1.ObjectName2 LIKE '%Austin%' THEN 'Austin Texas'
             WHEN t1.ObjectName2 LIKE '%Miami%' THEN 'Miami'
             WHEN t1.ObjectName2 LIKE '%NYC%' THEN 'New York'
             ELSE t1.PartitionName2
           END
         WHEN (UPPER(ISNULL(@regionKey,'')) = 'APAC' OR t1.PartitionName2 LIKE 'APAC.%' OR t1.PartitionName2 LIKE 'APAC%')
         THEN
           CASE
             WHEN t1.ObjectName2 LIKE 'APAC_PI%' THEN 'Taguig City'
             WHEN t1.ObjectName2 LIKE 'APAC_PH%' THEN 'Quezon City'
             WHEN t1.ObjectName2 LIKE '%PUN%' THEN 'Pune'
             WHEN t1.ObjectName2 LIKE '%HYD%' THEN 'Hyderabad'
             ELSE t1.PartitionName2
           END
         ELSE t1.PartitionName2
       END AS Location
    FROM AllLogs AS t1
    LEFT JOIN ACVSCore.Access.Personnel     AS t2 ON t1.ObjectIdentity1 = t2.GUID
    LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeId   = t3.ObjectID
    LEFT JOIN AllShred       AS t5_admit
      ON t1.XmlGUID = t5_admit.GUID AND t5_admit.Name = 'AdmitCode'
    LEFT JOIN AllShred       AS t5_dir
      ON t1.XmlGUID = t5_dir.GUID AND t5_dir.Value IN ('InDirection','OutDirection')
    LEFT JOIN AllXml         AS t_xml ON t1.XmlGUID = t_xml.GUID
    LEFT JOIN (
      SELECT GUID, [value]
      FROM AllShred
      WHERE [Name] IN ('Card','CHUID')
    ) AS SCard ON t1.XmlGUID = SCard.GUID
    OUTER APPLY (
      SELECT COALESCE(
        TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]', 'varchar(50)'),
        TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]', 'varchar(50)'),
        SCard.[value]
      ) AS CardNumber
    ) AS t5_card
    LEFT JOIN AllShred AS t5_rej
      ON t1.XmlGUID = t5_rej.GUID AND t5_rej.Name = 'RejectCode'
    WHERE
      t1.MessageType IN ('CardAdmitted' , 'CardRejected')
      AND (
        @location IS NULL
        OR t1.PartitionName2 = @location
        OR (
          (CASE
            WHEN (UPPER(ISNULL(@regionKey,'')) = 'NAMER' OR t1.PartitionName2 LIKE 'US.%' OR t1.PartitionName2 LIKE 'USA%')
            THEN
              CASE
                WHEN t1.ObjectName2 LIKE '%HQ%' THEN 'Denver-HQ'
                WHEN t1.ObjectName2 LIKE '%Austin%' THEN 'Austin Texas'
                WHEN t1.ObjectName2 LIKE '%Miami%' THEN 'Miami'
                WHEN t1.ObjectName2 LIKE '%NYC%' THEN 'New York'
                ELSE NULL
              END
            WHEN (UPPER(ISNULL(@regionKey,'')) = 'APAC' OR t1.PartitionName2 LIKE 'APAC.%' OR t1.PartitionName2 LIKE 'APAC%')
            THEN
              CASE
                WHEN t1.ObjectName2 LIKE 'APAC_PI%' THEN 'Taguig City'
                WHEN t1.ObjectName2 LIKE 'APAC_PH%' THEN 'Quezon City'
                WHEN t1.ObjectName2 LIKE '%PUN%' THEN 'Pune'
                WHEN t1.ObjectName2 LIKE '%HYD%' THEN 'Hyderabad'
                ELSE NULL
              END
            ELSE NULL
          END) = @location
        )
      )
      AND (
        UPPER(ISNULL(@admitFilter,'all')) = 'ALL'
        OR (UPPER(@admitFilter) = 'ADMIT'  AND t1.MessageType = 'CardAdmitted')
        OR (UPPER(@admitFilter) = 'REJECT' AND t1.MessageType = 'CardRejected')
      )
      AND CONVERT(date, DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC)) BETWEEN @startDate AND @endDate
      /* optional employees filter: CSV of tokens - match against EmployeeID, Name or CardNumber */
     
     
         AND (
        @employees IS NULL
        OR EXISTS (
          SELECT 1
          FROM STRING_SPLIT(@employees,',') AS E
          WHERE LTRIM(RTRIM(E.value)) <> ''
            AND (
                /* EmployeeID (contractors use Text12, others use Int1) - case insensitive */
                UPPER(
                  COALESCE(
                    CASE WHEN t3.Name IN ('Contractor','Terminated Contractor') THEN ISNULL(t2.Text12,'') ELSE ISNULL(CAST(t2.Int1 AS NVARCHAR), '') END,
                    ''
                  )
                ) LIKE '%' + UPPER(LTRIM(RTRIM(E.value))) + '%'
                /* Employee name - case insensitive */
                OR UPPER(ISNULL(t2.Text1,'')) LIKE '%' + UPPER(LTRIM(RTRIM(E.value))) + '%'
                /* Card number (from shredding or parsed xml) - case insensitive */
                OR UPPER(ISNULL(SCard.[value], '')) LIKE '%' + UPPER(LTRIM(RTRIM(E.value))) + '%'
            )
        )
      )
)
  SELECT
    LocaleMessageTime,
    CONVERT(date,    LocaleMessageTime) AS DateOnly,
    CONVERT(time(0), LocaleMessageTime) AS Swipe_Time,
    EmployeeID,
    ObjectName1,
    PersonnelType,
    Location,
    CardNumber,
    AdmitCode,
    Direction,
    Door,
    Rejection_Type
  FROM CombinedQuery
  ORDER BY LocaleMessageTime ASC;
  `;



  // const filename = `Raw_${region}_${startDateIso.replace(/-/g,'')}_to_${endDateIso.replace(/-/g,'')}.xlsx`;
  // res.setHeader('Content-Type', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet');
  // res.setHeader('Content-Disposition', `attachment; filename="${filename}"`);
  // res.setHeader('Cache-Control', 'no-cache');

  // // create streaming workbook with styles enabled
  // const workbook = new ExcelJS.stream.xlsx.WorkbookWriter({ stream: res, useSharedStrings: true, useStyles: true });


  const filename = `Raw_${region}_${startDateIso.replace(/-/g,'')}_to_${endDateIso.replace(/-/g,'')}.xlsx`;
  res.setHeader('Content-Type', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet');
  res.setHeader('Content-Disposition', `attachment; filename="${filename}"`);
  res.setHeader('Cache-Control', 'no-cache');

  // tell client the headers are sent (helpful for long streaming exports; prevents some client-side network errors)
  if (typeof res.flushHeaders === 'function') {
    try { res.flushHeaders(); } catch (e) { /* ignore */ }
  }

  // create streaming workbook with styles enabled
  const workbook = new ExcelJS.stream.xlsx.WorkbookWriter({ stream: res, useSharedStrings: true, useStyles: true });




  // configuration
  const MAX_ROWS_PER_SHEET = 700000;

  let headers = ['LocaleMessageTime','DateOnly','Swipe_Time','EmployeeID','ObjectName1','PersonnelType','Location','CardNumber'];

  const admit = 'AdmitCode';
  const rej = 'Rejection_Type';
  const tail = ['Direction','Door'];

  const af = (String(admitFilter || 'all')).toLowerCase();
  if (af === 'admit') {
    headers = headers.concat([admit, ...tail]);
  } else if (af === 'reject') {
    headers = headers.concat([...tail, rej]);
  } else {
    headers = headers.concat([admit, ...tail, rej]);
  }

  const defaultWidths = {
    LocaleMessageTime: 22, DateOnly: 12, Swipe_Time: 12, EmployeeID: 15,
    ObjectName1: 30, PersonnelType: 18, Location: 20, CardNumber: 20,
    AdmitCode: 14, Direction: 10, Door: 22, Rejection_Type: 22
  };

  // helpers
  const MONTH_ABBR = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'];
  function humanMonthNameFromKey(key) {
    if (!key || key === 'unknown') return 'Unknown';
    const [y,m] = key.split('-');
    const mm = parseInt(m,10);
    const mon = MONTH_ABBR[mm - 1] || m;
    return `${mon}-${y}`;
  }

  let currentMonthKey = null;
  let worksheet = null;
  let sheetIndexForMonth = {};

  function addFooterRowForOutsideBorder(ws, headersLen) {
    const footerValues = new Array(headersLen).fill('');
    const footer = ws.addRow(footerValues);
    const lastColIndex = headersLen;
    footer.eachCell((cell, colNumber) => {
      const border = {
        top: { style: 'thick' },
        left: colNumber === 1 ? { style: 'thick' } : (cell.border && cell.border.left ? cell.border.left : { style: 'thin' }),
        right: colNumber === lastColIndex ? { style: 'thick' } : (cell.border && cell.border.right ? cell.border.right : { style: 'thin' }),
        bottom: { style: 'thin' }
      };
      cell.border = border;
      cell.alignment = { horizontal: 'center', vertical: 'middle' };
    });
    footer.commit();
  }

  function createWorksheetFor(monthKey) {
    sheetIndexForMonth[monthKey] = (sheetIndexForMonth[monthKey] || 0) + 1;
    const part = sheetIndexForMonth[monthKey];
    const human = humanMonthNameFromKey(monthKey);
    const name = part === 1 ? `${human} Sheet 1` : `${human} Sheet ${part}`;

    const ws = workbook.addWorksheet(name);
    ws.columns = headers.map(h => ({ key: h, width: defaultWidths[h] || 18 }));

    const headerRow = ws.addRow(headers);
    const doorIndex = headers.indexOf('Door') + 1;
    headerRow.eachCell((cell, colNumber) => {
      cell.font = { bold: true };
      cell.alignment = { horizontal: (colNumber === doorIndex ? 'left' : 'center'), vertical: 'middle' };
      cell.fill = { type: 'pattern', pattern: 'solid', fgColor: { argb: 'FFCCE8FF' } };
      cell.border = {
        top: { style: 'thick' },
        left: (colNumber === 1) ? { style: 'thick' } : { style: 'thin' },
        bottom: { style: 'thin' },
        right: (colNumber === headers.length) ? { style: 'thick' } : { style: 'thin' }
      };
    });
    headerRow.commit();

    ws.__written = 1;
    return ws;
  }

  function monthKeyForRow(row) {
    try {
      const iso = row.LocaleMessageTime || row.DateOnly;
      if (iso) {
        const d = new Date(iso);
        if (!isNaN(d.getTime())) {
          const y = d.getUTCFullYear();
          const m = String(d.getUTCMonth() + 1).padStart(2, '0');
          return `${y}-${m}`;
        }
      }
    } catch (e) { /* ignore */ }
    return 'unknown';
  }

  // safe commit helper
  let committed = false;
  async function safeCommit() {
    if (committed) return;
    committed = true;
    try {
      await workbook.commit();
    } catch (e) {
      console.error('safeCommit: workbook.commit failed', e);
    }
  }

  request.stream = true;

  request.on('error', async (err) => {
    console.error('SQL stream error', err);
    try { await safeCommit(); } catch (e) {}
    try { res.end(); } catch (e) {}
  });

  request.on('row', (row) => {
    const monthKey = monthKeyForRow(row);
    if (!worksheet || monthKey !== currentMonthKey || (worksheet.__written >= MAX_ROWS_PER_SHEET)) {
      if (worksheet) {
        try { addFooterRowForOutsideBorder(worksheet, headers.length); } catch (e) {}
        try { worksheet.commit(); } catch (e) {}
      }
      currentMonthKey = monthKey;
      worksheet = createWorksheetFor(monthKey);
    }

    let localeDate = null;
    if (row.LocaleMessageTime) {
      localeDate = (row.LocaleMessageTime instanceof Date) ? row.LocaleMessageTime : new Date(row.LocaleMessageTime);
      if (isNaN(localeDate.getTime())) localeDate = null;
    }
    let dateOnlyDate = null;
    if (row.DateOnly) {
      const d = new Date(row.DateOnly);
      if (!isNaN(d.getTime())) dateOnlyDate = d;
    }
    let swipeDate = null;
    if (row.Swipe_Time) {
      if (row.Swipe_Time instanceof Date && !isNaN(row.Swipe_Time.getTime())) swipeDate = row.Swipe_Time;
      else {
        const t = String(row.Swipe_Time || '').trim();
        if (t) {
          const parts = t.split(':').map(p => parseInt(p || '0', 10));
          if (parts.length >= 2) swipeDate = new Date(Date.UTC(1970,0,1, parts[0], parts[1], parts[2]||0));
        }
      }
    }

    const fullObj = {
      LocaleMessageTime: localeDate || (row.LocaleMessageTime || ''),
      DateOnly: dateOnlyDate || (row.DateOnly || ''),
      Swipe_Time: swipeDate || (row.Swipe_Time || ''),
      EmployeeID: row.EmployeeID || '',
      ObjectName1: row.ObjectName1 || '',
      PersonnelType: row.PersonnelType || '',
      Location: row.Location || row.PartitionName2 || '',
      CardNumber: row.CardNumber || '',
      AdmitCode: row.AdmitCode || '',
      Direction: row.Direction || '',
      Door: row.Door || '',
      Rejection_Type: row.Rejection_Type || ''
    };

    const rowVals = headers.map(h => (fullObj[h] !== undefined && fullObj[h] !== null) ? fullObj[h] : '');
    const wrow = worksheet.addRow(rowVals);

    try {
      const doorIndex = headers.indexOf('Door') + 1;
      for (let ci = 1; ci <= headers.length; ci++) {
        const cell = wrow.getCell(ci);
        const headerName = headers[ci - 1];
        if (headerName === 'LocaleMessageTime' && localeDate) {
          cell.numFmt = 'dd-mmm-yyyy h:mm:ss AM/PM';
        } else if (headerName === 'DateOnly' && dateOnlyDate) {
          cell.numFmt = 'dd-mmm-yy';
        } else if (headerName === 'Swipe_Time' && swipeDate) {
          cell.numFmt = 'h:mm:ss AM/PM';
        }
        const horizontal = (ci === doorIndex) ? 'left' : 'center';
        cell.alignment = { vertical: 'middle', horizontal };
        const border = {
          top: { style: 'thin' },
          bottom: { style: 'thin' },
          left: (ci === 1) ? { style: 'thick' } : { style: 'thin' },
          right: (ci === headers.length) ? { style: 'thick' } : { style: 'thin' }
        };
        cell.border = border;
      }
    } catch (e) {
      console.warn('row styling warning', e && e.message ? e.message : e);
    }

    wrow.commit();
    worksheet.__written++;
  });

  request.on('done', async () => {
    try {
      if (worksheet) {
        try { addFooterRowForOutsideBorder(worksheet, headers.length); } catch (e) {}
        try { worksheet.commit(); } catch (e) {}
      }
      await safeCommit();
    } catch (err) {
      console.error('Workbook commit error', err);
    } finally {
      try { res.end(); } catch (e) {}
    }
  });

  // start streaming query (single invocation)
  try {
    // await so we catch immediate failures; streaming will invoke 'row' events
    await request.query(query);
  } catch (err) {
    console.error('request.query failed', err);
    try { await safeCommit(); } catch (e) {}
    try { res.end(); } catch (e) {}
  }
}





C:\Users\326131\OneDrive - Western Union\Desktop\Trend_backend\app.py

'''
from flask import Flask, jsonify
from db import fetch_live_swipe
from logic import flag_employee
import pandas as pd
import numpy as np
from datetime import datetime

app = Flask(__name__)

def make_json_safe(obj):
    if isinstance(obj,dict):
        return {k: make_json_safe(v) for k, v in obj.items()}
    elif isinstance(obj,list):
        return [make_json_safe(item) for item in obj]
    elif isinstance(obj, (pd.Timestamp,datetime)):
        return obj.isoformat()
    elif pd.isna(obj) or obj is pd.NaT:
        return None
    elif isinstance(obj,(np.integer,np.floating)):
        return obj.item()
    else:
        return obj


@app.route('/')
def root():
    return "Employee alert API is running"

@app.route('/check', methods=['GET'])
def check_employee():
    try:
        swipe_data = fetch_live_swipe()
        print(f"Received swipe_data type: {type(swipe_data)}")
        print(f"Swipe_data shape: {swipe_data.shape if isinstance(swipe_data, pd.DataFrame) else 'Not a DataFrame'}")
        print(f"Swipe_data columns: {swipe_data.columns.tolist() if isinstance(swipe_data, pd.DataFrame) else 'No columns'}")
        
        if swipe_data is None or swipe_data.empty:
            print("No swipe data found or DataFrame is empty")
            return jsonify({'error': 'No swipe data found', 'rows_fetched': 0}), 404

        # Log first few rows for debugging
        print("First 3 rows of swipe_data:\n", swipe_data.head(3).to_dict(orient='records'))

        # Process all rows and collect results
        results = []
        flagged_count=0
        for index, row in swipe_data.iterrows():
            row_dict = row.apply(lambda x: x[0] if isinstance(x, list) else x).to_dict()
            
            for field in ['InTime','OutTime','SwipeDate']:
                try:
                    if pd.notnull(row_dict.get(field)):
                        row_dict[field]=pd.to_datetime(row_dict[field])
                    else:
                        row_dict[field]=pd.NaT
                except Exception as dt_err:
                    print(f"Warning: could not parse datetime for {field} : {dt_err}")
                    row_dict[field] = pd.NaT      

            print(f"Processing row {index}: EmployeeID={row_dict.get('EmployeeID')}")
            flag, reasons = flag_employee(row_dict)
            if flag:
                flagged_count+=1
            
            # convert row dict to JSON-Serializable
            serializable_row_dict = make_json_safe(row_dict) 
                        
            results.append({
                'EmployeeID': serializable_row_dict.get('EmployeeID'),
                'Flagged': flag,
                'Reasons': reasons,
                'LiveSwipeData': serializable_row_dict
            })

        response = {
            'results': results,
            'total_records': len(results),
            'flagged_employees':flagged_count,
            'columns': swipe_data.columns.tolist()
        }
        print(f"Returning {len(results)} records")
        return jsonify(response)

    except Exception as e:
        print(f"Error in check_employee: {str(e)}")
        return jsonify({'error': str(e), 'rows_fetched': 0}), 500

if __name__ == '__main__':
    app.run(debug=True)
    '''
from flask import Flask, jsonify, request
from db import fetch_live_swipe
from logic import flag_employee
import pandas as pd
import numpy as np
from datetime import datetime

app = Flask(__name__)

def make_json_safe(obj):
    if isinstance(obj, dict):
        return {k: make_json_safe(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [make_json_safe(item) for item in obj]
    elif isinstance(obj, (pd.Timestamp, datetime)):
        return obj.isoformat()
    elif pd.isna(obj) or obj is pd.NaT:
        return None
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    else:
        return obj

@app.route('/')
def root():
    return "Employee alert API is running"

@app.route('/check', methods=['GET'])
def check_employee():
    try:
        swipe_data = fetch_live_swipe()
        if swipe_data is None or swipe_data.empty:
            return jsonify({'error': 'No swipe data found', 'rows_fetched': 0}), 404

        # ========== NEW: Filter by selected date ==========
        date_str = request.args.get("date", None)
        selected_date = None

        if date_str:
            try:
                selected_date = pd.to_datetime(date_str).normalize()
                swipe_data['SwipeDate'] = pd.to_datetime(swipe_data['SwipeDate'])
                swipe_data = swipe_data[swipe_data['SwipeDate'].dt.normalize() == selected_date]
            except Exception as e:
                return jsonify({'error': f"Invalid date format: {e}"}), 400

        if swipe_data.empty:
            return jsonify({'error': 'No swipe data found for the given date', 'rows_fetched': 0}), 404

        # ========== PROCESS EACH EMPLOYEE ==========
        results = []
        flagged_count = 0

        for index, row in swipe_data.iterrows():
            row_dict = row.apply(lambda x: x[0] if isinstance(x, list) else x).to_dict()
            
            for field in ['InTime', 'OutTime', 'SwipeDate']:
                try:
                    if pd.notnull(row_dict.get(field)):
                        row_dict[field] = pd.to_datetime(row_dict[field])
                    else:
                        row_dict[field] = pd.NaT
                except Exception as dt_err:
                    print(f"Warning: could not parse datetime for {field} : {dt_err}")
                    row_dict[field] = pd.NaT

            print(f"Processing row {index}: EmployeeID={row_dict.get('EmployeeID')}")
            flag, reasons = flag_employee(row_dict)
            if flag:
                flagged_count += 1

            serializable_row_dict = make_json_safe(row_dict)

            results.append({
                'EmployeeID': serializable_row_dict.get('EmployeeID'),
                'Flagged': flag,
                'Reasons': reasons,
                'LiveSwipeData': serializable_row_dict
            })

        response = {
            'results': results,
            'selected_date': selected_date.strftime("%Y-%m-%d") if selected_date else "All Dates",
            'total_records': len(results),
            'flagged_employees': flagged_count,
            'columns': swipe_data.columns.tolist()
        }

        return jsonify(response)

    except Exception as e:
        print(f"Error in check_employee: {str(e)}")
        return jsonify({'error': str(e), 'rows_fetched': 0}), 500


# ========== NEW ENDPOINT: RETURN ALL UNIQUE DATES ==========
@app.route('/available-dates', methods=['GET'])
def get_available_dates():
    try:
        swipe_data = fetch_live_swipe()
        if swipe_data is None or swipe_data.empty or 'SwipeDate' not in swipe_data.columns:
            return jsonify({'error': 'No swipe data available'}), 404

        swipe_data['SwipeDate'] = pd.to_datetime(swipe_data['SwipeDate'], errors='coerce')
        today=pd.Timestamp.today().normalize()
        available_dates = swipe_data['SwipeDate'].dropna()
        available_dates = available_dates[available_dates.dt.normalize() <today]
        
        unique_dates= sorted(available_dates.dt.date.unique(),reverse=True)

        return jsonify({'available_dates': [d.isoformat() for d in unique_dates]})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


if __name__ == '__main__':
    app.run(debug=True)








C:\Users\326131\OneDrive - Western Union\Desktop\Trend_backend\config.py


# config.py the details for connecting with the server

DB_CONFIG = {
    'driver':'OBDC Driver 17 for SQL Server',
    'server':'SRVWUPNQ0986V',
    'database':'ACVSUJournal_00010028',
    'username':'GSOC_Test',
    'password':'Westernuniongsoc@2025'
}







C:\Users\326131\OneDrive - Western Union\Desktop\Trend_backend\db.py

# db.py , we are connecting and fetching data from the database here

from sqlalchemy import create_engine
import urllib
import pyodbc
import pandas as pd
from config import DB_CONFIG

def fetch_live_swipe():
    print("step 1: entering the function")
    try:
        connection_string=(
            f"DRIVER={{ODBC Driver 17 for SQL Server}};"
            f"SERVER={DB_CONFIG['server']};"
            f"DATABASE={DB_CONFIG['database']};"
            f"UID={DB_CONFIG['username']};"
            f"PWD={DB_CONFIG['password']};"
        )
        connection_uri= f"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(connection_string)}"
        engine=create_engine(connection_uri)
        print("connection successful")
        query = """
WITH MainDoorSwipes AS (
        SELECT
            t1.ObjectName1 AS EmployeeName,
			t3.Name AS PersonnelType,
            CASE 
            WHEN t3.Name IN ('Contractor','Terminated Contractor') THEN t2.Text12
            ELSE CAST(t2.Int1 AS NVARCHAR)
        END                                                                             AS EmployeeID,
        DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS LocaleMessageTime,
        x.Value AS Direction,
        t1.ObjectName2 AS DoorName,
        -- Compute WorkDayKey based on 2 AM cut
        CASE
            WHEN CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS TIME) >= '02:00:00'
                THEN CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS DATE)
            ELSE DATEADD(DAY, -1, CAST(DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS DATE))
        END AS WorkDayKey
        FROM
        [ACVSUJournal_00010028].[dbo].[ACVSUJournalLog] AS t1
        INNER JOIN [ACVSCore].[Access].[Personnel] AS t2
        ON t1.ObjectIdentity1 = t2.GUID
        INNER JOIN [ACVSCore].[Access].[PersonnelType] AS t3
        ON t2.PersonnelTypeID = t3.ObjectID
        INNER JOIN [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxmlShred] AS x
        ON t1.XMLGuid = x.GUID
        WHERE
        t1.MessageType = 'CardAdmitted'
        AND t1.PartitionName2 = 'APAC.Default'
        AND t3.Name = 'Employee'
        AND DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) >= '2025-06-01'
        AND t1.ObjectName2 IN (
            -- your main door list here
            'APAC_IN_PUN_2NDFLR_RECPTION TO WORKSTATION DOOR_10:05:4B',
            'APAC_IN_PUN_PODIUM_MAIN PODIUM RIGHT ENTRY-DOOR NEW',
            'APAC_IN_PUN_PODIUM_MAIN PODIUM LEFT ENTRY-DOOR NEW',
            'APAC_IN_PUN_TOWER B_MAIN RECEPTION DOOR',
            'APAC_IN_PUN_TOWER B_LIFT LOBBY DOOR',
            'APAC_IN_PUN_PODIUM_YELLOW_MAIN LIFT LOBBY-DOOR NEW',
            'APAC_IN_PUN_PODIUM_RED_MAIN LIFT LOBBY ENTRY 1-DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 1-DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2-DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 3-DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4-DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2 -OUT DOOR',
            'APAC_IN_PUN-PODIUM_P-1 TURNSTILE 3 -OUT DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 1-OUT DOOR',
            'APAC_IN_PUN_PODIUM_P-1 TURNSTILE 4 -OUT DOOR',
            'APAC_IN_PUN_PODIUM_ST 1-DOOR 1 (RED)',
            'APAC_IN_PUN_PODIUM_ST 1 DOOR 2 (YELLOW)'
        )
        AND x.Value IN ('InDirection', 'OutDirection')
        ),
        RankedSwipes AS (
        SELECT
        *,
        LEAD(LocaleMessageTime) OVER (PARTITION BY EmployeeID, WorkDayKey ORDER BY LocaleMessageTime) AS NextSwipeTime,
        LEAD(Direction) OVER (PARTITION BY EmployeeID, WorkDayKey ORDER BY LocaleMessageTime) AS NextDirection,
        LEAD(DoorName) OVER (PARTITION BY EmployeeID ORDER BY LocaleMessageTime) AS NextDoor
        FROM
        MainDoorSwipes
        ),
        FlaggedBreaks AS (
        SELECT
        EmployeeName,
        EmployeeID,
		PersonnelType,
        DoorName AS OutDoor,
        LocaleMessageTime AS OutTime,
		NextDoor AS Indoor,
        NextSwipeTime AS InTime, 
		CONVERT(date, LocaleMessageTime)         AS SwipeDate,
        DATEDIFF(MINUTE, LocaleMessageTime, NextSwipeTime) AS MinutesDiff
        FROM
        RankedSwipes
        WHERE
        Direction = 'OutDirection'
        AND NextDirection = 'InDirection'
        AND DATEDIFF(MINUTE, LocaleMessageTime, NextSwipeTime) >= 120
        ),
        CombinedQuery AS (
        SELECT 
        DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])                 AS LocaleMessageTime,
        t1.ObjectName1                                                                  AS EmployeeName,
        CASE
            WHEN t3.Name IN ('Contractor','Terminated Contractor') THEN t2.Text12
			WHEN t3.Name IN ('Property Management', 'Visitor', 'Temp Badge') THEN t2.Text9
            ELSE CAST(t2.Int1 AS NVARCHAR)											END AS EmployeeID,
        t3.Name                                                                         AS PersonnelType,
        t1.PartitionName2                                                               AS Location,
        t5_card.CardNumber,
        t5_admit.Value                                                                  AS AdmitCode,
        t5_dir.Value                                                                    AS Direction,
        t1.ObjectName2                                                                  AS Door,
        t5_rej.Value                                                                    AS Rejection_Type,
        t1.XmlGUID
        FROM [ACVSUJournal_00010028].[dbo].[ACVSUJournalLog] AS t1
        LEFT JOIN [ACVSCore].[Access].[Personnel]     AS t2 ON t1.ObjectIdentity1 = t2.GUID
        LEFT JOIN [ACVSCore].[Access].[PersonnelType] AS t3 ON t2.PersonnelTypeId   = t3.ObjectID

        -- AdmitCode shred
        LEFT JOIN [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxmlShred] AS t5_admit
        ON t1.XmlGUID = t5_admit.GUID AND t5_admit.Name = 'AdmitCode'

        -- Direction shred
        LEFT JOIN [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxmlShred] AS t5_dir
        ON t1.XmlGUID = t5_dir.GUID AND t5_dir.Name = 'Direction'

        -- RejectCode shred
        LEFT JOIN [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxmlShred] AS t5_rej
        ON t1.XmlGUID = t5_rej.GUID AND t5_rej.Name = 'RejectCode'

        -- Pull entire XML and shredded “Card” fallback
        LEFT JOIN [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxml]      AS t_xml
        ON t1.XmlGUID = t_xml.GUID

        LEFT JOIN (
        SELECT GUID,[Value] FROM [ACVSUJournal_00010028].[dbo].[ACVSUJournalLogxmlShred]
        WHERE [Name] IN ('Card','CHUID')
        ) AS SCard ON t1.XmlGUID = SCard.GUID

        OUTER APPLY (
        SELECT COALESCE(
            TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
            TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
            SCard.[Value]
        ) AS CardNumber
        ) AS t5_card

        WHERE 
        t1.MessageType   IN ('CardAdmitted','CardRejected')
        AND t1.PartitionName2 = 'APAC.Default'
        AND CONVERT(date,
                DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])
            ) >= '2025-05-01'
        ),


--------------------------------------------------------------------------------
-- 2) Swipe counts per day
DailySwipes AS (
    SELECT
        EmployeeID,
        EmployeeName,
		PersonnelType,
        CONVERT(date, LocaleMessageTime)         AS SwipeDate,
        COUNT(DISTINCT LocaleMessageTime)        AS TotalSwipes
    FROM CombinedQuery
    WHERE PersonnelType IS NOT NULL
    AND PersonnelType NOT IN ('Visitor','Temp Badge')
    GROUP BY EmployeeID, EmployeeName,PersonnelType, CONVERT(date, LocaleMessageTime)
),


--------------------------------------------------------------------------------
-- 3) Only In / Only Out
DirectionSummary AS (
    SELECT
        EmployeeID,
        CONVERT(date, LocaleMessageTime)         AS SwipeDate,
        MAX(CASE WHEN Direction = 'InDirection'  THEN 1 ELSE 0 END)  AS InSwipe,
        MAX(CASE WHEN Direction = 'OutDirection' THEN 1 ELSE 0 END)  AS OutSwipe
    FROM CombinedQuery
    GROUP BY EmployeeID, CONVERT(date, LocaleMessageTime)
),


--------------------------------------------------------------------------------
-- 4) Single-door usage
SingleDoor AS (
    SELECT
        EmployeeID,
        CONVERT(date, LocaleMessageTime)         AS SwipeDate,
        COUNT(DISTINCT Door)                     AS DoorCount
    FROM CombinedQuery
    GROUP BY EmployeeID, CONVERT(date, LocaleMessageTime)
    HAVING COUNT(DISTINCT Door) = 1
),

--------------------------------------------------------------------------------
-- 5) Shift duration (PH/LT/CR locations break into shifts)
SwipeSequence AS (
    SELECT
        EmployeeID,
        EmployeeName,
        PersonnelType,
        Location,
        LocaleMessageTime,
        LAG(LocaleMessageTime) OVER (PARTITION BY EmployeeID ORDER BY LocaleMessageTime) AS PrevTime
    FROM CombinedQuery
),
ShiftIdentification AS (
    SELECT
        EmployeeID,
        EmployeeName,
        PersonnelType,
        Location,
        LocaleMessageTime,
        CASE
        WHEN PrevTime IS NULL OR DATEDIFF(MINUTE, PrevTime, LocaleMessageTime) > 360 THEN 1
        ELSE 0
        END AS IsNewShift
    FROM SwipeSequence
),
ShiftGroups AS (
    SELECT
        EmployeeID,
        EmployeeName,
        PersonnelType,
        Location,
        LocaleMessageTime,
        SUM(IsNewShift) OVER (PARTITION BY EmployeeID ORDER BY LocaleMessageTime) AS ShiftID
    FROM ShiftIdentification
),
ShiftBoundaries AS (
    SELECT
        EmployeeID,
        EmployeeName,
        PersonnelType,
        Location,
        ShiftID,
        MIN(LocaleMessageTime)                     AS ShiftStart,
        MAX(LocaleMessageTime)                     AS ShiftEnd,
        DATEDIFF(MINUTE, MIN(LocaleMessageTime), MAX(LocaleMessageTime)) AS DurationMinutes
    FROM ShiftGroups
    WHERE Location IN ('PH.Manila','LT.Vilnius','CR.Costa Rica Partition')
    GROUP BY EmployeeID,EmployeeName,PersonnelType,Location,ShiftID
),
DailyDuration AS (
    SELECT
        EmployeeID,
        EmployeeName,
        PersonnelType,
        Location,
        CONVERT(date, LocaleMessageTime)           AS AttendanceDate,
        MIN(LocaleMessageTime)                     AS ShiftStart,
        MAX(LocaleMessageTime)                     AS ShiftEnd,
        DATEDIFF(MINUTE, MIN(LocaleMessageTime), MAX(LocaleMessageTime)) AS DurationMinutes
    FROM CombinedQuery
    WHERE Location NOT IN ('PH.Manila','LT.Vilnius','CR.Costa Rica Partition')
    GROUP BY EmployeeID,EmployeeName,PersonnelType,Location,CONVERT(date, LocaleMessageTime)
),
AllShifts AS (
    SELECT EmployeeID, EmployeeName, PersonnelType, Location,
        CONVERT(date,ShiftStart) AS AttendanceDate, ShiftStart, ShiftEnd, DurationMinutes
    FROM ShiftBoundaries
UNION ALL
    SELECT EmployeeID, EmployeeName, PersonnelType, Location,
        AttendanceDate, FirstSwipeTime = ShiftStart, LastSwipeTime = ShiftEnd, DurationMinutes
    FROM DailyDuration
),

FinalMerged AS(
	SELECT 
		d.EmployeeID,
		d.EmployeeName,
		d.PersonnelType,
		d.SwipeDate,
		d.TotalSwipes,
		CASE WHEN ds.InSwipe  = 1 AND ds.OutSwipe = 0 THEN 1 ELSE 0 END AS OnlyIn,
		CASE WHEN ds.OutSwipe = 1 AND ds.InSwipe  = 0 THEN 1 ELSE 0 END AS OnlyOut,
		CASE WHEN sd.DoorCount = 1             THEN 1 ELSE 0 END AS SingleDoor,
		sh.DurationMinutes,
		fb.OutTime,
		fb.OutDoor,
		fb.InTime,
		fb.Indoor,
		fb.MinutesDiff
		FROM DailySwipes AS d
		LEFT JOIN DirectionSummary   AS ds ON ds.EmployeeID = d.EmployeeID AND ds.SwipeDate = d.SwipeDate
		LEFT JOIN SingleDoor         AS sd ON sd.EmployeeID = d.EmployeeID AND sd.SwipeDate = d.SwipeDate
		LEFT JOIN AllShifts          AS sh ON sh.EmployeeID = d.EmployeeID AND sh.AttendanceDate = d.SwipeDate
		LEFT JOIN FlaggedBreaks      AS fb ON fb.EmployeeID = d.EmployeeID AND fb.SwipeDate = sh.AttendanceDate 
),
-- Add this after FinalMerged CTE
WeeklyStatus AS (
    SELECT
        ds.EmployeeID,
        ds.EmployeeName,
		ds.PersonnelType,
        DATEADD(
            DAY,
            1 - DATEPART(WEEKDAY, ds.SwipeDate),  -- Week starts on Sunday
            ds.SwipeDate
        ) AS WeekOf,
        COUNT(DISTINCT ds.SwipeDate) AS DaysPresentInWeek,
        SUM(CASE WHEN ds.OnlyIn = 0 AND ds.OnlyOut = 0 THEN 1 ELSE 0 END) AS DaysWithoutDefault,
        CASE
            WHEN SUM(CASE WHEN ds.OnlyIn = 0 AND ds.OnlyOut = 0 THEN 1 ELSE 0 END) < 3 THEN 'Yes'
            ELSE 'No'
        END AS Defaulter
    FROM FinalMerged ds
    GROUP BY
        ds.EmployeeID,
        ds.EmployeeName,
		ds.PersonnelType,
        DATEADD(
            DAY,
            1 - DATEPART(WEEKDAY, ds.SwipeDate),
            ds.SwipeDate
        )
)

-- Final SELECT with WeeklyStatus join
SELECT
    fm.EmployeeID,
    fm.EmployeeName,
	fm.PersonnelType,
    fm.SwipeDate,
    fm.TotalSwipes,
    fm.OnlyIn,
    fm.OnlyOut,
    fm.SingleDoor,
    fm.DurationMinutes,
    fm.OutTime,
    fm.OutDoor,
    fm.InTime,
    fm.Indoor,
    fm.MinutesDiff,
    ws.WeekOf,
    ws.DaysPresentInWeek,
    ws.Defaulter
FROM FinalMerged fm
LEFT JOIN WeeklyStatus ws
ON fm.EmployeeID = ws.EmployeeID
AND fm.SwipeDate BETWEEN ws.WeekOf AND DATEADD(DAY, 6, ws.WeekOf)
ORDER BY fm.SwipeDate DESC; 
        """
        print(" step 3: query prepared")
        print("Query preview: \n",query[:300], "...")
        df=pd.read_sql(query,engine)
        print("step 4: query executed")
        print(f"total fetched rows :", df.shape[0])
        print("columns : ",df.columns.tolist())
        if df.empty:
            print("warning : query ran but return 0 rows")    
        else:
            print("preview of the result:",df.head(3))    
        return df

    except Exception as e:
        print("Connection error" ,e)
        













C:\Users\326131\OneDrive - Western Union\Desktop\Trend_backend\logic.py



'''
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Load weekly trend analysis CSV
employee_profile = pd.read_csv("current_analysis.csv")

# ========================
# 1. FILTER OUT INCOMPLETE WEEK (1-Jun-25)
# ========================
employee_profile = employee_profile[employee_profile['WeekOf'] != '01-Jun-25']

# ========================
# 2. WEEKLY TO DAILY CONVERSION
# ========================
# We'll convert weekly metrics to average daily values for comparison with live swipe data.
def weekly_to_daily(row):
    days = row['DaysPresentInWeek'] if row['DaysPresentInWeek'] > 0 else 1  # Avoid division by zero
    return pd.Series({
        'DailySwipesAvg': row['TotalSwipes'] / days,
        'DailyDurationAvg': row['AvgDurationMins'],  # already a daily average in some datasets
        'DailyCoffeeBadgingAvg': row['CoffeeBadgingCount'] / days,
        'DailySingleDoorAvg': row['SingleDoorEntries'] / days,
        'DailyOnlyInAvg': row['OnlyInCount'] / days,
        'DailyOnlyOutAvg': row['OnlyOutCount'] / days
    })

# Apply conversion row-wise and join back to the original dataframe
daily_metrics = employee_profile.apply(weekly_to_daily, axis=1)
employee_profile = pd.concat([employee_profile[['EmployeeID', 'EmployeeName', 'WeekOf']], daily_metrics], axis=1)

# ========================
# 3. FLAGGING LOGIC
# ========================
def flag_employee(row_dict):
    in_time=row_dict.get('InTime')
    out_time=row_dict.get('OutTime')
    if pd.notnull(in_time):
        in_time.hour
    else:
        in_time= None
    if pd.notnull(out_time):
        out_time.hour
    else:
        out_time= None

                            
    employee_id = row_dict.get('EmployeeID')
    personnel_type = row_dict.get('PersonnelType')  # Ensure this exists in your live swipe data

    print(f"\n\n--- Checking EmployeeID: {employee_id} ---")
    print("Live row:", row_dict)

    emp_hist = employee_profile[employee_profile['EmployeeID'] == employee_id]
    if emp_hist.empty:
        return False, ["No historical trend data"]

    # Initialize flags
    reasons = []
    if pd.notnull(row_dict.get('InTime')) and pd.notnull(row_dict.get('OutTime')):
        reasons.append("Coffee badging count detected(both intime and outtime present")
    
    if row_dict.get("OnlyIn",0) == 1:
        reasons.append("Onlyin entry detected")
    if row_dict.get("OnlyOut",0) == 1:
        reasons.append("OnlyOut entry detected")
    if row_dict.get("SingleDoor",0) == 1:
        reasons.append("SingleDoor entry detected")   

    # ============ 3A. DEFUALTER LOGIC (ONLY for Employees) ============
    if personnel_type == "Employee":
        is_defaulter = row_dict.get("Defaulter","No")  # Already calculated in live data
        if str(is_defaulter).strip().lower()=="yes":
            reasons.append("Flagged as Defaulter by company policy")

    # ============ 3B. DAILY BEHAVIOR ANOMALY ============
    # Compare current values to historical daily averages using z-score (robust)
    for metric, daily_col in [
        ('TotalSwipes', 'DailySwipesAvg'),
        ('DurationMinutes', 'DailyDurationAvg')
        #('CoffeeBadgingCount', 'DailyCoffeeBadgingAvg')
    ]:
        try:
            live_val = row_dict.get(metric)
            if live_val is None or pd.isna(live_val):
                reasons.append(f"{metric} missing or null in live data")
            hist_median = emp_hist[daily_col].median()
            hist_mad = (emp_hist[daily_col] - hist_median).abs().median() + 1e-9

            z = (live_val - hist_median) / hist_mad
            print(f"Z-score for {metric}: {z:.2f}")

            if abs(z) > 4:  # Adjust threshold as needed
                reasons.append(f"Abnormal {metric} detected (Z = {z:.2f})")
        except Exception as e:
            reasons.append(f"Error analyzing {metric}: {str(e)}")

    # ============ 3C. SHORT DURATION FLAG (e.g., coffee badging)
    try:
        duration = row_dict.get('DurationMinutes', 0)
        days_present = row_dict.get('DaysPresentInWeek', 0)
        if days_present < 3 and duration < 480:
            reasons.append("Duration < 8 hours on limited office days")
    except Exception as e:
        reasons.append(f"Error checking duration logic: {str(e)}")

    return len(reasons) > 0, reasons
'''
import pandas as pd
import numpy as np
from datetime import datetime

# Load employee historical profile
employee_profile = pd.read_csv("current_analysis.csv")

# ========================
# FLAGGING LOGIC
# ========================
def flag_employee(row_dict):
    employee_id = row_dict.get('EmployeeID')
    personnel_type = row_dict.get('PersonnelType')
    days_present = max(row_dict.get('DaysPresentInWeek', 1), 1)  # Prevent divide-by-zero

    print(f"\n--- Checking EmployeeID: {employee_id} ---")
    print("Live row:", row_dict)

    # Filter past data for the same employee
    emp_hist = employee_profile[employee_profile['EmployeeID'] == employee_id]
    if emp_hist.empty:
        return False, ["No historical trend data found"]

    reasons = []

    # ========== 1. Coffee Badging Check ==========
    if pd.notnull(row_dict.get('InTime')) and pd.notnull(row_dict.get('OutTime')):
        reasons.append("Coffee badging pattern detected (both InTime and OutTime present)")

    # ========== 2. Swipe Type Flags ==========
    if row_dict.get("OnlyIn", 0) == 1:
        reasons.append("OnlyIn entry detected")
    if row_dict.get("OnlyOut", 0) == 1:
        reasons.append("OnlyOut entry detected")
    if row_dict.get("SingleDoor", 0) == 1:
        reasons.append("SingleDoor entry detected")

    # ========== 3. Company Defaulter Policy ==========
    if personnel_type == "Employee":
        is_defaulter = row_dict.get("Defaulter", "No")
        if str(is_defaulter).strip().lower() == "yes":
            reasons.append("Flagged as Defaulter by company policy")

    # ========== 4. Behavior Anomaly Based on Median ± Std ==========
    metric_column_map = {
        'DurationMinutes': ('AvgDurationMins_median', 'AvgDurationMins_std'),
        'TotalSwipes': ('TotalSwipes_median', 'TotalSwipes_std')
    }

    for metric, (median_col, std_col) in metric_column_map.items():
        try:
            live_val = row_dict.get(metric)
            if live_val is None or pd.isna(live_val):
                reasons.append(f"{metric} missing or null in live data")
                continue

            median_val = emp_hist[median_col].values[0] #/ days_present
            std_val = emp_hist[std_col].values[0] #/ days_present
            buffer_multiplier = 2.5

            lower_bound = median_val - buffer_multiplier * std_val
            upper_bound = median_val + buffer_multiplier * std_val

            if live_val < lower_bound or live_val > upper_bound:
                reasons.append(
                    f"Abnormal {metric}: current={live_val}, expected ∈ [{lower_bound:.1f}, {upper_bound:.1f}] "
                    f"(median={median_val:.1f} ± {buffer_multiplier}×std={std_val:.1f})"
                )

        except Exception as e:
            reasons.append(f"Error analyzing {metric}: {str(e)}")

    # ========== 5. Short Duration on Few Office Days ==========
    try:
        duration = row_dict.get('DurationMinutes', 0)
        if days_present < 3 and duration < 480:
            reasons.append("Duration < 8 hours on limited office days")
    except Exception as e:
        reasons.append(f"Error checking duration logic: {str(e)}")

    return len(reasons) > 0, reasons
