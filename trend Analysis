Check Each file carefully here 

Fix our 4 issue carefully Display Employee- Employee Name here 
Employee ID , Card Number is also Blank need to correct ..

Display Evidance file only for Specific Employee ...
Details — Evidence
Name: 011A2336-D0A9-466F-A5E9-647785757F03
EmployeeID: —
Card: —
Date: 28/10/2025, 05:30:00
Duration: 0:54:01
Reasons: short_duration_<4hzero_swipesrepeated_short_breaks
OverlapWith: —
Why highlighted
short_duration_<4h — Short total duration in office (< 4 hours).
zero_swipes — No swipes recorded.
repeated_short_breaks — Multiple short breaks within the da


Employee Name	Employee ID	Card	Date	Time	Door	Direction	Note
Hajare, Nagnath	-	409947	2025-10-28	00:04:00	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	InDirection	- (swipes_pune_20251028.csv)
Gavle, Pravin	-	410369	2025-10-28	00:05:22	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	OutDirection	- (swipes_pune_20251028.csv)
Vasulkar, Sachin	-	410317	2025-10-28	00:05:23	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	OutDirection	- (swipes_pune_20251028.csv)
Vasulkar, Sachin	-	410317	2025-10-28	00:05:27	APAC_IN_PUN_TOWER B_RECEPTION LEFT DOOR	OutDirection	- (swipes_pune_20251028.csv



If Swapnil is Flag and when click Ecidance for swapnil display only Swapnil Related Evidance only ..

Fix above mention issue carefully and share me fully upadted file so i can easily swap file each other



{
  "aggregated_rows_total": 597,
  "end_date": "2025-10-28",
  "files": [
    "trend_pune_20251028.csv"
  ],
  "flagged_rows": 421,
  "rows": 597,
  "sample": [
    {
      "AnomalyScore": 1.9,
      "CardNumber": null,
      "CompanyName": "WU Technology Engineering Services Private Limited",
      "CountSwipes": 0,
      "Date": "2025-10-28",
      "DetectedScenarios": "short_duration_<4h; zero_swipes; repeated_short_breaks",
      "Duration": "0:54:01",
      "DurationMinutes": 54.016666666666666,
      "DurationSeconds": 3241.0,
      "EmpHistoryPresent": false,
      "EmployeeID": null,
      "EmployeeIdentity": null,
      "EmployeeName": null,
      "FirstDirection": "InDirection",
      "FirstDoor": "APAC_IN_PUN_PODIUM_P-1 TURNSTILE 2-DOOR",
      "FirstSwipe": "2025-10-28T09:34:30",
      "InCount": 5,
      "IsFlagged": true,
      "LastDirection": "InDirection",
      "LastDoor": "APAC_IN_PUN_PODIUM_ORANGE_KITCHENETTE FIRE EXIT-DOOR NEW",
      "LastSwipe": "2025-10-28T10:28:31",
      "MaxSwipeGapSeconds": 1414,
      "OnlyIn": 0,
      "OnlyOut": 0,
      "OutCount": 2,
      "OverlapWith": null,
      "PartitionName2": "APAC.Default",
      "PersonnelType": "Employee",
      "PersonnelTypeName": "Employee",
      "PrimaryLocation": "Pune - Business Bay",
      "Reasons": "short_duration_<4h; zero_swipes; repeated_short_breaks",
      "RejectionCount": 0,
      "ShortGapCount": 3,
      "SingleDoor": 0,
      "UniqueDoors": 5,
      "UniqueLocations": 1,
      "badge_sharing_suspected": false,
      "coffee_badging": false,
      "consecutive_absent_days": false,
      "early_arrival_before_06": false,
      "high_variance_duration": false,
      "late_exit_after_22": false,
      "long_gap_>=90min": false,
      "low_swipe_count_<=2": false,
      "multiple_location_same_day": false,
      "only_in": false,
      "only_out": false,
      "overtime_>=10h": false,
      "person_uid": "011A2336-D0A9-466F-A5E9-647785757F03",
      "repeated_rejection_count": false,
      "repeated_short_breaks": true,
      "shift_inconsistency": false,
      "short_duration_<4h": true,
      "short_duration_on_high_presence_days": false,
      "single_door": false,
      "swipe_overlap": false,
      "trending_decline": false,
      "unusually_high_swipes": false,
      "very_long_duration_>=16h": false,
      "weekend_activity": false,
      "zero_swipes": true
    },




# app.py (UPDATED)
from flask import Flask, jsonify, request, send_from_directory
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math

from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)


def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        # convert floats NaN handled above
        return x
    try:
        # fallback to string
        return str(x)
    except Exception:
        return None


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Clean a dataframe for JSON output (convert NaN -> None, pandas types -> native, format datetimes).
    """
    if df is None or df.empty:
        return []
    df = df.copy()

    # remove duplicate suffix columns
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    # Date normalization
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    # Datetime columns
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # Replace NaN/inf -> None
    df = df.where(pd.notnull(df), None)

    # Convert records to safe Python types
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)
        cleaned.append(out)
    return cleaned


@app.route('/')
def root():
    return "Trend Analysis API — Pune test"


@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.json or {}

    date_str = params.get('date')
    start_str = params.get('start')
    end_str = params.get('end')

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            dates = [datetime.now().date()]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    combined_rows = []
    files = []
    samples = []

    for d in dates:
        try:
            df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        if df is None or df.empty:
            continue

        # Ensure IsFlagged exists; Reasons only when flagged
        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        # Prefer sample of flagged rows (makes QA easier); else generic sample
        flagged = df[df['IsFlagged'] == True]
        sample_df = flagged.head(10) if not flagged.empty else df.head(10)
        samples.extend(_clean_sample_df(sample_df, max_rows=10))

        combined_rows.append(df)

    combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()

    # Determine best identifier column to count unique persons (priority)
    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity']
    id_col = next((c for c in id_candidates if c in combined_df.columns), None)

    def _norm_id_val(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        # convert floats like "320172.0" -> "320172"
        try:
            if '.' in s:
                f = float(s)
                if math.isfinite(f) and f.is_integer():
                    s = str(int(f))
        except Exception:
            pass
        return s

    if id_col is None or combined_df.empty:
        analysis_count = int(len(combined_df))
        flagged_count = int(combined_df['IsFlagged'].sum()) if 'IsFlagged' in combined_df.columns else 0
    else:
        ids_series = combined_df[id_col].apply(_norm_id_val)
        unique_ids = set([x for x in ids_series.unique() if x])
        analysis_count = int(len(unique_ids))

        # flagged unique persons (using IsFlagged)
        if 'IsFlagged' in combined_df.columns:
            flagged_series = combined_df.loc[combined_df['IsFlagged'] == True, id_col].apply(_norm_id_val)
            flagged_unique = set([x for x in flagged_series.unique() if x])
            flagged_count = int(len(flagged_unique))
        else:
            flagged_count = 0

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "files": files,
        # legacy totals
        "aggregated_rows_total": int(len(combined_df)),
        # new semantics (distinct persons)
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "sample": samples[:20]
    }
    return jsonify(resp)


@app.route('/latest', methods=['GET'])
def latest_results():
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    sample = _clean_sample_df(df, max_rows=5)
    return jsonify({
        "file": latest.name,
        "rows": int(len(df)),
        "sample": sample
    })


@app.route('/record', methods=['GET'])
def get_record():
    """
    /record?employee_id=... or /record?person_uid=...
    Returns matching aggregated trend rows and filtered raw swipe rows (only for flagged persons).
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200
    try:
        latest = csvs[0]
        try:
            df = pd.read_csv(latest, parse_dates=['FirstSwipe','LastSwipe'], infer_datetime_format=True)
        except Exception:
            df = pd.read_csv(latest, dtype=str)
    except Exception:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    if q is None:
        cleaned = _clean_sample_df(df, max_rows=10)
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    q_str = str(q).strip()

    def normalize_series(s):
        if s is None:
            return pd.Series([None]*len(df))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if math.isfinite(fv) and fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    found_mask = pd.Series(False, index=df.index)

    if 'EmployeeID' in df.columns:
        emp_series = normalize_series(df['EmployeeID'])
        found_mask = found_mask | (emp_series == q_str)

    if 'person_uid' in df.columns:
        uid_series = normalize_series(df['person_uid'])
        found_mask = found_mask | (uid_series == q_str)

    if not found_mask.any():
        # try numeric equivalence
        try:
            q_numeric = float(q_str)
            if 'EmployeeID' in df.columns:
                emp_numeric = pd.to_numeric(df['EmployeeID'], errors='coerce')
                found_mask = found_mask | (emp_numeric == q_numeric)
        except Exception:
            pass

    matched = df[found_mask]
    if matched.empty:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    cleaned_matched = _clean_sample_df(matched, max_rows=len(matched))

    # Resolve raw swipe file names by Date
    raw_files = set()
    date_vals = set()
    if 'Date' in matched.columns:
        try:
            dates_parsed = pd.to_datetime(matched['Date'], errors='coerce').dropna().dt.date.unique()
            for d in dates_parsed:
                date_vals.add(str(d.isoformat()))
        except Exception:
            pass

    if not date_vals:
        for col in ('FirstSwipe', 'LastSwipe'):
            if col in matched.columns:
                try:
                    vals = pd.to_datetime(matched[col], errors='coerce').dropna().dt.date.unique()
                    for d in vals:
                        date_vals.add(str(d.isoformat()))
                except Exception:
                    pass

    # build filtered raw swipe rows ONLY for flagged persons
    raw_swipes_out = []
    # pick first matched row to decide identifiers to filter raw swipes
    # (we will collect swipes for each matched aggregated row)
    for idx, agg_row in matched.iterrows():
        # gather the identifying values
        person_uid = agg_row.get('person_uid') if 'person_uid' in agg_row else None
        empid = agg_row.get('EmployeeID') if 'EmployeeID' in agg_row else None
        card = agg_row.get('CardNumber') if 'CardNumber' in agg_row else None
        # determine date string(s) to look for raw files using Date / FirstSwipe / LastSwipe
        dates_for_row = set()
        if 'Date' in agg_row and pd.notna(agg_row['Date']):
            try:
                d = pd.to_datetime(agg_row['Date']).date()
                dates_for_row.add(d.isoformat())
            except Exception:
                pass
        for col in ('FirstSwipe','LastSwipe'):
            if col in agg_row and pd.notna(agg_row[col]):
                try:
                    d = pd.to_datetime(agg_row[col]).date()
                    dates_for_row.add(d.isoformat())
                except Exception:
                    pass

        # only fetch raw swipe evidence when this aggregated row is flagged
        is_flagged = bool(agg_row.get('IsFlagged', False))
        if not is_flagged:
            continue

        for d in dates_for_row:
            try:
                dd = d[:10]
                raw_name = f"swipes_pune_{dd.replace('-','')}.csv"
                fp = DEFAULT_OUTDIR / raw_name
                if not fp.exists():
                    continue
                raw_files.add(raw_name)
                # read the raw swipe CSV (try to parse datetimes if possible)
                try:
                    raw_df = pd.read_csv(fp, parse_dates=['LocaleMessageTime'], infer_datetime_format=True)
                except Exception:
                    try:
                        raw_df = pd.read_csv(fp, dtype=str)
                    except Exception:
                        continue

                # normalize columns names to lowercase for searching
                cols_lower = {c.lower(): c for c in raw_df.columns}

                # candidate column names
                tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
                emp_col = cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
                card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or None
                door_col = cols_lower.get('door') or cols_lower.get('doorname') or None
                dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or None
                note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None

                # build filter mask: match person_uid OR empid OR card (whichever present)
                mask = pd.Series(False, index=raw_df.index)
                if person_uid is not None and 'person_uid' in raw_df.columns:
                    mask = mask | (raw_df['person_uid'].astype(str).str.strip() == str(person_uid).strip())
                if emp_col:
                    mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(empid).strip()) if empid is not None else mask
                if card_col and card is not None:
                    mask = mask | (raw_df[card_col].astype(str).str.strip() == str(card).strip())

                # Also attempt to match by EmployeeName if no empid found
                if (not mask.any()) and name_col and 'EmployeeName' in agg_row and pd.notna(agg_row.get('EmployeeName')):
                    mask = mask | (raw_df[name_col].astype(str).str.strip() == str(agg_row.get('EmployeeName')).strip())

                # Also filter by date: ensure the timestamp's date equals dd
                if tcol and tcol in raw_df.columns:
                    try:
                        # ensure parsed datetimes
                        raw_df[tcol] = pd.to_datetime(raw_df[tcol], errors='coerce')
                        mask = mask & (raw_df[tcol].dt.date == pd.to_datetime(dd).date())
                    except Exception:
                        pass

                filtered = raw_df[mask]
                if filtered.empty:
                    continue

                # Convert filtered rows into the standardized output structure requested by frontend
                for _, r in filtered.iterrows():
                    # build output row
                    out = {}
                    # EmployeeName
                    if name_col and name_col in raw_df.columns:
                        out['EmployeeName'] = _to_python_scalar(r.get(name_col))
                    else:
                        out['EmployeeName'] = _to_python_scalar(agg_row.get('EmployeeName') or agg_row.get('person_uid'))

                    # EmployeeID
                    if emp_col and emp_col in raw_df.columns:
                        out['EmployeeID'] = _to_python_scalar(r.get(emp_col))
                    else:
                        out['EmployeeID'] = _to_python_scalar(agg_row.get('EmployeeID'))

                    # CardNumber
                    if card_col and card_col in raw_df.columns:
                        out['CardNumber'] = _to_python_scalar(r.get(card_col))
                    else:
                        out['CardNumber'] = _to_python_scalar(agg_row.get('CardNumber'))

                    # Date and Time
                    if tcol and tcol in raw_df.columns:
                        ts = r.get(tcol)
                        try:
                            ts_py = pd.to_datetime(ts)
                            out['Date'] = ts_py.date().isoformat()
                            out['Time'] = ts_py.time().isoformat()
                        except Exception:
                            # fallback: try to split a string
                            txt = str(r.get(tcol))
                            out['Date'] = txt[:10]
                            out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                    else:
                        out['Date'] = d
                        out['Time'] = None

                    # Door
                    if door_col and door_col in raw_df.columns:
                        out['Door'] = _to_python_scalar(r.get(door_col))
                    else:
                        out['Door'] = None

                    # Direction
                    if dir_col and dir_col in raw_df.columns:
                        out['Direction'] = _to_python_scalar(r.get(dir_col))
                    else:
                        out['Direction'] = _to_python_scalar(r.get('Direction')) if 'Direction' in r else None

                    # Note (rejection / source)
                    if note_col and note_col in raw_df.columns:
                        out['Note'] = _to_python_scalar(r.get(note_col))
                    else:
                        out['Note'] = None

                    out['_source'] = raw_name
                    raw_swipes_out.append(out)
            except Exception as e:
                logging.exception("Error processing raw swipe file for date %s: %s", d, e)
                continue

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": sorted(list(raw_files)),
        "raw_swipes": raw_swipes_out
    }), 200


@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    """
    Serve raw swipe CSVs from outputs/ (filename should be the file name only).
    """
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    # send file
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)


@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)











# trend_runner.py
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math

# IMPORTANT: duration_report must exist and expose run_for_date(date, regions, outdir, city)
from duration_report import run_for_date

# historical profile (optional)
HIST_PATH = Path(__file__).parent / "current_analysis.csv"
if HIST_PATH.exists():
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()
else:
    logging.warning("Historical profile file current_analysis.csv not found; history-based scenarios will fallback.")
    HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)


def _normalize_id_val(v):
    """
    Normalize an id-like value: strip, convert floats like '320172.0' -> '320172'
    """
    if pd.isna(v):
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan":
        return None
    # strip .0 if integer-like
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s


def _canonical_person_uid(row):
    """
    Create canonical person uid:
      - prefer EmployeeID (fast numeric normalized) -> 'emp:<id>'
      - else EmployeeIdentity -> 'uid:<val>'
      - else EmployeeName -> hash-based 'name:<shorthash>'
    This avoids having multiple different person_uid values for same employee due to missing fields.
    """
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n:
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        # stable short hash of name
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None


def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    Filters: keep only rows where personnel type/name indicates 'Employee' or 'Terminated Personnel'.
    Returns DataFrame per (person_uid, Date) with feature columns.
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()
    # Normalize timestamp column
    if 'LocaleMessageTime' in sw.columns:
        sw['LocaleMessageTime'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce')
        sw['Date'] = sw['LocaleMessageTime'].dt.date
    else:
        # fallback if other time column exists
        for col in ('MessageTime', 'Timestamp', 'Time'):
            if col in sw.columns:
                sw['LocaleMessageTime'] = pd.to_datetime(sw[col], errors='coerce')
                sw['Date'] = sw['LocaleMessageTime'].dt.date
                break
        if 'LocaleMessageTime' not in sw.columns:
            # can't group by date; try to use a provided 'Date' column
            if 'Date' in sw.columns:
                sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
            else:
                sw['Date'] = None

    # Filter personnel types: prefer PersonnelTypeName, fallback to PersonnelType
    if 'PersonnelTypeName' in sw.columns:
        sw = sw[sw['PersonnelTypeName'].isin(['Employee', 'Terminated Personnel'])]
    elif 'PersonnelType' in sw.columns:
        sw = sw[sw['PersonnelType'].isin(['Employee', 'Terminated Personnel'])]
    # else keep everything (no personnel type present)

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # ensure stable person_uid (canonical)
    if 'person_uid' not in sw.columns:
        # create from EmployeeID/EmployeeIdentity/EmployeeName/objectname
        def make_person_uid_local(r):
            return _canonical_person_uid({
                'EmployeeID': r.get('EmployeeID'),
                'EmployeeIdentity': r.get('EmployeeIdentity'),
                'EmployeeName': r.get('EmployeeName') or r.get('ObjectName1')
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    # selection columns for aggregation
    sel_cols = [c for c in [
        'LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
        'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
        'EmployeeIdentity'
    ] if c in sw.columns]

    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0
        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0
        card_numbers = list(pd.unique(g['CardNumber'].dropna())) if 'CardNumber' in g.columns else []
        card_number = card_numbers[0] if card_numbers else None

        # stable id/name
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None
        if 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna()
            employee_id = vals.iloc[0] if not vals.empty else None
        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna()
            employee_identity = vals.iloc[0] if not vals.empty else None
        if 'EmployeeName' in g.columns:
            vals = g['EmployeeName'].dropna()
            if not vals.empty:
                employee_name = vals.iloc[0]
        if employee_name is None and 'ObjectName1' in g.columns:
            vals = g['ObjectName1'].dropna()
            if not vals.empty:
                employee_name = vals.iloc[0]
        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = grouped.apply(agg_swipe_group).reset_index()

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # coalesce helpers (ensure column existence)
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)

    # numeric normalization
    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)

    # ensure FirstSwipe/LastSwipe are datetimes
    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # EmpHistoryPresent
    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    return merged


# ---------------- SCENARIOS (boolean functions) ----------------
def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    # short = under 4 hours
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    # multiple short gaps (>=4 swipes and short total duration)
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    # single door by itself is low severity
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    # > 10 hours = overtime
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
    except Exception:
        pass
    if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    return cur > 50

def scenario_repeated_short_breaks(row):
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    # badge_map expected externally as {(date,card):distinct_users}
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    # if today count swipes==0 and historical flag present
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map


# scenario list (name, fn)
SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap)
]


def run_trend_for_date(target_date: date, outdir: str = "./outputs", city='Pune'):
    logging.info("run_trend_for_date: date=%s (city=%s)", target_date, city)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city=city)
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    # save raw swipes for evidence (filtered swipes if compute_features filters again)
    try:
        if swipes is not None and not swipes.empty:
            sw_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception as e:
        logging.warning("Failed to save raw swipes: %s", e)

    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # Build badge map and swipe overlap maps for higher-severity scenarios
    badge_map = {}
    if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}

    swipe_overlap_map = {}
    overlap_window_seconds = 2
    if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
        tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
        if not tmp.empty:
            tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
            for (d, door), g in tmp.groupby(['Date', 'Door']):
                items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                n = len(items)
                for i in range(n):
                    t_i, uid_i = items[i]
                    j = i+1
                    while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                        uid_j = items[j][1]
                        if uid_i != uid_j:
                            swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                            swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                        j += 1

    # Evaluate scenarios (use weighting to compute anomaly score)
    # weights chosen conservatively to reduce false positives — adjust as needed
    WEIGHTS = {
        "long_gap_>=90min": 0.3,
        "short_duration_<4h": 1.0,
        "coffee_badging": 1.0,
        "low_swipe_count_<=2": 0.5,
        "single_door": 0.25,
        "only_in": 0.8,
        "only_out": 0.8,
        "overtime_>=10h": 0.2,
        "very_long_duration_>=16h": 1.5,
        "zero_swipes": 0.4,
        "unusually_high_swipes": 1.5,
        "repeated_short_breaks": 0.5,
        "multiple_location_same_day": 0.6,
        "weekend_activity": 0.6,
        "repeated_rejection_count": 0.8,
        "badge_sharing_suspected": 2.0,
        "early_arrival_before_06": 0.4,
        "late_exit_after_22": 0.4,
        "shift_inconsistency": 1.2,
        "trending_decline": 0.7,
        "consecutive_absent_days": 1.2,
        "high_variance_duration": 0.8,
        "short_duration_on_high_presence_days": 1.1,
        "swipe_overlap": 2.0
    }
    # threshold for a row to be considered flagged
    ANOMALY_THRESHOLD = 1.5

    # evaluate scenarios and compute score
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            features[name] = features.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
        elif name == "swipe_overlap":
            features[name] = features.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map), axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    # compute AnomalyScore and apply thresholding
    def compute_score(r):
        score = 0.0
        detected = []
        for name, _ in SCENARIOS:
            val = bool(r.get(name))
            w = WEIGHTS.get(name, 0.0)
            if val and w > 0:
                score += float(w)
                detected.append(name)
        return score, detected

    scores = features.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
    # Scores field 'DetectedScenarios' will be list-like; convert to list
    features['AnomalyScore'] = scores['AnomalyScore'].astype(float)
    features['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
    features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

    # Only surface 'Reasons' if flagged (keeps frontend clean)
    def reasons_for_row(r):
        if not bool(r.get('IsFlagged')):
            return None
        # include scenarios that were detected (join)
        ds = r.get('DetectedScenarios')
        if ds:
            return ds
        return None
    features['Reasons'] = features.apply(reasons_for_row, axis=1)

    # add OverlapWith details (string) if present
    if 'OverlapWith' not in features.columns:
        # if swipe_overlap_map produced sets, map them
        def overlap_with_fn(r):
            d = r.get('Date')
            uid = r.get('person_uid')
            if (d, uid) in swipe_overlap_map:
                return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
            return None
        features['OverlapWith'] = features.apply(overlap_with_fn, axis=1)

    # Remove suffix columns and fix duplicates
    cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
    if cols_to_drop:
        for c in cols_to_drop:
            base = c[:-2]
            if base in features.columns:
                try:
                    features.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                try:
                    features.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass
    features = features.loc[:, ~features.columns.duplicated()]

    # ensure booleans are native Python (avoid numpy.bool_)
    for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
        if col in features.columns:
            features[col] = features[col].astype(bool)

    # write CSV with native types
    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    try:
        # convert lists/sets to strings and ensure datetimes convert properly
        write_df = features.copy()
        # FirstSwipe/LastSwipe -> ISO strings
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        # Date -> ISO date
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        # ensure no numpy types when saving
        write_df = write_df.where(pd.notnull(write_df), None)
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception as e:
        logging.exception("Failed to write trend CSV: %s", e)

    return features


# ---------------- training dataset builder ----------------
def build_monthly_training(end_date: date = None, months: int = 3, min_unique_employees: int = 1000,
                           outdir: str = "./outputs", city: str = "Pune"):
    """
    Aggregate daily feature CSVs into a person-month dataset ready for ML.
    Label logic: for each scenario, label person-month as 1 if the person had >=1 day in that month where scenario flagged.
    Aggregates numeric features via median/mean and counts.
    """
    if end_date is None:
        end_date = datetime.now().date()
    logging.info("build_monthly_training: end_date=%s months=%d min_unique=%d", end_date, months, min_unique_employees)
    outdir = Path(outdir)
    # build month windows (most recent 'months' months)
    month_windows = []
    cur = end_date.replace(day=1)
    for _ in range(months):
        start = cur
        # compute last day
        next_month = (cur.replace(day=28) + timedelta(days=4)).replace(day=1)
        last = next_month - timedelta(days=1)
        month_windows.append((start, last))
        cur = (start - timedelta(days=1)).replace(day=1)

    person_month_rows = []
    unique_persons = set()

    for start, last in month_windows:
        # iterate days in month
        d = start
        month_dfs = []
        while d <= last:
            csv_path = outdir / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
            if csv_path.exists():
                try:
                    df = pd.read_csv(csv_path)
                    month_dfs.append(df)
                except Exception:
                    try:
                        df = pd.read_csv(csv_path, dtype=str)
                        month_dfs.append(df)
                    except Exception as e:
                        logging.warning("Failed reading %s: %s", csv_path, e)
            d = d + timedelta(days=1)

        if not month_dfs:
            logging.info("No daily trend CSVs found for month %s - %s", start.isoformat(), last.isoformat())
            continue

        month_df = pd.concat(month_dfs, ignore_index=True)
        # ensure person_uid exists
        if 'person_uid' not in month_df.columns:
            def make_person_uid(row):
                parts = []
                for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                    v = row.get(c)
                    if pd.notna(v) and str(v).strip():
                        parts.append(str(v).strip())
                return "|".join(parts) if parts else None
            month_df['person_uid'] = month_df.apply(make_person_uid, axis=1)

        # convert boolean columns to int for aggregation if necessary
        for name, _ in SCENARIOS:
            if name in month_df.columns:
                month_df[name] = month_df[name].astype(int)

        # per-person aggregation for this month
        agg_funcs = {
            'CountSwipes': ['median', 'mean', 'sum'],
            'DurationMinutes': ['median', 'mean', 'sum'],
            'MaxSwipeGapSeconds': ['max', 'median'],
            'ShortGapCount': ['sum'],
            'UniqueDoors': ['median'],
            'UniqueLocations': ['median'],
            'RejectionCount': ['sum']
        }
        # include scenario counts: how many days in month flagged for scenario
        scenario_cols = [name for name,_ in SCENARIOS if name in month_df.columns]
        group_cols = ['person_uid']
        grp = month_df.groupby(group_cols)

        for person, g in grp:
            row = {}
            row['person_uid'] = person
            # pick stable identifiers (EmployeeID/EmployeeName) from first non-null occurrence
            row['EmployeeID'] = next((v for v in g.get('EmployeeID', []) if pd.notna(v)), None)
            row['EmployeeName'] = next((v for v in g.get('EmployeeName', []) if pd.notna(v)), None)
            row['MonthStart'] = start.isoformat()
            row['MonthEnd'] = last.isoformat()
            # numeric aggregates
            for col, funcs in agg_funcs.items():
                if col in g.columns:
                    for f in funcs:
                        key = f"{col}_{f}"
                        try:
                            val = getattr(g[col], f)()
                            row[key] = float(val) if pd.notna(val) else None
                        except Exception:
                            row[key] = None
                else:
                    for f in funcs:
                        row[f"{col}_{f}"] = None
            # scenario labels: 1 if any day in month flagged for that scenario
            for s in scenario_cols:
                row[f"{s}_days"] = int(g[s].sum())
                row[f"{s}_label"] = int(g[s].sum() > 0)
            # counts
            row['days_present'] = int(g.shape[0])
            person_month_rows.append(row)
            unique_persons.add(person)

        if len(unique_persons) >= min_unique_employees:
            logging.info("Reached min unique employees=%d, stopping aggregation early", min_unique_employees)
            break

    if not person_month_rows:
        logging.warning("No person-month rows created (no data).")
        return None

    training_df = pd.DataFrame(person_month_rows)
    # save CSV
    train_out = outdir / "training_person_month.csv"
    training_df.to_csv(train_out, index=False)
    logging.info("Saved training CSV to %s (rows=%d unique_persons=%d)", train_out, len(training_df), len(unique_persons))
    return train_out


if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today)
    print("Completed; rows:", len(df) if df is not None else 0)









<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Trend Analysis — Dashboard</title>
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <!-- React + ReactDOM + Babel (quick prototyping) -->
    <script crossorigin src="https://unpkg.com/react@18/umd/react.development.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.development.js"></script>
    <script crossorigin src="https://unpkg.com/babel-standalone@6.26.0/babel.min.js"></script>
    <!-- Chart.js -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
      /* your existing styles kept but with a few class tweaks */
      body { font-family: Inter, Roboto, Arial, sans-serif; margin: 0; padding: 0; background:#f6f7fb; color:#1f2937; }
      .container { max-width:1200px; margin:24px auto; padding:20px; background:#fff; border-radius:8px; box-shadow:0 6px 18px rgba(16,24,40,0.06); }
      header { display:flex; align-items:center; justify-content:space-between; gap:12px; flex-wrap:wrap; }
      header h1 { margin:0; font-size:20px; }
      .controls { display:flex; gap:8px; align-items:center; flex-wrap:wrap; }
      input[type="date"] { padding:8px; border-radius:6px; border:1px solid #e2e8f0; }
      button { padding:8px 12px; border-radius:6px; border:0; background:#2563eb; color:#fff; cursor:pointer; }
      button.secondary { background:#64748b; }
      button.ghost { background:transparent; color:#0f172a; border:1px solid #e2e8f0; }
      .cards { display:flex; gap:12px; margin-top:16px; }
      .card { flex:1; background:#f8fafc; padding:12px; border-radius:8px; text-align:center; }
      .card h3 { margin:4px 0; font-size:18px; }
      .card p { margin:0; color:#6b7280; }
      .main { display:flex; gap:18px; margin-top:18px; }
      .left { flex: 2; }
      .right { flex: 1; min-width:260px; }
      .chart-wrap { background:#fff; padding:10px; border-radius:8px; box-shadow: inset 0 0 0 1px #f1f5f9; }
      table { width:100%; border-collapse:collapse; margin-top:12px; }
      th, td { padding:8px 6px; border-bottom:1px solid #eef2f7; font-size:13px; vertical-align:middle; }
      th { background:#fafafa; position:sticky; top:0; z-index:1; text-align:left; }
      .small { font-size:12px; color:#475569; }
      .pill { display:inline-block; padding:4px 8px; border-radius:999px; background:#e6f2ff; color:#075985; font-size:12px; cursor:pointer; margin:2px; }
      .chip { display:inline-block; padding:6px 10px; border-radius:999px; background:#eef2ff; color:#034f84; font-size:13px; cursor:pointer; margin:3px; }
      .searchbar { margin-top:12px; display:flex; gap:8px; align-items:center; }
      .searchbar input { padding:8px; border-radius:6px; border:1px solid #e2e8f0; width:280px; }
      .pagination { margin-top:10px; display:flex; gap:8px; align-items:center; justify-content:flex-start; }
      .muted { color:#64748b; font-size:13px; }
      .spinner { position:fixed; left:0; right:0; top:0; bottom:0; display:flex; align-items:center; justify-content:center; background:rgba(255,255,255,0.6); z-index:999; }
      .modal { position:fixed; left:0; right:0; top:0; bottom:0; display:flex; align-items:center; justify-content:center; z-index:1000; }
      .modal-inner { width:900px; max-width:95%; max-height:85%; overflow:auto; background:#fff; border-radius:8px; padding:16px; box-shadow:0 10px 40px rgba(2,6,23,0.3); }
      .close-btn { float:right; background:#ef4444; color:#fff; border-radius:6px; padding:6px 8px; border:0; cursor:pointer; }
      pre { white-space:pre-wrap; word-wrap:break-word; background:#0f172a; color:#fff; padding:12px; border-radius:8px; }
      .row-click { cursor:pointer; }
      .overlap-pill { display:inline-block; padding:4px 8px; border-radius:6px; background:#fff2dd; color:#92400e; font-size:12px; }
      .evidence-table { width:100%; border-collapse:collapse; margin-top:8px; }
      .evidence-table th, .evidence-table td { padding:6px 8px; border:1px solid #e6edf3; font-size:13px; text-align:left; }
      .gap-flag { background:#fff5f0; color:#9a3412; padding:4px 6px; border-radius:6px; display:inline-block; margin-left:8px; }
      .explain { background:#f8fafc; padding:10px; border-radius:8px; margin-top:8px; }
      .evidence-btn { padding:6px 8px; background:#0ea5a4; color:#fff; border-radius:6px; border:0; cursor:pointer; }
      .flagged-row { background: linear-gradient(90deg,#fffaf0,#fff); }
      .highlight-swipe { background:#fffbeb; }
    </style>
  </head>
  <body>
    <div id="root"></div>

    <script type="text/babel">
      (function(){
        const { useState, useEffect, useRef } = React;

        // CHANGE THIS IF YOUR API HOST DIFFERS
        const API_BASE = "http://10.199.45.239:8002";

        // Map scenario keys -> human friendly explanation (edit as needed)
        const SCENARIO_EXPLANATIONS = {
          "long_gap_>=90min": "Long gap between swipes (>= 90 minutes) — could indicate long out-of-office break.",
          "short_duration_<4h": "Short total duration in office (< 4 hours).",
          "coffee_badging": "Frequent short badge cycles (>=4) with short duration — possible 'coffee badging'.",
          "low_swipe_count_<=2": "Low swipe count (<=2) for the day.",
          "single_door": "All swipes used the same door — single-door behavior.",
          "only_in": "Only IN swipe(s) recorded for the day.",
          "only_out": "Only OUT swipe(s) recorded for the day.",
          "overtime_>=10h": "Long duration (>=10 hours) — overtime.",
          "very_long_duration_>=16h": "Very long duration (>=16 hours) — suspiciously long presence.",
          "zero_swipes": "No swipes recorded.",
          "unusually_high_swipes": "Unusually high number of swipes versus historical median.",
          "repeated_short_breaks": "Multiple short breaks within the day.",
          "multiple_location_same_day": "Swipes recorded at multiple locations same day.",
          "weekend_activity": "Activity recorded on weekend.",
          "repeated_rejection_count": "Several card rejections.",
          "badge_sharing_suspected": "Badge sharing suspected (same card used by multiple persons on same day).",
          "early_arrival_before_06": "First swipe before 06:00.",
          "late_exit_after_22": "Last swipe after 22:00.",
          "shift_inconsistency": "Duration inconsistent with historical shift patterns.",
          "trending_decline": "Historical trending decline flagged.",
          "consecutive_absent_days": "Marked absent for consecutive days historically.",
          "high_variance_duration": "High variance in durations historically.",
          "short_duration_on_high_presence_days": "Short duration even though employee usually attends many days.",
          "swipe_overlap": "Simultaneous swipe(s) near the same time with other uid(s) (possible tailgating or collusion)."
        };

        function pad(n){ return n.toString().padStart(2,'0'); }
        function formatDateISO(d){
          if(!d) return "";
          const dt = (d instanceof Date) ? d : new Date(d);
          return dt.getFullYear() + "-" + pad(dt.getMonth()+1) + "-" + pad(dt.getDate());
        }
        function datesBetween(start, end){
          var out = [];
          var cur = new Date(start);
          while(cur <= end){
            out.push(new Date(cur));
            cur.setDate(cur.getDate()+1);
          }
          return out;
        }
        function safeDateDisplay(val){
          if(!val && val !== 0) return "";
          try {
            var d = (val instanceof Date) ? val : new Date(val);
            if(isNaN(d.getTime())) return String(val);
            return d.toLocaleString();
          } catch(e) {
            return String(val);
          }
        }
        function sanitizeName(row){
          return row.EmployeeName || row.EmployeeName_x || row.EmployeeName_y || row.person_uid || "";
        }
        function downloadCSV(rows, filename){
          if(!rows || !rows.length) { alert("No rows to export"); return; }
          var cols = Object.keys(rows[0]);
          var lines = [cols.join(",")];
          rows.forEach(function(r){
            var row = cols.map(function(c){
              var v = (r[c] === undefined || r[c] === null) ? "" : String(r[c]).replace(/\n/g,' ');
              return JSON.stringify(v);
            }).join(",");
            lines.push(row);
          });
          var blob = new Blob([lines.join("\n")], {type:'text/csv'});
          var url = URL.createObjectURL(blob);
          var a = document.createElement('a'); a.href = url; a.download = filename || 'export.csv'; a.click(); URL.revokeObjectURL(url);
        }

        function parseCSV(text){
          // kept but no longer used for main evidence flow (left for debug)
          const rows = [];
          let i = 0, N = text.length;
          let cur = '', inQuotes = false, curRow = [];
          while(i < N){
            const ch = text[i];
            if(inQuotes){
              if(ch === '"'){
                if(i+1 < N && text[i+1] === '"'){ cur += '"'; i += 2; continue; } // escaped quote
                inQuotes = false;
                i++; continue;
              } else {
                cur += ch; i++; continue;
              }
            } else {
              if(ch === '"'){ inQuotes = true; i++; continue; }
              if(ch === ','){ curRow.push(cur); cur = ''; i++; continue; }
              if(ch === '\r'){ i++; continue; }
              if(ch === '\n'){ curRow.push(cur); rows.push(curRow); curRow = []; cur = ''; i++; continue; }
              cur += ch; i++;
            }
          }
          if(cur !== '' || inQuotes || curRow.length){
            curRow.push(cur);
            rows.push(curRow);
          }
          if(rows.length === 0) return [];
          const headers = rows[0].map(h => (h || '').trim());
          const out = [];
          for(let r=1; r<rows.length; r++){
            const row = rows[r];
            if(row.length === 1 && row[0] === '') continue;
            const obj = {};
            for(let c=0;c<headers.length;c++){
              obj[headers[c]] = (row[c] !== undefined) ? row[c] : null;
            }
            out.push(obj);
          }
          return out;
        }

        function App(){
          var yesterday = new Date();
          yesterday.setDate(yesterday.getDate()-1);

          const [dateFrom, setDateFrom] = useState(formatDateISO(yesterday));
          const [dateTo, setDateTo] = useState(formatDateISO(new Date()));
          const [loading, setLoading] = useState(false);
          const [summary, setSummary] = useState({rows:0, flagged_rows:0, files:[], end_date:null});
          const [rows, setRows] = useState([]);
          const [reasonsCount, setReasonsCount] = useState({});
          const [filterText, setFilterText] = useState("");
          const [page, setPage] = useState(1);
          const [selectedReason, setSelectedReason] = useState("");
          const [modalRow, setModalRow] = useState(null);
          const [modalDetails, setModalDetails] = useState(null); // {aggregated_rows, raw_swipes, raw_swipe_files}
          const [modalLoading, setModalLoading] = useState(false);
          const pageSize = 25;
          const chartRef = useRef(null);
          const chartInst = useRef(null);

          useEffect(function(){
            // load latest on mount
            loadLatest();
          }, []);

          async function runForRange(){
            setLoading(true);
            setRows([]);
            setSummary({rows:0, flagged_rows:0, files:[], end_date:null});
            setReasonsCount({});
            try {
              var start = new Date(dateFrom);
              var end = new Date(dateTo);
              var dateList = datesBetween(start, end).map(d => formatDateISO(d));
              var accRows = [];
              var totalRows = 0, totalFlagged = 0, files = [];
              for(var i=0;i<dateList.length;i++){
                var d = dateList[i];
                var url = API_BASE + "/run?date=" + d;
                var r = await fetch(url, { method:'GET' });
                if(!r.ok){
                  var txt = await r.text();
                  throw new Error("API returned " + r.status + ": " + txt);
                }
                var js = await r.json();
                var sample = js.sample || [];
                if(Array.isArray(sample) && sample.length) accRows = accRows.concat(sample);
                if(typeof js.rows === 'number') totalRows += js.rows; else totalRows += (Array.isArray(sample) ? sample.length : 0);
                totalFlagged += (js.flagged_rows || 0);
                if(js.files) files = files.concat(js.files);
              }
              setRows(accRows);
              setSummary({rows: totalRows, flagged_rows: totalFlagged, files: files, end_date: formatDateISO(new Date(dateTo))});
              computeReasons(accRows);
              setPage(1);
            } catch(err){
              alert("Error: " + err.message);
              console.error(err);
            } finally {
              setLoading(false);
            }
          }

          async function loadLatest(){
            setLoading(true);
            try{
              var r = await fetch(API_BASE + "/latest");
              if(!r.ok) throw new Error("latest failed: " + r.status);
              var js = await r.json();
              var sample = js.sample || [];
              if(!Array.isArray(sample)) sample = [];
              setRows(sample);
              setSummary({rows: (js.rows || sample.length || 0), flagged_rows: (sample.filter(function(x){ return !!x.Reasons; }).length || 0), files:[js.file]});
              computeReasons(sample);
              setPage(1);
            } catch(err){
              alert("Error: " + err.message + (err.message === 'latest failed: 0' ? " (check backend/CORS)" : ""));
              console.error(err);
            } finally {
              setLoading(false);
            }
          }

          function computeReasons(dataRows){
            var counts = {};
            (dataRows || []).forEach(function(r){
              if(!r.Reasons) return;
              var parts = String(r.Reasons).split(";").map(function(s){ return s.trim(); }).filter(Boolean);
              parts.forEach(function(p){ counts[p] = (counts[p] || 0) + 1; });
            });
            setReasonsCount(counts);
            buildChart(counts);
          }

          function buildChart(counts){
            var labels = Object.keys(counts).sort(function(a,b){ return counts[b] - counts[a]; });
            var values = labels.map(function(l){ return counts[l]; });
            var ctx = chartRef.current && chartRef.current.getContext ? chartRef.current.getContext('2d') : null;
            if(!ctx) return;
            try { if(chartInst.current) chartInst.current.destroy(); } catch(e){}
            chartInst.current = new Chart(ctx, {
              type:'bar',
              data:{ labels: labels, datasets:[{ label:'Events', data: values, backgroundColor:'rgba(37,99,235,0.85)' }] },
              options:{ responsive:true, maintainAspectRatio:false, plugins:{ legend:{ display:false } }, scales:{ y:{ beginAtZero:true } } }
            });
          }

          // filtering & pagination
          var filtered = (rows || []).filter(function(r){
            var hay = (sanitizeName(r) + " " + (r.EmployeeID||"") + " " + (r.CardNumber||"") + " " + (r.Reasons||"")).toLowerCase();
            var textOk = !filterText || hay.indexOf(filterText.toLowerCase()) !== -1;
            var reasonOk = !selectedReason || (r.Reasons && ((";" + String(r.Reasons) + ";").indexOf(selectedReason) !== -1));
            return textOk && reasonOk;
          });
          var totalPages = Math.max(1, Math.ceil(filtered.length / pageSize));
          var pageRows = filtered.slice((page-1)*pageSize, page*pageSize);

          function exportFiltered(){
            downloadCSV(filtered, "trend_filtered_export.csv");
          }

          function onReasonClick(reason){
            if(!reason) { setSelectedReason(""); return; }
            if(selectedReason === reason) setSelectedReason(""); else setSelectedReason(reason);
            setPage(1);
          }

          // open evidence modal (explicit Evidence button)
          async function openEvidence(row){
            setModalRow(row);
            setModalDetails(null);
            setModalLoading(true);
            try {
              const q = encodeURIComponent(row.EmployeeID || row.person_uid || "");
              const resp = await fetch(API_BASE + "/record?employee_id=" + q);
              if(!resp.ok){
                const txt = await resp.text();
                throw new Error("record failed: " + resp.status + " - " + txt);
              }
              const js = await resp.json();
              // js.raw_swipes is already filtered and shaped by backend
              const details = { aggregated_rows: js.aggregated_rows || [], raw_swipe_files: js.raw_swipe_files || [], raw_swipes: js.raw_swipes || [] };
              setModalDetails(details);
            } catch(e){
              alert("Failed loading details: " + e.message);
              console.error(e);
            } finally {
              setModalLoading(false);
            }
          }

          function closeModal(){ setModalRow(null); setModalDetails(null); }

          // convenience counts for cards
          var rowsCount = (summary && typeof summary.rows === 'number') ? summary.rows : (rows ? rows.length : 0);
          var flaggedCount = (summary && typeof summary.flagged_rows === 'number') ? summary.flagged_rows : (rows ? rows.filter(function(r){ return !!r.Reasons; }).length : 0);
          var flaggedPct = rowsCount ? Math.round((flaggedCount*100)/(rowsCount||1)) : 0;

          // overlap render
          function renderOverlapCell(r){
            var ov = r.OverlapWith || r.swipe_overlap || r.overlap_with || null;
            if(ov && typeof ov === 'string'){
              var parts = ov.split(";").map(function(s){ return s.trim(); }).filter(Boolean);
              if(parts.length === 0) return <span className="muted">—</span>;
              return <span className="overlap-pill" title={ov}>{parts.length} overlap</span>;
            } else if(r.swipe_overlap === true || r.swipe_overlap === "True"){
              return <span className="overlap-pill">overlap</span>;
            }
            return <span className="muted">—</span>;
          }

          function renderReasonChips(reasonText){
            if(!reasonText) return <span className="muted">—</span>;
            const parts = String(reasonText).split(";").map(s=>s.trim()).filter(Boolean);
            return parts.map((p,idx)=>(<span key={idx} className="pill" title={SCENARIO_EXPLANATIONS[p] || p}>{p}</span>));
          }

          function renderReasonExplanations(reasonText){
            if(!reasonText) return <div className="muted">No flags</div>;
            const parts = String(reasonText).split(";").map(s=>s.trim()).filter(Boolean);
            return (
              <div>
                {parts.map((p,idx)=>(
                  <div key={idx} style={{marginBottom:6}}>
                    <b>{p}</b> — <span className="small">{ SCENARIO_EXPLANATIONS[p] || "No explanation available." }</span>
                  </div>
                ))}
              </div>
            );
          }

          function renderSwipeTimeline(details){
            if(!details || !details.raw_swipes || details.raw_swipes.length === 0){
              return <div className="muted">No raw swipe evidence available (person not flagged or raw file missing).</div>;
            }
            // sort by Date+Time
            let all = details.raw_swipes.slice();
            all.sort(function(a,b){
              const at = a.Date && a.Time ? new Date(a.Date + "T" + a.Time) : null;
              const bt = b.Date && b.Time ? new Date(b.Date + "T" + b.Time) : null;
              if(at && bt) return at - bt;
              if(at) return -1;
              if(bt) return 1;
              return 0;
            });

            // If aggregated row contains FirstSwipe/LastSwipe we can highlight those matching entries
            let aggFirst = null, aggLast = null;
            if(modalRow && modalRow.FirstSwipe) aggFirst = new Date(modalRow.FirstSwipe);
            if(modalRow && modalRow.LastSwipe) aggLast = new Date(modalRow.LastSwipe);

            return (
              <div>
                <table className="evidence-table">
                  <thead>
                    <tr>
                      <th>Employee Name</th><th>Employee ID</th><th>Card</th><th>Date</th><th>Time</th><th>Door</th><th>Direction</th><th>Note</th>
                    </tr>
                  </thead>
                  <tbody>
                    { all.map(function(rObj, idx){
                        const r = rObj;
                        let highlight = false;
                        try {
                          if(aggFirst && r.Date && r.Time){
                            highlight = highlight || (Math.abs(new Date(r.Date + "T" + r.Time) - aggFirst) <= 1500);
                          }
                          if(aggLast && r.Date && r.Time){
                            highlight = highlight || (Math.abs(new Date(r.Date + "T" + r.Time) - aggLast) <= 1500);
                          }
                        } catch(e){}
                        return (
                          <tr key={idx} className={highlight ? 'highlight-swipe' : ''}>
                            <td className="small">{ r.EmployeeName || '-' }</td>
                            <td className="small">{ r.EmployeeID || '-' }</td>
                            <td className="small">{ r.CardNumber || '-' }</td>
                            <td className="small">{ r.Date || '-' }</td>
                            <td className="small">{ r.Time || '-' }</td>
                            <td className="small">{ r.Door || '-' }</td>
                            <td className="small">{ r.Direction || '-' }</td>
                            <td className="small">{ r.Note || '-' } { r._source ? <span className="muted">({r._source})</span> : null }</td>
                          </tr>
                        );
                      }) }
                  </tbody>
                </table>
              </div>
            );
          }

          return (
            <div className="container" role="main" aria-live="polite">
              { loading && <div className="spinner"><div style={{padding:16, background:'#fff', borderRadius:8, boxShadow:'0 4px 20px rgba(2,6,23,0.1)'}}>Loading…</div></div> }

              <header>
                <h1>Trend Analysis — Pune</h1>
                <div className="controls">
                  <label className="small">From</label>
                  <input type="date" value={dateFrom} onChange={function(e){ setDateFrom(e.target.value); }} disabled={loading} />
                  <label className="small">To</label>
                  <input type="date" value={dateTo} onChange={function(e){ setDateTo(e.target.value); }} disabled={loading} />
                  <button onClick={runForRange} disabled={loading}>Run (date/range)</button>
                  <button className="secondary" onClick={loadLatest} disabled={loading}>Load latest</button>
                  <button className="ghost" onClick={exportFiltered} disabled={loading}>Export filtered</button>
                </div>
              </header>

              <div className="cards" aria-hidden={loading}>
                <div className="card">
                  <h3>{ (rowsCount !== undefined && rowsCount !== null) ? rowsCount : 0 }</h3>
                  <p>Rows analysed</p>
                </div>
                <div className="card">
                  <h3>{ (flaggedCount !== undefined && flaggedCount !== null) ? flaggedCount : 0 }</h3>
                  <p>Flagged rows</p>
                </div>
                <div className="card">
                  <h3>{ flaggedPct }%</h3>
                  <p>Flagged rate</p>
                </div>
              </div>

              <div style={{marginTop:12}}>
                <strong>Top reasons (click to filter)</strong>
                <div style={{marginTop:8}}>
                  { Object.keys(reasonsCount).length === 0 && <span className="muted">No flags found</span> }
                  { Object.entries(reasonsCount).sort(function(a,b){ return b[1]-a[1]; }).slice(0,18).map(function(kv){
                      var name = kv[0], count = kv[1];
                      var active = selectedReason === name;
                      return <button key={name} className="chip" style={{background: active ? '#dbeafe' : undefined, border: active ? '1px solid #60a5fa' : undefined}} onClick={function(){ onReasonClick(name); }}>{name} <span style={{marginLeft:8, opacity:0.8}} className="small">({count})</span></button>;
                  }) }
                </div>
              </div>

              <div className="main">
                <div className="left">
                  <div className="chart-wrap" style={{height:260}}>
                    <canvas ref={chartRef}></canvas>
                  </div>

                  <div className="searchbar">
                    <input placeholder="Search name, employee id, card or reason..." value={filterText} onChange={function(e){ setFilterText(e.target.value); setPage(1); }} />
                    <div className="muted">Showing { filtered.length } / { rows.length } rows</div>
                  </div>

                  <table>
                    <thead>
                      <tr>
                        <th>Employee</th>
                        <th className="small">ID</th>
                        <th className="small">Card</th>
                        <th className="small">Date</th>
                        <th className="small">Duration</th>
                        <th className="small">Reasons</th>
                        <th className="small">Overlap</th>
                        <th className="small">Evidence</th>
                      </tr>
                    </thead>
                    <tbody>
                      { pageRows.map(function(r, idx){
                          var empName = sanitizeName(r);
                          var displayDate = safeDateDisplay(r.Date || r.FirstSwipe || r.LastSwipe);
                          var durText = r.Duration || (r.DurationMinutes ? Math.round(r.DurationMinutes) + " min" : "");
                          var flagged = r.Reasons && String(r.Reasons).trim();
                          return (
                            <tr key={idx} className={ flagged ? "flagged-row" : "" }>
                              <td className="row-click" onClick={function(){ openEvidence(r); }}>{ empName || <span className="muted">—</span> }</td>
                              <td className="small">{ r.EmployeeID || "" }</td>
                              <td className="small">{ r.CardNumber || "" }</td>
                              <td className="small">{ displayDate }</td>
                              <td className="small">{ durText }</td>
                              <td className="small">{ renderReasonChips(r.Reasons) }</td>
                              <td className="small">{ renderOverlapCell(r) }</td>
                              <td className="small">
                                <button className="evidence-btn" onClick={function(){ openEvidence(r); }}>Evidence</button>
                              </td>
                            </tr>
                        );
                      }) }
                    </tbody>
                  </table>

                  <div className="pagination">
                    <button onClick={function(){ setPage(function(p){ return Math.max(1,p-1); }); }} disabled={page<=1}>Prev</button>
                    <div className="muted">Page { page } / { totalPages }</div>
                    <button onClick={function(){ setPage(function(p){ return Math.min(totalPages,p+1); }); }} disabled={page>=totalPages}>Next</button>
                  </div>
                </div>

                <aside className="right">
                  <div style={{marginBottom:12}}>
                    <strong>Files:</strong>
                    <div className="muted">{ (summary.files || []).join(", ") }</div>
                  </div>

                  <div style={{marginBottom:12}}>
                    <strong>Top reasons summary</strong>
                    <ul>
                      { Object.entries(reasonsCount).sort(function(a,b){ return b[1]-a[1]; }).slice(0,10).map(function(kv){
                          return <li key={kv[0]}><b>{kv[1]}</b> — <span className="small">{kv[0]}</span></li>;
                        }) }
                      { Object.keys(reasonsCount).length === 0 && <div className="muted">No flags found</div> }
                    </ul>
                  </div>

                  <div>
                    <strong>Help</strong>
                    <div className="small muted" style={{marginTop:6}}>
                      - Click <b>Run</b> to trigger analysis for chosen date(s).<br/>
                      - Range calls `/run?date=YYYY-MM-DD` for each date in the range sequentially.<br/>
                      - Click the Evidence button to view raw swipe evidence filtered to the selected person/date.<br/>
                      - If you see CORS errors, enable Flask-Cors on the backend or run the frontend from the same host as API.
                    </div>
                  </div>
                </aside>
              </div>

              { modalRow &&
                <div className="modal" onClick={closeModal}>
                  <div className="modal-inner" onClick={function(e){ e.stopPropagation(); }}>
                    <button className="close-btn" onClick={closeModal}>Close</button>
                    <h3>Details — Evidence</h3>

                    { modalLoading && <div className="muted">Loading evidence…</div> }

                    <div style={{marginTop:8}}>
                      <strong>Name:</strong> { sanitizeName(modalRow) } <br/>
                      <strong>EmployeeID:</strong> { modalRow.EmployeeID || "—" } <br/>
                      <strong>Card:</strong> { modalRow.CardNumber || "—" } <br/>
                      <strong>Date:</strong> { safeDateDisplay(modalRow.Date || modalRow.FirstSwipe) } <br/>
                      <strong>Duration:</strong> { modalRow.Duration || (modalRow.DurationMinutes ? Math.round(modalRow.DurationMinutes) + " min" : "—") } <br/>
                      <strong>Reasons:</strong> { renderReasonChips(modalRow.Reasons) } <br/>
                      <strong>OverlapWith:</strong> { modalRow.OverlapWith || modalRow.swipe_overlap || "—" } <br/>
                    </div>

                    <div className="explain">
                      <strong>Why highlighted</strong>
                      <div style={{marginTop:6}}>
                        { renderReasonExplanations(modalRow.Reasons) }
                      </div>
                    </div>

                    <hr/>

                    <h4>Available evidence files</h4>
                    <div style={{marginTop:8}}>
                      { modalDetails && modalDetails.raw_swipe_files && modalDetails.raw_swipe_files.length > 0
                        ? <div>
                            <ul>
                              { modalDetails.raw_swipe_files.map((f,i)=>(
                                <li key={i}><b>{f}</b> — <button onClick={function(){ window.location = API_BASE + "/swipes/" + encodeURIComponent(f); }}>Download</button></li>
                              )) }
                            </ul>
                          </div>
                        : <div className="muted">No raw swipe files found for this person/date.</div>
                      }
                    </div>

                    <div style={{marginTop:12}}>
                      <strong>Swipe timeline (filtered for this person/date)</strong>
                      <div style={{marginTop:8}}>
                        { modalDetails ? renderSwipeTimeline(modalDetails) : <div className="muted">Evidence not loaded yet.</div> }
                      </div>
                    </div>

                    <hr/>
                    <div style={{marginTop:8}}>
                      <label><input type="checkbox" id="showraw" onChange={function(e){
                        const el = document.getElementById('rawpayload');
                        if(el) el.style.display = e.target.checked ? 'block' : 'none';
                      }} /> Show raw aggregated JSON</label>
                      <div id="rawpayload" style={{display:'none', marginTop:8}}><pre>{ JSON.stringify(modalRow, null, 2) }</pre></div>
                    </div>

                  </div>
                </div>
              }

            </div>
          );
        }

        const root = ReactDOM.createRoot(document.getElementById('root'));
        root.render(React.createElement(App));
      })();
    </script>
  </body>
</html>







# backend/logic.py
import pandas as pd
import numpy as np
import logging
from pathlib import Path

PROFILE_PATH = Path(__file__).parent / "current_analysis.csv"
if PROFILE_PATH.exists():
    try:
        employee_profile = pd.read_csv(PROFILE_PATH)
        logging.info("logic.py: loaded historical profile (%d rows)", len(employee_profile))
    except Exception as e:
        logging.warning("logic.py: failed to read current_analysis.csv: %s", e)
        employee_profile = pd.DataFrame()
else:
    logging.warning("logic.py: current_analysis.csv not found; history-based checks will be limited.")
    employee_profile = pd.DataFrame()


def _safe_get_emp_hist(employee_id):
    if employee_profile.empty or employee_id is None or (isinstance(employee_id, float) and np.isnan(employee_id)):
        return pd.DataFrame()
    # normalize employee_id comparison as string/int
    try:
        empid_str = str(employee_id).strip()
    except Exception:
        empid_str = employee_id
    if 'EmployeeID' in employee_profile.columns:
        # try numeric match first
        try:
            # if both numeric/int equal
            emp_numeric = None
            try:
                emp_numeric = float(empid_str)
            except Exception:
                emp_numeric = None
            if emp_numeric is not None and not np.isnan(emp_numeric):
                # try numeric comparison when employee_profile EmployeeID numeric-like
                try:
                    profile_nums = pd.to_numeric(employee_profile['EmployeeID'], errors='coerce')
                    mask = (profile_nums == emp_numeric)
                    res = employee_profile[mask]
                    if not res.empty:
                        return res
                except Exception:
                    pass
        except Exception:
            pass
        # fallback to string match
        try:
            return employee_profile[employee_profile['EmployeeID'].astype(str).str.strip() == empid_str]
        except Exception:
            return pd.DataFrame()
    return pd.DataFrame()


def flag_employee(row_dict):
    """
    Return (is_flagged: bool, reasons: list[str])
    This is an extra per-employee heuristic (uses historical profile if available).
    It will return (False, []) when there is no historical data; do not return stray reasons.
    """
    employee_id = row_dict.get('EmployeeID')
    personnel_type = row_dict.get('PersonnelType') or row_dict.get('PersonnelTypeName')
    try:
        days_present = int(row_dict.get('DaysPresentInWeek') or 0)
    except Exception:
        days_present = 0

    logging.info("flag_employee: checking EmployeeID=%s", employee_id)

    emp_hist = _safe_get_emp_hist(employee_id)
    if emp_hist.empty:
        # do not fabricate a reason when history missing — caller can still combine other signals
        return False, []

    reasons = []

    # example checks
    try:
        if row_dict.get('InTime') and row_dict.get('OutTime'):
            reasons.append("Has both InTime and OutTime (possible short cycles)")
    except Exception:
        pass

    if int(row_dict.get("OnlyIn", 0) or 0) == 1:
        reasons.append("OnlyIn entry detected")
    if int(row_dict.get("OnlyOut", 0) or 0) == 1:
        reasons.append("OnlyOut entry detected")
    if int(row_dict.get("SingleDoor", 0) or 0) == 1:
        reasons.append("SingleDoor usage detected")

    if personnel_type and str(personnel_type).strip().lower() == "employee":
        is_defaulter = row_dict.get("Defaulter", "No")
        if str(is_defaulter).strip().lower() == "yes":
            reasons.append("Flagged as Defaulter by company policy")

    # historical median/std checks for numeric metrics
    metric_column_map = {
        'DurationMinutes': ('AvgDurationMins_median', 'AvgDurationMins_std'),
        'TotalSwipes': ('TotalSwipes_median', 'TotalSwipes_std')
    }

    for metric, (median_col, std_col) in metric_column_map.items():
        try:
            live_val = row_dict.get(metric)
            if live_val is None or (isinstance(live_val, float) and np.isnan(live_val)):
                # don't automatically add reason; only when metric is required
                continue
            if median_col in emp_hist.columns and std_col in emp_hist.columns:
                median_val = emp_hist.iloc[0].get(median_col, np.nan)
                std_val = emp_hist.iloc[0].get(std_col, np.nan)
                if pd.notna(median_val) and pd.notna(std_val) and std_val >= 0:
                    try:
                        median_val = float(median_val)
                        std_val = float(std_val)
                        buffer = 2.5 * std_val
                        if float(live_val) < median_val - buffer or float(live_val) > median_val + buffer:
                            reasons.append(f"Abnormal {metric}: {live_val} outside expected [{median_val-buffer:.1f}, {median_val+buffer:.1f}]")
                    except Exception:
                        pass
        except Exception as e:
            logging.debug("flag_employee: error evaluating metric %s: %s", metric, e)

    try:
        duration = float(row_dict.get('DurationMinutes') or 0)
        if days_present < 3 and duration < 480:
            reasons.append("Duration < 8 hours on limited office days")
    except Exception:
        pass

    return (len(reasons) > 0), reasons




