# ----------------- START OF FILE: trend_runner.py -----------------
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re

# IMPORTANT: duration_report must exist and expose run_for_date(date, regions, outdir, city)
from duration_report import run_for_date

# historical profile (optional)
HIST_PATH = Path(__file__).parent / "current_analysis.csv"
if HIST_PATH.exists():
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()
else:
    logging.warning("Historical profile file current_analysis.csv not found; history-based scenarios will fallback.")
    HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)


# ----- small shared helpers: treat empty/placeholder tokens as None -----
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


def _normalize_id_val(v):
    """
    Normalize an id-like value: strip, convert floats like '320172.0' -> '320172'
    Return None for NaN/empty/placeholder.
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    # strip .0 if integer-like
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s


# prefer to avoid emp:<GUID> person_uids — only treat emp: if value looks like a human id (not GUID)
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')

def _looks_like_guid(s: object) -> bool:
    """Return True if s looks like a GUID/UUID string."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    """Heuristic: treat as a plausible human name if it contains letters and not a GUID."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        # reject GUIDs and obviously numeric ids
        if _looks_like_guid(st):
            return False
        # require at least one alphabetic character
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    """Pick the first non-null, non-GUID, non-placeholder value from a pandas Series (as str) or None."""
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    """
    Create canonical person uid:
      - prefer EmployeeID (normalized) -> 'emp:<id>' only if it is not a GUID
      - else EmployeeIdentity -> 'uid:<val>' (GUID allowed)
      - else EmployeeName -> hash-based 'name:<shorthash>'
    """
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        # stable short hash of name
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None


# small helper to extract Card from XML-like strings
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        # fallback: look for CHUID ... Card: pattern or Card: 12345
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


# --- compute_features (replaced/updated) ---
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    Returns DataFrame per (person_uid, Date) with feature columns and normalized IDs/names.
    This implementation follows the duration_report column conventions and avoids
    treating GUIDs or placeholder tokens as EmployeeName/EmployeeID/CardNumber.
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # Build lowercase->actual column map for flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}

    # detect time column
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)
    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
        sw['Date'] = sw['LocaleMessageTime'].dt.date
    else:
        if 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
        else:
            sw['Date'] = None

    # find these earlier in compute_features — prefer Int1/Text12 for EmployeeID and CHUID/Card for CardNumber
    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    # Filter personnel types: prefer PersonnelTypeName, fallback to PersonnelType
    if 'PersonnelTypeName' in sw.columns:
        sw = sw[sw['PersonnelTypeName'].isin(['Employee', 'Terminated Personnel'])]
    elif 'PersonnelType' in sw.columns:
        sw = sw[sw['PersonnelType'].isin(['Employee', 'Terminated Personnel'])]
    # else keep everything

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # ensure stable person_uid (canonical)
    if 'person_uid' not in sw.columns:
        def make_person_uid_local(r):
            # prefer canonical EmployeeID (normalized, non-GUID) then EmployeeIdentity then EmployeeName
            empid_val = None
            if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
                empid_val = r.get(empid_col)
            elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
                empid_val = r.get('EmployeeID')

            empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
            name_val = None
            if name_col and name_col in r:
                name_val = r.get(name_col)
            elif 'EmployeeName' in r:
                name_val = r.get('EmployeeName')
            elif 'ObjectName1' in r:
                name_val = r.get('ObjectName1')

            return _canonical_person_uid({
                'EmployeeID': empid_val,
                'EmployeeIdentity': empident_val,
                'EmployeeName': name_val
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    # selection columns for aggregation: include discovered columns
    sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
                    'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
                    'EmployeeIdentity'])
    if name_col:
        sel_cols.add(name_col)
    if empid_col:
        sel_cols.add(empid_col)
    if card_col:
        sel_cols.add(card_col)
    if door_col:
        sel_cols.add(door_col)
    if dir_col:
        sel_cols.add(dir_col)
    sel_cols = [c for c in sel_cols if c in sw.columns]

    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0

        # Direction counts (default to column names present)
        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

        # pick first non-placeholder, non-guid card number if present (prefer cardnumber/chuid)
        card_numbers = []
        # 1) direct known column
        if card_col and card_col in g.columns:
            card_numbers = list(pd.unique(g[card_col].dropna()))
        # 2) explicit 'CardNumber' output column (from SQL COALESCE)
        if not card_numbers and 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna()))
        # 3) some XML-shred columns may appear as 'value' or other column names
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
                    try:
                        vals = list(pd.unique(g[c].dropna()))
                        if vals:
                            # extend but prefer first real-looking
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        # 4) lastly try to extract from XmlMessage fields
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'xml' in cl or 'xmlmessage' in cl or 'xml_msg' in cl or 'xmlmessage' in cl:
                    for raw in g[c].dropna().astype(str):
                        extracted = _extract_card_from_xml(raw)
                        if extracted:
                            card_numbers.append(extracted)
        # 5) final unique
        card_numbers = list(dict.fromkeys(card_numbers))  # preserve order, unique

        card_number = None
        for c in card_numbers:
            n = _normalize_id_val(c)
            # explicitly reject GUIDs as card numbers
            if n and not _looks_like_guid(n):
                card_number = n
                break

        # stable id/name from the group using discovered columns first
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None

        # Employee ID: prefer Int1/Text12 then EmployeeID; DO NOT use EmployeeIdentity as EmployeeID
        # use _pick_first_non_guid_value to skip GUIDs automatically
        if empid_col and empid_col in g.columns:
            vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                # final trial: numeric normalization (strip .0) but still reject GUIDs
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        elif 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized

        # If still no employee_id and PersonnelType indicates contractor -> prefer Text12 explicitly
        if (not employee_id) and 'PersonnelType' in g.columns:
            try:
                pvals = g['PersonnelType'].dropna().astype(str)
                if not pvals.empty:
                    p0 = pvals.iloc[0]
                    if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
                        # look for text12 explicitly (case-insensitive)
                        for c in g.columns:
                            if c.lower() == 'text12':
                                vals = g[c].dropna().astype(str).map(lambda x: x.strip())
                                employee_id = _pick_first_non_guid_value(vals)
                                if employee_id:
                                    break
            except Exception:
                pass

        # Employee identity (GUID) — keep but do not promote to EmployeeID
        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]

        # Employee name: pick non-GUID candidate
        candidate_name_vals = None
        if name_col and name_col in g.columns:
            candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
        elif 'EmployeeName' in g.columns:
            candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
        elif 'ObjectName1' in g.columns:
            candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

        if candidate_name_vals is not None and not candidate_name_vals.empty:
            employee_name = _pick_first_non_guid_value(candidate_name_vals)
            if employee_name is None:
                # accept any value that looks like a name
                for v in candidate_name_vals:
                    if _looks_like_name(v) and not _is_placeholder_str(v):
                        employee_name = str(v).strip()
                        break

        # personnel type
        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        # First/Last swipe times
        first_swipe = None
        last_swipe = None
        if times:
            first_swipe = times[0]
            last_swipe = times[-1]

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type,
            'FirstSwipe': first_swipe,
            'LastSwipe': last_swipe
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = grouped.apply(agg_swipe_group).reset_index()

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # --- START PATCH: coalesce duplicate columns produced by merge ---
    # When merging grouped + durations we may get *_x / *_y columns.
    # Coalesce them into canonical column names (prefer _x then _y), then remove suffixes.
    def _coalesce_merge_columns(df, bases):
        for base in bases:
            x = base + "_x"
            y = base + "_y"
            # If canonical exists and has non-null values, keep it; else build from x/y
            try:
                has_base = base in df.columns
                base_all_null = False
                if has_base:
                    base_all_null = df[base].isnull().all()
            except Exception:
                has_base = base in df.columns
                base_all_null = True

            if (not has_base) or base_all_null:
                if x in df.columns and y in df.columns:
                    try:
                        df[base] = df[x].combine_first(df[y])
                    except Exception:
                        # fallback: prefer x then y by simple fillna
                        try:
                            df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
                        except Exception:
                            # last resort - assign x or y raw
                            if x in df.columns:
                                df[base] = df[x]
                            elif y in df.columns:
                                df[base] = df[y]
                elif x in df.columns:
                    df[base] = df[x]
                elif y in df.columns:
                    df[base] = df[y]
    # coalesce common id/name/card columns we expect
    _coalesce_merge_columns(merged, [
        "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
    ])
    # Drop any remaining suffix columns
    drop_cols = [c for c in merged.columns if c.endswith("_x") or c.endswith("_y")]
    if drop_cols:
        try:
            merged.drop(columns=drop_cols, inplace=True)
        except Exception:
            for c in drop_cols:
                if c in merged.columns:
                    try:
                        merged.drop(columns=[c], inplace=True)
                    except Exception:
                        pass
    # --- END PATCH ---

    # coalesce helpers (ensure column existence)
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)

    # If EmployeeName is missing or a GUID, try to get a better name from durations (durations typically has EmployeeName)
    # After coalescing we no longer expect _x/_y suffixes; prefer a valid human name from available columns.
    if 'EmployeeName' in merged.columns:
        def choose_best_name(row):
            gname = row.get('EmployeeName')
            # try also duration-side name tokens that may have different column names
            dname = None
            for cand in ('EmployeeName', 'employee_name', 'objectname1', 'ObjectName1'):
                if cand in row and row.get(cand) is not None:
                    dname = row.get(cand)
                    break
            if _looks_like_name(gname):
                return str(gname).strip()
            if _looks_like_name(dname):
                return str(dname).strip()
            # if grouped name exists but is GUID-like, prefer duration's name if available
            if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
                return str(gname).strip()
            if dname and not _is_placeholder_str(dname):
                return str(dname).strip()
            return None
        merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
    else:
        # fallback: try to use 'EmployeeName' from durations if our EmployeeName is missing/invalid
        if not dur.empty:
            def fill_name_from_dur(row):
                gname = row.get('EmployeeName')
                if _looks_like_name(gname) and not _is_placeholder_str(gname):
                    return gname
                # try find matching row in durations (person_uid + date) - already merged, so duration name may be in other columns
                for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
                    if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
                        return row[cand]
                return None
            merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

    # numeric normalization for EmployeeID: ensure not GUIDs/placeholder, convert floats like '320172.0' -> '320172'
    def normalize_empid(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            # strip .0 integer floats
            try:
                if '.' in s:
                    f = float(s)
                    if math.isfinite(f) and f.is_integer():
                        return str(int(f))
            except Exception:
                pass
            return s
        except Exception:
            return None

    merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

    # normalize card numbers: reject GUIDs and placeholder tokens
    def normalize_card(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            return s
        except Exception:
            return None

    merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

    # numeric normalization
    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)

    # ensure FirstSwipe/LastSwipe are datetimes
    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # EmpHistoryPresent
    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    # normalize string columns for safe downstream use; EmployeeName keep as readable-only
    for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
        if c in merged.columns:
            def _clean_str_val(v):
                if v is None:
                    return None
                try:
                    s = str(v).strip()
                    if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                        return None
                    return s
                except Exception:
                    return None
            merged[c] = merged[c].apply(_clean_str_val)

    # EmployeeName: keep None if empty or GUID/placeholder; otherwise string.
    if 'EmployeeName' in merged.columns:
        merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

    return merged


# ---------------- SCENARIOS (boolean functions) ----------------
def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
    except Exception:
        pass
    if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    return cur > 50

def scenario_repeated_short_breaks(row):
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map


# scenario list (name, fn)
SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap)
]


def run_trend_for_date(target_date: date, outdir: str = "./outputs", city='Pune'):
    logging.info("run_trend_for_date: date=%s (city=%s)", target_date, city)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city=city)
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    # save raw swipes for evidence (full raw)
    try:
        if swipes is not None and not swipes.empty:
            sw_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception as e:
        logging.warning("Failed to save raw swipes: %s", e)

    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # Build badge map and swipe overlap maps for higher-severity scenarios
    badge_map = {}
    if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}

    swipe_overlap_map = {}
    overlap_window_seconds = 2
    if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
        tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
        if not tmp.empty:
            tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
            for (d, door), g in tmp.groupby(['Date', 'Door']):
                items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                n = len(items)
                for i in range(n):
                    t_i, uid_i = items[i]
                    j = i+1
                    while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                        uid_j = items[j][1]
                        if uid_i != uid_j:
                            swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                            swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                        j += 1

    # Evaluate scenarios (use weighting to compute anomaly score)
    WEIGHTS = {
        "long_gap_>=90min": 0.3,
        "short_duration_<4h": 1.0,
        "coffee_badging": 1.0,
        "low_swipe_count_<=2": 0.5,
        "single_door": 0.25,
        "only_in": 0.8,
        "only_out": 0.8,
        "overtime_>=10h": 0.2,
        "very_long_duration_>=16h": 1.5,
        "zero_swipes": 0.4,
        "unusually_high_swipes": 1.5,
        "repeated_short_breaks": 0.5,
        "multiple_location_same_day": 0.6,
        "weekend_activity": 0.6,
        "repeated_rejection_count": 0.8,
        "badge_sharing_suspected": 2.0,
        "early_arrival_before_06": 0.4,
        "late_exit_after_22": 0.4,
        "shift_inconsistency": 1.2,
        "trending_decline": 0.7,
        "consecutive_absent_days": 1.2,
        "high_variance_duration": 0.8,
        "short_duration_on_high_presence_days": 1.1,
        "swipe_overlap": 2.0
    }
    ANOMALY_THRESHOLD = 1.5

    # evaluate scenarios and compute score
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            features[name] = features.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
        elif name == "swipe_overlap":
            features[name] = features.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map), axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    def compute_score(r):
        score = 0.0
        detected = []
        for name, _ in SCENARIOS:
            val = bool(r.get(name))
            w = WEIGHTS.get(name, 0.0)
            if val and w > 0:
                score += float(w)
                detected.append(name)
        return score, detected

    scores = features.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
    features['AnomalyScore'] = scores['AnomalyScore'].astype(float)
    features['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
    features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

    def reasons_for_row(r):
        if not bool(r.get('IsFlagged')):
            return None
        ds = r.get('DetectedScenarios')
        if ds:
            return ds
        return None
    features['Reasons'] = features.apply(reasons_for_row, axis=1)

    if 'OverlapWith' not in features.columns:
        def overlap_with_fn(r):
            d = r.get('Date')
            uid = r.get('person_uid')
            if (d, uid) in swipe_overlap_map:
                return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
            return None
        features['OverlapWith'] = features.apply(overlap_with_fn, axis=1)

    # Remove suffix columns and fix duplicates
    cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
    if cols_to_drop:
        for c in cols_to_drop:
            base = c[:-2]
            if base in features.columns:
                try:
                    features.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                try:
                    features.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass
    features = features.loc[:, ~features.columns.duplicated()]

    # ensure booleans are native Python (avoid numpy.bool_)
    for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
        if col in features.columns:
            features[col] = features[col].astype(bool)

    # write CSV with native types
    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    try:
        write_df = features.copy()
        # FirstSwipe/LastSwipe -> ISO strings
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        # Date -> ISO date
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        write_df = write_df.where(pd.notnull(write_df), None)
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception as e:
        logging.exception("Failed to write trend CSV: %s", e)

    return features


# ---------------- training dataset builder (restored) ----------------
def build_monthly_training(end_date: date = None, months: int = 3, min_unique_employees: int = 1000,
                           outdir: str = "./outputs", city: str = "Pune"):
    if end_date is None:
        end_date = datetime.now().date()
    logging.info("build_monthly_training: end_date=%s months=%d min_unique=%d", end_date, months, min_unique_employees)
    outdir = Path(outdir)
    month_windows = []
    cur = end_date.replace(day=1)
    for _ in range(months):
        start = cur
        next_month = (cur.replace(day=28) + timedelta(days=4)).replace(day=1)
        last = next_month - timedelta(days=1)
        month_windows.append((start, last))
        cur = (start - timedelta(days=1)).replace(day=1)

    person_month_rows = []
    unique_persons = set()

    for start, last in month_windows:
        d = start
        month_dfs = []
        while d <= last:
            csv_path = outdir / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
            if csv_path.exists():
                try:
                    df = pd.read_csv(csv_path)
                    month_dfs.append(df)
                except Exception:
                    try:
                        df = pd.read_csv(csv_path, dtype=str)
                        month_dfs.append(df)
                    except Exception as e:
                        logging.warning("Failed reading %s: %s", csv_path, e)
            d = d + timedelta(days=1)

        if not month_dfs:
            logging.info("No daily trend CSVs found for month %s - %s", start.isoformat(), last.isoformat())
            continue

        month_df = pd.concat(month_dfs, ignore_index=True)
        # ensure person_uid exists
        if 'person_uid' not in month_df.columns:
            def make_person_uid(row):
                parts = []
                for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                    v = row.get(c)
                    if pd.notna(v) and str(v).strip():
                        parts.append(str(v).strip())
                return "|".join(parts) if parts else None
            month_df['person_uid'] = month_df.apply(make_person_uid, axis=1)

        # convert boolean columns to int for aggregation if necessary
        for name, _ in SCENARIOS:
            if name in month_df.columns:
                month_df[name] = month_df[name].astype(int)

        agg_funcs = {
            'CountSwipes': ['median', 'mean', 'sum'],
            'DurationMinutes': ['median', 'mean', 'sum'],
            'MaxSwipeGapSeconds': ['max', 'median'],
            'ShortGapCount': ['sum'],
            'UniqueDoors': ['median'],
            'UniqueLocations': ['median'],
            'RejectionCount': ['sum']
        }
        scenario_cols = [name for name,_ in SCENARIOS if name in month_df.columns]
        group_cols = ['person_uid']
        grp = month_df.groupby(group_cols)

        for person, g in grp:
            row = {}
            row['person_uid'] = person
            row['EmployeeID'] = next((v for v in g.get('EmployeeID', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['EmployeeName'] = next((v for v in g.get('EmployeeName', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['MonthStart'] = start.isoformat()
            row['MonthEnd'] = last.isoformat()
            for col, funcs in agg_funcs.items():
                if col in g.columns:
                    for f in funcs:
                        key = f"{col}_{f}"
                        try:
                            val = getattr(g[col], f)()
                            row[key] = float(val) if pd.notna(val) else None
                        except Exception:
                            row[key] = None
                else:
                    for f in funcs:
                        row[f"{col}_{f}"] = None
            for s in scenario_cols:
                row[f"{s}_days"] = int(g[s].sum())
                row[f"{s}_label"] = int(g[s].sum() > 0)
            row['days_present'] = int(g.shape[0])
            person_month_rows.append(row)
            unique_persons.add(person)

        if len(unique_persons) >= min_unique_employees:
            logging.info("Reached min unique employees=%d, stopping aggregation early", min_unique_employees)
            break

    if not person_month_rows:
        logging.warning("No person-month rows created (no data).")
        return None

    training_df = pd.DataFrame(person_month_rows)
    train_out = outdir / "training_person_month.csv"
    training_df.to_csv(train_out, index=False)
    logging.info("Saved training CSV to %s (rows=%d unique_persons=%d)", train_out, len(training_df), len(unique_persons))
    return train_out


if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today)
    print("Completed; rows:", len(df) if df is not None else 0)
# ----------------- END OF FILE: trend_runner.py -----------------
















When i update this we got below error fix this error carefully and share me Fully updated file so i can easily swap file each other

(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python app.py
>>
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 12, in <module>
    from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 466
    def ensure_col(df, col, default=None):
IndentationError: unexpected indent
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> 





#C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py

# ----------------- START OF FILE: trend_runner.py -----------------
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re

# IMPORTANT: duration_report must exist and expose run_for_date(date, regions, outdir, city)
from duration_report import run_for_date

# historical profile (optional)
HIST_PATH = Path(__file__).parent / "current_analysis.csv"
if HIST_PATH.exists():
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()
else:
    logging.warning("Historical profile file current_analysis.csv not found; history-based scenarios will fallback.")
    HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)


# ----- small shared helpers: treat empty/placeholder tokens as None -----
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


def _normalize_id_val(v):
    """
    Normalize an id-like value: strip, convert floats like '320172.0' -> '320172'
    Return None for NaN/empty/placeholder.
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    # strip .0 if integer-like
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s


# prefer to avoid emp:<GUID> person_uids — only treat emp: if value looks like a human id (not GUID)
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')

def _looks_like_guid(s: object) -> bool:
    """Return True if s looks like a GUID/UUID string."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    """Heuristic: treat as a plausible human name if it contains letters and not a GUID."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        # reject GUIDs and obviously numeric ids
        if _looks_like_guid(st):
            return False
        # require at least one alphabetic character
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    """Pick the first non-null, non-GUID, non-placeholder value from a pandas Series (as str) or None."""
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    """
    Create canonical person uid:
      - prefer EmployeeID (normalized) -> 'emp:<id>' only if it is not a GUID
      - else EmployeeIdentity -> 'uid:<val>' (GUID allowed)
      - else EmployeeName -> hash-based 'name:<shorthash>'
    """
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        # stable short hash of name
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None


# small helper to extract Card from XML-like strings
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        # fallback: look for CHUID ... Card: pattern or Card: 12345
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


# --- compute_features (replaced/updated) ---
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    Returns DataFrame per (person_uid, Date) with feature columns and normalized IDs/names.
    This implementation follows the duration_report column conventions and avoids
    treating GUIDs or placeholder tokens as EmployeeName/EmployeeID/CardNumber.
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # Build lowercase->actual column map for flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}

    # detect time column
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)
    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
        sw['Date'] = sw['LocaleMessageTime'].dt.date
    else:
        if 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
        else:
            sw['Date'] = None

    # find these earlier in compute_features — prefer Int1/Text12 for EmployeeID and CHUID/Card for CardNumber
    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    # Filter personnel types: prefer PersonnelTypeName, fallback to PersonnelType
    if 'PersonnelTypeName' in sw.columns:
        sw = sw[sw['PersonnelTypeName'].isin(['Employee', 'Terminated Personnel'])]
    elif 'PersonnelType' in sw.columns:
        sw = sw[sw['PersonnelType'].isin(['Employee', 'Terminated Personnel'])]
    # else keep everything

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # ensure stable person_uid (canonical)
    if 'person_uid' not in sw.columns:
        def make_person_uid_local(r):
            # prefer canonical EmployeeID (normalized, non-GUID) then EmployeeIdentity then EmployeeName
            empid_val = None
            if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
                empid_val = r.get(empid_col)
            elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
                empid_val = r.get('EmployeeID')

            empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
            name_val = None
            if name_col and name_col in r:
                name_val = r.get(name_col)
            elif 'EmployeeName' in r:
                name_val = r.get('EmployeeName')
            elif 'ObjectName1' in r:
                name_val = r.get('ObjectName1')

            return _canonical_person_uid({
                'EmployeeID': empid_val,
                'EmployeeIdentity': empident_val,
                'EmployeeName': name_val
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    # selection columns for aggregation: include discovered columns
    sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
                    'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
                    'EmployeeIdentity'])
    if name_col:
        sel_cols.add(name_col)
    if empid_col:
        sel_cols.add(empid_col)
    if card_col:
        sel_cols.add(card_col)
    if door_col:
        sel_cols.add(door_col)
    if dir_col:
        sel_cols.add(dir_col)
    sel_cols = [c for c in sel_cols if c in sw.columns]

    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0

        # Direction counts (default to column names present)
        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

        # pick first non-placeholder, non-guid card number if present (prefer cardnumber/chuid)
        card_numbers = []
        # 1) direct known column
        if card_col and card_col in g.columns:
            card_numbers = list(pd.unique(g[card_col].dropna()))
        # 2) explicit 'CardNumber' output column (from SQL COALESCE)
        if not card_numbers and 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna()))
        # 3) some XML-shred columns may appear as 'value' or other column names
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
                    try:
                        vals = list(pd.unique(g[c].dropna()))
                        if vals:
                            # extend but prefer first real-looking
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        # 4) lastly try to extract from XmlMessage fields
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'xml' in cl or 'xmlmessage' in cl or 'xml_msg' in cl or 'xmlmessage' in cl:
                    for raw in g[c].dropna().astype(str):
                        extracted = _extract_card_from_xml(raw)
                        if extracted:
                            card_numbers.append(extracted)
        # 5) final unique
        card_numbers = list(dict.fromkeys(card_numbers))  # preserve order, unique

        card_number = None
        for c in card_numbers:
            n = _normalize_id_val(c)
            # explicitly reject GUIDs as card numbers
            if n and not _looks_like_guid(n):
                card_number = n
                break

        # stable id/name from the group using discovered columns first
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None

        # Employee ID: prefer Int1/Text12 then EmployeeID; DO NOT use EmployeeIdentity as EmployeeID
        # use _pick_first_non_guid_value to skip GUIDs automatically
        if empid_col and empid_col in g.columns:
            vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                # final trial: numeric normalization (strip .0) but still reject GUIDs
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        elif 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized

        # If still no employee_id and PersonnelType indicates contractor -> prefer Text12 explicitly
        if (not employee_id) and 'PersonnelType' in g.columns:
            try:
                pvals = g['PersonnelType'].dropna().astype(str)
                if not pvals.empty:
                    p0 = pvals.iloc[0]
                    if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
                        # look for text12 explicitly (case-insensitive)
                        for c in g.columns:
                            if c.lower() == 'text12':
                                vals = g[c].dropna().astype(str).map(lambda x: x.strip())
                                employee_id = _pick_first_non_guid_value(vals)
                                if employee_id:
                                    break
            except Exception:
                pass

        # Employee identity (GUID) — keep but do not promote to EmployeeID
        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]

        # Employee name: pick non-GUID candidate
        candidate_name_vals = None
        if name_col and name_col in g.columns:
            candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
        elif 'EmployeeName' in g.columns:
            candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
        elif 'ObjectName1' in g.columns:
            candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

        if candidate_name_vals is not None and not candidate_name_vals.empty:
            employee_name = _pick_first_non_guid_value(candidate_name_vals)
            if employee_name is None:
                # accept any value that looks like a name
                for v in candidate_name_vals:
                    if _looks_like_name(v) and not _is_placeholder_str(v):
                        employee_name = str(v).strip()
                        break

        # personnel type
        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        # First/Last swipe times
        first_swipe = None
        last_swipe = None
        if times:
            first_swipe = times[0]
            last_swipe = times[-1]

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type,
            'FirstSwipe': first_swipe,
            'LastSwipe': last_swipe
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = grouped.apply(agg_swipe_group).reset_index()

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

# --- START PATCH: coalesce duplicate columns produced by merge ---
# When merging grouped + durations we may get *_x / *_y columns.
# Coalesce them into canonical column names (prefer _x then _y), then remove suffixes.
def _coalesce_merge_columns(df, bases):
    for base in bases:
        x = base + "_x"
        y = base + "_y"
        # If canonical exists and has non-null values, keep it; else build from x/y
        if base not in df.columns or df[base].isnull().all():
            if x in df.columns and y in df.columns:
                try:
                    df[base] = df[x].combine_first(df[y])
                except Exception:
                    # fallback: prefer x then y by simple fillna
                    df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
            elif x in df.columns:
                df[base] = df[x]
            elif y in df.columns:
                df[base] = df[y]
    # Drop suffix columns after coalescing
    drop_cols = [c for c in df.columns if c.endswith("_x") or c.endswith("_y")]
    if drop_cols:
        try:
            df.drop(columns=drop_cols, inplace=True)
        except Exception:
            # best-effort drop
            for c in drop_cols:
                if c in df.columns:
                    try:
                        df.drop(columns=[c], inplace=True)
                    except Exception:
                        pass

# coalesce common id/name/card columns we expect
_coalesce_merge_columns(merged, [
    "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
])
# --- END PATCH ---


    # coalesce helpers (ensure column existence)
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)

    # If EmployeeName is missing or a GUID, try to get a better name from durations (durations typically has EmployeeName)
    if 'EmployeeName' in merged.columns and 'EmployeeName_y' in merged.columns:
        # if merged has suffix columns from merge, prefer the durations value if the grouped employee name is empty or GUID-like
        def choose_best_name(row):
            gname = row.get('EmployeeName')
            dname = row.get('EmployeeName_y')
            if _looks_like_name(gname):
                return str(gname).strip()
            if _looks_like_name(dname):
                return str(dname).strip()
            # if grouped name exists but is GUID-like, prefer duration's name if available
            if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
                return str(gname).strip()
            if dname and not _is_placeholder_str(dname):
                return str(dname).strip()
            return None
        merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
        # drop suffix if present
        try:
            merged.drop(columns=[c for c in merged.columns if c.endswith('_y') or c.endswith('_x')], inplace=True)
        except Exception:
            pass
    else:
        # fallback: try to use 'EmployeeName' from durations if our EmployeeName is missing/invalid
        if 'EmployeeName' in merged.columns and not dur.empty:
            def fill_name_from_dur(row):
                gname = row.get('EmployeeName')
                if _looks_like_name(gname) and not _is_placeholder_str(gname):
                    return gname
                # try find matching row in durations (person_uid + date) - already merged, so duration name may be in other columns
                for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
                    if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
                        return row[cand]
                return None
            merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

    # numeric normalization for EmployeeID: ensure not GUIDs/placeholder, convert floats like '320172.0' -> '320172'
    def normalize_empid(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            # strip .0 integer floats
            try:
                if '.' in s:
                    f = float(s)
                    if math.isfinite(f) and f.is_integer():
                        return str(int(f))
            except Exception:
                pass
            return s
        except Exception:
            return None

    merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

    # normalize card numbers: reject GUIDs and placeholder tokens
    def normalize_card(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            return s
        except Exception:
            return None

    merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

    # numeric normalization
    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)

    # ensure FirstSwipe/LastSwipe are datetimes
    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # EmpHistoryPresent
    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    # normalize string columns for safe downstream use; EmployeeName keep as readable-only
    for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
        if c in merged.columns:
            def _clean_str_val(v):
                if v is None:
                    return None
                try:
                    s = str(v).strip()
                    if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                        return None
                    return s
                except Exception:
                    return None
            merged[c] = merged[c].apply(_clean_str_val)

    # EmployeeName: keep None if empty or GUID/placeholder; otherwise string.
    if 'EmployeeName' in merged.columns:
        merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

    return merged


# ---------------- SCENARIOS (boolean functions) ----------------
def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
    except Exception:
        pass
    if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    return cur > 50

def scenario_repeated_short_breaks(row):
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map


# scenario list (name, fn)
SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap)
]


def run_trend_for_date(target_date: date, outdir: str = "./outputs", city='Pune'):
    logging.info("run_trend_for_date: date=%s (city=%s)", target_date, city)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city=city)
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    # save raw swipes for evidence (full raw)
    try:
        if swipes is not None and not swipes.empty:
            sw_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception as e:
        logging.warning("Failed to save raw swipes: %s", e)

    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # Build badge map and swipe overlap maps for higher-severity scenarios
    badge_map = {}
    if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}

    swipe_overlap_map = {}
    overlap_window_seconds = 2
    if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
        tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
        if not tmp.empty:
            tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
            for (d, door), g in tmp.groupby(['Date', 'Door']):
                items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                n = len(items)
                for i in range(n):
                    t_i, uid_i = items[i]
                    j = i+1
                    while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                        uid_j = items[j][1]
                        if uid_i != uid_j:
                            swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                            swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                        j += 1

    # Evaluate scenarios (use weighting to compute anomaly score)
    WEIGHTS = {
        "long_gap_>=90min": 0.3,
        "short_duration_<4h": 1.0,
        "coffee_badging": 1.0,
        "low_swipe_count_<=2": 0.5,
        "single_door": 0.25,
        "only_in": 0.8,
        "only_out": 0.8,
        "overtime_>=10h": 0.2,
        "very_long_duration_>=16h": 1.5,
        "zero_swipes": 0.4,
        "unusually_high_swipes": 1.5,
        "repeated_short_breaks": 0.5,
        "multiple_location_same_day": 0.6,
        "weekend_activity": 0.6,
        "repeated_rejection_count": 0.8,
        "badge_sharing_suspected": 2.0,
        "early_arrival_before_06": 0.4,
        "late_exit_after_22": 0.4,
        "shift_inconsistency": 1.2,
        "trending_decline": 0.7,
        "consecutive_absent_days": 1.2,
        "high_variance_duration": 0.8,
        "short_duration_on_high_presence_days": 1.1,
        "swipe_overlap": 2.0
    }
    ANOMALY_THRESHOLD = 1.5

    # evaluate scenarios and compute score
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            features[name] = features.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
        elif name == "swipe_overlap":
            features[name] = features.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map), axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    def compute_score(r):
        score = 0.0
        detected = []
        for name, _ in SCENARIOS:
            val = bool(r.get(name))
            w = WEIGHTS.get(name, 0.0)
            if val and w > 0:
                score += float(w)
                detected.append(name)
        return score, detected

    scores = features.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
    features['AnomalyScore'] = scores['AnomalyScore'].astype(float)
    features['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
    features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

    def reasons_for_row(r):
        if not bool(r.get('IsFlagged')):
            return None
        ds = r.get('DetectedScenarios')
        if ds:
            return ds
        return None
    features['Reasons'] = features.apply(reasons_for_row, axis=1)

    if 'OverlapWith' not in features.columns:
        def overlap_with_fn(r):
            d = r.get('Date')
            uid = r.get('person_uid')
            if (d, uid) in swipe_overlap_map:
                return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
            return None
        features['OverlapWith'] = features.apply(overlap_with_fn, axis=1)

    # Remove suffix columns and fix duplicates
    cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
    if cols_to_drop:
        for c in cols_to_drop:
            base = c[:-2]
            if base in features.columns:
                try:
                    features.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                try:
                    features.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass
    features = features.loc[:, ~features.columns.duplicated()]

    # ensure booleans are native Python (avoid numpy.bool_)
    for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
        if col in features.columns:
            features[col] = features[col].astype(bool)

    # write CSV with native types
    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    try:
        write_df = features.copy()
        # FirstSwipe/LastSwipe -> ISO strings
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        # Date -> ISO date
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        write_df = write_df.where(pd.notnull(write_df), None)
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception as e:
        logging.exception("Failed to write trend CSV: %s", e)

    return features


# ---------------- training dataset builder (restored) ----------------
def build_monthly_training(end_date: date = None, months: int = 3, min_unique_employees: int = 1000,
                           outdir: str = "./outputs", city: str = "Pune"):
    if end_date is None:
        end_date = datetime.now().date()
    logging.info("build_monthly_training: end_date=%s months=%d min_unique=%d", end_date, months, min_unique_employees)
    outdir = Path(outdir)
    month_windows = []
    cur = end_date.replace(day=1)
    for _ in range(months):
        start = cur
        next_month = (cur.replace(day=28) + timedelta(days=4)).replace(day=1)
        last = next_month - timedelta(days=1)
        month_windows.append((start, last))
        cur = (start - timedelta(days=1)).replace(day=1)

    person_month_rows = []
    unique_persons = set()

    for start, last in month_windows:
        d = start
        month_dfs = []
        while d <= last:
            csv_path = outdir / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
            if csv_path.exists():
                try:
                    df = pd.read_csv(csv_path)
                    month_dfs.append(df)
                except Exception:
                    try:
                        df = pd.read_csv(csv_path, dtype=str)
                        month_dfs.append(df)
                    except Exception as e:
                        logging.warning("Failed reading %s: %s", csv_path, e)
            d = d + timedelta(days=1)

        if not month_dfs:
            logging.info("No daily trend CSVs found for month %s - %s", start.isoformat(), last.isoformat())
            continue

        month_df = pd.concat(month_dfs, ignore_index=True)
        # ensure person_uid exists
        if 'person_uid' not in month_df.columns:
            def make_person_uid(row):
                parts = []
                for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                    v = row.get(c)
                    if pd.notna(v) and str(v).strip():
                        parts.append(str(v).strip())
                return "|".join(parts) if parts else None
            month_df['person_uid'] = month_df.apply(make_person_uid, axis=1)

        # convert boolean columns to int for aggregation if necessary
        for name, _ in SCENARIOS:
            if name in month_df.columns:
                month_df[name] = month_df[name].astype(int)

        agg_funcs = {
            'CountSwipes': ['median', 'mean', 'sum'],
            'DurationMinutes': ['median', 'mean', 'sum'],
            'MaxSwipeGapSeconds': ['max', 'median'],
            'ShortGapCount': ['sum'],
            'UniqueDoors': ['median'],
            'UniqueLocations': ['median'],
            'RejectionCount': ['sum']
        }
        scenario_cols = [name for name,_ in SCENARIOS if name in month_df.columns]
        group_cols = ['person_uid']
        grp = month_df.groupby(group_cols)

        for person, g in grp:
            row = {}
            row['person_uid'] = person
            row['EmployeeID'] = next((v for v in g.get('EmployeeID', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['EmployeeName'] = next((v for v in g.get('EmployeeName', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['MonthStart'] = start.isoformat()
            row['MonthEnd'] = last.isoformat()
            for col, funcs in agg_funcs.items():
                if col in g.columns:
                    for f in funcs:
                        key = f"{col}_{f}"
                        try:
                            val = getattr(g[col], f)()
                            row[key] = float(val) if pd.notna(val) else None
                        except Exception:
                            row[key] = None
                else:
                    for f in funcs:
                        row[f"{col}_{f}"] = None
            for s in scenario_cols:
                row[f"{s}_days"] = int(g[s].sum())
                row[f"{s}_label"] = int(g[s].sum() > 0)
            row['days_present'] = int(g.shape[0])
            person_month_rows.append(row)
            unique_persons.add(person)

        if len(unique_persons) >= min_unique_employees:
            logging.info("Reached min unique employees=%d, stopping aggregation early", min_unique_employees)
            break

    if not person_month_rows:
        logging.warning("No person-month rows created (no data).")
        return None

    training_df = pd.DataFrame(person_month_rows)
    train_out = outdir / "training_person_month.csv"
    training_df.to_csv(train_out, index=False)
    logging.info("Saved training CSV to %s (rows=%d unique_persons=%d)", train_out, len(training_df), len(unique_persons))
    return train_out


if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today)
    print("Completed; rows:", len(df) if df is not None else 0)
# ----------------- END OF FILE: trend_runner.py -----------------



















When i update this we got below error fix this error carefully and share me Fully updated file so i can easily swap file each other

(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python app.py
>>
Traceback (most recent call last):
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py", line 12, in <module>
    from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR
  File "C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py", line 466
    def ensure_col(df, col, default=None):
IndentationError: unexpected indent
(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> 





#C:\Users\W0024618\Desktop\Trend Analysis\backend\trend_runner.py

# ----------------- START OF FILE: trend_runner.py -----------------
from datetime import date, datetime, time, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import hashlib
import math
import re

# IMPORTANT: duration_report must exist and expose run_for_date(date, regions, outdir, city)
from duration_report import run_for_date

# historical profile (optional)
HIST_PATH = Path(__file__).parent / "current_analysis.csv"
if HIST_PATH.exists():
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()
else:
    logging.warning("Historical profile file current_analysis.csv not found; history-based scenarios will fallback.")
    HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
MODELS_DIR = Path("./models")
MODELS_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)


# ----- small shared helpers: treat empty/placeholder tokens as None -----
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


def _normalize_id_val(v):
    """
    Normalize an id-like value: strip, convert floats like '320172.0' -> '320172'
    Return None for NaN/empty/placeholder.
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == "" or s.lower() == "nan" or _is_placeholder_str(s):
        return None
    # strip .0 if integer-like
    try:
        if '.' in s:
            f = float(s)
            if math.isfinite(f) and f.is_integer():
                return str(int(f))
    except Exception:
        pass
    return s


# prefer to avoid emp:<GUID> person_uids — only treat emp: if value looks like a human id (not GUID)
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')

def _looks_like_guid(s: object) -> bool:
    """Return True if s looks like a GUID/UUID string."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _looks_like_name(s: object) -> bool:
    """Heuristic: treat as a plausible human name if it contains letters and not a GUID."""
    if s is None:
        return False
    try:
        st = str(s).strip()
        if not st:
            return False
        # reject GUIDs and obviously numeric ids
        if _looks_like_guid(st):
            return False
        # require at least one alphabetic character
        return bool(re.search(r'[A-Za-z]', st))
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    """Pick the first non-null, non-GUID, non-placeholder value from a pandas Series (as str) or None."""
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
            if not s:
                continue
            if _is_placeholder_str(s):
                continue
            if _looks_like_guid(s):
                continue
            return s
        except Exception:
            continue
    return None

def _canonical_person_uid(row):
    """
    Create canonical person uid:
      - prefer EmployeeID (normalized) -> 'emp:<id>' only if it is not a GUID
      - else EmployeeIdentity -> 'uid:<val>' (GUID allowed)
      - else EmployeeName -> hash-based 'name:<shorthash>'
    """
    empid = row.get('EmployeeID', None)
    empident = row.get('EmployeeIdentity', None)
    name = row.get('EmployeeName', None)
    empid_n = _normalize_id_val(empid)
    if empid_n and not _looks_like_guid(empid_n):
        return f"emp:{empid_n}"
    empident_n = _normalize_id_val(empident)
    if empident_n:
        return f"uid:{empident_n}"
    if name and str(name).strip():
        # stable short hash of name
        h = hashlib.sha1(str(name).strip().lower().encode('utf8')).hexdigest()[:10]
        return f"name:{h}"
    return None


# small helper to extract Card from XML-like strings
_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        # fallback: look for CHUID ... Card: pattern or Card: 12345
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


# --- compute_features (replaced/updated) ---
def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    Returns DataFrame per (person_uid, Date) with feature columns and normalized IDs/names.
    This implementation follows the duration_report column conventions and avoids
    treating GUIDs or placeholder tokens as EmployeeName/EmployeeID/CardNumber.
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()

    # Build lowercase->actual column map for flexible column detection
    cols_lower = {c.lower(): c for c in sw.columns}

    # detect time column
    time_candidates = ['localemessagetime', 'messagetime', 'timestamp', 'time', 'localemessagetimestamp']
    found_time_col = next((cols_lower[c] for c in time_candidates if c in cols_lower), None)
    if found_time_col:
        sw['LocaleMessageTime'] = pd.to_datetime(sw[found_time_col], errors='coerce')
        sw['Date'] = sw['LocaleMessageTime'].dt.date
    else:
        if 'Date' in sw.columns:
            sw['Date'] = pd.to_datetime(sw['Date'], errors='coerce').dt.date
        else:
            sw['Date'] = None

    # find these earlier in compute_features — prefer Int1/Text12 for EmployeeID and CHUID/Card for CardNumber
    name_candidates = ['employeename', 'objectname1', 'objectname', 'employee_name', 'name', 'object_name']
    employeeid_candidates = ['int1', 'text12', 'employeeid', 'employee_id', 'empid', 'id']
    card_candidates = ['cardnumber', 'chuid', 'card', 'card_no', 'cardnum', 'value']
    door_candidates = ['door', 'doorname', 'door_name']
    direction_candidates = ['direction', 'directionname', 'direction_name']

    name_col = next((cols_lower[c] for c in name_candidates if c in cols_lower), None)
    empid_col = next((cols_lower[c] for c in employeeid_candidates if c in cols_lower), None)
    card_col = next((cols_lower[c] for c in card_candidates if c in cols_lower), None)
    door_col = next((cols_lower[c] for c in door_candidates if c in cols_lower), None)
    dir_col = next((cols_lower[c] for c in direction_candidates if c in cols_lower), None)

    # Filter personnel types: prefer PersonnelTypeName, fallback to PersonnelType
    if 'PersonnelTypeName' in sw.columns:
        sw = sw[sw['PersonnelTypeName'].isin(['Employee', 'Terminated Personnel'])]
    elif 'PersonnelType' in sw.columns:
        sw = sw[sw['PersonnelType'].isin(['Employee', 'Terminated Personnel'])]
    # else keep everything

    if sw.empty:
        logging.info("compute_features: no rows after PersonnelType filter")
        return pd.DataFrame()

    # ensure stable person_uid (canonical)
    if 'person_uid' not in sw.columns:
        def make_person_uid_local(r):
            # prefer canonical EmployeeID (normalized, non-GUID) then EmployeeIdentity then EmployeeName
            empid_val = None
            if empid_col and empid_col in r and pd.notna(r.get(empid_col)):
                empid_val = r.get(empid_col)
            elif 'EmployeeID' in r and pd.notna(r.get('EmployeeID')):
                empid_val = r.get('EmployeeID')

            empident_val = r.get('EmployeeIdentity') if 'EmployeeIdentity' in r else None
            name_val = None
            if name_col and name_col in r:
                name_val = r.get(name_col)
            elif 'EmployeeName' in r:
                name_val = r.get('EmployeeName')
            elif 'ObjectName1' in r:
                name_val = r.get('ObjectName1')

            return _canonical_person_uid({
                'EmployeeID': empid_val,
                'EmployeeIdentity': empident_val,
                'EmployeeName': name_val
            })
        sw['person_uid'] = sw.apply(make_person_uid_local, axis=1)

    # selection columns for aggregation: include discovered columns
    sel_cols = set(['LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
                    'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType', 'PersonnelTypeName',
                    'EmployeeIdentity'])
    if name_col:
        sel_cols.add(name_col)
    if empid_col:
        sel_cols.add(empid_col)
    if card_col:
        sel_cols.add(card_col)
    if door_col:
        sel_cols.add(door_col)
    if dir_col:
        sel_cols.add(dir_col)
    sel_cols = [c for c in sel_cols if c in sw.columns]

    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist()) if 'LocaleMessageTime' in g else []
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0

        # Direction counts (default to column names present)
        in_count = int((g['Direction'] == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g['Direction'] == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0

        # pick first non-placeholder, non-guid card number if present (prefer cardnumber/chuid)
        card_numbers = []
        # 1) direct known column
        if card_col and card_col in g.columns:
            card_numbers = list(pd.unique(g[card_col].dropna()))
        # 2) explicit 'CardNumber' output column (from SQL COALESCE)
        if not card_numbers and 'CardNumber' in g.columns:
            card_numbers = list(pd.unique(g['CardNumber'].dropna()))
        # 3) some XML-shred columns may appear as 'value' or other column names
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'value' == cl or 'xml' in cl or 'msg' in cl or 'shred' in cl:
                    try:
                        vals = list(pd.unique(g[c].dropna()))
                        if vals:
                            # extend but prefer first real-looking
                            card_numbers.extend(vals)
                    except Exception:
                        continue
        # 4) lastly try to extract from XmlMessage fields
        if not card_numbers:
            for c in g.columns:
                cl = c.lower()
                if 'xml' in cl or 'xmlmessage' in cl or 'xml_msg' in cl or 'xmlmessage' in cl:
                    for raw in g[c].dropna().astype(str):
                        extracted = _extract_card_from_xml(raw)
                        if extracted:
                            card_numbers.append(extracted)
        # 5) final unique
        card_numbers = list(dict.fromkeys(card_numbers))  # preserve order, unique

        card_number = None
        for c in card_numbers:
            n = _normalize_id_val(c)
            # explicitly reject GUIDs as card numbers
            if n and not _looks_like_guid(n):
                card_number = n
                break

        # stable id/name from the group using discovered columns first
        employee_id = None
        employee_name = None
        employee_identity = None
        personnel_type = None

        # Employee ID: prefer Int1/Text12 then EmployeeID; DO NOT use EmployeeIdentity as EmployeeID
        # use _pick_first_non_guid_value to skip GUIDs automatically
        if empid_col and empid_col in g.columns:
            vals = g[empid_col].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                # final trial: numeric normalization (strip .0) but still reject GUIDs
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized
        elif 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna().astype(str).map(lambda x: x.strip())
            employee_id = _pick_first_non_guid_value(vals)
            if employee_id is None and not vals.empty:
                v0 = vals.iloc[0]
                normalized = _normalize_id_val(v0)
                if normalized and not _looks_like_guid(normalized):
                    employee_id = normalized

        # If still no employee_id and PersonnelType indicates contractor -> prefer Text12 explicitly
        if (not employee_id) and 'PersonnelType' in g.columns:
            try:
                pvals = g['PersonnelType'].dropna().astype(str)
                if not pvals.empty:
                    p0 = pvals.iloc[0]
                    if str(p0).strip().lower() in ('contractor', 'terminated contractor', 'contractor '):
                        # look for text12 explicitly (case-insensitive)
                        for c in g.columns:
                            if c.lower() == 'text12':
                                vals = g[c].dropna().astype(str).map(lambda x: x.strip())
                                employee_id = _pick_first_non_guid_value(vals)
                                if employee_id:
                                    break
            except Exception:
                pass

        # Employee identity (GUID) — keep but do not promote to EmployeeID
        if 'EmployeeIdentity' in g.columns:
            vals = g['EmployeeIdentity'].dropna().astype(str).map(lambda x: x.strip())
            if not vals.empty:
                employee_identity = vals.iloc[0]

        # Employee name: pick non-GUID candidate
        candidate_name_vals = None
        if name_col and name_col in g.columns:
            candidate_name_vals = g[name_col].dropna().astype(str).map(lambda x: x.strip())
        elif 'EmployeeName' in g.columns:
            candidate_name_vals = g['EmployeeName'].dropna().astype(str).map(lambda x: x.strip())
        elif 'ObjectName1' in g.columns:
            candidate_name_vals = g['ObjectName1'].dropna().astype(str).map(lambda x: x.strip())

        if candidate_name_vals is not None and not candidate_name_vals.empty:
            employee_name = _pick_first_non_guid_value(candidate_name_vals)
            if employee_name is None:
                # accept any value that looks like a name
                for v in candidate_name_vals:
                    if _looks_like_name(v) and not _is_placeholder_str(v):
                        employee_name = str(v).strip()
                        break

        # personnel type
        if 'PersonnelTypeName' in g.columns:
            vals = g['PersonnelTypeName'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]
        elif 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            if not vals.empty:
                personnel_type = vals.iloc[0]

        # First/Last swipe times
        first_swipe = None
        last_swipe = None
        if times:
            first_swipe = times[0]
            last_swipe = times[-1]

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeIdentity': employee_identity,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type,
            'FirstSwipe': first_swipe,
            'LastSwipe': last_swipe
        })

    grouped = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = grouped.apply(agg_swipe_group).reset_index()

    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date'], errors='coerce').dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

# --- START PATCH: coalesce duplicate columns produced by merge ---
# When merging grouped + durations we may get *_x / *_y columns.
# Coalesce them into canonical column names (prefer _x then _y), then remove suffixes.
def _coalesce_merge_columns(df, bases):
    for base in bases:
        x = base + "_x"
        y = base + "_y"
        # If canonical exists and has non-null values, keep it; else build from x/y
        if base not in df.columns or df[base].isnull().all():
            if x in df.columns and y in df.columns:
                try:
                    df[base] = df[x].combine_first(df[y])
                except Exception:
                    # fallback: prefer x then y by simple fillna
                    df[base] = df[x].where(df[x].notna(), df[y] if y in df.columns else None)
            elif x in df.columns:
                df[base] = df[x]
            elif y in df.columns:
                df[base] = df[y]
    # Drop suffix columns after coalescing
    drop_cols = [c for c in df.columns if c.endswith("_x") or c.endswith("_y")]
    if drop_cols:
        try:
            df.drop(columns=drop_cols, inplace=True)
        except Exception:
            # best-effort drop
            for c in drop_cols:
                if c in df.columns:
                    try:
                        df.drop(columns=[c], inplace=True)
                    except Exception:
                        pass

# coalesce common id/name/card columns we expect
_coalesce_merge_columns(merged, [
    "EmployeeID", "Int1", "Text12", "CardNumber", "EmployeeName", "EmployeeIdentity"
])
# --- END PATCH ---


    # coalesce helpers (ensure column existence)
    def ensure_col(df, col, default=None):
        if col not in df.columns:
            df[col] = default

    ensure_col(merged, 'DurationSeconds', 0)
    ensure_col(merged, 'FirstSwipe', pd.NaT)
    ensure_col(merged, 'LastSwipe', pd.NaT)
    ensure_col(merged, 'CountSwipes', 0)
    ensure_col(merged, 'MaxSwipeGapSeconds', 0)
    ensure_col(merged, 'ShortGapCount', 0)
    ensure_col(merged, 'RejectionCount', 0)
    ensure_col(merged, 'UniqueLocations', 0)
    ensure_col(merged, 'UniqueDoors', 0)
    ensure_col(merged, 'CardNumber', None)
    ensure_col(merged, 'EmployeeID', None)
    ensure_col(merged, 'EmployeeName', None)
    ensure_col(merged, 'EmployeeIdentity', None)
    ensure_col(merged, 'PersonnelType', None)

    # If EmployeeName is missing or a GUID, try to get a better name from durations (durations typically has EmployeeName)
    if 'EmployeeName' in merged.columns and 'EmployeeName_y' in merged.columns:
        # if merged has suffix columns from merge, prefer the durations value if the grouped employee name is empty or GUID-like
        def choose_best_name(row):
            gname = row.get('EmployeeName')
            dname = row.get('EmployeeName_y')
            if _looks_like_name(gname):
                return str(gname).strip()
            if _looks_like_name(dname):
                return str(dname).strip()
            # if grouped name exists but is GUID-like, prefer duration's name if available
            if gname and not _looks_like_guid(gname) and not _is_placeholder_str(gname):
                return str(gname).strip()
            if dname and not _is_placeholder_str(dname):
                return str(dname).strip()
            return None
        merged['EmployeeName'] = merged.apply(choose_best_name, axis=1)
        # drop suffix if present
        try:
            merged.drop(columns=[c for c in merged.columns if c.endswith('_y') or c.endswith('_x')], inplace=True)
        except Exception:
            pass
    else:
        # fallback: try to use 'EmployeeName' from durations if our EmployeeName is missing/invalid
        if 'EmployeeName' in merged.columns and not dur.empty:
            def fill_name_from_dur(row):
                gname = row.get('EmployeeName')
                if _looks_like_name(gname) and not _is_placeholder_str(gname):
                    return gname
                # try find matching row in durations (person_uid + date) - already merged, so duration name may be in other columns
                for cand in ('EmployeeName', 'EmployeeName_y', 'EmployeeName_x'):
                    if cand in row and _looks_like_name(row[cand]) and not _is_placeholder_str(row[cand]):
                        return row[cand]
                return None
            merged['EmployeeName'] = merged.apply(fill_name_from_dur, axis=1)

    # numeric normalization for EmployeeID: ensure not GUIDs/placeholder, convert floats like '320172.0' -> '320172'
    def normalize_empid(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            # strip .0 integer floats
            try:
                if '.' in s:
                    f = float(s)
                    if math.isfinite(f) and f.is_integer():
                        return str(int(f))
            except Exception:
                pass
            return s
        except Exception:
            return None

    merged['EmployeeID'] = merged['EmployeeID'].apply(normalize_empid)

    # normalize card numbers: reject GUIDs and placeholder tokens
    def normalize_card(v):
        if v is None:
            return None
        try:
            s = str(v).strip()
            if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                return None
            if _looks_like_guid(s):
                return None
            return s
        except Exception:
            return None

    merged['CardNumber'] = merged['CardNumber'].apply(normalize_card)

    # numeric normalization
    merged['DurationSeconds'] = pd.to_numeric(merged['DurationSeconds'], errors='coerce').fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)

    # ensure FirstSwipe/LastSwipe are datetimes
    for col in ['FirstSwipe', 'LastSwipe']:
        try:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')
        except Exception:
            merged[col] = pd.NaT

    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # EmpHistoryPresent
    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: _normalize_id_val(x) in hist_map if pd.notna(x) else False)

    # normalize string columns for safe downstream use; EmployeeName keep as readable-only
    for c in ['EmployeeID', 'CardNumber', 'EmployeeIdentity', 'PersonnelType']:
        if c in merged.columns:
            def _clean_str_val(v):
                if v is None:
                    return None
                try:
                    s = str(v).strip()
                    if s == '' or s.lower() == 'nan' or _is_placeholder_str(s):
                        return None
                    return s
                except Exception:
                    return None
            merged[c] = merged[c].apply(_clean_str_val)

    # EmployeeName: keep None if empty or GUID/placeholder; otherwise string.
    if 'EmployeeName' in merged.columns:
        merged['EmployeeName'] = merged['EmployeeName'].apply(lambda v: None if (v is None or (isinstance(v, float) and np.isnan(v)) or _looks_like_guid(v) or _is_placeholder_str(v)) else str(v).strip())

    return merged


# ---------------- SCENARIOS (boolean functions) ----------------
def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return 0 < (row.get('CountSwipes') or 0) <= 2

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    try:
        if not HIST_DF.empty and empid is not None and empid in HIST_DF['EmployeeID'].values:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
    except Exception:
        pass
    if not HIST_DF.empty and 'TotalSwipes_median' in HIST_DF.columns:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    return cur > 50

def scenario_repeated_short_breaks(row):
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, badge_map=None):
    card = row.get('CardNumber')
    d = row.get('Date')
    if card is None or pd.isna(card) or d is None:
        return False
    if badge_map is None:
        return False
    return badge_map.get((d, card), 0) > 1

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row, swipe_overlap_map=None):
    d = row.get('Date')
    uid = row.get('person_uid')
    if swipe_overlap_map is None or d is None or uid is None:
        return False
    return (d, uid) in swipe_overlap_map


# scenario list (name, fn)
SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap)
]


def run_trend_for_date(target_date: date, outdir: str = "./outputs", city='Pune'):
    logging.info("run_trend_for_date: date=%s (city=%s)", target_date, city)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city=city)
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    # save raw swipes for evidence (full raw)
    try:
        if swipes is not None and not swipes.empty:
            sw_out = Path(outdir) / f"swipes_{city.lower().replace(' ','_')}_{target_date.strftime('%Y%m%d')}.csv"
            swipes.to_csv(sw_out, index=False)
            logging.info("Saved raw swipes to %s", sw_out)
    except Exception as e:
        logging.warning("Failed to save raw swipes: %s", e)

    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # Build badge map and swipe overlap maps for higher-severity scenarios
    badge_map = {}
    if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            badge_map = {(row.Date, row.CardNumber): int(row.distinct_users) for row in grouped_card.itertuples(index=False)}

    swipe_overlap_map = {}
    overlap_window_seconds = 2
    if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
        tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna()
        if not tmp.empty:
            tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
            for (d, door), g in tmp.groupby(['Date', 'Door']):
                items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                n = len(items)
                for i in range(n):
                    t_i, uid_i = items[i]
                    j = i+1
                    while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                        uid_j = items[j][1]
                        if uid_i != uid_j:
                            swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                            swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                        j += 1

    # Evaluate scenarios (use weighting to compute anomaly score)
    WEIGHTS = {
        "long_gap_>=90min": 0.3,
        "short_duration_<4h": 1.0,
        "coffee_badging": 1.0,
        "low_swipe_count_<=2": 0.5,
        "single_door": 0.25,
        "only_in": 0.8,
        "only_out": 0.8,
        "overtime_>=10h": 0.2,
        "very_long_duration_>=16h": 1.5,
        "zero_swipes": 0.4,
        "unusually_high_swipes": 1.5,
        "repeated_short_breaks": 0.5,
        "multiple_location_same_day": 0.6,
        "weekend_activity": 0.6,
        "repeated_rejection_count": 0.8,
        "badge_sharing_suspected": 2.0,
        "early_arrival_before_06": 0.4,
        "late_exit_after_22": 0.4,
        "shift_inconsistency": 1.2,
        "trending_decline": 0.7,
        "consecutive_absent_days": 1.2,
        "high_variance_duration": 0.8,
        "short_duration_on_high_presence_days": 1.1,
        "swipe_overlap": 2.0
    }
    ANOMALY_THRESHOLD = 1.5

    # evaluate scenarios and compute score
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            features[name] = features.apply(lambda r: scenario_badge_sharing_suspected(r, badge_map=badge_map), axis=1)
        elif name == "swipe_overlap":
            features[name] = features.apply(lambda r: scenario_swipe_overlap(r, swipe_overlap_map), axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    def compute_score(r):
        score = 0.0
        detected = []
        for name, _ in SCENARIOS:
            val = bool(r.get(name))
            w = WEIGHTS.get(name, 0.0)
            if val and w > 0:
                score += float(w)
                detected.append(name)
        return score, detected

    scores = features.apply(lambda r: pd.Series(compute_score(r), index=['AnomalyScore', 'DetectedScenarios']), axis=1)
    features['AnomalyScore'] = scores['AnomalyScore'].astype(float)
    features['DetectedScenarios'] = scores['DetectedScenarios'].apply(lambda x: "; ".join(x) if (isinstance(x, (list, tuple)) and len(x)>0) else None)
    features['IsFlagged'] = features['AnomalyScore'].apply(lambda s: bool(s >= ANOMALY_THRESHOLD))

    def reasons_for_row(r):
        if not bool(r.get('IsFlagged')):
            return None
        ds = r.get('DetectedScenarios')
        if ds:
            return ds
        return None
    features['Reasons'] = features.apply(reasons_for_row, axis=1)

    if 'OverlapWith' not in features.columns:
        def overlap_with_fn(r):
            d = r.get('Date')
            uid = r.get('person_uid')
            if (d, uid) in swipe_overlap_map:
                return ";".join(sorted(str(x) for x in swipe_overlap_map[(d, uid)]))
            return None
        features['OverlapWith'] = features.apply(overlap_with_fn, axis=1)

    # Remove suffix columns and fix duplicates
    cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
    if cols_to_drop:
        for c in cols_to_drop:
            base = c[:-2]
            if base in features.columns:
                try:
                    features.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                try:
                    features.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass
    features = features.loc[:, ~features.columns.duplicated()]

    # ensure booleans are native Python (avoid numpy.bool_)
    for col in [name for name, _ in SCENARIOS] + ['IsFlagged']:
        if col in features.columns:
            features[col] = features[col].astype(bool)

    # write CSV with native types
    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    try:
        write_df = features.copy()
        # FirstSwipe/LastSwipe -> ISO strings
        for dtcol in ('FirstSwipe', 'LastSwipe'):
            if dtcol in write_df.columns:
                write_df[dtcol] = pd.to_datetime(write_df[dtcol], errors='coerce').dt.strftime('%Y-%m-%dT%H:%M:%S')
        # Date -> ISO date
        if 'Date' in write_df.columns:
            try:
                write_df['Date'] = pd.to_datetime(write_df['Date'], errors='coerce').dt.date
                write_df['Date'] = write_df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
            except Exception:
                pass
        write_df = write_df.where(pd.notnull(write_df), None)
        write_df.to_csv(out_csv, index=False)
        logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(write_df))
    except Exception as e:
        logging.exception("Failed to write trend CSV: %s", e)

    return features


# ---------------- training dataset builder (restored) ----------------
def build_monthly_training(end_date: date = None, months: int = 3, min_unique_employees: int = 1000,
                           outdir: str = "./outputs", city: str = "Pune"):
    if end_date is None:
        end_date = datetime.now().date()
    logging.info("build_monthly_training: end_date=%s months=%d min_unique=%d", end_date, months, min_unique_employees)
    outdir = Path(outdir)
    month_windows = []
    cur = end_date.replace(day=1)
    for _ in range(months):
        start = cur
        next_month = (cur.replace(day=28) + timedelta(days=4)).replace(day=1)
        last = next_month - timedelta(days=1)
        month_windows.append((start, last))
        cur = (start - timedelta(days=1)).replace(day=1)

    person_month_rows = []
    unique_persons = set()

    for start, last in month_windows:
        d = start
        month_dfs = []
        while d <= last:
            csv_path = outdir / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
            if csv_path.exists():
                try:
                    df = pd.read_csv(csv_path)
                    month_dfs.append(df)
                except Exception:
                    try:
                        df = pd.read_csv(csv_path, dtype=str)
                        month_dfs.append(df)
                    except Exception as e:
                        logging.warning("Failed reading %s: %s", csv_path, e)
            d = d + timedelta(days=1)

        if not month_dfs:
            logging.info("No daily trend CSVs found for month %s - %s", start.isoformat(), last.isoformat())
            continue

        month_df = pd.concat(month_dfs, ignore_index=True)
        # ensure person_uid exists
        if 'person_uid' not in month_df.columns:
            def make_person_uid(row):
                parts = []
                for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                    v = row.get(c)
                    if pd.notna(v) and str(v).strip():
                        parts.append(str(v).strip())
                return "|".join(parts) if parts else None
            month_df['person_uid'] = month_df.apply(make_person_uid, axis=1)

        # convert boolean columns to int for aggregation if necessary
        for name, _ in SCENARIOS:
            if name in month_df.columns:
                month_df[name] = month_df[name].astype(int)

        agg_funcs = {
            'CountSwipes': ['median', 'mean', 'sum'],
            'DurationMinutes': ['median', 'mean', 'sum'],
            'MaxSwipeGapSeconds': ['max', 'median'],
            'ShortGapCount': ['sum'],
            'UniqueDoors': ['median'],
            'UniqueLocations': ['median'],
            'RejectionCount': ['sum']
        }
        scenario_cols = [name for name,_ in SCENARIOS if name in month_df.columns]
        group_cols = ['person_uid']
        grp = month_df.groupby(group_cols)

        for person, g in grp:
            row = {}
            row['person_uid'] = person
            row['EmployeeID'] = next((v for v in g.get('EmployeeID', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['EmployeeName'] = next((v for v in g.get('EmployeeName', []) if pd.notna(v) and not _is_placeholder_str(v)), None)
            row['MonthStart'] = start.isoformat()
            row['MonthEnd'] = last.isoformat()
            for col, funcs in agg_funcs.items():
                if col in g.columns:
                    for f in funcs:
                        key = f"{col}_{f}"
                        try:
                            val = getattr(g[col], f)()
                            row[key] = float(val) if pd.notna(val) else None
                        except Exception:
                            row[key] = None
                else:
                    for f in funcs:
                        row[f"{col}_{f}"] = None
            for s in scenario_cols:
                row[f"{s}_days"] = int(g[s].sum())
                row[f"{s}_label"] = int(g[s].sum() > 0)
            row['days_present'] = int(g.shape[0])
            person_month_rows.append(row)
            unique_persons.add(person)

        if len(unique_persons) >= min_unique_employees:
            logging.info("Reached min unique employees=%d, stopping aggregation early", min_unique_employees)
            break

    if not person_month_rows:
        logging.warning("No person-month rows created (no data).")
        return None

    training_df = pd.DataFrame(person_month_rows)
    train_out = outdir / "training_person_month.csv"
    training_df.to_csv(train_out, index=False)
    logging.info("Saved training CSV to %s (rows=%d unique_persons=%d)", train_out, len(training_df), len(unique_persons))
    return train_out


if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today)
    print("Completed; rows:", len(df) if df is not None else 0)
# ----------------- END OF FILE: trend_runner.py -----------------






