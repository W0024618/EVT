from datetime import date, datetime, time
from pathlib import Path
import pandas as pd
import numpy as np
import logging

# IMPORTANT: import duration_report as a top-level module (file in same folder).
from duration_report import run_for_date

# try to load historical profile (current_analysis.csv) if present
HIST_PATH = Path(__file__).parent / "current_analysis.csv"
if HIST_PATH.exists():
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()
else:
    logging.warning("Historical profile file current_analysis.csv not found; history-based scenarios will fallback.")
    HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)


def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()
    sw['LocaleMessageTime'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce')
    sw['Date'] = sw['LocaleMessageTime'].dt.date

    # ensure person_uid exists (same logic as duration_report)
    if 'person_uid' not in sw.columns:
        def make_person_uid(row):
            parts = []
            for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                v = row.get(c)
                if pd.notna(v) and str(v).strip():
                    parts.append(str(v).strip())
            return "|".join(parts) if parts else None
        sw['person_uid'] = sw.apply(make_person_uid, axis=1)

    # Choose the columns present in swipes for aggregation
    sel_cols = [c for c in [
        'LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
        'CardNumber', 'EmployeeID', 'EmployeeName', 'ObjectName1', 'PersonnelType'
    ] if c in sw.columns]

    # aggregator
    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist())
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0
        in_count = int((g.get('Direction') == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g.get('Direction') == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0
        card_numbers = list(pd.unique(g['CardNumber'].dropna())) if 'CardNumber' in g.columns else []
        card_number = card_numbers[0] if card_numbers else None

        # pick name/id/personnel from any available column (prefer the explicit ones)
        employee_id = None
        employee_name = None
        personnel_type = None
        if 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna()
            employee_id = vals.iloc[0] if not vals.empty else None
        # prefer EmployeeName then ObjectName1
        if 'EmployeeName' in g.columns:
            vals = g['EmployeeName'].dropna()
            if not vals.empty:
                employee_name = vals.iloc[0]
        if employee_name is None and 'ObjectName1' in g.columns:
            vals = g['ObjectName1'].dropna()
            if not vals.empty:
                employee_name = vals.iloc[0]
        if 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            personnel_type = vals.iloc[0] if not vals.empty else None

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type
        })

    # run groupby using selected columns (prevents pandas future warning)
    gb = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = gb.apply(agg_swipe_group).reset_index()

    # normalize durations side
    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date']).dt.date

    # merge features with durations
    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # Coalesce function that consolidates _x/_y into base if needed
    def coalesce_field(df: pd.DataFrame, base: str, fallback_order: list = None, default=None):
        """
        Ensure df[base] exists. Priority:
         - if base exists, keep it
         - else try base_x, base_y
         - else if fallback_order provided, attempt those columns in order
         - else set default
        """
        if base in df.columns:
            return
        candidates = [base + '_x', base + '_y']
        if fallback_order:
            candidates = candidates + fallback_order
        for c in candidates:
            if c in df.columns:
                df[base] = df[c]
                return
        df[base] = default

    # coalesce common fields (including EmployeeName variants)
    coalesce_field(merged, 'CountSwipes', default=0)
    coalesce_field(merged, 'DurationSeconds', default=0)
    coalesce_field(merged, 'FirstSwipe', default=pd.NaT)
    coalesce_field(merged, 'LastSwipe', default=pd.NaT)
    coalesce_field(merged, 'CardNumber', default=None)
    coalesce_field(merged, 'EmployeeID', default=None, fallback_order=['ObjectName1'])
    # EmployeeName: prefer EmployeeName, then ObjectName1, then EmployeeName_x/_y, then EmployeeName_y
    coalesce_field(merged, 'EmployeeName', fallback_order=['ObjectName1', 'EmployeeName_x', 'EmployeeName_y'], default=None)
    coalesce_field(merged, 'PersonnelType', default=None)

    # Now ensure types & defaults
    merged['DurationSeconds'] = merged.get('DurationSeconds', 0).fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged.get('MaxSwipeGapSeconds', 0).fillna(0).astype(int)
    merged['ShortGapCount'] = merged.get('ShortGapCount', 0).fillna(0).astype(int)
    merged['RejectionCount'] = merged.get('RejectionCount', 0).fillna(0).astype(int)
    merged['UniqueLocations'] = merged.get('UniqueLocations', 0).fillna(0).astype(int)
    merged['UniqueDoors'] = merged.get('UniqueDoors', 0).fillna(0).astype(int)

    # ensure FirstSwipe/LastSwipe exist and convert to datetime
    for col in ['FirstSwipe', 'LastSwipe']:
        if col not in merged.columns:
            merged[col] = pd.NaT
        else:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')

    # boolean flags
    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # historical presence map
    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')
    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: x in hist_map if pd.notna(x) else False)

    return merged


# ----------------- Scenarios -----------------
def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return (row.get('CountSwipes') or 0) <= 2 and (row.get('CountSwipes') or 0) > 0

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        try:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
        except Exception:
            pass
    if 'TotalSwipes_median' in HIST_DF.columns and not HIST_DF.empty:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    return cur > 50

def scenario_repeated_short_breaks(row):
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, full_df=None):
    return False  # evaluated later using badge_map

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row):
    # boolean version - actual membership will be applied in run_trend_for_date
    return False


SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap)
]


def run_trend_for_date(target_date: date, outdir: str = "./outputs"):
    logging.info("run_trend_for_date: date=%s (Pune test)", target_date)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city='Pune')
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # Build badge map (Date, CardNumber) => distinct person count (badge-sharing)
    badge_map = {}
    if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            badge_map = {(row.Date, row.CardNumber): row.distinct_users for row in grouped_card.itertuples(index=False)}

    # Build swipe-overlap details: map (Date, person_uid) -> set(other_person_uid)
    swipe_overlap_map = {}
    overlap_window_seconds = 2  # adjustable window for "simultaneous" swipes
    if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
        tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna(subset=['Door', 'LocaleMessageTime', 'person_uid'])
        if not tmp.empty:
            tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
            for (d, door), g in tmp.groupby(['Date', 'Door']):
                # list of tuples (time, uid)
                items = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                n = len(items)
                j = 0
                for i in range(n):
                    t_i, uid_i = items[i]
                    j = max(j, i+1)
                    while j < n and (items[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                        uid_j = items[j][1]
                        if uid_i != uid_j:
                            swipe_overlap_map.setdefault((d, uid_i), set()).add(uid_j)
                            swipe_overlap_map.setdefault((d, uid_j), set()).add(uid_i)
                        j += 1

    # Apply scenarios — special handling for badge_sharing and swipe_overlap
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            def badge_fn(r):
                card = r.get('CardNumber')
                d = r.get('Date')
                if pd.isna(card) or card is None:
                    return False
                return badge_map.get((d, card), 0) > 1
            features[name] = features.apply(badge_fn, axis=1)
        elif name == "swipe_overlap":
            # boolean flag
            def overlap_flag_fn(r):
                d = r.get('Date')
                uid = r.get('person_uid')
                if pd.isna(uid) or d is None:
                    return False
                return (d, uid) in swipe_overlap_map
            features[name] = features.apply(overlap_flag_fn, axis=1)
            # create readable OverlapWith column listing other uids
            def overlap_with_fn(r):
                d = r.get('Date')
                uid = r.get('person_uid')
                if pd.isna(uid) or d is None:
                    return None
                others = swipe_overlap_map.get((d, uid), None)
                if not others:
                    return None
                # join sorted uids
                try:
                    return ";".join(sorted(str(o) for o in others))
                except Exception:
                    return ";".join(str(o) for o in others)
            features['OverlapWith'] = features.apply(overlap_with_fn, axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    # Create reason text
    def reasons_for_row(r):
        reasons = [name for name, _ in SCENARIOS if bool(r.get(name))]
        return "; ".join(reasons) if reasons else None

    features['Reasons'] = features.apply(reasons_for_row, axis=1)

    # Coalesce employee name once more if merge produced weird suffixes
    # prefer EmployeeName > ObjectName1 > EmployeeName_x/_y
    for candidate in ('EmployeeName', 'ObjectName1', 'EmployeeName_x', 'EmployeeName_y'):
        if candidate in features.columns and 'EmployeeName' not in features.columns:
            features.rename(columns={candidate: 'EmployeeName'}, inplace=True)

    # --- cleanup duplicate suffixed columns before saving ---
    cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
    if cols_to_drop:
        for c in cols_to_drop:
            base = c[:-2]
            if base in features.columns:
                try:
                    features.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                try:
                    features.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass
    features = features.loc[:, ~features.columns.duplicated()]

    # Ensure boolean columns are standard Python booleans (helps JSON output)
    for col in [name for name, _ in SCENARIOS]:
        if col in features.columns:
            features[col] = features[col].astype(bool)

    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    features.to_csv(out_csv, index=False)
    logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(features))
    return features


if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today)
    print("Completed; rows:", len(df))




















from datetime import date, datetime, time
from pathlib import Path
import pandas as pd
import numpy as np
import logging

# IMPORTANT: import duration_report as a top-level module (file in same folder).
from duration_report import run_for_date

# try to load historical profile (current_analysis.csv) if present
HIST_PATH = Path(__file__).parent / "current_analysis.csv"
if HIST_PATH.exists():
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()
else:
    logging.warning("Historical profile file current_analysis.csv not found; history-based scenarios will fallback.")
    HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)


def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()
    sw['LocaleMessageTime'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce')
    sw['Date'] = sw['LocaleMessageTime'].dt.date

    # create person_uid if missing (same logic as duration_report)
    if 'person_uid' not in sw.columns:
        def make_person_uid(row):
            parts = []
            for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                v = row.get(c)
                if pd.notna(v) and str(v).strip():
                    parts.append(str(v).strip())
            return "|".join(parts) if parts else None
        sw['person_uid'] = sw.apply(make_person_uid, axis=1)

    # columns we will use for aggregation (choose those present to avoid apply warnings)
    sel_cols = [c for c in [
        'LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
        'CardNumber', 'EmployeeID', 'ObjectName1', 'PersonnelType'
    ] if c in sw.columns]

    # aggregator for each person/date
    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist())
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0
        in_count = int((g.get('Direction') == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g.get('Direction') == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0
        card_numbers = list(pd.unique(g['CardNumber'].dropna())) if 'CardNumber' in g.columns else []
        card_number = card_numbers[0] if card_numbers else None

        employee_id = None
        employee_name = None
        personnel_type = None
        if 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna()
            employee_id = vals.iloc[0] if not vals.empty else None
        if 'ObjectName1' in g.columns:
            vals = g['ObjectName1'].dropna()
            employee_name = vals.iloc[0] if not vals.empty else None
        if 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            personnel_type = vals.iloc[0] if not vals.empty else None

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type
        })

    # Use only the selected columns in the groupby to avoid FutureWarning
    gb = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = gb.apply(agg_swipe_group).reset_index()

    # normalize durations side
    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date']).dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # helper to coalesce possible _x/_y duplicates created by merge
    def coalesce_col(df, base, default=0):
        if base in df.columns:
            return
        a = base + '_x'
        b = base + '_y'
        if a in df.columns:
            df[base] = df[a]
        elif b in df.columns:
            df[base] = df[b]
        else:
            # create column with default scalar or series
            df[base] = default

    # coalesce likely duplicated fields coming from durations
    for col in ['CountSwipes', 'DurationSeconds', 'FirstSwipe', 'LastSwipe', 'CardNumber', 'EmployeeID', 'EmployeeName', 'PersonnelType']:
        coalesce_col(merged, col, default=np.nan if 'Swipe' in col else 0)

    # ensure types and fill defaults
    merged['DurationSeconds'] = merged.get('DurationSeconds', 0).fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged.get('MaxSwipeGapSeconds', 0).fillna(0).astype(int)
    merged['ShortGapCount'] = merged.get('ShortGapCount', 0).fillna(0).astype(int)
    merged['RejectionCount'] = merged.get('RejectionCount', 0).fillna(0).astype(int)
    merged['UniqueLocations'] = merged.get('UniqueLocations', 0).fillna(0).astype(int)
    merged['UniqueDoors'] = merged.get('UniqueDoors', 0).fillna(0).astype(int)

    # ensure FirstSwipe/LastSwipe exist (they come from durations) and are datetime
    for col in ['FirstSwipe', 'LastSwipe']:
        if col not in merged.columns:
            merged[col] = pd.NaT
        else:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')

    # easy boolean flags
    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # helper: map historical medians/stds for lookups (if available)
    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')

    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: x in hist_map if pd.notna(x) else False)

    return merged


# ----------------- Scenarios -----------------
def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return (row.get('CountSwipes') or 0) <= 2 and (row.get('CountSwipes') or 0) > 0

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        try:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
        except Exception:
            pass
    if 'TotalSwipes_median' in HIST_DF.columns and not HIST_DF.empty:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    return cur > 50

def scenario_repeated_short_breaks(row):
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, full_df=None):
    return False  # evaluated later using badge_map

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)

def scenario_swipe_overlap(row):
    # placeholder; actual overlap membership is computed in run_trend_for_date and applied later
    return False


SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days),
    ("swipe_overlap", scenario_swipe_overlap)
]


def run_trend_for_date(target_date: date, outdir: str = "./outputs"):
    logging.info("run_trend_for_date: date=%s (Pune test)", target_date)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city='Pune')
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # Build badge map (Date, CardNumber) => distinct person count (badge-sharing)
    badge_map = {}
    if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            badge_map = {(row.Date, row.CardNumber): row.distinct_users for row in grouped_card.itertuples(index=False)}

    # Build swipe-overlap set: identify person_uids that were involved in near-simultaneous swipes
    # Definition: same Door, same Date, different person_uid AND timestamps within overlap_window_seconds (default 2s)
    swipe_overlap_set = set()
    overlap_window_seconds = 2  # adjustable
    if {'Door', 'LocaleMessageTime', 'person_uid', 'Date'}.issubset(swipes.columns):
        tmp = swipes[['Door', 'LocaleMessageTime', 'person_uid', 'Date']].dropna(subset=['Door', 'LocaleMessageTime', 'person_uid'])
        if not tmp.empty:
            tmp = tmp.sort_values(['Door', 'LocaleMessageTime'])
            # group by Date+Door to search for overlap
            for (d, door), g in tmp.groupby(['Date', 'Door']):
                times = list(g[['LocaleMessageTime', 'person_uid']].itertuples(index=False, name=None))
                # two-pointer scan
                n = len(times)
                j = 0
                for i in range(n):
                    t_i, uid_i = times[i]
                    # advance j to first index > i where time difference > window
                    j = max(j, i+1)
                    while j < n and (times[j][0] - t_i).total_seconds() <= overlap_window_seconds:
                        # different persons within window -> mark both uids
                        uid_j = times[j][1]
                        if uid_i != uid_j:
                            swipe_overlap_set.add((d, uid_i))
                            swipe_overlap_set.add((d, uid_j))
                        j += 1

    # Apply scenarios
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            def badge_fn(r):
                card = r.get('CardNumber')
                d = r.get('Date')
                if pd.isna(card) or card is None:
                    return False
                key = (d, card)
                return badge_map.get(key, 0) > 1
            features[name] = features.apply(badge_fn, axis=1)
        elif name == "swipe_overlap":
            def overlap_fn(r):
                d = r.get('Date')
                uid = r.get('person_uid')
                if pd.isna(uid) or d is None:
                    return False
                return (d, uid) in swipe_overlap_set
            features[name] = features.apply(overlap_fn, axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    # Create reason text
    def reasons_for_row(r):
        reasons = [name for name, _ in SCENARIOS if bool(r.get(name))]
        return "; ".join(reasons) if reasons else None

    features['Reasons'] = features.apply(reasons_for_row, axis=1)

    # --- cleanup duplicate suffixed columns before saving ---
    cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
    if cols_to_drop:
        for c in cols_to_drop:
            base = c[:-2]
            if base in features.columns:
                try:
                    features.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                try:
                    features.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass
    features = features.loc[:, ~features.columns.duplicated()]

    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    features.to_csv(out_csv, index=False)
    logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(features))
    return features










from flask import Flask, jsonify, request
from datetime import datetime, timedelta
from pathlib import Path
import logging
import pandas as pd

# Try to enable CORS if available; otherwise continue without it.
try:
    from flask_cors import CORS
    _HAS_CORS = True
except Exception:
    CORS = None
    _HAS_CORS = False

# import runner (local)
from trend_runner import run_trend_for_date

app = Flask(__name__)
if _HAS_CORS:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS. Install Flask-Cors to enable cross-origin access from browser.")

logging.basicConfig(level=logging.INFO)

# Resolve output directory relative to this file
BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Return a clean JSON-serializable sample. Drop any _x/_y duplicates and convert datetimes.
    """
    if df is None or df.empty:
        return []
    # drop _x/_y columns
    cols_to_drop = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    if cols_to_drop:
        # prefer base name (already present) otherwise rename _x/_y -> base
        for c in cols_to_drop:
            base = c[:-2]
            if base in df.columns:
                try:
                    df = df.drop(columns=[c])
                except Exception:
                    pass
            else:
                try:
                    df = df.rename(columns={c: base})
                except Exception:
                    pass
    df = df.loc[:, ~df.columns.duplicated()]
    # convert datetimes if present
    for dtcol in ('FirstSwipe','LastSwipe','LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
            except Exception:
                pass
    return df.head(max_rows).to_dict(orient='records')


@app.route('/')
def root():
    return "Trend Analysis API — Pune test"

@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    """
    /run?date=YYYY-MM-DD                 -> single date
    /run?start=YYYY-MM-DD&end=YYYY-MM-DD -> date range inclusive
    POST json: { "date": "..."} or { "start": "...", "end":"..." }
    Returns summary + sample rows + list of generated files.
    """
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.json or {}

    # parse date(s)
    date_str = params.get('date')
    start_str = params.get('start')
    end_str = params.get('end')

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            # default: today
            dates = [datetime.now().date()]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    combined_rows = []
    files = []
    total_rows = 0
    total_flagged = 0
    samples = []

    for d in dates:
        try:
            df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
        files.append(csv_path.name if csv_path.exists() else None)

        if df is None or df.empty:
            continue

        # ensure Reasons column exists
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        # count
        total_rows += len(df)
        total_flagged += int(df['Reasons'].notna().sum())

        # sample top flagged first, else top rows
        flagged = df[df['Reasons'].notna()]
        sample_df = flagged.head(10) if not flagged.empty else df.head(10)
        samples.extend(_clean_sample_df(sample_df, max_rows=10))

        # keep combined rows if user wants full (we won't return full by default)
        combined_rows.append(df)

    # combine for convenience if needed
    combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()

    # produce response
    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "files": [f for f in files if f],
        "rows": int(total_rows),
        "flagged_rows": int(total_flagged),
        "sample": samples[:20]  # limit sample size
    }
    return jsonify(resp)


@app.route('/latest', methods=['GET'])
def latest_results():
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    sample = _clean_sample_df(df, max_rows=5)
    return jsonify({
        "file": latest.name,
        "rows": int(len(df)),
        "sample": sample
    })


@app.route('/record', methods=['GET'])
def get_record():
    """
    /record?employee_id=... or /record?person_uid=...
    Returns matching rows from latest output (cleaned).
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({'error':'no outputs'}), 404
    try:
        df = pd.read_csv(csvs[0], parse_dates=['FirstSwipe','LastSwipe'], infer_datetime_format=True)
    except Exception:
        df = pd.read_csv(csvs[0])
    # clean suffixes
    df_clean = pd.DataFrame(_clean_sample_df(df, max_rows=len(df)))
    if q is None:
        return jsonify(df_clean.head(10).to_dict(orient='records'))
    mask = (df_clean.get('EmployeeID', '').astype(str) == str(q)) | (df_clean.get('person_uid', '').astype(str) == str(q))
    rows = df_clean[mask]
    if rows.empty:
        return jsonify({'error':'not found'}), 404
    return jsonify(rows.to_dict(orient='records'))


if __name__ == "__main__":
    # bind 0.0.0.0 so it is reachable on LAN (if firewall / network allows)
    app.run(host="0.0.0.0", port=8002, debug=True)


















from datetime import date, datetime, time
from pathlib import Path
import pandas as pd
import numpy as np
import logging

# IMPORTANT: import duration_report as a top-level module (file in same folder).
from duration_report import run_for_date

# try to load historical profile (current_analysis.csv) if present
HIST_PATH = Path(__file__).parent / "current_analysis.csv"
if HIST_PATH.exists():
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()
else:
    logging.warning("Historical profile file current_analysis.csv not found; history-based scenarios will fallback.")
    HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)


def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()
    sw['LocaleMessageTime'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce')
    sw['Date'] = sw['LocaleMessageTime'].dt.date

    # create person_uid if missing (same logic as duration_report)
    if 'person_uid' not in sw.columns:
        def make_person_uid(row):
            parts = []
            for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                v = row.get(c)
                if pd.notna(v) and str(v).strip():
                    parts.append(str(v).strip())
            return "|".join(parts) if parts else None
        sw['person_uid'] = sw.apply(make_person_uid, axis=1)

    # columns we will use for aggregation (choose those present to avoid apply warnings)
    sel_cols = [c for c in [
        'LocaleMessageTime', 'Direction', 'Door', 'PartitionName2', 'Rejection_Type',
        'CardNumber', 'EmployeeID', 'ObjectName1', 'PersonnelType'
    ] if c in sw.columns]

    # aggregator for each person/date
    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist())
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0
        in_count = int((g.get('Direction') == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g.get('Direction') == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0
        card_numbers = list(pd.unique(g['CardNumber'].dropna())) if 'CardNumber' in g.columns else []
        card_number = card_numbers[0] if card_numbers else None

        employee_id = None
        employee_name = None
        personnel_type = None
        if 'EmployeeID' in g.columns:
            vals = g['EmployeeID'].dropna()
            employee_id = vals.iloc[0] if not vals.empty else None
        if 'ObjectName1' in g.columns:
            vals = g['ObjectName1'].dropna()
            employee_name = vals.iloc[0] if not vals.empty else None
        if 'PersonnelType' in g.columns:
            vals = g['PersonnelType'].dropna()
            personnel_type = vals.iloc[0] if not vals.empty else None

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type
        })

    # Use only the selected columns in the groupby to avoid the FutureWarning
    gb = sw[['person_uid', 'Date'] + sel_cols].groupby(['person_uid', 'Date'])
    grouped = gb.apply(agg_swipe_group).reset_index()

    # normalize durations side
    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date']).dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # helper to coalesce possible _x/_y duplicates created by merge
    def coalesce_col(df, base, default=0):
        if base in df.columns:
            return
        a = base + '_x'
        b = base + '_y'
        if a in df.columns:
            df[base] = df[a]
        elif b in df.columns:
            df[base] = df[b]
        else:
            # create column with default scalar or series
            df[base] = default

    # coalesce likely duplicated fields coming from durations
    for col in ['CountSwipes', 'DurationSeconds', 'FirstSwipe', 'LastSwipe', 'CardNumber', 'EmployeeID', 'EmployeeName', 'PersonnelType']:
        coalesce_col(merged, col, default=np.nan if 'Swipe' in col else 0)

    # ensure types and fill defaults
    merged['DurationSeconds'] = merged.get('DurationSeconds', 0).fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged.get('MaxSwipeGapSeconds', 0).fillna(0).astype(int)
    merged['ShortGapCount'] = merged.get('ShortGapCount', 0).fillna(0).astype(int)
    merged['RejectionCount'] = merged.get('RejectionCount', 0).fillna(0).astype(int)
    merged['UniqueLocations'] = merged.get('UniqueLocations', 0).fillna(0).astype(int)
    merged['UniqueDoors'] = merged.get('UniqueDoors', 0).fillna(0).astype(int)

    # ensure FirstSwipe/LastSwipe exist (they come from durations) and are datetime
    for col in ['FirstSwipe', 'LastSwipe']:
        if col not in merged.columns:
            merged[col] = pd.NaT
        else:
            merged[col] = pd.to_datetime(merged[col], errors='coerce')

    # easy boolean flags
    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # helper: map historical medians/stds for lookups (if available)
    hist_map = {}
    if not HIST_DF.empty and 'EmployeeID' in HIST_DF.columns:
        hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')

    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: x in hist_map if pd.notna(x) else False)

    return merged


# ----------------- Scenarios -----------------
def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return (row.get('CountSwipes') or 0) <= 2 and (row.get('CountSwipes') or 0) > 0

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        try:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
        except Exception:
            pass
    if 'TotalSwipes_median' in HIST_DF.columns and not HIST_DF.empty:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    return cur > 50

def scenario_repeated_short_breaks(row):
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, full_df=None):
    return False  # resolved later in run_trend_for_date

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        return False
    return False

def scenario_high_variance_duration(row):
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            return (std / med) > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)


SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days)
]


def run_trend_for_date(target_date: date, outdir: str = "./outputs"):
    logging.info("run_trend_for_date: date=%s (Pune test)", target_date)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city='Pune')
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # Build badge map (Date, CardNumber) => distinct person count
    badge_map = {}
    if 'CardNumber' in swipes.columns and 'person_uid' in swipes.columns and 'Date' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            badge_map = {(row.Date, row.CardNumber): row.distinct_users for row in grouped_card.itertuples(index=False)}

    # Apply scenarios
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            def badge_fn(r):
                card = r.get('CardNumber')
                d = r.get('Date')
                if pd.isna(card) or card is None:
                    return False
                key = (d, card)
                return badge_map.get(key, 0) > 1
            features[name] = features.apply(badge_fn, axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    # Create reason text
    def reasons_for_row(r):
        reasons = [name for name, _ in SCENARIOS if bool(r.get(name))]
        return "; ".join(reasons) if reasons else None

    features['Reasons'] = features.apply(reasons_for_row, axis=1)

    # --- cleanup duplicate suffixed columns before saving ---
    cols_to_drop = [c for c in features.columns if c.endswith("_x") or c.endswith("_y")]
    if cols_to_drop:
        # prefer non-suffixed version if exists; otherwise keep _x as canonical
        for c in cols_to_drop:
            base = c[:-2]
            if base in features.columns:
                # we already have base, drop the suffixed column
                try:
                    features.drop(columns=[c], inplace=True)
                except Exception:
                    pass
            else:
                # rename suffixed column to base (prefer _x over _y if both present)
                try:
                    features.rename(columns={c: base}, inplace=True)
                except Exception:
                    pass
    # lastly drop any remaining duplicate-named columns (keep first)
    features = features.loc[:, ~features.columns.duplicated()]

    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    features.to_csv(out_csv, index=False)
    logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(features))
    return features


if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today)
    print("Completed; rows:", len(df))




