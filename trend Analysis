# db.py - connecting & fetching live swipe data
from sqlalchemy import create_engine
import urllib
import pandas as pd
from config import DB_CONFIG
from pathlib import Path

def fetch_live_swipe():
    """
    Fetch a raw 'live' swipe result using the SQL defined below.
    Uses DB_CONFIG['database'] so update config.py to change the target DB.
    Returns a pandas.DataFrame (may be empty if an error occurs).
    """
    print("db.fetch_live_swipe: starting")
    try:
        driver = DB_CONFIG.get('driver', 'ODBC Driver 17 for SQL Server')
        connection_string = (
            f"DRIVER={{{driver}}};"
            f"SERVER={DB_CONFIG['server']};"
            f"DATABASE={DB_CONFIG['database']};"
            f"UID={DB_CONFIG['username']};"
            f"PWD={DB_CONFIG['password']};"
            "TrustServerCertificate=Yes;"
        )

        connection_uri = f"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(connection_string)}"
        engine = create_engine(connection_uri)
        print("db.fetch_live_swipe: SQLAlchemy engine created")

        db = DB_CONFIG['database']

        # Query adapted to use configured database name (select columns used by downstream code)
        query = f"""
WITH CombinedQuery AS (
  SELECT 
    DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS LocaleMessageTime,
    t1.ObjectName1 AS ObjectName1,
    t1.PartitionName2 AS PartitionName2,
    CASE WHEN t3.Name IN ('Contractor','Terminated Contractor') THEN t2.Text12
         WHEN t3.Name IN ('Property Management', 'Visitor', 'Temp Badge') THEN t2.Text9
         ELSE CAST(t2.Int1 AS NVARCHAR) END AS EmployeeID,
    t3.Name AS PersonnelType,
    t1.ObjectName2 AS Door,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.[Value]
    ) AS CardNumber,
    adm.Value AS AdmitCode,
    dir.Value AS Direction,
    rej.Value AS Rejection_Type
  FROM [{db}].dbo.ACVSUJournalLog AS t1
  LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
  LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeId = t3.ObjectID
  LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred AS adm ON t1.XmlGUID = adm.GUID AND adm.Name = 'AdmitCode'
  LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred AS dir ON t1.XmlGUID = dir.GUID AND dir.Name = 'Direction'
  LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred AS rej ON t1.XmlGUID = rej.GUID AND rej.Name = 'RejectCode'
  LEFT JOIN [{db}].dbo.ACVSUJournalLogxml AS t_xml ON t1.XmlGUID = t_xml.GUID
  LEFT JOIN (
    SELECT GUID, [Value] FROM [{db}].dbo.ACVSUJournalLogxmlShred WHERE [Name] IN ('Card','CHUID')
  ) AS sc ON t1.XmlGUID = sc.GUID
  WHERE t1.MessageType IN ('CardAdmitted','CardRejected')
    AND t1.PartitionName2 = 'APAC.Default'
    AND CONVERT(date, DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC)) >= '2025-05-01'
)
SELECT
  LocaleMessageTime,
  CONVERT(date, LocaleMessageTime) AS DateOnly,
  CONVERT(time(0), LocaleMessageTime) AS Swipe_Time,
  EmployeeID,
  ObjectName1,
  PersonnelType,
  PartitionName2,
  Door,
  CardNumber,
  AdmitCode,
  Direction,
  Rejection_Type
FROM CombinedQuery
ORDER BY LocaleMessageTime ASC;
"""
        print("db.fetch_live_swipe: SQL prepared (preview):")
        print(query[:400], "...")
        df = pd.read_sql(query, engine)
        print("db.fetch_live_swipe: query executed; rows =", df.shape[0])
        print("db.fetch_live_swipe: columns =", df.columns.tolist())
        return df

    except Exception as e:
        print("db.fetch_live_swipe: connection/query error:", e)
        return pd.DataFrame()









# trend_runner.py (updated)
from datetime import date, datetime, time
from pathlib import Path
import pandas as pd
import numpy as np
import logging
from duration_report import run_for_date

# try to load historical profile (current_analysis.csv) if present
HIST_PATH = Path(__file__).parent / "current_analysis.csv"
if HIST_PATH.exists():
    try:
        HIST_DF = pd.read_csv(HIST_PATH)
        logging.info("Loaded historical profile from %s (rows=%d)", HIST_PATH, len(HIST_DF))
    except Exception as e:
        logging.warning("Failed to load historical profile: %s", e)
        HIST_DF = pd.DataFrame()
else:
    logging.warning("Historical profile file current_analysis.csv not found; history-based scenarios will fallback.")
    HIST_DF = pd.DataFrame()

OUTDIR = Path("./outputs")
OUTDIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(level=logging.INFO)


def compute_features(swipes: pd.DataFrame, durations: pd.DataFrame) -> pd.DataFrame:
    """
    Compute per person-per-date features used by scenarios.
    - swipes: raw swipe logs (LocaleMessageTime, Direction, Door, EmployeeID, CardNumber, PartitionName2, ...)
    - durations: result from compute_daily_durations (person_uid, Date, FirstSwipe, LastSwipe, CountSwipes, DurationSeconds, ...)
    Returns merged feature DataFrame
    """
    if swipes is None or swipes.empty:
        return pd.DataFrame()

    sw = swipes.copy()
    sw['LocaleMessageTime'] = pd.to_datetime(sw['LocaleMessageTime'], errors='coerce')
    sw['Date'] = sw['LocaleMessageTime'].dt.date

    # create person_uid if missing (same logic as duration_report)
    if 'person_uid' not in sw.columns:
        def make_person_uid(row):
            parts = []
            for c in ('EmployeeIdentity', 'EmployeeID', 'EmployeeName'):
                v = row.get(c)
                if pd.notna(v) and str(v).strip():
                    parts.append(str(v).strip())
            return "|".join(parts) if parts else None
        sw['person_uid'] = sw.apply(make_person_uid, axis=1)

    # compute per-person-date aggregated swipe stats (including gaps and short-gap count)
    def agg_swipe_group(g):
        times = sorted(g['LocaleMessageTime'].dropna().tolist())
        gaps = []
        short_gap_count = 0
        for i in range(1, len(times)):
            s = (times[i] - times[i-1]).total_seconds()
            gaps.append(s)
            if s <= 5*60:  # <= 5 minutes
                short_gap_count += 1
        max_gap = int(max(gaps)) if gaps else 0
        in_count = int((g.get('Direction') == 'InDirection').sum()) if 'Direction' in g.columns else 0
        out_count = int((g.get('Direction') == 'OutDirection').sum()) if 'Direction' in g.columns else 0
        unique_doors = int(g['Door'].nunique()) if 'Door' in g.columns else 0
        unique_locations = int(g['PartitionName2'].nunique()) if 'PartitionName2' in g.columns else 0
        rejection_count = int(g['Rejection_Type'].notna().sum()) if 'Rejection_Type' in g.columns else 0
        card_numbers = list(pd.unique(g['CardNumber'].dropna())) if 'CardNumber' in g.columns else []
        card_number = card_numbers[0] if card_numbers else None

        # EmployeeID/Name keep-first
        employee_id = None
        employee_name = None
        personnel_type = None
        if 'EmployeeID' in g.columns:
            employee_id = g['EmployeeID'].dropna().iloc[0] if not g['EmployeeID'].dropna().empty else None
        if 'ObjectName1' in g.columns:
            employee_name = g['ObjectName1'].dropna().iloc[0] if not g['ObjectName1'].dropna().empty else None
        if 'PersonnelType' in g.columns:
            personnel_type = g['PersonnelType'].dropna().iloc[0] if not g['PersonnelType'].dropna().empty else None

        return pd.Series({
            'CountSwipes': int(len(g)),
            'MaxSwipeGapSeconds': max_gap,
            'ShortGapCount': int(short_gap_count),
            'InCount': in_count,
            'OutCount': out_count,
            'UniqueDoors': unique_doors,
            'UniqueLocations': unique_locations,
            'RejectionCount': rejection_count,
            'CardNumber': card_number,
            'EmployeeID': employee_id,
            'EmployeeName': employee_name,
            'PersonnelType': personnel_type
        })

    grouped = sw.groupby(['person_uid', 'Date']).apply(agg_swipe_group).reset_index()

    # normalize durations
    dur = pd.DataFrame() if durations is None else durations.copy()
    if not dur.empty and 'Date' in dur.columns:
        dur['Date'] = pd.to_datetime(dur['Date']).dt.date

    merged = pd.merge(grouped, dur, how='left', on=['person_uid', 'Date'])

    # ensure important columns exist and cast types
    merged['DurationSeconds'] = merged.get('DurationSeconds', 0).fillna(0).astype(float)
    merged['DurationMinutes'] = (merged['DurationSeconds'] / 60.0).astype(float)
    merged['CountSwipes'] = merged['CountSwipes'].fillna(0).astype(int)
    merged['MaxSwipeGapSeconds'] = merged['MaxSwipeGapSeconds'].fillna(0).astype(int)
    merged['ShortGapCount'] = merged['ShortGapCount'].fillna(0).astype(int)
    merged['RejectionCount'] = merged['RejectionCount'].fillna(0).astype(int)
    merged['UniqueLocations'] = merged['UniqueLocations'].fillna(0).astype(int)
    merged['UniqueDoors'] = merged['UniqueDoors'].fillna(0).astype(int)

    # ensure FirstSwipe/LastSwipe exist (they come from durations)
    for col in ['FirstSwipe', 'LastSwipe']:
        if col not in merged.columns:
            merged[col] = pd.NaT

    # easy boolean flags
    merged['OnlyIn'] = ((merged.get('InCount', 0) > 0) & (merged.get('OutCount', 0) == 0)).astype(int)
    merged['OnlyOut'] = ((merged.get('OutCount', 0) > 0) & (merged.get('InCount', 0) == 0)).astype(int)
    merged['SingleDoor'] = (merged.get('UniqueDoors', 0) <= 1).astype(int)

    # helper: map historical medians/stds for lookups (if available)
    hist_map = {}
    if not HIST_DF.empty:
        # ensure consistent types/columns - prefer EmployeeID as key
        if 'EmployeeID' in HIST_DF.columns:
            hist_map = HIST_DF.set_index('EmployeeID').to_dict(orient='index')

    merged['EmpHistoryPresent'] = merged['EmployeeID'].apply(lambda x: x in hist_map if pd.notna(x) else False)

    return merged


# ----------------- Scenarios -----------------
# Each scenario returns True/False for a row (input: pandas Series)

def scenario_long_gap(row):
    return (row.get('MaxSwipeGapSeconds') or 0) >= 90 * 60

def scenario_short_duration(row):
    return (row.get('DurationMinutes') or 0) < 240

def scenario_coffee_badging(row):
    # many small swipes but short overall duration
    return (row.get('CountSwipes') or 0) >= 4 and (row.get('DurationMinutes') or 0) < 60

def scenario_low_swipe_count(row):
    return (row.get('CountSwipes') or 0) <= 2 and (row.get('CountSwipes') or 0) > 0

def scenario_single_door(row):
    return (row.get('UniqueDoors') or 0) <= 1

def scenario_only_in(row):
    return int(row.get('OnlyIn', 0)) == 1

def scenario_only_out(row):
    return int(row.get('OnlyOut', 0)) == 1

def scenario_overtime(row):
    # DurationMinutes already in minutes
    return (row.get('DurationMinutes') or 0) >= 10 * 60

def scenario_very_long_duration(row):
    return (row.get('DurationMinutes') or 0) >= 16 * 60

def scenario_zero_swipes(row):
    return int(row.get('CountSwipes', 0)) == 0

def scenario_unusually_high_swipes(row):
    # compare to historical median if present, fallback to population median
    cur = row.get('CountSwipes') or 0
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        try:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
            median = float(rec.get('TotalSwipes_median', np.nan))
            if np.isfinite(median) and median > 0:
                return cur > 3 * median
        except Exception:
            pass
    # fallback: global median from HIST_DF TotalSwipes_median if exists
    if 'TotalSwipes_median' in HIST_DF.columns and not HIST_DF.empty:
        global_med = HIST_DF['TotalSwipes_median'].median()
        if pd.notna(global_med) and global_med > 0:
            return cur > 3 * global_med
    # final fallback: > 50 swipes in day is suspicious
    return cur > 50

def scenario_repeated_short_breaks(row):
    # many short gaps <= 5 minutes
    return (row.get('ShortGapCount') or 0) >= 3

def scenario_multiple_location_same_day(row):
    return (row.get('UniqueLocations') or 0) > 1

def scenario_weekend_activity(row):
    try:
        d = pd.to_datetime(row['Date'])
        return d.weekday() >= 5  # 5=Saturday,6=Sunday
    except Exception:
        return False

def scenario_repeated_rejection_count(row):
    return (row.get('RejectionCount') or 0) >= 2

def scenario_badge_sharing_suspected(row, full_df=None):
    """
    Badge sharing: same CardNumber for >1 person_uid on same date.
    To implement this we need access to the whole day's swipes; trend_runner will build a helper map.
    We implement a placeholder returning False here; the engine will set BadgeSharingMap and call this variant.
    """
    return False  # real check done below with map

def scenario_early_arrival_before_06(row):
    fs = row.get('FirstSwipe')
    if pd.isna(fs) or fs is None:
        return False
    try:
        t = pd.to_datetime(fs).time()
        return t < time(hour=6, minute=0, second=0)
    except Exception:
        return False

def scenario_late_exit_after_22(row):
    ls = row.get('LastSwipe')
    if pd.isna(ls) or ls is None:
        return False
    try:
        t = pd.to_datetime(ls).time()
        return t >= time(hour=22, minute=0, second=0)
    except Exception:
        return False

def scenario_shift_inconsistency(row):
    # compare DurationMinutes with historical median/std if present
    empid = row.get('EmployeeID')
    dur = row.get('DurationMinutes') or 0
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std):
            # inconsistent if outside median +/- 2.5*std
            return (dur < med - 2.5 * std) or (dur > med + 2.5 * std)
    return False

def scenario_trending_decline(row):
    # attempts to detect `trending decline` if historical file contains weekly sliding averages
    empid = row.get('EmployeeID')
    if HIST_DF is None or HIST_DF.empty:
        return False
    if 'TrendingDecline' in HIST_DF.columns:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
        if not rec.empty:
            val = rec.iloc[0].get('TrendingDecline')
            return str(val).strip().lower() == 'yes' if pd.notna(val) else False
    return False

def scenario_consecutive_absent_days(row):
    # fallback: check if CountSwipes == 0 and historical field exists marking consecutive absences
    if row.get('CountSwipes') == 0:
        empid = row.get('EmployeeID')
        if HIST_DF is not None and not HIST_DF.empty and 'ConsecAbsent3Plus' in HIST_DF.columns:
            rec = HIST_DF[HIST_DF['EmployeeID'] == empid]
            if not rec.empty:
                v = rec.iloc[0].get('ConsecAbsent3Plus')
                return str(v).strip().lower() in ('yes', 'true', '1')
        # otherwise, treat single zero-swipe as not necessarily consecutive
        return False
    return False

def scenario_high_variance_duration(row):
    # if historical std is high relative to median (CV > 1.0)
    empid = row.get('EmployeeID')
    if HIST_DF is not None and not HIST_DF.empty and empid in HIST_DF['EmployeeID'].values:
        rec = HIST_DF[HIST_DF['EmployeeID'] == empid].iloc[0]
        med = rec.get('AvgDurationMins_median', np.nan)
        std = rec.get('AvgDurationMins_std', np.nan)
        if pd.notna(med) and pd.notna(std) and med > 0:
            cv = std / med
            return cv > 1.0
    return False

def scenario_short_duration_on_high_presence_days(row):
    days_present = row.get('DaysPresentInWeek') or 0
    dur = row.get('DurationMinutes') or 0
    return (days_present >= 4) and (dur < 240)


# SCENARIO LIST (ordered)
SCENARIOS = [
    ("long_gap_>=90min", scenario_long_gap),
    ("short_duration_<4h", scenario_short_duration),
    ("coffee_badging", scenario_coffee_badging),
    ("low_swipe_count_<=2", scenario_low_swipe_count),
    ("single_door", scenario_single_door),
    ("only_in", scenario_only_in),
    ("only_out", scenario_only_out),
    ("overtime_>=10h", scenario_overtime),
    ("very_long_duration_>=16h", scenario_very_long_duration),
    ("zero_swipes", scenario_zero_swipes),
    ("unusually_high_swipes", scenario_unusually_high_swipes),
    ("repeated_short_breaks", scenario_repeated_short_breaks),
    ("multiple_location_same_day", scenario_multiple_location_same_day),
    ("weekend_activity", scenario_weekend_activity),
    ("repeated_rejection_count", scenario_repeated_rejection_count),
    ("badge_sharing_suspected", scenario_badge_sharing_suspected),  # special: resolved later with map
    ("early_arrival_before_06", scenario_early_arrival_before_06),
    ("late_exit_after_22", scenario_late_exit_after_22),
    ("shift_inconsistency", scenario_shift_inconsistency),
    ("trending_decline", scenario_trending_decline),
    ("consecutive_absent_days", scenario_consecutive_absent_days),
    ("high_variance_duration", scenario_high_variance_duration),
    ("short_duration_on_high_presence_days", scenario_short_duration_on_high_presence_days)
]


# ---------- Main run function ----------
def run_trend_for_date(target_date: date, outdir: str = "./outputs"):
    logging.info("run_trend_for_date: date=%s (Pune test)", target_date)
    results = run_for_date(target_date, regions=['apac'], outdir=outdir, city='Pune')
    apac = results.get('apac', {})
    swipes = apac.get('swipes', pd.DataFrame())
    durations = apac.get('durations', pd.DataFrame())

    features = compute_features(swipes, durations)
    if features.empty:
        logging.warning("run_trend_for_date: no features computed")
        return pd.DataFrame()

    # Build a map for badge sharing: for the day's swipes check CardNumber -> distinct person_uid count
    badge_map = {}
    if 'CardNumber' in swipes.columns:
        tmp = swipes[['CardNumber', 'person_uid', 'Date']].dropna(subset=['CardNumber'])
        if not tmp.empty:
            grouped_card = tmp.groupby(['Date', 'CardNumber'])['person_uid'].nunique().reset_index(name='distinct_users')
            # convert to dict keyed by (Date, CardNumber)
            badge_map = {(row.Date, row.CardNumber): row.distinct_users for row in grouped_card.itertuples(index=False)}

    # Apply scenarios
    for name, fn in SCENARIOS:
        if name == "badge_sharing_suspected":
            # custom application using badge_map
            def badge_fn(r):
                card = r.get('CardNumber')
                d = r.get('Date')
                if pd.isna(card) or card is None:
                    return False
                key = (d, card)
                return badge_map.get(key, 0) > 1
            features[name] = features.apply(badge_fn, axis=1)
        else:
            features[name] = features.apply(lambda r, f=fn: bool(f(r)), axis=1)

    # Create reason text
    def reasons_for_row(r):
        reasons = [name for name, _ in SCENARIOS if bool(r.get(name))]
        return "; ".join(reasons) if reasons else None

    features['Reasons'] = features.apply(reasons_for_row, axis=1)

    out_csv = Path(outdir) / f"trend_pune_{target_date.strftime('%Y%m%d')}.csv"
    features.to_csv(out_csv, index=False)
    logging.info("run_trend_for_date: wrote %s (rows=%d)", out_csv, len(features))
    return features


if __name__ == "__main__":
    today = datetime.now().date()
    df = run_trend_for_date(today)
    print("Completed; rows:", len(df))












# logic.py - historical profile flagging logic used by app.py
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import logging

PROFILE_PATH = Path(__file__).parent / "current_analysis.csv"
if PROFILE_PATH.exists():
    try:
        employee_profile = pd.read_csv(PROFILE_PATH)
        logging.info("logic.py: loaded historical profile (%d rows)", len(employee_profile))
    except Exception as e:
        logging.warning("logic.py: failed to read current_analysis.csv: %s", e)
        employee_profile = pd.DataFrame()
else:
    logging.warning("logic.py: current_analysis.csv not found; history-based checks will be limited.")
    employee_profile = pd.DataFrame()

def _safe_get_emp_hist(employee_id):
    if employee_profile.empty or pd.isna(employee_id):
        return pd.DataFrame()
    return employee_profile[employee_profile['EmployeeID'] == employee_id]

def flag_employee(row_dict):
    """
    Input: row_dict (one live row with keys such as EmployeeID, InTime, OutTime, DurationMinutes, TotalSwipes, PersonnelType, DaysPresentInWeek, etc.)
    Returns: (flag_boolean, [reasons])
    """
    employee_id = row_dict.get('EmployeeID')
    personnel_type = row_dict.get('PersonnelType') or row_dict.get('PersonnelTypeName')
    days_present = int(row_dict.get('DaysPresentInWeek') or 0)

    logging.info("flag_employee: checking EmployeeID=%s", employee_id)

    emp_hist = _safe_get_emp_hist(employee_id)
    if emp_hist.empty:
        return False, ["No historical trend data found"]

    reasons = []

    # 1. Coffee badging
    if pd.notnull(row_dict.get('InTime')) and pd.notnull(row_dict.get('OutTime')):
        reasons.append("Coffee badging pattern detected (both InTime and OutTime present)")

    # 2. Swipe type flags
    if int(row_dict.get("OnlyIn", 0) or 0) == 1:
        reasons.append("OnlyIn entry detected")
    if int(row_dict.get("OnlyOut", 0) or 0) == 1:
        reasons.append("OnlyOut entry detected")
    if int(row_dict.get("SingleDoor", 0) or 0) == 1:
        reasons.append("SingleDoor entry detected")

    # 3. Defaulter
    if personnel_type == "Employee":
        is_defaulter = row_dict.get("Defaulter", "No")
        if str(is_defaulter).strip().lower() == "yes":
            reasons.append("Flagged as Defaulter by company policy")

    # 4. Behavior anomaly based on historical median/std (if columns exist)
    metric_column_map = {
        'DurationMinutes': ('AvgDurationMins_median', 'AvgDurationMins_std'),
        'TotalSwipes': ('TotalSwipes_median', 'TotalSwipes_std')
    }

    for metric, (median_col, std_col) in metric_column_map.items():
        try:
            live_val = row_dict.get(metric)
            if live_val is None or (isinstance(live_val, float) and np.isnan(live_val)):
                reasons.append(f"{metric} missing or null in live data")
                continue
            if median_col in emp_hist.columns and std_col in emp_hist.columns:
                median_val = float(emp_hist.iloc[0].get(median_col, np.nan))
                std_val = float(emp_hist.iloc[0].get(std_col, np.nan))
                if pd.notna(median_val) and pd.notna(std_val):
                    buffer = 2.5 * std_val
                    if live_val < median_val - buffer or live_val > median_val + buffer:
                        reasons.append(f"Abnormal {metric}: {live_val} outside expected [{median_val-buffer:.1f}, {median_val+buffer:.1f}]")
        except Exception as e:
            reasons.append(f"Error analyzing {metric}: {e}")

    # 5. Short duration on few office days
    try:
        duration = float(row_dict.get('DurationMinutes') or 0)
        if days_present < 3 and duration < 480:
            reasons.append("Duration < 8 hours on limited office days")
    except Exception as e:
        reasons.append(f"Error checking duration logic: {e}")

    return (len(reasons) > 0), reasons














# app.py - Flask API to run trend and fetch latest result
from flask import Flask, jsonify, request
from datetime import datetime
from trend_runner import run_trend_for_date
import pandas as pd
import logging
from pathlib import Path

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

@app.route('/')
def root():
    return "Trend Analysis API — Pune test"

@app.route('/run', methods=['POST', 'GET'])
def run_trend():
    # Accept date param (YYYY-MM-DD). If not provided, default to today.
    date_str = None
    if request.method == 'GET':
        date_str = request.args.get('date')
    else:
        if request.is_json:
            date_str = (request.json or {}).get('date')

    if date_str:
        try:
            target_date = datetime.strptime(date_str, "%Y-%m-%d").date()
        except Exception as e:
            return jsonify({"error": f"Invalid date format: {e}"}), 400
    else:
        target_date = datetime.now().date()

    df = run_trend_for_date(target_date, outdir="./backend/outputs")
    if df is None or df.empty:
        return jsonify({"message":"No records computed", "rows":0}), 200

    flagged = df[df['Reasons'].notna()]
    return jsonify({
        "date": target_date.isoformat(),
        "rows": int(len(df)),
        "flagged_rows": int(len(flagged))
    })

@app.route('/latest', methods=['GET'])
def latest_results():
    p = Path("./backend/outputs")
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error":"no outputs found"}), 404
    df = pd.read_csv(csvs[0])
    sample = df.head(5).to_dict(orient='records') if not df.empty else []
    return jsonify({
        "file": csvs[0].name,
        "rows": int(len(df)),
        "sample": sample
    })

if __name__ == "__main__":
    app.run(debug=True, port=8002)























I have Update all file as per above suggestion  now we got 

(.venv) PS C:\Users\W0024618\Desktop\Trend Analysis\backend> python app.py
 * Serving Flask app 'app'
 * Debug mode: on
INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on http://127.0.0.1:8002
INFO:werkzeug:Press CTRL+C to quit
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 134-209-644



Check each file line by line and fix this 
Alos Strickly Build below Mention Scenario carefully 





I implemented 10 sample scenarios in trend_runner.py. Here’s a list of 20+ you can add (copy the pattern and implement function; most rely on computed features or can be added by adding fields to compute_features):
	1.	long_gap_>=90min (done)
	2.	short_duration_<4h (done)
	3.	coffee_badging (done)
	4.	low_swipe_count_<=2 (done)
	5.	single_door (done)
	6.	only_in (done)
	7.	only_out (done)
	8.	overtime_>=10h (done)
	9.	very_long_duration_>=16h (done)
	10.	zero_swipes (done)
	11.	unusually_high_swipes (e.g., > median*3)
	12.	repeated_short_breaks (many gaps < 5 min)
	13.	multiple_location_same_day (visits recorded in multiple Partitions)
	14.	weekend_activity (swipes on Sat/Sun)
	15.	repeated_rejection_count (use Rejection_Type from raw if present)
	16.	badge_sharing_suspected (same card used by different person_uid)
	17.	early_arrival_before_06 (needs FirstSwipe field)
	18.	late_exit_after_22 (needs LastSwipe field)
	19.	shift_inconsistency (duration deviates from historical median)
	20.	trending_decline (3-week sliding avg decline)
	21.	consecutive_absent_days (zero swipes 3+ days)
	22.	high_variance_duration (std dev high vs historical)
	23.	short_duration_on_high_presence_days (duration<4h though DaysPresentInWeek high)



Refer Raw Report Query carefully and Build Scenarion Strickly....



Refer Database details file 
And Connect all Server

// //C:\Users\W0024618\Desktop\global-page\backend\config\dbConfig.js
// import sql from 'mssql';

// // shared options
// const commonOpts = {
//   options: {
//     encrypt: true,
//     trustServerCertificate: true
//   },
//   // increase timeouts (milliseconds)
//   // requestTimeout: maximum time for a single request to complete
//   requestTimeout: 300000,      // 5 minutes
//   // connectionTimeout: time to wait while establishing connection
//   connectionTimeout: 30000,    // 30 seconds
//   // pool defaults
//   pool: {
//     max: 10,
//     min: 0,
//     idleTimeoutMillis: 30000
//   }
// };

// // lowercase keys only

// const regions = {
//   apac: {
//     user: 'GSOC_Test',
//     password: 'Westernccure@2025',
//     server: 'SRVWUPNQ0986V',
//     database: 'ACVSUJournal_00010030',
//     ...commonOpts
//   },
//   emea: {
//     user: 'GSOC_Test',
//     password: 'Westernccure@2025',
//     server: 'SRVWUFRA0986V',
//     database: 'ACVSUJournal_00011029',
//     ...commonOpts
//   },
//   laca: {
//     user: 'GSOC_Test',
//     password: 'Westernccure@2025',
//     server: 'SRVWUSJO0986V',
//     database: 'ACVSUJournal_00010030',
//     ...commonOpts
//   },
//   namer: {
//     user: 'GSOC_Test',
//     password: 'Westernccure@2025',
//     server: 'SRVWUDEN0891V',
//     database: 'ACVSUJournal_00010030',
//     ...commonOpts
//   }
// };

// const pools = {};

// /**
//  * @param {string} regionKey  one of the keys in `regions`, case-insensitive
//  */
// export async function getPool(regionKey) {
//   const key = (regionKey || '').toLowerCase();
//   const cfg = regions[key];
//   if (!cfg) {
//     throw new Error(`Unknown region: ${regionKey}`);
//   }
//   if (!pools[key]) {
//     pools[key] = await new sql.ConnectionPool(cfg).connect();
//   }
//   return pools[key];
// }

// export { sql };









//Test

//C:\Users\W0024618\Desktop\global-page\backend\config\dbConfig.js
import sql from 'mssql';

// shared options
const commonOpts = {
  options: {
    encrypt: true,
    trustServerCertificate: true
  },
  // increase timeouts (milliseconds)
  requestTimeout: 300000,      // 5 minutes
  connectionTimeout: 30000,    // 30 seconds
  pool: {
    max: 10,
    min: 0,
    idleTimeoutMillis: 30000
  }
};

// lowercase keys only
// NOTE: moved to `databases` array so we can reference previous/other ACVSUJournal DBs easily.
const regions = {
  apac: {
    user: 'GSOC_Test',
    password: 'Westernccure@2025',
    server: 'SRVWUPNQ0986V',
    // primary DB first; add previous/other ACVSUJournal DB names to this array as needed
    databases: ['ACVSUJournal_00010030','ACVSUJournal_00010029','ACVSUJournal_00010028','ACVSUJournal_00010027','ACVSUJournal_00010026','ACVSUJournal_00010025'],
    ...commonOpts
  },
  emea: {
    user: 'GSOC_Test',
    password: 'Westernccure@2025',
    server: 'SRVWUFRA0986V',
    databases: ['ACVSUJournal_00011029','ACVSUJournal_00011028','ACVSUJournal_00011027','ACVSUJournal_00011026','ACVSUJournal_00011025','ACVSUJournal_00011024','ACVSUJournal_00011023'],
    ...commonOpts
  },
  laca: {
    user: 'GSOC_Test',
    password: 'Westernccure@2025',
    server: 'SRVWUSJO0986V',
    databases: ['ACVSUJournal_00010030','ACVSUJournal_00010029','ACVSUJournal_00010028','ACVSUJournal_00010027','ACVSUJournal_00010026','ACVSUJournal_00010025'],
    ...commonOpts
  },
  namer: {
    user: 'GSOC_Test',
    password: 'Westernccure@2025',
    server: 'SRVWUDEN0891V',
    databases: ['ACVSUJournal_00010030','ACVSUJournal_00010029','ACVSUJournal_00010028','ACVSUJournal_00010027','ACVSUJournal_00010026','ACVSUJournal_00010025'],
    ...commonOpts
  }
};

const pools = {};

/**
 * @param {string} regionKey  one of the keys in `regions`, case-insensitive
 */
export async function getPool(regionKey) {
  const key = (regionKey || '').toLowerCase();
  const cfg = regions[key];
  if (!cfg) {
    throw new Error(`Unknown region: ${regionKey}`);
  }

  // Use the first database in the array as the connection default (so queries without explicit db still work).
  if (Array.isArray(cfg.databases) && cfg.databases.length) {
    cfg.database = cfg.databases[0];
  } else if (!cfg.database) {
    // If no database specified, leave it alone (the server's default DB will be used).
    cfg.database = undefined;
  }

  if (!pools[key]) {
    pools[key] = await new sql.ConnectionPool(cfg).connect();
  }
  return pools[key];
}

/**
 * Return array of database names configured for a region (always returns array).
 * This is intentionally exported for other modules to dynamically build fully-qualified table references.
 */
export function getRegionDatabases(regionKey) {
  const key = (regionKey || '').toLowerCase();
  const cfg = regions[key];
  if (!cfg) return [];
  if (Array.isArray(cfg.databases) && cfg.databases.length) return cfg.databases.slice();
  if (cfg.database) return [cfg.database];
  return [];
}

/**
 * Safe bracketed DB name for embedding into SQL identifiers.
 */
export function bracketDb(dbName) {
  if (!dbName) return '';
  // protect against accidental injection: remove closing bracket then bracket it
  const safe = String(dbName).replace(/]/g, '');
  return `[${safe}]`;
}

export { sql };




  const query = `
  WITH ${regionCTEs},
  CombinedQuery AS(
    SELECT 
       DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC) AS LocaleMessageTime,
       t1.ObjectName1,
       t1.PartitionName2       AS PartitionName2,
       t5_card.CardNumber,
       t5_admit.value          AS AdmitCode,
       t5_dir.value            AS Direction,
       t1.ObjectName2          AS Door,
       t5_rej.value            AS Rejection_Type,
       CASE WHEN t3.Name IN ('Contractor','Terminated Contractor')
            THEN t2.Text12
            ELSE CAST(t2.Int1 AS NVARCHAR)
       END                       AS EmployeeID,
       t3.Name                  AS PersonnelType,
       t1.MessageType,
       t1.XmlGUID,
       CASE
         WHEN (UPPER(ISNULL(@regionKey,'')) = 'NAMER' OR t1.PartitionName2 LIKE 'US.%' OR t1.PartitionName2 LIKE 'USA%')
         THEN
           CASE
             WHEN t1.ObjectName2 LIKE '%HQ%' THEN 'Denver-HQ'
             WHEN t1.ObjectName2 LIKE '%Austin%' THEN 'Austin Texas'
             WHEN t1.ObjectName2 LIKE '%Miami%' THEN 'Miami'
             WHEN t1.ObjectName2 LIKE '%NYC%' THEN 'New York'
             ELSE t1.PartitionName2
           END
         WHEN (UPPER(ISNULL(@regionKey,'')) = 'APAC' OR t1.PartitionName2 LIKE 'APAC.%' OR t1.PartitionName2 LIKE 'APAC%')
         THEN
           CASE
             WHEN t1.ObjectName2 LIKE 'APAC_PI%' THEN 'Taguig City'
             WHEN t1.ObjectName2 LIKE 'APAC_PH%' THEN 'Quezon City'
             WHEN t1.ObjectName2 LIKE '%PUN%' THEN 'Pune'
             WHEN t1.ObjectName2 LIKE '%HYD%' THEN 'Hyderabad'
             ELSE t1.PartitionName2
           END
         ELSE t1.PartitionName2
       END AS Location
    FROM AllLogs AS t1
    LEFT JOIN ACVSCore.Access.Personnel     AS t2 ON t1.ObjectIdentity1 = t2.GUID
    LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeId   = t3.ObjectID
    LEFT JOIN AllShred       AS t5_admit
      ON t1.XmlGUID = t5_admit.GUID AND t5_admit.Name = 'AdmitCode'
    LEFT JOIN AllShred       AS t5_dir
      ON t1.XmlGUID = t5_dir.GUID AND t5_dir.Value IN ('InDirection','OutDirection')
    LEFT JOIN AllXml         AS t_xml ON t1.XmlGUID = t_xml.GUID
    LEFT JOIN (
      SELECT GUID, [value]
      FROM AllShred
      WHERE [Name] IN ('Card','CHUID')
    ) AS SCard ON t1.XmlGUID = SCard.GUID
    OUTER APPLY (
      SELECT COALESCE(
        TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]', 'varchar(50)'),
        TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]', 'varchar(50)'),
        SCard.[value]
      ) AS CardNumber
    ) AS t5_card
    LEFT JOIN AllShred AS t5_rej
      ON t1.XmlGUID = t5_rej.GUID AND t5_rej.Name = 'RejectCode'
    WHERE
      t1.MessageType IN ('CardAdmitted' , 'CardRejected')
      AND (
        @location IS NULL
        OR t1.PartitionName2 = @location
        OR (
          (CASE
            WHEN (UPPER(ISNULL(@regionKey,'')) = 'NAMER' OR t1.PartitionName2 LIKE 'US.%' OR t1.PartitionName2 LIKE 'USA%')
            THEN
              CASE
                WHEN t1.ObjectName2 LIKE '%HQ%' THEN 'Denver-HQ'
                WHEN t1.ObjectName2 LIKE '%Austin%' THEN 'Austin Texas'
                WHEN t1.ObjectName2 LIKE '%Miami%' THEN 'Miami'
                WHEN t1.ObjectName2 LIKE '%NYC%' THEN 'New York'
                ELSE NULL
              END
            WHEN (UPPER(ISNULL(@regionKey,'')) = 'APAC' OR t1.PartitionName2 LIKE 'APAC.%' OR t1.PartitionName2 LIKE 'APAC%')
            THEN
              CASE
                WHEN t1.ObjectName2 LIKE 'APAC_PI%' THEN 'Taguig City'
                WHEN t1.ObjectName2 LIKE 'APAC_PH%' THEN 'Quezon City'
                WHEN t1.ObjectName2 LIKE '%PUN%' THEN 'Pune'
                WHEN t1.ObjectName2 LIKE '%HYD%' THEN 'Hyderabad'
                ELSE NULL
              END
            ELSE NULL
          END) = @location
        )
      )
      AND (
        UPPER(ISNULL(@admitFilter,'all')) = 'ALL'
        OR (UPPER(@admitFilter) = 'ADMIT'  AND t1.MessageType = 'CardAdmitted')
        OR (UPPER(@admitFilter) = 'REJECT' AND t1.MessageType = 'CardRejected')
      )
      AND CONVERT(date, DATEADD(MINUTE, -1 * t1.MessageLocaleOffset, t1.MessageUTC)) BETWEEN @startDate AND @endDate
      /* optional employees filter: CSV of tokens - match against EmployeeID, Name or CardNumber */
     
     
         AND (
        @employees IS NULL
        OR EXISTS (
          SELECT 1
          FROM STRING_SPLIT(@employees,',') AS E
          WHERE LTRIM(RTRIM(E.value)) <> ''
            AND (
                /* EmployeeID (contractors use Text12, others use Int1) - case insensitive */
                UPPER(
                  COALESCE(
                    CASE WHEN t3.Name IN ('Contractor','Terminated Contractor') THEN ISNULL(t2.Text12,'') ELSE ISNULL(CAST(t2.Int1 AS NVARCHAR), '') END,
                    ''
                  )
                ) LIKE '%' + UPPER(LTRIM(RTRIM(E.value))) + '%'
                /* Employee name - case insensitive */
                OR UPPER(ISNULL(t2.Text1,'')) LIKE '%' + UPPER(LTRIM(RTRIM(E.value))) + '%'
                /* Card number (from shredding or parsed xml) - case insensitive */
                OR UPPER(ISNULL(SCard.[value], '')) LIKE '%' + UPPER(LTRIM(RTRIM(E.value))) + '%'
            )
        )
      )
)
  SELECT
    LocaleMessageTime,
    CONVERT(date,    LocaleMessageTime) AS DateOnly,
    CONVERT(time(0), LocaleMessageTime) AS Swipe_Time,
    EmployeeID,
    ObjectName1,
    PersonnelType,
    Location,
    CardNumber,
    AdmitCode,
    Direction,
    Door,
    Rejection_Type
  FROM CombinedQuery
  ORDER BY LocaleMessageTime ASC;
  `;











