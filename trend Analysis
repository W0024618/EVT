For Employee ID and card number
refer below Schema 
ACVSCore.Access.Personnel -- database
Personnel - Table name 
Int1 - (as Employee ID)column name 

for Card number 
ACVSCore.Access.Credential - database 
Credential -- table name 
CardNumber - Column name 

refer this Schema and update it carefully...





#C:\Users\W0024618\Desktop\Trend Analysis\backend\app.py

from flask import Flask, jsonify, request, send_from_directory
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re

from trend_runner import run_trend_for_date, build_monthly_training, OUTDIR

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data

# Try to enable CORS
try:
    from flask_cors import CORS
    has_cors = True
except Exception:
    CORS = None
    has_cors = False

app = Flask(__name__, static_folder=None)
if has_cors:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")

logging.basicConfig(level=logging.INFO)

BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)


def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        # convert floats NaN handled above
        return x
    try:
        # fallback to string
        return str(x)
    except Exception:
        return None


_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Clean a dataframe for JSON output (convert NaN -> None, pandas types -> native, format datetimes).
    Also ensures EmployeeName fallback uses EmployeeID first and avoids showing GUIDs as names.
    """
    if df is None or df.empty:
        return []
    df = df.copy()

    # remove duplicate suffix columns
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    # Date normalization
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    # Datetime columns
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # Replace NaN/inf -> None
    df = df.where(pd.notnull(df), None)

    # Convert records to safe Python types
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        # ensure EmployeeName: prefer real name; avoid GUID-like fallbacks
        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        # if EmployeeName empty or looks like GUID/person_uid, try to use EmployeeID or a better source
        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            # prefer EmployeeID (human id) if available
            if emp_id not in (None, '', 'nan'):
                out['EmployeeName'] = str(emp_id)
            else:
                # last resort: do not set a GUID as name; keep None so frontend can show '-'
                out['EmployeeName'] = None
        cleaned.append(out)
    return cleaned


@app.route('/')
def root():
    return "Trend Analysis API â€” Pune test"


@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.json or {}

    date_str = params.get('date')
    start_str = params.get('start')
    end_str = params.get('end')

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            dates = [datetime.now().date()]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    combined_rows = []
    files = []
    samples = []

    for d in dates:
        try:
            df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        csv_path = DEFAULT_OUTDIR / f"trend_pune_{d.strftime('%Y%m%d')}.csv"
        if csv_path.exists():
            files.append(csv_path.name)

        if df is None or df.empty:
            continue

        # Ensure IsFlagged exists; Reasons only when flagged
        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        # Prefer sample of flagged rows (makes QA easier); else generic sample
        flagged = df[df['IsFlagged'] == True]
        sample_df = flagged.head(10) if not flagged.empty else df.head(10)
        samples.extend(_clean_sample_df(sample_df, max_rows=10))

        combined_rows.append(df)

    combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()

    # Determine best identifier column to count unique persons (priority)
    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity']
    id_col = next((c for c in id_candidates if c in combined_df.columns), None)

    def _norm_id_val(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        # convert floats like "320172.0" -> "320172"
        try:
            if '.' in s:
                f = float(s)
                if math.isfinite(f) and f.is_integer():
                    s = str(int(f))
        except Exception:
            pass
        return s

    if id_col is None or combined_df.empty:
        analysis_count = int(len(combined_df))
        flagged_count = int(combined_df['IsFlagged'].sum()) if 'IsFlagged' in combined_df.columns else 0
    else:
        ids_series = combined_df[id_col].apply(_norm_id_val)
        unique_ids = set([x for x in ids_series.unique() if x])
        analysis_count = int(len(unique_ids))

        # flagged unique persons (using IsFlagged)
        if 'IsFlagged' in combined_df.columns:
            flagged_series = combined_df.loc[combined_df['IsFlagged'] == True, id_col].apply(_norm_id_val)
            flagged_unique = set([x for x in flagged_series.unique() if x])
            flagged_count = int(len(flagged_unique))
        else:
            flagged_count = 0

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "files": files,
        # legacy totals
        "aggregated_rows_total": int(len(combined_df)),
        # new semantics (distinct persons)
        "rows": int(analysis_count),
        "flagged_rows": int(flagged_count),
        "sample": samples[:20]
    }
    return jsonify(resp)


@app.route('/latest', methods=['GET'])
def latest_results():
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    sample = _clean_sample_df(df, max_rows=5)
    return jsonify({
        "file": latest.name,
        "rows": int(len(df)),
        "sample": sample
    })


@app.route('/record', methods=['GET'])
def get_record():
    """
    /record?employee_id=... or /record?person_uid=...
    Returns matching aggregated trend rows and filtered raw swipe rows (only for flagged persons).
    """
    q = request.args.get('employee_id') or request.args.get('person_uid')
    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob("trend_pune_*.csv"), reverse=True)
    if not csvs:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200
    try:
        latest = csvs[0]
        try:
            df = pd.read_csv(latest, parse_dates=['FirstSwipe','LastSwipe'], infer_datetime_format=True)
        except Exception:
            df = pd.read_csv(latest, dtype=str)
    except Exception:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    if q is None:
        cleaned = _clean_sample_df(df, max_rows=10)
        return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

    q_str = str(q).strip()

    def normalize_series(s):
        if s is None:
            return pd.Series([None]*len(df))
        s = s.fillna('').astype(str).str.strip()
        def _norm_val(v):
            if not v:
                return ''
            try:
                if '.' in v:
                    fv = float(v)
                    if math.isfinite(fv) and fv.is_integer():
                        return str(int(fv))
            except Exception:
                pass
            return v
        return s.map(_norm_val)

    found_mask = pd.Series(False, index=df.index)

    if 'EmployeeID' in df.columns:
        emp_series = normalize_series(df['EmployeeID'])
        found_mask = found_mask | (emp_series == q_str)

    if 'person_uid' in df.columns:
        uid_series = normalize_series(df['person_uid'])
        found_mask = found_mask | (uid_series == q_str)

    if not found_mask.any():
        # try numeric equivalence
        try:
            q_numeric = float(q_str)
            if 'EmployeeID' in df.columns:
                emp_numeric = pd.to_numeric(df['EmployeeID'], errors='coerce')
                found_mask = found_mask | (emp_numeric == q_numeric)
        except Exception:
            pass

    matched = df[found_mask]
    if matched.empty:
        return jsonify({'aggregated_rows': [], 'raw_swipe_files': [], 'raw_swipes': []}), 200

    cleaned_matched = _clean_sample_df(matched, max_rows=len(matched))

    # Resolve raw swipe file names by Date
    raw_files = set()
    date_vals = set()
    if 'Date' in matched.columns:
        try:
            dates_parsed = pd.to_datetime(matched['Date'], errors='coerce').dropna().dt.date.unique()
            for d in dates_parsed:
                date_vals.add(str(d.isoformat()))
        except Exception:
            pass

    if not date_vals:
        for col in ('FirstSwipe', 'LastSwipe'):
            if col in matched.columns:
                try:
                    vals = pd.to_datetime(matched[col], errors='coerce').dropna().dt.date.unique()
                    for d in vals:
                        date_vals.add(str(d.isoformat()))
                except Exception:
                    pass

    # build filtered raw swipe rows ONLY for flagged persons
    raw_swipes_out = []
    # pick first matched row to decide identifiers to filter raw swipes
    for idx, agg_row in matched.iterrows():
        # gather the identifying values
        person_uid = agg_row.get('person_uid') if 'person_uid' in agg_row else None
        empid = agg_row.get('EmployeeID') if 'EmployeeID' in agg_row else None
        card = agg_row.get('CardNumber') if 'CardNumber' in agg_row else None
        # determine date string(s) to look for raw files using Date / FirstSwipe / LastSwipe
        dates_for_row = set()
        if 'Date' in agg_row and pd.notna(agg_row['Date']):
            try:
                d = pd.to_datetime(agg_row['Date']).date()
                dates_for_row.add(d.isoformat())
            except Exception:
                pass
        for col in ('FirstSwipe','LastSwipe'):
            if col in agg_row and pd.notna(agg_row[col]):
                try:
                    d = pd.to_datetime(agg_row[col]).date()
                    dates_for_row.add(d.isoformat())
                except Exception:
                    pass

        # only fetch raw swipe evidence when this aggregated row is flagged
        is_flagged = bool(agg_row.get('IsFlagged', False))
        if not is_flagged:
            continue

        for d in dates_for_row:
            try:
                dd = d[:10]
                raw_name = f"swipes_pune_{dd.replace('-','')}.csv"
                fp = DEFAULT_OUTDIR / raw_name
                if not fp.exists():
                    continue
                raw_files.add(raw_name)
                # read the raw swipe CSV (try to parse datetimes if possible)
                try:
                    raw_df = pd.read_csv(fp, parse_dates=['LocaleMessageTime'], infer_datetime_format=True)
                except Exception:
                    try:
                        raw_df = pd.read_csv(fp, dtype=str)
                    except Exception:
                        continue

                # normalize columns names to lowercase for searching
                cols_lower = {c.lower(): c for c in raw_df.columns}

                # candidate column names
                tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
                emp_col = cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
                card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or None
                door_col = cols_lower.get('door') or cols_lower.get('doorname') or None
                dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or None
                note_col = cols_lower.get('rejection_type') or cols_lower.get('note') or cols_lower.get('source') or None

                # build filter mask: match person_uid OR empid OR card (whichever present)
                mask = pd.Series(False, index=raw_df.index)
                if person_uid is not None and 'person_uid' in raw_df.columns:
                    mask = mask | (raw_df['person_uid'].astype(str).str.strip() == str(person_uid).strip())
                if emp_col:
                    if empid is not None:
                        mask = mask | (raw_df[emp_col].astype(str).str.strip() == str(empid).strip())
                if card_col and card is not None:
                    mask = mask | (raw_df[card_col].astype(str).str.strip() == str(card).strip())

                # Also attempt to match by EmployeeName if agg has name and no other match
                if (not mask.any()) and name_col and 'EmployeeName' in agg_row and pd.notna(agg_row.get('EmployeeName')):
                    mask = mask | (raw_df[name_col].astype(str).str.strip() == str(agg_row.get('EmployeeName')).strip())

                # Also filter by date: ensure the timestamp's date equals dd
                if tcol and tcol in raw_df.columns:
                    try:
                        raw_df[tcol] = pd.to_datetime(raw_df[tcol], errors='coerce')
                        mask = mask & (raw_df[tcol].dt.date == pd.to_datetime(dd).date())
                    except Exception:
                        pass

                filtered = raw_df[mask]
                if filtered.empty:
                    continue

                # Convert filtered rows into the standardized output structure requested by frontend
                for _, r in filtered.iterrows():
                    out = {}
                    # EmployeeName
                    if name_col and name_col in raw_df.columns:
                        out['EmployeeName'] = _to_python_scalar(r.get(name_col))
                    else:
                        out['EmployeeName'] = _to_python_scalar(agg_row.get('EmployeeName') or agg_row.get('person_uid'))

                    # EmployeeID
                    if emp_col and emp_col in raw_df.columns:
                        out['EmployeeID'] = _to_python_scalar(r.get(emp_col))
                    else:
                        out['EmployeeID'] = _to_python_scalar(agg_row.get('EmployeeID'))

                    # CardNumber
                    if card_col and card_col in raw_df.columns:
                        out['CardNumber'] = _to_python_scalar(r.get(card_col))
                    else:
                        out['CardNumber'] = _to_python_scalar(agg_row.get('CardNumber'))

                    # Date and Time
                    if tcol and tcol in raw_df.columns:
                        ts = r.get(tcol)
                        try:
                            ts_py = pd.to_datetime(ts)
                            out['Date'] = ts_py.date().isoformat()
                            out['Time'] = ts_py.time().isoformat()
                        except Exception:
                            txt = str(r.get(tcol))
                            out['Date'] = txt[:10]
                            out['Time'] = txt[11:19] if len(txt) >= 19 else txt
                    else:
                        out['Date'] = d
                        out['Time'] = None

                    # Door
                    if door_col and door_col in raw_df.columns:
                        out['Door'] = _to_python_scalar(r.get(door_col))
                    else:
                        out['Door'] = None

                    # Direction
                    if dir_col and dir_col in raw_df.columns:
                        out['Direction'] = _to_python_scalar(r.get(dir_col))
                    else:
                        out['Direction'] = _to_python_scalar(r.get('Direction')) if 'Direction' in r else None

                    # Note (rejection / source)
                    if note_col and note_col in raw_df.columns:
                        out['Note'] = _to_python_scalar(r.get(note_col))
                    else:
                        out['Note'] = None

                    out['_source'] = raw_name
                    raw_swipes_out.append(out)
            except Exception as e:
                logging.exception("Error processing raw swipe file for date %s: %s", d, e)
                continue

    return jsonify({
        "aggregated_rows": cleaned_matched,
        "raw_swipe_files": sorted(list(raw_files)),
        "raw_swipes": raw_swipes_out
    }), 200


@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    """
    Serve raw swipe CSVs from outputs/ (filename should be the file name only).
    """
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    # send file
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)


@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)




