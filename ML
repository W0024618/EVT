attendance-analytics/
├─ app.py                      # FastAPI app (upload, compare, reports)
├─ requirements.txt
├─ settings.py                 # config (DB, paths, API endpoints)
├─ models.py                   # SQLAlchemy models
├─ db.py                       # DB session setup
├─ ingest_excel.py             # helpers to parse & normalize Excel sheets
├─ compare_service.py          # core logic to compare headcount -> active lists
├─ reports.py                  # report generation functions (CSV/JSON)
├─ ml/
│  ├─ train_model.py           # training pipeline (IsolationForest / classifier)
│  └─ predict.py               # run predictions/anomaly scoring
├─ data/
│  ├─ raw_uploads/             # store uploaded excel files (rotating)
│  └─ outputs/                 # generated CSV reports
└─ README.md





cd C:\path\to\attendance-analytics
python -m venv .venv
# activate
.\.venv\Scripts\Activate.ps1
# upgrade pip and install
python -m pip install --upgrade pip
pip install -r requirements.txt





$env:ATT_DB_URL = "postgresql://user:pass@dbhost:5432/attendance_db"



# with venv activated
python - <<'PY'
from db import Base, engine
Base.metadata.create_all(bind=engine)
print("DB tables created.")
PY







# from project root, venv active
uvicorn app:app --reload --host 0.0.0.0 --port 8000





INFO:     Uvicorn running on http://0.0.0.0:8000 (CTRL+C to quit)




curl -X POST "http://localhost:8000/upload/active-contractors" \
  -F "file=@/full/path/to/Active_Contractors_-_Badge_Issuing_20250810-230009226115.xlsx"




python ingest_excel.py



from ingest_excel import ingest_employee_excel, ingest_contractor_excel
ingest_employee_excel("data/raw_uploads/Copy of Excel 2025-07-01 15_31 GMT+5_30.xlsx")
ingest_contractor_excel("data/raw_uploads/Active_Contractors_-_Badge_Issuing_20250810-230009226115.xlsx")







curl http://localhost:8000/reports/daily/20250817



curl http://localhost:8000/reports/daily/20250817 > daily_report_20250817.json



python ml/train_model.py






sqlite3 attendance.db
sqlite> .tables
sqlite> select count(*) from active_employees;





gunicorn -k uvicorn.workers.UvicornWorker app:app -w 4 -b 0.0.0.0:8000












fastapi
uvicorn[standard]
pandas
openpyxl
sqlalchemy
alembic
python-multipart
scikit-learn
joblib
python-dotenv





import os
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent
UPLOAD_DIR = BASE_DIR / "data" / "raw_uploads"
OUTPUT_DIR = BASE_DIR / "data" / "outputs"
DB_URL = os.environ.get("ATT_DB_URL", "sqlite:///./attendance.db")  # change to postgres in prod
LIVE_API_URL = os.environ.get("LIVE_API_URL", "http://localhost:3008/api/occupancy/live-summary")

UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)





from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from settings import DB_URL

engine = create_engine(DB_URL, connect_args={"check_same_thread": False} if DB_URL.startswith("sqlite") else {})
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
Base = declarative_base()






from sqlalchemy import Column, Integer, String, DateTime, JSON, Boolean, Date
from sqlalchemy import ForeignKey, UniqueConstraint
from sqlalchemy.orm import relationship
from db import Base

class ActiveEmployee(Base):
    __tablename__ = "active_employees"
    id = Column(Integer, primary_key=True)
    employee_id = Column(String, index=True, unique=True, nullable=False)
    full_name = Column(String, index=True)
    email = Column(String)
    location_city = Column(String, index=True)
    location_desc = Column(String)
    current_status = Column(String)
    raw_row = Column(JSON)  # store original row for reference
    uploaded_at = Column(DateTime)

class ActiveContractor(Base):
    __tablename__ = "active_contractors"
    id = Column(Integer, primary_key=True)
    worker_system_id = Column(String, index=True, unique=True, nullable=False)
    ipass_id = Column(String, index=True)
    full_name = Column(String, index=True)
    vendor = Column(String)
    location = Column(String)
    status = Column(String)
    raw_row = Column(JSON)
    uploaded_at = Column(DateTime)

class LiveSwipe(Base):
    __tablename__ = "live_swipes"
    id = Column(Integer, primary_key=True)
    timestamp = Column(DateTime, index=True)
    employee_id = Column(String, index=True, nullable=True)
    card_number = Column(String, index=True, nullable=True)
    full_name = Column(String)
    partition = Column(String, index=True)
    floor = Column(String)
    door = Column(String)
    region = Column(String, index=True)
    raw = Column(JSON)

class AttendanceSummary(Base):
    __tablename__ = "attendance_summary"
    id = Column(Integer, primary_key=True)
    employee_id = Column(String, index=True)
    date = Column(Date, index=True)
    presence_count = Column(Integer)
    first_seen = Column(DateTime)
    last_seen = Column(DateTime)
    derived = Column(JSON)  # extra stats










import pandas as pd
from datetime import datetime
from sqlalchemy.exc import IntegrityError
from db import SessionLocal, engine
from models import Base, ActiveEmployee, ActiveContractor
from settings import UPLOAD_DIR
import uuid, os

Base.metadata.create_all(bind=engine)

def normalize_colname(c):
    return ''.join(ch for ch in c).strip()

def ingest_employee_excel(path, uploaded_by="system"):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    important = ['Employee ID','Full Name','Location City','Location Description','Current Status','Employee Type','Employee\'s Email']
    # adapt column mapping if names differ
    with SessionLocal() as db:
        for _, row in df.iterrows():
            emp_id = (row.get('Employee ID') or row.get('EmployeeID') or '').strip()
            if not emp_id:
                continue
            full_name = (row.get('Full Name') or f"{row.get('First Name','')} {row.get('Last Name','')}").strip()
            rec = ActiveEmployee(
                employee_id=emp_id,
                full_name=full_name,
                email=row.get("Employee's Email"),
                location_city=row.get('Location City') or row.get('Location Description'),
                location_desc=row.get('Location Description'),
                current_status=row.get('Current Status'),
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)  # upsert logic with merge
                db.commit()
            except IntegrityError:
                db.rollback()

def ingest_contractor_excel(path):
    df = pd.read_excel(path, sheet_name=0, dtype=str)
    df.columns = [c.strip() for c in df.columns]
    with SessionLocal() as db:
        for _, row in df.iterrows():
            wsid = (row.get('Worker System Id') or row.get('Worker System ID') or '').strip()
            if not wsid:
                continue
            rec = ActiveContractor(
                worker_system_id=wsid,
                ipass_id=row.get('iPass ID') or row.get('"W" iPass ID'),
                full_name=row.get('Full Name'),
                vendor=row.get('Vendor Company Name'),
                location=row.get('Worker Location'),
                status=row.get('Status'),
                raw_row=row.to_dict(),
                uploaded_at=datetime.utcnow()
            )
            try:
                db.merge(rec)
                db.commit()
            except IntegrityError:
                db.rollback()

if __name__ == "__main__":
    # example usage
    for f in os.listdir(UPLOAD_DIR):
        p = UPLOAD_DIR / f
        if 'contractor' in f.lower():
            ingest_contractor_excel(p)
        else:
            ingest_employee_excel(p)
    print("Ingestion completed.")









import pandas as pd
from sqlalchemy import select
from db import SessionLocal
from models import ActiveEmployee, ActiveContractor, LiveSwipe, AttendanceSummary
from datetime import datetime, date

def ingest_live_details_list(details_list):
    """
    details_list = array of detail objects from live-summary /api response (data.details)
    Each item should contain keys: LocaleMessageTime, EmployeeID, CardNumber, ObjectName1, PartitionName2, Floor, Door, Direction
    """
    with SessionLocal() as db:
        for d in details_list:
            ts = d.get("LocaleMessageTime")
            try:
                ts_parsed = datetime.fromisoformat(ts.replace("Z", "+00:00"))
            except:
                ts_parsed = datetime.utcnow()
            rec = LiveSwipe(
                timestamp=ts_parsed,
                employee_id=(d.get("EmployeeID") or "").strip() or None,
                card_number=(d.get("CardNumber") or "").strip() or None,
                full_name=d.get("ObjectName1"),
                partition=d.get("PartitionName2") or d.get("PartitionName1"),
                floor=d.get("Floor"),
                door=d.get("Door") or d.get("DoorName"),
                region=d.get("PartitionName2"),
                raw=d
            )
            db.add(rec)
        db.commit()

def compute_daily_attendance(target_date: date):
    """
    Build attendance summary for target_date (UTC date of swipe)
    """
    with SessionLocal() as db:
        start = datetime.combine(target_date, datetime.min.time())
        end = datetime.combine(target_date, datetime.max.time())
        q = db.query(LiveSwipe).filter(LiveSwipe.timestamp >= start, LiveSwipe.timestamp <= end)
        df = pd.read_sql(q.statement, q.session.bind)
        if df.empty:
            return []
        # group by employee or card
        df['key'] = df['employee_id'].fillna(df['card_number'])
        grouped = df.groupby('key').agg(
            presence_count=('id','count'),
            first_seen=('timestamp','min'),
            last_seen=('timestamp','max'),
            full_name=('full_name','first'),
            partition=('partition','first')
        ).reset_index()
        # store or return
        for _, row in grouped.iterrows():
            rec = AttendanceSummary(
                employee_id=row['key'],
                date=target_date,
                presence_count=int(row['presence_count']),
                first_seen=row['first_seen'],
                last_seen=row['last_seen'],
                derived={"partition": row['partition'], "full_name": row['full_name']}
            )
            db.merge(rec)
        db.commit()
        return grouped.to_dict(orient='records')

def compare_with_active(target_date: date):
    """
    Join attendance_summary with active employees/contractors to compute percentages and flags.
    """
    with SessionLocal() as db:
        # get attendance for date
        att = db.query(AttendanceSummary).filter(AttendanceSummary.date == target_date).all()
        att_df = pd.DataFrame([{
            "employee_id": a.employee_id,
            "presence_count": a.presence_count,
            "first_seen": a.first_seen,
            "last_seen": a.last_seen,
            **(a.derived or {})
        } for a in att])
        # get active employees
        act_q = db.query(ActiveEmployee).all()
        act_df = pd.DataFrame([{
            "employee_id": e.employee_id,
            "full_name": e.full_name,
            "location_city": e.location_city,
            "status": e.current_status
        } for e in act_q])
        # merge
        merged = act_df.merge(att_df, how='left', left_on='employee_id', right_on='employee_id')
        merged['present_today'] = merged['presence_count'].fillna(0) > 0
        # compute location wise percent present
        loc_group = merged.groupby('location_city').agg(
            total_n=('employee_id','count'),
            present_n=('present_today','sum')
        ).reset_index()
        loc_group['percent_present'] = (loc_group['present_n'] / loc_group['total_n'] * 100).round(2)
        return {"by_location": loc_group.to_dict(orient='records'), "merged": merged.to_dict(orient='records')}







from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
import shutil, uuid
from settings import UPLOAD_DIR, OUTPUT_DIR
from ingest_excel import ingest_employee_excel, ingest_contractor_excel
from compare_service import ingest_live_details_list, compute_daily_attendance, compare_with_active
import os, json

app = FastAPI(title="Attendance Analytics")

@app.post("/upload/active-employees")
async def upload_active_employees(file: UploadFile = File(...)):
    if not file.filename.endswith(('.xls','.xlsx')):
        raise HTTPException(400, "Please upload an Excel file")
    dest = UPLOAD_DIR / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    ingest_employee_excel(dest)
    return {"status":"ok", "path": str(dest)}

@app.post("/upload/active-contractors")
async def upload_active_contractors(file: UploadFile = File(...)):
    dest = UPLOAD_DIR / f"{uuid.uuid4().hex}_{file.filename}"
    with open(dest, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    ingest_contractor_excel(dest)
    return {"status":"ok", "path": str(dest)}

@app.post("/ingest/live-details")
async def ingest_live(details: list):
    # accept the data.details array from live-summary API
    ingest_live_details_list(details)
    return {"status":"ok", "inserted": len(details)}

@app.get("/reports/daily/{yyyymmdd}")
def daily_report(yyyymmdd: str):
    import datetime
    dt = datetime.datetime.strptime(yyyymmdd, "%Y%m%d").date()
    compute_daily_attendance(dt)
    summary = compare_with_active(dt)
    return JSONResponse(summary)






import pandas as pd
from sklearn.ensemble import IsolationForest
from joblib import dump, load
from db import SessionLocal
from models import AttendanceSummary
from datetime import date, timedelta

def build_feature_table(last_n_days=30):
    with SessionLocal() as db:
        # pull attendance summary for last N days
        end = date.today()
        start = end - timedelta(days=last_n_days)
        q = db.query(AttendanceSummary).filter(AttendanceSummary.date >= start, AttendanceSummary.date <= end)
        df = pd.read_sql(q.statement, q.session.bind)
    if df.empty:
        return None
    # pivot: rows=employee_id, cols=date, values=presence_count>0
    df['present'] = df['presence_count'] > 0
    pivot = df.pivot_table(index='employee_id', columns='date', values='present', aggfunc='max', fill_value=0)
    pivot['days_present'] = pivot.sum(axis=1)
    pivot['presence_rate'] = pivot['days_present'] / last_n_days
    features = pivot[['days_present','presence_rate']].fillna(0)
    return features

def train_isolationforest(save_path="models/isojob.joblib"):
    features = build_feature_table()
    if features is None:
        raise RuntimeError("No data")
    clf = IsolationForest(contamination=0.05, random_state=42)
    clf.fit(features)
    dump(clf, save_path)
    return save_path

if __name__ == "__main__":
    print("Training...")
    print(train_isolationforest())







