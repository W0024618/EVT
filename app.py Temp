# backend/app.py
from flask import Flask, jsonify, request, send_from_directory, jsonify, send_file
from datetime import datetime, timedelta, date
from pathlib import Path
import logging
import pandas as pd
import numpy as np
import joblib
import math
import re
import io
import base64
import os
import difflib
from io import BytesIO
from flask import send_file  # add if not present
from pathlib import Path
from typing import Optional, List, Dict, Any
from duration_report import REGION_CONFIG
from datetime import date, timedelta, datetime
from flask import jsonify, request
import logging
logging.basicConfig(level=logging.INFO)



# new import to reuse canonicalization helpers from duration_report
try:
    from duration_report import _strip_person_uid_prefix
except Exception:
    # defensive fallback (shouldn't happen if duration_report present)
    def _strip_person_uid_prefix(token):
        if token is None:
            return None
        try:
            s = str(token).strip()
            if not s:
                return None
            if ':' in s:
                prefix, rest = s.split(':', 1)
                if prefix.lower() in ('emp', 'uid', 'name'):
                    rest = rest.strip()
                    if rest:
                        return rest
            return s
        except Exception:
            return None



# expose helper functions for other modules (trend_runner expects app.get_personnel_info)
try:
    from .employeeimage import get_personnel_info, get_person_image_bytes  # relative import if package
except Exception:
    try:
        from employeeimage import get_personnel_info, get_person_image_bytes
    except Exception:
        get_personnel_info = None
        get_person_image_bytes = None



# Robust import of employeeimage helpers (use fallback if unavailable)
try:
    from employeeimage import get_person_image_bytes, get_personnel_info
except Exception:
    def get_person_image_bytes(pid):
        return None
    def get_personnel_info(pid):
        return {}


from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE


from trend_runner import run_trend_for_date, build_monthly_training


from io import BytesIO
# try to import imghdr (may be missing in some minimal Python builds)
try:
    import imghdr as _imghdr
except Exception:
    _imghdr = None

def _guess_image_kind(data_bytes):
    """Return image kind like 'jpeg', 'png', 'gif', 'webp' or None."""
    # 1) try imghdr if available
    try:
        if _imghdr:
            k = _imghdr.what(None, h=data_bytes)
            if k:
                return k
    except Exception:
        pass

    # 2) try Pillow if installed
    try:
        from io import BytesIO as _BytesIO
        from PIL import Image as _Image
        bio = _BytesIO(data_bytes)
        img = _Image.open(bio)
        fmt = getattr(img, 'format', None)
        if fmt:
            return fmt.lower()
    except Exception:
        pass

    # 3) last-resort: quick magic-bytes sniff for common formats
    try:
        header = data_bytes[:12]
        if header.startswith(b'\xff\xd8\xff'):
            return 'jpeg'
        if header.startswith(b'\x89PNG\r\n\x1a\n'):
            return 'png'
        if header[:6] in (b'GIF87a', b'GIF89a'):
            return 'gif'
        if header.startswith(b'RIFF') and header[8:12] == b'WEBP':
            return 'webp'
    except Exception:
        pass

    return None


# import helpers that exist in your employeeimage.py
try:
    from employeeimage import get_person_image_bytes, get_personnel_info
except Exception:
    # if module missing, define fallbacks so app still runs
    def get_person_image_bytes(pid):
        return None
    def get_personnel_info(pid):
        return {}


# create Flask app early (must exist before any @app.route usage)
from flask import Flask
try:
    from flask_cors import CORS
    _HAS_CORS = True
except Exception:
    CORS = None
    _HAS_CORS = False

app = Flask(__name__, static_folder=None)
if _HAS_CORS:
    CORS(app)
else:
    logging.warning("flask_cors not available; continuing without CORS.")


# --- ensure CORS headers are present even if flask_cors isn't installed ---
@app.after_request
def _add_cors_headers(response):
    try:
        # Only add if not already present (flask_cors will add these if available)
        if 'Access-Control-Allow-Origin' not in response.headers:
            response.headers['Access-Control-Allow-Origin'] = '*'
        if 'Access-Control-Allow-Methods' not in response.headers:
            response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
        if 'Access-Control-Allow-Headers' not in response.headers:
            response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization'
        # Allow caching negotiation but recommend no-cache for images to avoid stale pictures during dev
        response.headers.setdefault('Cache-Control', 'no-cache, no-store, must-revalidate')
    except Exception:
        pass
    return response




def _safe_read_csv(fp):
    try:
        return pd.read_csv(fp, parse_dates=['LocaleMessageTime'], low_memory=False)
    except Exception:
        try:
            return pd.read_csv(fp, low_memory=False)
        except Exception:
            return pd.DataFrame()

# path 
BASE_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTDIR = BASE_DIR / "outputs"
DEFAULT_OUTDIR.mkdir(parents=True, exist_ok=True)

OUTDIR = DEFAULT_OUTDIR

# -------------------------------------------------------------------

# Can be overridden by env var VIOLATION_DAYS (int)
DEFAULT_VIOLATION_DAYS = int(os.getenv("VIOLATION_DAYS", "90"))
# -------------------------------------------------------------------

OVERRIDES_FILE = DEFAULT_OUTDIR / "overrides.csv"


def _load_overrides():
    if not OVERRIDES_FILE.exists():
        return {}
    try:
        df = pd.read_csv(OVERRIDES_FILE, dtype=str)
        out = {}
        for _, r in df.iterrows():
            emp = str(r.get('EmployeeID') or r.get('person_uid') or '').strip()
            if not emp:
                continue
            out[emp] = {
                'level': str(r.get('OverrideLevel') or '').strip(),
                'reason': str(r.get('Reason') or '').strip(),
                'ts': str(r.get('Timestamp') or '').strip()
            }
        return out
    except Exception:
        logging.exception("Failed reading overrides file")
        return {}

def _save_override(employee_key, level, reason):
    now = datetime.now().isoformat()
    row = {'EmployeeID': employee_key, 'OverrideLevel': level, 'Reason': reason or '', 'Timestamp': now}
    try:
        if OVERRIDES_FILE.exists():
            df = pd.read_csv(OVERRIDES_FILE, dtype=str)
            # pandas.DataFrame.append is deprecated -> use concat
            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
        else:
            df = pd.DataFrame([row])
        df.to_csv(OVERRIDES_FILE, index=False)
        return True
    except Exception:
        logging.exception("Failed to save override")
        return False

def _slug_city(s):
    """
    Convert a city/site string into a safe slug: lowercase, alphanumeric+hyphen
    """
    if not s:
        return ''
    # Remove special chars, spaces to hyphens, lower
    slug = re.sub(r'[^\w\s-]', '', str(s)).strip().lower()
    slug = re.sub(r'[\s_]+', '-', slug)
    return slug

_acvscore_backoff = {"ts": None, "failed": False}
_ACVSCORE_BACKOFF_SECONDS = 20
# ODBC driver (keep existing env-based driver)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

MODELS_DIR = Path(__file__).parent / "models"
_loaded_models = {}

def load_model(name):
    if name in _loaded_models:
        return _loaded_models[name]
    p = MODELS_DIR / f"{name}.joblib"
    if not p.exists():
        return None
    data = joblib.load(p)
    _loaded_models[name] = data
    return data



# send_file is needed for Excel responses
from flask import send_file
try:
    # optional import; used for styling
    from openpyxl import load_workbook
    from openpyxl.styles import Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except Exception:
    OPENPYXL_AVAILABLE = False

def _to_python_scalar(x):
    """
    Convert numpy/pandas scalar types to built-in Python types and
    convert NaN-like values to None so JSON is safe.
    """
    try:
        import pandas as _pd
        if isinstance(x, _pd.Timestamp):
            return x.to_pydatetime().isoformat()
    except Exception:
        pass

    try:
        import numpy as _np
        if isinstance(x, _np.generic):
            v = x.item()
            if isinstance(v, float) and _np.isnan(v):
                return None
            return v
    except Exception:
        pass

    try:
        if isinstance(x, float) and math.isnan(x):
            return None
    except Exception:
        pass

    if isinstance(x, (datetime,)):
        return x.isoformat()
    if isinstance(x, (bool, int, str, type(None), float)):
        # convert floats NaN handled above
        return x
    try:
        # fallback to string
        return str(x)
    except Exception:
        return None


_uuid_like_re = re.compile(r'^[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}$')

def _looks_like_guid(s):
    try:
        if not s or not isinstance(s, str):
            return False
        s = s.strip()
        return bool(_uuid_like_re.match(s)) or s.startswith('name:') or s.startswith('emp:') or s.startswith('uid:')
    except Exception:
        return False


# Helper: format seconds to HH:MM:SS
def format_seconds_to_hms(seconds):
    try:
        if seconds is None:
            return None
        # guard against floats and NaN
        s = int(float(seconds))
        if s < 0:
            s = 0
        hh = s // 3600
        mm = (s % 3600) // 60
        ss = s % 60
        return f"{hh:02d}:{mm:02d}:{ss:02d}"
    except Exception:
        return None


# Placeholder tokens (keep consistent with trend_runner expectations)
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False


_CARD_XML_RE = re.compile(r'<Card>([^<]+)</Card>', re.IGNORECASE | re.DOTALL)
def _extract_card_from_xml_text(txt):
    try:
        if not txt or not isinstance(txt, str):
            return None
        m = _CARD_XML_RE.search(txt)
        if m:
            return m.group(1).strip()
        m2 = re.search(r'CHUID.*?Card.*?[:=]\s*([0-9A-Za-z\-\_]+)', txt, re.IGNORECASE | re.DOTALL)
        if m2:
            return m2.group(1).strip()
    except Exception:
        pass
    return None


def _resolve_field_from_record(record: dict, candidate_tokens: list):
    """
    Search a single row `record` (dict) for likely columns listed in candidate_tokens.
    Return first non-placeholder value found (converted to Python scalar), else None.
    """
    if record is None:
        return None

    # 1) exact key matches (case-sensitive & common casing)
    for key in candidate_tokens:
        if key in record:
            v = record.get(key)
            if v is None:
                continue
            if isinstance(v, float) and math.isnan(v):
                continue
            sval = str(v).strip()
            if sval and not _is_placeholder_str(sval):
                return _to_python_scalar(v)

    # 2) case-insensitive contains match
    lower_keys = {k.lower(): k for k in record.keys()}
    for tok in candidate_tokens:
        tok_l = tok.lower()
        for lk, orig_key in lower_keys.items():
            if tok_l in lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                if isinstance(v, float) and math.isnan(v):
                    continue
                sval = str(v).strip()
                if sval and not _is_placeholder_str(sval):
                    return _to_python_scalar(v)

    # 3) xml / value parsing fallback for CardNumber
    card_like = any(tok.lower() in ('cardnumber', 'chuid', 'card') for tok in candidate_tokens)
    if card_like:
        for lk, orig_key in lower_keys.items():
            if 'xml' in lk or 'xmlmessage' in lk or 'xml_msg' in lk or 'msg' in lk or 'value' == lk:
                v = record.get(orig_key)
                if v is None:
                    continue
                try:
                    txt = str(v)
                    extracted = _extract_card_from_xml_text(txt)
                    if extracted and not _is_placeholder_str(extracted):
                        return _to_python_scalar(extracted)
                except Exception:
                    continue

    # 4) final fallback: first non-placeholder value
    for k, v in record.items():
        if v is None:
            continue
        if isinstance(v, float) and math.isnan(v):
            continue
        sval = str(v).strip()
        if sval and not _is_placeholder_str(sval):
            return _to_python_scalar(v)

    return None


def _clean_sample_df(df: pd.DataFrame, max_rows: int = 10):
    """
    Clean a dataframe for JSON output (convert NaN -> None, pandas types -> native, format datetimes).
    """
    if df is None or df.empty:
        return []
    df = df.copy()

    # remove duplicate suffix columns
    cols_to_fix = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]
    for c in cols_to_fix:
        base = c[:-2]
        if base in df.columns:
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass
        else:
            try:
                df.rename(columns={c: base}, inplace=True)
            except Exception:
                pass

    # Date normalization
    if 'Date' in df.columns:
        try:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.date
            df['Date'] = df['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
        except Exception:
            pass

    # Datetime columns to ISO strings
    for dtcol in ('FirstSwipe', 'LastSwipe', 'LocaleMessageTime'):
        if dtcol in df.columns:
            try:
                df[dtcol] = pd.to_datetime(df[dtcol], errors='coerce')
                df[dtcol] = df[dtcol].apply(lambda t: t.to_pydatetime().isoformat() if pd.notna(t) else None)
            except Exception:
                try:
                    df[dtcol] = df[dtcol].astype(str).replace('NaT', None)
                except Exception:
                    pass

    # Replace NaN/inf -> None
    df = df.where(pd.notnull(df), None)

    # Convert records to safe Python types
    rows = df.head(max_rows).to_dict(orient='records')
    cleaned = []
    for r in rows:
        out = {}
        for k, v in r.items():
            out[k] = _to_python_scalar(v)

        # Typical fields
        emp_name = out.get('EmployeeName')
        emp_id = out.get('EmployeeID') or out.get('EmployeeIdentity')
        person_uid = out.get('person_uid')

        # ----- Schema-aware fallback resolution -----
        if not emp_id:
            emp_tokens = ['Int1', 'Text12', 'EmployeeID', 'empid', 'id']
            resolved_emp = _resolve_field_from_record(r, emp_tokens)
            if resolved_emp is not None:
                try:
                    s = str(resolved_emp).strip()
                    # remove trailing .0 for floats
                    if '.' in s:
                        f = float(s)
                        if math.isfinite(f) and f.is_integer():
                            s = str(int(f))
                    if _looks_like_guid(s):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = s
                        emp_id = s
                except Exception:
                    if _looks_like_guid(resolved_emp):
                        out['EmployeeID'] = None
                        emp_id = None
                    else:
                        out['EmployeeID'] = resolved_emp
                        emp_id = resolved_emp

        # Prefer Credential.CardNumber / CHUID / Card as CardNumber when missing — reject GUIDs/placeholders
        if out.get('CardNumber') in (None, '', 'nan'):
            card_tokens = ['CardNumber', 'CHUID', 'Card', 'card_no', 'cardnum']
            resolved_card = _resolve_field_from_record(r, card_tokens)
            if resolved_card is not None:
                try:
                    cs = str(resolved_card).strip()
                    if _looks_like_guid(cs) or _is_placeholder_str(cs):
                        out['CardNumber'] = None
                    else:
                        out['CardNumber'] = cs
                except Exception:
                    out['CardNumber'] = None

        # final safety: ensure EmployeeID/CardNumber are not GUID-like tokens
        if 'EmployeeID' in out and isinstance(out['EmployeeID'], str) and _looks_like_guid(out['EmployeeID']):
            out['EmployeeID'] = None
        if 'CardNumber' in out and isinstance(out['CardNumber'], str) and _looks_like_guid(out['CardNumber']):
            out['CardNumber'] = None

        # If EmployeeName empty or looks like a GUID, prefer EmployeeID (human id) over GUIDs
        if (emp_name in (None, '', 'nan')) or (isinstance(emp_name, str) and _looks_like_guid(emp_name)):
            if emp_id not in (None, '', 'nan') and not _looks_like_guid(emp_id):
                out['EmployeeName'] = str(emp_id)
            else:
                out['EmployeeName'] = None

        cleaned.append(out)
    return cleaned

# ----- Helpers added to match commented (Pune) file functionality but multi-city-aware -----

def _replace_placeholder_strings(obj):
    """
    If obj is a DataFrame, replace known placeholder strings with None (NaN).
    If obj is a scalar/string, return None for placeholder strings else return obj.
    """
    if obj is None:
        return obj
    try:
        if isinstance(obj, pd.DataFrame):
            df = obj.copy()
            for col in df.columns:
                try:
                    # Replace placeholder strings (case-insensitive)
                    df[col] = df[col].apply(lambda x: None if _is_placeholder_str(x) else x)
                except Exception:
                    continue
            return df
        else:
            # scalar
            return None if _is_placeholder_str(obj) else obj
    except Exception:
        return obj

def _normalize_id_local(v):
    """
    Normalize an identifier for robust matching/counting:
    - treat NaN/None/empty as None
    - strip and convert float-like integers to integer strings
    """
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    if v is None:
        return None
    s = str(v).strip()
    if s == '' or s.lower() == 'nan':
        return None
    try:
        if '.' in s:
            fv = float(s)
            if math.isfinite(fv) and fv.is_integer():
                s = str(int(fv))
    except Exception:
        pass
    return s


def _parse_employees_param(params) -> List[str]:
    """
    Parse employees param from request params (supports JSON list or comma/;| separated string).
    Returns list of normalized tokens (suffixes stripped: e.g. 'emp:123' -> '123').

    Extended: accept more keys commonly used by frontends:
      - employees, employee, emp
      - employee_id, employeeid, emp_id, empid
      - employee_ids, employeeids
      - person_uid, person
      - id
    """
    if not params:
        return []

    # candidate param keys (in order of preference)
    keys = [
        'employees', 'employee', 'emp',
        'employee_id', 'employeeid', 'emp_id', 'empid',
        'employee_ids', 'employeeids', 'employees_ids',
        'person_uid', 'person', 'personid',
        'id'
    ]

    vals = None
    for k in keys:
        try:
            if k in params and params.get(k) not in (None, ''):
                vals = params.get(k)
                break
        except Exception:
            continue

    # fallback: accept any param that exactly matches 'employee' ignoring case
    if vals is None:
        for k, v in (params.items() if isinstance(params, dict) else []):
            try:
                if str(k).strip().lower() == 'employee' and v not in (None, ''):
                    vals = v
                    break
            except Exception:
                continue

    if vals is None:
        return []

    out = []
    # if passed as a list (JSON body)
    if isinstance(vals, (list, tuple)):
        cand_list = list(vals)
    else:
        # string: split by common separators (comma, semicolon, pipe, newline)
        s = str(vals).strip()
        if not s:
            return []
        cand_list = [p.strip() for p in re.split(r'[;,|\n]+', s) if p.strip()]

    for c in cand_list:
        try:
            norm = _strip_person_uid_prefix(c)
            if norm:
                out.append(str(norm))
        except Exception:
            try:
                sc = str(c).strip()
                if sc:
                    out.append(sc)
            except Exception:
                continue

    # unique preserve order
    seen = set()
    uniq = []
    for x in out:
        if x not in seen:
            seen.add(x)
            uniq.append(x)
    return uniq


def _row_matches_tokens(row, tokens: List[str]) -> bool:
    """
    Return True if pandas Series `row` matches any token in tokens list.
    Matching considerations:
      - compare person_uid, EmployeeID/Int1/Text12, EmployeeIdentity, CardNumber (string equality)
      - numeric tokens compared as ints/floats to EmployeeID/Int1 when possible
      - name tokens: case-insensitive substring match against EmployeeName
    """
    if row is None or not tokens:
        return False

    # gather candidate fields (stringified)
    def _safe_val(key):
        try:
            v = row.get(key) if hasattr(row, 'get') else row.get(key, None)
        except Exception:
            try:
                v = row[key] if key in row else None
            except Exception:
                v = None
        if v is None:
            return ''
        try:
            s = str(v).strip()
            # strip trailing .0 for floats
            if '.' in s:
                try:
                    f = float(s)
                    if math.isfinite(f) and float(f).is_integer():
                        s = str(int(f))
                except Exception:
                    pass
            return s
        except Exception:
            return ''

    person_uid_val = _safe_val('person_uid')
    empid_val = _safe_val('EmployeeID') or _safe_val('Int1') or _safe_val('Text12') or ''
    empident_val = _safe_val('EmployeeIdentity')
    card_val = _safe_val('CardNumber') or _safe_val('Card') or ''
    name_val = _safe_val('EmployeeName') or _safe_val('ObjectName1') or ''

    # prepare normalized set
    row_set = set()
    for v in (person_uid_val, empid_val, empident_val, card_val):
        if v:
            row_set.add(v)
    # lower name for substring compare
    name_lower = name_val.lower() if name_val else ''

    for t in tokens:
        if t is None:
            continue
        tt = str(t).strip()
        if not tt:
            continue
        # numeric attempt
        matched = False
        # If token looks numeric integer, compare to numeric employee id fields
        try:
            if '.' in tt:
                f = float(tt)
                if math.isfinite(f) and f.is_integer():
                    tt_num = str(int(f))
                else:
                    tt_num = None
            else:
                # if digits-only
                tt_num = tt if re.fullmatch(r'\d+', tt) else None
        except Exception:
            tt_num = None

        if tt_num:
            if empid_val == tt_num or empident_val == tt_num or person_uid_val == tt_num:
                return True

        # direct exact match (person_uid, empid, card, identity)
        if tt in row_set:
            return True
        # sometimes person_uid may include prefixes like emp:123 or uid:GUID
        if person_uid_val and tt in person_uid_val:
            return True

        # GUID-like tokens may match EmployeeIdentity or person_uid
        try:
            if _looks_like_guid(tt):
                if empident_val and tt.lower() == empident_val.lower():
                    return True
                if person_uid_val and tt.lower() in person_uid_val.lower():
                    return True
        except Exception:
            pass

        # name substring (case-insensitive) match
        try:
            if name_lower and tt.lower() in name_lower:
                return True
        except Exception:
            pass

    return False


def _find_swipe_files(outdir: Path, date_obj: Optional[date] = None, city_slug: Optional[str] = None, include_shifted: bool = True):
    """
    Robust swipe-file discovery.
    - If include_shifted is False, files with 'shift' in the filename (e.g. _shifted) are excluded.
    - Supports various filename patterns; if date_obj is None returns recent swipe-like files.
    - Returns list of Path objects sorted by mtime (newest first).
    """
    p = Path(outdir)
    files_set = set()
    try:
        city_slug_l = (city_slug or "").lower().strip()

        def add_glob(pattern):
            try:
                for fp in p.glob(pattern):
                    if fp.is_file():
                        files_set.add(fp)
            except Exception:
                pass

        if date_obj is None:
            add_glob("*_swipes_*.csv")
            add_glob("swipes_*.csv")
            add_glob("*swipes*.csv")
            add_glob("*_swipes.csv")
            add_glob("*swipe*.csv")
            if city_slug_l:
                add_glob(f"*{city_slug_l}*_swipes_*.csv")
                add_glob(f"*{city_slug_l}*swipes*.csv")
                add_glob(f"*{city_slug_l}*.csv")
        else:
            target = date_obj.strftime("%Y%m%d")
            patterns = [
                f"*_{target}.csv",
                f"*_swipes_{target}.csv",
                f"swipes*_{target}.csv",
                f"swipes_{target}.csv",
                f"*swipes*_{target}.csv",
                f"*{city_slug_l}*_{target}.csv",
                f"*{city_slug_l}*swipes*_{target}.csv",
                f"*{city_slug_l}_{target}.csv"
            ]
            for pat in patterns:
                add_glob(pat)

        # fallback: any CSV containing 'swipe'/'swipes' in the name
        try:
            for fp in p.iterdir():
                if not fp.is_file():
                    continue
                name = fp.name.lower()
                if ('_swipe' in name) or ('swipe' in name and name.endswith('.csv')):
                    files_set.add(fp)
        except Exception:
            pass

    except Exception:
        logging.exception("Error while searching for swipe files in %s", outdir)

    # Filter out shifted files if requested
    files = sorted(list(files_set), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
    if not include_shifted:
        files = [f for f in files if 'shift' not in f.name.lower()]

    return files



# def _consolidate_trend_rows(df: pd.DataFrame, combine_dates: bool = False) -> pd.DataFrame:
#     """
#     Consolidate multiple rows per person (and optionally per-date) into a single row.
#     If combine_dates == False: behave as before (group by person_key + Date).
#     If combine_dates == True: group by person_key only and aggregate Dates, Durations and Reasons
#     into single columns (semi-colon separated), while preserving the best representative row's
#     fields (using your existing priority rules).
#     """
#     if df is None or df.empty:
#         return df

#     df2 = df.copy()

#     # Normalize Date -> date objects
#     if 'Date' in df2.columns:
#         try:
#             df2['Date'] = pd.to_datetime(df2['Date'], errors='coerce').dt.date
#         except Exception:
#             pass
#     else:
#         for c in ('FirstSwipe', 'LastSwipe'):
#             if c in df2.columns:
#                 try:
#                     df2['Date'] = pd.to_datetime(df2[c], errors='coerce').dt.date
#                     break
#                 except Exception:
#                     pass
#         if 'Date' not in df2.columns:
#             df2['Date'] = None

#     # Build stable consolidation key: prefer person_uid, then EmployeeID, EmployeeIdentity, CardNumber, EmployeeName
#     id_cols = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'EmployeeName']
#     def _pick_key(row):
#         for c in id_cols:
#             try:
#                 if c in row and row.get(c) not in (None, '', float('nan')):
#                     v = row.get(c)
#                     s = str(v).strip()
#                     if s:
#                         return s
#             except Exception:
#                 continue
#         return None

#     df2['_trend_key'] = df2.apply(_pick_key, axis=1)

#     # helper to format duration (use existing format_seconds_to_hms if available)
#     def _fmt_duration_from_row(r):
#         try:
#             # prefer explicit 'Duration' string if present
#             if 'Duration' in r and r.get('Duration') not in (None, '', float('nan')):
#                 return str(r.get('Duration')).strip()
#             if 'DurationSeconds' in r and r.get('DurationSeconds') not in (None, '', float('nan')):
#                 try:
#                     return format_seconds_to_hms(float(r.get('DurationSeconds')))
#                 except Exception:
#                     pass
#             if 'DurationMinutes' in r and r.get('DurationMinutes') not in (None, '', float('nan')):
#                 try:
#                     mins = float(r.get('DurationMinutes'))
#                     secs = int(round(mins * 60.0))
#                     return format_seconds_to_hms(secs)
#                 except Exception:
#                     pass
#             # fallback to any duration-like column
#             for c in ('DurSeconds','Dur','Duration_str'):
#                 if c in r and r.get(c) not in (None, '', float('nan')):
#                     return str(r.get(c))
#         except Exception:
#             pass
#         return None

#     # grouping keys: if combine_dates -> group by key only; else group by key+Date
#     group_cols = ['_trend_key'] if combine_dates else ['_trend_key', 'Date']

#     out_rows = []
#     try:
#         grouped = df2.groupby(group_cols, sort=False, dropna=False)
#     except Exception:
#         grouped = [(None, df2)]

#     for gkeys, group in grouped:
#         g = group.copy()  # unified group dataframe

#         # If there is only one row in this group -> keep it (still normalize Date type)
#         if len(g) == 1:
#             out_rows.append(g.iloc[0])
#             continue

#         # pick a representative row using priority rules
#         picked = None
#         try:
#             if 'IsFlagged' in g.columns:
#                 flagged = g[g['IsFlagged'] == True]
#                 if not flagged.empty:
#                     if 'AnomalyScore' in flagged.columns:
#                         try:
#                             idx = pd.to_numeric(flagged['AnomalyScore'], errors='coerce').fillna(-1).astype(float).idxmax()
#                             picked = flagged.loc[idx].copy()
#                         except Exception:
#                             picked = flagged.iloc[0].copy()
#                     else:
#                         picked = flagged.iloc[0].copy()
#         except Exception:
#             picked = None

#         if picked is None:
#             if 'CountSwipes' in g.columns:
#                 try:
#                     idx = pd.to_numeric(g['CountSwipes'], errors='coerce').fillna(-1).astype(float).idxmax()
#                     picked = g.loc[idx].copy()
#                 except Exception:
#                     picked = None

#         if picked is None:
#             if 'DurationSeconds' in g.columns:
#                 try:
#                     idx = pd.to_numeric(g['DurationSeconds'], errors='coerce').fillna(-1).astype(float).idxmax()
#                     picked = g.loc[idx].copy()
#                 except Exception:
#                     picked = None

#         if picked is None:
#             if 'LastSwipe' in g.columns:
#                 try:
#                     g['__ls__'] = pd.to_datetime(g['LastSwipe'], errors='coerce')
#                     idx = g['__ls__'].idxmax()
#                     picked = g.loc[idx].copy()
#                 except Exception:
#                     picked = None
#                 finally:
#                     try:
#                         if '__ls__' in g.columns:
#                             g = g.drop(columns=['__ls__'])
#                     except Exception:
#                         pass

#         if picked is None:
#             picked = g.iloc[0].copy()

#         # If not combining across dates: keep representative row
#         if not combine_dates:
#             out_rows.append(picked)
#             continue

#         # --- combine multiple dates into single row (combine_dates == True) ---
#         try:
#             # sort group (best-effort)
#             g_sorted = g.copy()
#             try:
#                 g_sorted = g_sorted.sort_values(by='Date')
#             except Exception:
#                 pass

#             dates_list = []
#             durations_list = []
#             reasons_set = set()
#             anomaly_scores = []
#             flagged_any = False
#             countswipes_list = []

#             # detect ViolationDaysLast* column (prefer highest numeric suffix)
#             def _detect_violation_col(cols):
#                 cand = [c for c in cols if isinstance(c, str) and c.startswith('ViolationDaysLast')]
#                 if not cand:
#                     return None
#                 def _suffix_num(name):
#                     m = re.search(r'ViolationDaysLast(\d+)', name)
#                     if m:
#                         try:
#                             return int(m.group(1))
#                         except Exception:
#                             return 0
#                     return 0
#                 cand_sorted = sorted(cand, key=_suffix_num, reverse=True)
#                 return cand_sorted[0]

#             violation_col = _detect_violation_col(g_sorted.columns)

#             # collect per-row values
#             for _, rr in g_sorted.iterrows():
#                 # date string
#                 try:
#                     dval = rr.get('Date')
#                     if pd.notna(dval):
#                         ds = pd.to_datetime(dval, errors='coerce')
#                         if pd.notna(ds):
#                             dates_list.append(ds.date().strftime("%d/%m/%Y"))
#                         else:
#                             dates_list.append(str(dval))
#                     else:
#                         dates_list.append('')
#                 except Exception:
#                     dates_list.append(str(rr.get('Date')))

#                 # duration
#                 dstr = _fmt_duration_from_row(rr)
#                 durations_list.append(dstr if dstr else "")

#                 # reasons
#                 try:
#                     rs = rr.get('Reasons') or rr.get('DetectedScenarios') or None
#                     if rs:
#                         for part in re.split(r'[;,\|]', str(rs)):
#                             p = part.strip()
#                             if p and p.lower() not in ('nan','none',''):
#                                 reasons_set.add(p)
#                 except Exception:
#                     pass

#                 # anomaly score
#                 try:
#                     if 'AnomalyScore' in rr and rr.get('AnomalyScore') not in (None, '', float('nan')):
#                         anomaly_scores.append(float(rr.get('AnomalyScore')))
#                 except Exception:
#                     pass

#                 # flagged
#                 try:
#                     if 'IsFlagged' in rr and bool(rr.get('IsFlagged')):
#                         flagged_any = True
#                 except Exception:
#                     pass

#                 # CountSwipes
#                 try:
#                     if 'CountSwipes' in rr and rr.get('CountSwipes') not in (None, '', float('nan')):
#                         countswipes_list.append(int(float(rr.get('CountSwipes'))))
#                 except Exception:
#                     pass

#             # ---- compute ViolationDays properly: number of distinct Dates where IsFlagged == True ----
#             violation_days_total = 0
#             try:
#                 if 'IsFlagged' in g_sorted.columns and 'Date' in g_sorted.columns:
#                     flagged_rows = g_sorted[g_sorted['IsFlagged'] == True]
#                     if not flagged_rows.empty:
#                         norm_set = set()
#                         for _, fr in flagged_rows.iterrows():
#                             dd = fr.get('Date')
#                             if pd.isna(dd) or dd in (None, '', float('nan')):
#                                 continue
#                             try:
#                                 dnorm = pd.to_datetime(dd, errors='coerce').date()
#                                 if pd.notna(dnorm):
#                                     norm_set.add(dnorm)
#                                 else:
#                                     norm_set.add(str(dd))
#                             except Exception:
#                                 norm_set.add(str(dd))
#                         violation_days_total = int(len(norm_set))
#                 elif violation_col:
#                     vt = 0
#                     for _, rr in g_sorted.iterrows():
#                         try:
#                             v = rr.get(violation_col)
#                             if v not in (None, '', float('nan')):
#                                 vt += int(float(v))
#                         except Exception:
#                             pass
#                     violation_days_total = int(vt)
#                 else:
#                     violation_days_total = 0
#             except Exception:
#                 violation_days_total = 0

#             # attach aggregated fields to picked row
#             pairs = []
#             for d, du in zip(dates_list, durations_list):
#                 if d and du:
#                     pairs.append(f"{d} {du}")
#                 elif d and not du:
#                     pairs.append(f"{d}")
#                 elif du and not d:
#                     pairs.append(f"{du}")

#             picked['Dates'] = "; ".join([p for p in dates_list if p])
#             picked['Duration'] = "; ".join([p for p in durations_list if p])
#             picked['DurationByDate'] = "; ".join(pairs) if pairs else None

#             if reasons_set:
#                 picked['Reasons'] = "; ".join(sorted(reasons_set))
#             else:
#                 if 'Reasons' not in picked or picked.get('Reasons') in (None, '', float('nan')):
#                     picked['Reasons'] = None

#             # --- NEW: aggregate AnomalyScore & DetectedScenarios across dates (sum, union) ---
#             try:
#                 summed_score = float(sum([float(x) for x in anomaly_scores]) if anomaly_scores else 0.0)

#                 detected_set = set()
#                 try:
#                     for _, rr2 in g_sorted.iterrows():
#                         ds = rr2.get('DetectedScenarios') or rr2.get('DetectedScenario') or None
#                         if ds:
#                             for part in re.split(r'[;,\|]', str(ds)):
#                                 p = part.strip()
#                                 if p and p.lower() not in ('nan','none',''):
#                                     detected_set.add(p)
#                 except Exception:
#                     pass
#                 detected_set.update(reasons_set)

#                 picked['AnomalyScore'] = float(round(summed_score, 3))
#                 picked['DetectedScenarios'] = "; ".join(sorted(detected_set)) if detected_set else None
#                 picked['IsFlagged'] = bool(flagged_any) or bool(picked.get('IsFlagged'))
#                 picked['ViolationDays'] = int(violation_days_total)
#                 try:
#                     picked[f'ViolationDaysLast{int(DEFAULT_VIOLATION_DAYS)}'] = int(violation_days_total)
#                 except Exception:
#                     pass

#                 # ---- NEW: derive Risk primarily by ViolationDays, use AnomalyScore only to escalate ----
#                 try:
#                     # base mapping: violation days -> bucket
#                     vd = int(picked.get('ViolationDays') or 0)
#                     if vd >= 4:
#                         base_score = 5
#                     elif vd == 3:
#                         base_score = 4
#                     elif vd == 2:
#                         base_score = 3
#                     elif vd == 1:
#                         base_score = 2
#                     else:
#                         base_score = 1

#                     # optional escalation from aggregated anomaly score:
#                     # - if summed_score >= 4.0 -> force High (5)
#                     # - if summed_score >= 2.5 -> escalate by 1 bucket (up to 5)
#                     try:
#                         s = float(summed_score)
#                     except Exception:
#                         s = 0.0

#                     if s >= 4.0:
#                         final_score = 5
#                     elif s >= 2.5:
#                         final_score = min(5, base_score + 1)
#                     else:
#                         final_score = base_score

#                     # map numeric bucket to label
#                     label_map = {
#                         1: "Low",
#                         2: "Low Medium",
#                         3: "Medium",
#                         4: "Medium High",
#                         5: "High"
#                     }
#                     picked['RiskScore'] = int(final_score)
#                     picked['RiskLevel'] = label_map.get(int(final_score), "Low")
#                 except Exception:
#                     # fallback: try old mapping function
#                     try:
#                         if 'map_score_to_label' in globals() and callable(globals().get('map_score_to_label')):
#                             bucket, label = globals().get('map_score_to_label')(float(picked.get('AnomalyScore') or 0.0))
#                             picked['RiskScore'] = int(bucket) if bucket is not None else None
#                             picked['RiskLevel'] = label
#                         else:
#                             picked['RiskScore'] = None
#                             picked['RiskLevel'] = _map_score_to_label_fallback(float(picked.get('AnomalyScore') or 0.0))[1]
#                     except Exception:
#                         picked['RiskScore'] = None
#                         picked['RiskLevel'] = None

#             except Exception:
#                 logging.exception("Failed to aggregate anomaly & detected scenarios for key=%s", str(gkeys))

#             try:
#                 picked['CountSwipes'] = int(max(countswipes_list)) if countswipes_list else picked.get('CountSwipes', 0)
#             except Exception:
#                 pass

#         except Exception:
#             logging.exception("Failed to aggregate multi-date group for key=%s", str(gkeys))
#             # fallback: use representative row as-is (picked already assigned)
#         out_rows.append(picked)

#     try:
#         out_df = pd.DataFrame(out_rows).reset_index(drop=True)
#     except Exception:
#         return df

#     if '_trend_key' in out_df.columns:
#         try:
#             out_df = out_df.drop(columns=['_trend_key'])
#         except Exception:
#             pass

#     return out_df


def _consolidate_trend_rows(df: pd.DataFrame, combine_dates: bool = False) -> pd.DataFrame:
    """
    Consolidate multiple rows per person (and optionally per-date) into a single row.

    Fixed behavior:
      - Avoid summing anomaly scores across dates for RiskLevel. Use max (worst-day) as representative,
        keep sum/avg for reporting.
      - Prefer ViolationDays for risk bucket and escalate by anomaly max (not sum).
      - Defensive and backwards-compatible.
    """
    if df is None or df.empty:
        return df

    df2 = df.copy()

    # Normalize Date -> date objects
    if 'Date' in df2.columns:
        try:
            df2['Date'] = pd.to_datetime(df2['Date'], errors='coerce').dt.date
        except Exception:
            pass
    else:
        for c in ('FirstSwipe', 'LastSwipe'):
            if c in df2.columns:
                try:
                    df2['Date'] = pd.to_datetime(df2[c], errors='coerce').dt.date
                    break
                except Exception:
                    pass
        if 'Date' not in df2.columns:
            df2['Date'] = None

    # Build stable consolidation key: prefer person_uid, then EmployeeID, EmployeeIdentity, CardNumber, EmployeeName
    id_cols = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'EmployeeName']

    def _pick_key(row, idx):
        for c in id_cols:
            try:
                if c in row and row.get(c) not in (None, '', float('nan')):
                    v = row.get(c)
                    s = str(v).strip()
                    if s:
                        return s
            except Exception:
                continue
        try:
            name = row.get('EmployeeName')
            card = row.get('CardNumber')
            parts = []
            if name and str(name).strip():
                parts.append(str(name).strip())
            if card and str(card).strip():
                parts.append(str(card).strip())
            if parts:
                return "AUTO:" + "|".join(parts)
        except Exception:
            pass
        return f"ROWIDX:{idx}"

    df2['_trend_key'] = [_pick_key(row, idx) for idx, row in df2.iterrows()]

    def _fmt_duration_from_row(r):
        try:
            if 'Duration' in r and r.get('Duration') not in (None, '', float('nan')):
                return str(r.get('Duration')).strip()
            if 'DurationSeconds' in r and r.get('DurationSeconds') not in (None, '', float('nan')):
                try:
                    return format_seconds_to_hms(float(r.get('DurationSeconds')))
                except Exception:
                    pass
            if 'DurationMinutes' in r and r.get('DurationMinutes') not in (None, '', float('nan')):
                try:
                    mins = float(r.get('DurationMinutes'))
                    secs = int(round(mins * 60.0))
                    return format_seconds_to_hms(secs)
                except Exception:
                    pass
            for c in ('DurSeconds','Dur','Duration_str'):
                if c in r and r.get(c) not in (None, '', float('nan')):
                    return str(r.get(c))
        except Exception:
            pass
        return None

    group_cols = ['_trend_key'] if combine_dates else ['_trend_key', 'Date']

    out_rows = []
    try:
        grouped = df2.groupby(group_cols, sort=False, dropna=False)
    except Exception:
        grouped = [(None, df2)]

    for gkeys, group in grouped:
        g = group.copy()

        # Single row: keep as-is
        if len(g) == 1:
            out_rows.append(g.iloc[0])
            continue

        # Representative pick (same priority rules)
        picked = None
        try:
            if 'IsFlagged' in g.columns:
                flagged = g[g['IsFlagged'] == True]
                if not flagged.empty:
                    if 'AnomalyScore' in flagged.columns:
                        try:
                            idx = pd.to_numeric(flagged['AnomalyScore'], errors='coerce').fillna(-1).astype(float).idxmax()
                            picked = flagged.loc[idx].copy()
                        except Exception:
                            picked = flagged.iloc[0].copy()
                    else:
                        picked = flagged.iloc[0].copy()
        except Exception:
            picked = None

        if picked is None:
            if 'CountSwipes' in g.columns:
                try:
                    idx = pd.to_numeric(g['CountSwipes'], errors='coerce').fillna(-1).astype(float).idxmax()
                    picked = g.loc[idx].copy()
                except Exception:
                    picked = None

        if picked is None:
            if 'DurationSeconds' in g.columns:
                try:
                    idx = pd.to_numeric(g['DurationSeconds'], errors='coerce').fillna(-1).astype(float).idxmax()
                    picked = g.loc[idx].copy()
                except Exception:
                    picked = None

        if picked is None:
            if 'LastSwipe' in g.columns:
                try:
                    g['__ls__'] = pd.to_datetime(g['LastSwipe'], errors='coerce')
                    idx = g['__ls__'].idxmax()
                    picked = g.loc[idx].copy()
                except Exception:
                    picked = None
                finally:
                    try:
                        if '__ls__' in g.columns:
                            g = g.drop(columns=['__ls__'])
                    except Exception:
                        pass

        if picked is None:
            picked = g.iloc[0].copy()

        if not combine_dates:
            out_rows.append(picked)
            continue

        # --- combine multiple dates into single row ---
        try:
            g_sorted = g.copy()
            try:
                g_sorted = g_sorted.sort_values(by='Date')
            except Exception:
                pass

            dates_list = []
            durations_list = []
            reasons_set = set()
            anomaly_scores = []
            flagged_any = False
            countswipes_list = []

            def _detect_violation_col(cols):
                cand = [c for c in cols if isinstance(c, str) and c.startswith('ViolationDaysLast')]
                if not cand:
                    return None
                def _suffix_num(name):
                    m = re.search(r'ViolationDaysLast(\d+)', name)
                    if m:
                        try:
                            return int(m.group(1))
                        except Exception:
                            return 0
                    return 0
                cand_sorted = sorted(cand, key=_suffix_num, reverse=True)
                return cand_sorted[0]

            violation_col = _detect_violation_col(g_sorted.columns)

            for _, rr in g_sorted.iterrows():
                # Date
                try:
                    dval = rr.get('Date')
                    if pd.notna(dval):
                        ds = pd.to_datetime(dval, errors='coerce')
                        if pd.notna(ds):
                            dates_list.append(ds.date().strftime("%d/%m/%Y"))
                        else:
                            dates_list.append(str(dval))
                    else:
                        dates_list.append('')
                except Exception:
                    dates_list.append(str(rr.get('Date')))

                # duration
                dstr = _fmt_duration_from_row(rr)
                durations_list.append(dstr if dstr else "")

                # reasons
                try:
                    rs = rr.get('Reasons') or rr.get('DetectedScenarios') or None
                    if rs:
                        for part in re.split(r'[;,\|]', str(rs)):
                            p = part.strip()
                            if p and p.lower() not in ('nan','none',''):
                                reasons_set.add(p)
                except Exception:
                    pass

                # anomaly score (collect)
                try:
                    if 'AnomalyScore' in rr and rr.get('AnomalyScore') not in (None, '', float('nan')):
                        anomaly_scores.append(float(rr.get('AnomalyScore')))
                except Exception:
                    pass

                # flagged
                try:
                    if 'IsFlagged' in rr and bool(rr.get('IsFlagged')):
                        flagged_any = True
                except Exception:
                    pass

                # CountSwipes
                try:
                    if 'CountSwipes' in rr and rr.get('CountSwipes') not in (None, '', float('nan')):
                        countswipes_list.append(int(float(rr.get('CountSwipes'))))
                except Exception:
                    pass

            # compute distinct flagged Dates -> ViolationDays
            violation_days_total = 0
            try:
                if 'IsFlagged' in g_sorted.columns and 'Date' in g_sorted.columns:
                    flagged_rows = g_sorted[g_sorted['IsFlagged'] == True]
                    if not flagged_rows.empty:
                        norm_set = set()
                        for _, fr in flagged_rows.iterrows():
                            dd = fr.get('Date')
                            if pd.isna(dd) or dd in (None, '', float('nan')):
                                continue
                            try:
                                dnorm = pd.to_datetime(dd, errors='coerce').date()
                                if pd.notna(dnorm):
                                    norm_set.add(dnorm)
                                else:
                                    norm_set.add(str(dd))
                            except Exception:
                                norm_set.add(str(dd))
                        violation_days_total = int(len(norm_set))
                elif violation_col:
                    vt = 0
                    for _, rr in g_sorted.iterrows():
                        try:
                            v = rr.get(violation_col)
                            if v not in (None, '', float('nan')):
                                vt += int(float(v))
                        except Exception:
                            pass
                    violation_days_total = int(vt)
                else:
                    violation_days_total = 0
            except Exception:
                violation_days_total = 0

            # attach aggregated fields
            pairs = []
            for d, du in zip(dates_list, durations_list):
                if d and du:
                    pairs.append(f"{d} {du}")
                elif d and not du:
                    pairs.append(f"{d}")
                elif du and not d:
                    pairs.append(f"{du}")

            picked['Dates'] = "; ".join([p for p in dates_list if p])
            picked['Duration'] = "; ".join([p for p in durations_list if p])
            picked['DurationByDate'] = "; ".join(pairs) if pairs else None

            if reasons_set:
                picked['Reasons'] = "; ".join(sorted(reasons_set))
            else:
                if 'Reasons' not in picked or picked.get('Reasons') in (None, '', float('nan')):
                    picked['Reasons'] = None

            # --- Use max (worst-day) anomaly as representative, keep sum/avg for reporting ---
            try:
                summed_score = float(sum([float(x) for x in anomaly_scores]) if anomaly_scores else 0.0)
                max_score = float(max(anomaly_scores)) if anomaly_scores else 0.0
                avg_score = float(summed_score / len(anomaly_scores)) if anomaly_scores else 0.0

                picked['AnomalyScoreSum'] = float(round(summed_score, 3))
                picked['AnomalyScoreAvg'] = float(round(avg_score, 3))
                picked['AnomalyScore'] = float(round(max_score, 3))  # representative/worst-day

                # union of detected scenarios
                detected_set = set()
                try:
                    for _, rr2 in g_sorted.iterrows():
                        ds = rr2.get('DetectedScenarios') or rr2.get('DetectedScenario') or None
                        if ds:
                            for part in re.split(r'[;,\|]', str(ds)):
                                p = part.strip()
                                if p and p.lower() not in ('nan','none',''):
                                    detected_set.add(p)
                except Exception:
                    pass
                detected_set.update(reasons_set)
                picked['DetectedScenarios'] = "; ".join(sorted(detected_set)) if detected_set else None

                picked['IsFlagged'] = bool(flagged_any) or bool(picked.get('IsFlagged'))

                # ViolationDays and compatibility field
                picked['ViolationDays'] = int(violation_days_total)
                try:
                    default_days = globals().get('DEFAULT_VIOLATION_DAYS', globals().get('VIOLATION_WINDOW_DAYS', 90))
                    picked[f'ViolationDaysLast{int(default_days)}'] = int(violation_days_total)
                except Exception:
                    pass

                # ---- NEW: derive Risk primarily by ViolationDays, escalate by representative anomaly (max_score) ----
                try:
                    vd = int(picked.get('ViolationDays') or 0)
                    if vd >= 4:
                        base_score = 5
                    elif vd == 3:
                        base_score = 4
                    elif vd == 2:
                        base_score = 3
                    elif vd == 1:
                        base_score = 2
                    else:
                        base_score = 1

                    s = float(max_score or 0.0)
                    if s >= 4.0:
                        final_score = 5
                    elif s >= 2.5:
                        final_score = min(5, base_score + 1)
                    else:
                        final_score = base_score

                    label_map = {
                        1: "Low",
                        2: "Low Medium",
                        3: "Medium",
                        4: "Medium High",
                        5: "High"
                    }
                    picked['RiskScore'] = int(final_score)
                    picked['RiskLevel'] = label_map.get(int(final_score), "Low")
                except Exception:
                    # fallback to legacy mapping from anomaly if needed
                    try:
                        if 'map_score_to_label' in globals() and callable(globals().get('map_score_to_label')):
                            bucket, label = globals().get('map_score_to_label')(float(picked.get('AnomalyScore') or 0.0))
                            picked['RiskScore'] = int(bucket) if bucket is not None else None
                            picked['RiskLevel'] = label
                        else:
                            picked['RiskScore'] = None
                            picked['RiskLevel'] = None
                    except Exception:
                        picked['RiskScore'] = None
                        picked['RiskLevel'] = None

                # preserve old override
                try:
                    if int(picked.get('ViolationDays') or 0) >= 4:
                        picked['RiskScore'] = 5
                        picked['RiskLevel'] = 'High'
                except Exception:
                    pass

            except Exception:
                logging.exception("Failed to aggregate anomaly & detected scenarios for key=%s", str(gkeys))

            try:
                picked['CountSwipes'] = int(max(countswipes_list)) if countswipes_list else picked.get('CountSwipes', 0)
            except Exception:
                pass

        except Exception:
            logging.exception("Failed to aggregate multi-date group for key=%s", str(gkeys))
        out_rows.append(picked)

    try:
        out_df = pd.DataFrame(out_rows).reset_index(drop=True)
    except Exception:
        return df

    if '_trend_key' in out_df.columns:
        try:
            out_df = out_df.drop(columns=['_trend_key'])
        except Exception:
            pass

    return out_df

# -----------------------
# Routes
# -----------------------


@app.route('/')
def root():
    return "Trend Analysis API — Multi-city"

@app.route('/employee/<path:pid>/image', methods=['GET'])
@app.route('/api/employees/<path:pid>/image', methods=['GET'])
def serve_employee_image(pid):
    """
    Try to return image bytes for pid using employeeimage.get_person_image_bytes.
    Fallbacks:
      - try stripped prefix (emp:, uid:, name:)
      - try resolving pid -> Personnel.ObjectID / GUID using get_personnel_info()
    If none found -> 404 JSON.
    """
    try:
        b = None
        # attempt with raw pid and stripped prefix
        logging.info("serve_employee_image: requested pid=%s", pid)
        b = get_person_image_bytes(pid)
        if not b:
            # try stripping common prefixes if present
            try:
                if ':' in pid:
                    _, rest = pid.split(':', 1)
                    logging.debug("serve_employee_image: trying stripped pid=%s", rest.strip())
                    b = get_person_image_bytes(rest.strip())
            except Exception:
                pass

        # NEW: if still not found, try to resolve pid via personnel lookup (EmployeeID -> ObjectID)
        if not b:
            try:
                logging.debug("serve_employee_image: attempt personnel resolution for pid=%s", pid)
                pinfo = get_personnel_info(pid) or {}
                # prefer ObjectID, then GUID
                obj = pinfo.get("ObjectID")
                guid = pinfo.get("GUID")
                tried = []
                if obj is not None:
                    obj_s = str(obj).strip()
                    tried.append(obj_s)
                    logging.debug("serve_employee_image: trying image with resolved ObjectID=%s", obj_s)
                    b = get_person_image_bytes(obj_s)
                if not b and guid:
                    guid_s = str(guid).strip()
                    tried.append(guid_s)
                    logging.debug("serve_employee_image: trying image with resolved GUID=%s", guid_s)
                    b = get_person_image_bytes(guid_s)
                if b:
                    logging.info("serve_employee_image: resolved pid=%s -> used parent id(s)=%s to fetch image", pid, tried)
            except Exception:
                logging.exception("serve_employee_image: personnel resolution attempt failed for pid=%s", pid)

        if not b:
            logging.warning("serve_employee_image: no image found for pid=%s", pid)
            return jsonify({"error": "image not found"}), 404

        try:
            kind = _guess_image_kind(b)
            if not kind:
                kind = 'jpeg'
            mime = 'image/' + ('jpeg' if kind == 'jpg' else kind)
        except Exception:
            mime = 'image/jpeg'

        bio = BytesIO(b)
        bio.seek(0)
        response = send_file(bio, mimetype=mime, as_attachment=False)
        try:
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            response.headers['Pragma'] = 'no-cache'
            response.headers['Expires'] = '0'
        except Exception:
            pass
        return response

    except Exception as e:
        logging.exception("serve_employee_image failed for %s", pid)
        return jsonify({"error": "internal server error", "details": str(e)}), 500


@app.route('/run', methods=['GET', 'POST'])
def run_trend():
    params = {}
    if request.method == 'GET':
        params = request.args.to_dict()
    else:
        if request.is_json:
            params = request.get_json(force=True) or {}
        else:
            try:
                params = request.form.to_dict() or {}
            except Exception:
                params = {}

    date_str = (params.get('date') or params.get('Date') or '').strip() or None
    start_str = (params.get('start') or params.get('Start') or '').strip() or None
    end_str = (params.get('end') or params.get('End') or '').strip() or None

    dates = []
    try:
        if date_str:
            dt = datetime.strptime(date_str, "%Y-%m-%d").date()
            dates = [dt]
        elif start_str and end_str:
            s = datetime.strptime(start_str, "%Y-%m-%d").date()
            e = datetime.strptime(end_str, "%Y-%m-%d").date()
            if e < s:
                return jsonify({"error":"end must be >= start"}), 400
            cur = s
            while cur <= e:
                dates.append(cur)
                cur = cur + timedelta(days=1)
        else:
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            dates = [yesterday, today]
    except Exception as e:
        return jsonify({"error": f"Invalid date format: {e}"}), 400

    regions_param = params.get('regions') or params.get('region') or ''
    if regions_param:
        regions = [r.strip().lower() for r in re.split(r'[;,|]', str(regions_param)) if r.strip()]
    else:
        try:
            regions = [k.lower() for k in list(REGION_CONFIG.keys())]
        except Exception:
            regions = ['apac']

    valid_regions = []
    for r in regions:
        if r in (REGION_CONFIG or {}):
            valid_regions.append(r)
        else:
            logging.debug("Requested region '%s' not in REGION_CONFIG - skipping", r)
    if not valid_regions:
        valid_regions = [k.lower() for k in REGION_CONFIG.keys()] if REGION_CONFIG else ['apac']
    params['_regions_to_run'] = valid_regions



    city_param = params.get('city') or params.get('site') or params.get('site_name') or None
    city_slug = _slug_city(city_param) if city_param else None
    params['_city'] = city_slug
    force = str(params.get('force', '')).lower() in ('1', 'true', 'yes')
    city_slug_safe = city_slug or 'pune'


    combined_rows = []
    files = []

    
    # ---------------------------
    # Run trend for each requested date (updated: skip if CSV exists unless force=True)
    # ---------------------------
    for d in dates:
        try:
            csv_path = DEFAULT_OUTDIR / f"trend_{city_slug_safe}_{d.strftime('%Y%m%d')}.csv"

            # If CSV already exists and caller did not request force -> reuse it (cache)
            if csv_path.exists() and not force:
                logging.info("Skipping generation for %s (city=%s) because %s exists (use force to override)", d.isoformat(), city_slug_safe, csv_path.name)
                try:
                    # attempt to read CSV produced earlier
                    try:
                        df = pd.read_csv(csv_path, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
                    except Exception:
                        # fallback if parse_dates causes trouble
                        df = pd.read_csv(csv_path, dtype=str)
                except Exception:
                    logging.exception("Failed to read existing CSV %s; will attempt regeneration", csv_path)
                    df = None
                files.append(csv_path.name)
            else:
                # Need to generate (either file missing or force=True)
                if run_trend_for_date is None:
                    raise RuntimeError("run_trend_for_date helper not available in trend_runner")

                # try calling with the richer signature first (regions, outdir, city)
                df = None
                try:
                    # attempt most-common signature
                    df = run_trend_for_date(d, regions=valid_regions, outdir=str(DEFAULT_OUTDIR), city=city_slug_safe)
                except TypeError:
                    # fallback attempts for older signatures
                    try:
                        df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR), city=city_slug_safe)
                    except TypeError:
                        try:
                            df = run_trend_for_date(d, outdir=str(DEFAULT_OUTDIR))
                        except TypeError:
                            try:
                                df = run_trend_for_date(d)
                            except Exception as e_inner:
                                logging.exception("All call attempts to run_trend_for_date failed for %s: %s", d, e_inner)
                                raise

                # If run_trend_for_date returned a DataFrame, ensure we record csv name if file created
                try:
                    # if generator wrote csv to DEFAULT_OUTDIR, append it
                    if csv_path.exists():
                        files.append(csv_path.name)
                except Exception:
                    pass

        except Exception as e:
            logging.exception("run_trend_for_date failed for %s", d)
            return jsonify({"error": f"runner failed for {d}: {e}"}), 500

        # If we got here and df is None or empty -> continue without adding rows
        if df is None or (hasattr(df, 'empty') and df.empty):
            continue

        # Replace placeholder strings etc (same as before)
        try:
            df = _replace_placeholder_strings(df)
        except Exception:
            pass

        if 'IsFlagged' not in df.columns:
            df['IsFlagged'] = False
        if 'Reasons' not in df.columns:
            df['Reasons'] = None

        # still support hybrid store (existing behavior)
        try:
            if str(params.get('hybrid', '')).lower() in ('1','true','yes'):
                try:
                    per_city_dir = DEFAULT_OUTDIR / (city_slug_safe or 'unknown_city')
                    per_city_dir.mkdir(parents=True, exist_ok=True)
                    per_city_path = per_city_dir / f"trend_{city_slug_safe}_{d.strftime('%Y%m%d')}.csv"
                    # write CSV copy if df exists and not empty
                    if df is not None and not (hasattr(df, 'empty') and df.empty):
                        try:
                            df.to_csv(per_city_path, index=False)
                            logging.info("Hybrid store: wrote per-city file %s", per_city_path)
                        except Exception:
                            logging.exception("Failed writing per-city hybrid CSV %s", per_city_path)
                except Exception:
                    logging.exception("Hybrid-per-city write block failed")
        except Exception:
            pass

        combined_rows.append(df)
    

    # *** Important: combine after loop to avoid UnboundLocalError and extra repeated concat inside loop ***
    try:
        combined_df = pd.concat(combined_rows, ignore_index=True) if combined_rows else pd.DataFrame()
    except Exception:
        combined_df = pd.DataFrame()

    # ---------- NEW: employee filtering support ----------
    try:
        employees_tokens = _parse_employees_param(params)
    except Exception:
        employees_tokens = []

    if employees_tokens:
        try:
            before_count = int(len(combined_df))
            # if combined_df empty just fast-fail
            if combined_df is None or combined_df.empty:
                logging.info("Employee filter requested but no combined_df rows present.")
                return jsonify({"message": "No scenario met", "rows": 0}), 200

            # apply row filter
            try:
                mask = combined_df.apply(lambda r: _row_matches_tokens(r, employees_tokens), axis=1)
                combined_df = combined_df[mask].copy()
            except Exception:
                # per-row apply can fail on exotic frames, fallback to naive string contains across key cols
                logging.exception("Per-row employee filter failed; trying fallback contains filter.")
                cols = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'CardNumber', 'EmployeeName']
                mask2 = pd.Series(False, index=combined_df.index)
                for c in cols:
                    if c in combined_df.columns:
                        try:
                            mask2 = mask2 | combined_df[c].astype(str).fillna('').str.strip().isin(employees_tokens)
                        except Exception:
                            try:
                                for t in employees_tokens:
                                    mask2 = mask2 | combined_df[c].astype(str).str.contains(str(t), na=False, case=False)
                            except Exception:
                                continue
                combined_df = combined_df[mask2].copy()

            after_count = int(len(combined_df))
            logging.info("Employee filter tokens=%s applied: rows_before=%d rows_after=%d", employees_tokens, before_count, after_count)

            # If nothing matched, return friendly message for frontend
            if combined_df.empty:
                return jsonify({"message": "No scenario met", "rows": 0}), 200

        except Exception:
            logging.exception("Failed applying employee filter; continuing without employee filter.")
    # ---------- END employee filtering ----------


    # --- CONSOLIDATION PATCH APPLIED HERE ---
    try:
        # Keep raw count of aggregated rows before consolidation
        aggregated_rows_total_raw = int(len(combined_df)) if combined_df is not None else 0

        try:
            combined_agg_df = _consolidate_trend_rows(combined_df, combine_dates=True)
        except Exception:
            logging.exception("run_trend: consolidation failed; falling back to raw combined_df")
            combined_agg_df = combined_df.copy() if combined_df is not None else pd.DataFrame()

        # Compute unique persons from consolidated dataframe
        if combined_agg_df is not None and not combined_agg_df.empty:
            if 'person_uid' in combined_agg_df.columns:
                raw_unique_person_uids = int(combined_agg_df['person_uid'].dropna().astype(str).nunique())
            elif 'EmployeeID' in combined_agg_df.columns:
                raw_unique_person_uids = int(combined_agg_df['EmployeeID'].dropna().astype(str).nunique())
            else:
                raw_unique_person_uids = int(len(combined_agg_df))
        else:
            raw_unique_person_uids = 0
    except Exception:
        # conservative fallback
        aggregated_rows_total_raw = int(len(combined_df)) if combined_df is not None else 0
        raw_unique_person_uids = int(len(combined_df)) if combined_df is not None else 0
        combined_agg_df = combined_df.copy() if combined_df is not None else pd.DataFrame()
    # --- END CONSOLIDATION PATCH ---


    try:
        if not combined_agg_df.empty and 'IsFlagged' in combined_agg_df.columns:
            flagged_df = combined_agg_df[combined_agg_df['IsFlagged'] == True].copy()
        else:
            flagged_df = pd.DataFrame()
    except Exception:
        flagged_df = pd.DataFrame()

    try:
        analysis_count = int(raw_unique_person_uids)
    except Exception:
        analysis_count = int(len(combined_agg_df)) if combined_agg_df is not None else 0

    try:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = float((flagged_count / analysis_count * 100.0) if analysis_count and analysis_count > 0 else 0.0)
    except Exception:
        flagged_count = int(len(flagged_df))
        flagged_rate_pct = 0.0

    try:
        # If we have flagged rows, return ALL flagged rows (strict)
        if flagged_df is not None and not flagged_df.empty:
            sample_source = flagged_df
            # return exactly flagged_count rows (no hidden head(10) truncation)
            samples = _clean_sample_df(sample_source, max_rows=int(len(flagged_df)))
        else:
            # new behaviour: prefer sample from consolidated aggregated dataframe
            sample_source = combined_agg_df
            samples = _clean_sample_df(sample_source.head(10), max_rows=10) if sample_source is not None and not sample_source.empty else []
    except Exception:
        samples = []

    resp = {
        "start_date": dates[0].isoformat() if dates else None,
        "end_date": dates[-1].isoformat() if dates else None,
        "aggregated_rows_total_raw": aggregated_rows_total_raw,
        "aggregated_unique_persons": int(raw_unique_person_uids),
        "rows": int(raw_unique_person_uids),
        "flagged_rows": int(flagged_count),
        "flagged_rate_percent": float(flagged_rate_pct),
        "files": files,
         "sample": (samples if isinstance(samples, list) else samples),
   
        "reasons_count": {},
        "risk_counts": {},
      
         "flagged_persons": (samples if samples else []),
        "_raw_unique_person_uids": int(raw_unique_person_uids),
        "regions_run": params.get('_regions_to_run', []),
        "city_used": city_slug
    }

    return jsonify(resp)



@app.route('/latest', methods=['GET'])
def latest_results():
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    p = Path(DEFAULT_OUTDIR)
    csvs = sorted(p.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(p.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return jsonify({"error": "no outputs found"}), 404
    latest = csvs[0]

    start_date_iso = None
    end_date_iso = None
    try:
        m = re.search(r'(\d{8})', latest.name)
        if m:
            ymd = m.group(1)
            dt = datetime.strptime(ymd, "%Y%m%d").date()
            start_date_iso = dt.isoformat()
            end_date_iso = dt.isoformat()
    except Exception:
        start_date_iso = None
        end_date_iso = None

    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)

    df = _replace_placeholder_strings(df)

    id_candidates = ['person_uid', 'EmployeeID', 'EmployeeIdentity', 'Int1']
    id_col = next((c for c in id_candidates if c in df.columns), None)

    def _norm_val_for_latest(v):
        try:
            if pd.isna(v):
                return None
        except Exception:
            pass
        if v is None:
            return None
        s = str(v).strip()
        if s == '' or s.lower() == 'nan':
            return None
        try:
            if '.' in s:
                fv = float(s)
                if math.isfinite(fv) and fv.is_integer():
                    s = str(int(fv))
        except Exception:
            pass
        return s

    if id_col is None:
        unique_persons = int(len(df))
    else:
        ids_series = df[id_col].apply(_norm_val_for_latest) if id_col in df.columns else pd.Series([None]*len(df))
        if id_col != 'person_uid' and 'person_uid' in df.columns:
            ids_series = ids_series.fillna(df['person_uid'].astype(str).replace('nan','').replace('None',''))
        unique_persons = int(len(set([x for x in ids_series.unique() if x])))

    # build initial sample (list of dicts)
    sample = _clean_sample_df(df, max_rows=5)  # returns list



    resp = {
        
        "file": latest.name,
        "rows_raw": int(len(df)),
        "rows": unique_persons,
        "sample": sample,
        "start_date": start_date_iso,
        "end_date": end_date_iso,
        "city": city_slug
    }
    return jsonify(resp)


@app.route('/record', methods=['GET'])
def record():
    try:
        # --- BEGIN existing record() logic ---
        from pathlib import Path
        import pandas as pd
        import math
        import re
        from datetime import datetime, date
        try:
            q = request.args.get('employee_id') or request.args.get('person_uid')
        except Exception:
            q = None
        include_unflagged = str(request.args.get('include_unflagged', '')).lower() in ('1', 'true', 'yes')
        city_param = request.args.get('city') or request.args.get('site') or 'pune'

        # pick outdir consistently
        try:
            base_out = Path(DEFAULT_OUTDIR)
        except Exception:
            try:
                base_out = Path(OUTDIR)
            except Exception:
                base_out = Path.cwd()


            
        # helper safe wrappers (use existing ones if present)
        def _safe_read(fp, **kwargs):
            try:
                if '_safe_read_csv' in globals():
                    return _safe_read_csv(fp)
                return pd.read_csv(fp, **kwargs)
            except Exception:
                try:
                    return pd.read_csv(fp, dtype=str, **{k: v for k, v in kwargs.items() if k != 'parse_dates'})
                except Exception:
                    return pd.DataFrame()

        def _to_python_scalar(v):
            if pd.isna(v):
                return None
            try:
                return v.item() if hasattr(v, 'item') else v
            except Exception:
                return v

        # 1) find trend CSVs (city-specific first)
        def _slug(s):
            return re.sub(r'[^a-z0-9]+', '_', str(s or '').strip().lower()).strip('_')

        city_slug = _slug(city_param)
        trend_glob = list(base_out.glob(f"trend_{city_slug}_*.csv"))
        if not trend_glob:
            trend_glob = list(base_out.glob("trend_*.csv"))
        trend_glob = sorted(trend_glob, reverse=True)

        df_list = []
        for fp in trend_glob:
            try:
                tmp = pd.read_csv(fp, parse_dates=['Date', 'FirstSwipe', 'LastSwipe'])
            except Exception:
                try:
                    tmp = pd.read_csv(fp, dtype=str)
                    if 'Date' in tmp.columns:
                        tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                except Exception:
                    continue
            df_list.append(tmp)
        if df_list:
            trends_df = pd.concat(df_list, ignore_index=True)
            try:
                trends_df = _replace_placeholder_strings(trends_df)
            except Exception:
                pass
        else:
            trends_df = pd.DataFrame()

            


        # ---------- ADDED: enrichment helper ----------
        def _enrich_with_contact_info(rows):
            """Given list-of-dict rows, best-effort populate EmployeeEmail / Email if missing."""
            try:
                if not rows:
                    return rows
                out = []
                for r in rows:
                    # do not mutate original in-place (defensive)
                    rr = dict(r)
                    if not rr.get('EmployeeEmail') and not rr.get('Email'):
                        candidate = rr.get('EmployeeID') or rr.get('person_uid') or rr.get('ObjectID') or rr.get('GUID')
                        try:
                            if candidate:
                                pinfo = {}
                                try:
                                    pinfo = get_personnel_info(candidate) or {}
                                except Exception:
                                    pinfo = {}
                                # populate from keys commonly provided by get_personnel_info
                                for key in ('EmployeeEmail','EmailAddress','Email','WorkEmail'):
                                    if not rr.get('EmployeeEmail') and pinfo.get(key):
                                        rr['EmployeeEmail'] = pinfo.get(key)
                                        rr['Email'] = pinfo.get(key)
                                        break
                                # fallback: ManagerEmail if nothing else
                                if (not rr.get('EmployeeEmail')) and pinfo.get('ManagerEmail'):
                                    rr['ManagerEmail'] = pinfo.get('ManagerEmail')
                            # else no candidate => nothing we can do
                        except Exception:
                            pass
                    out.append(rr)
                return out
            except Exception:
                return rows
        # ---------- END enrichment helper ----------


        # if no query param, return a small sample of trend rows (if any)
        if q is None:
            try:
                if not trends_df.empty and '_clean_sample_df' in globals():
                    cleaned = _clean_sample_df(trends_df, max_rows=10)
                elif not trends_df.empty:
                    cleaned = trends_df.head(10).to_dict(orient='records')
                else:
                    cleaned = []
            except Exception:
                cleaned = []
            # ENRICH CONTACT INFO
            try:
                cleaned = _enrich_with_contact_info(cleaned)
            except Exception:
                pass
            return jsonify({'aggregated_rows': cleaned, 'raw_swipe_files': [], 'raw_swipes': []}), 200

        q_str = str(q).strip()

        # helper to normalise series values to comparable strings/numerics
        def normalize_series(s):
            if s is None:
                return pd.Series([''] * (len(trends_df) if not trends_df.empty else 0))
            s = s.fillna('').astype(str).str.strip()
            def _norm_val(v):
                if not v:
                    return ''
                try:
                    if '.' in v:
                        fv = float(v)
                        if math.isfinite(fv) and fv.is_integer():
                            return str(int(fv))
                except Exception:
                    pass
                return v
            return s.map(_norm_val)





        # find matching rows in trends_df
        found_mask = pd.Series(False, index=trends_df.index) if not trends_df.empty else pd.Series(dtype=bool)
        candidates_cols = ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'CardNumber', 'Int1', 'Text12', 'EmployeeName')
        for c in candidates_cols:
            if c in trends_df.columns:
                try:
                    ser = normalize_series(trends_df[c])
                    found_mask = found_mask | (ser == q_str)
                except Exception:
                    pass

        # numeric fallback
        if (found_mask is None) or (not found_mask.any() if len(found_mask) else True):
            try:
                q_numeric = float(q_str)
                for c in ('EmployeeID', 'Int1'):
                    if c in trends_df.columns:
                        try:
                            numser = pd.to_numeric(trends_df[c], errors='coerce')
                            found_mask = found_mask | (numser == q_numeric)
                        except Exception:
                            pass
            except Exception:
                pass

        matched = trends_df[found_mask].copy() if not trends_df.empty else pd.DataFrame()
        if matched.empty:
            cleaned_matched = []
        else:
            try:
                cleaned_matched = _clean_sample_df(matched, max_rows=len(matched)) if '_clean_sample_df' in globals() else matched.to_dict(orient='records')
            except Exception:
                cleaned_matched = matched.to_dict(orient='records')

        # ENRICH CONTACT INFO for matched aggregated rows
        try:
            cleaned_matched = _enrich_with_contact_info(cleaned_matched)
        except Exception:
            pass


        # build list of dates to scan for swipe files (from matched rows)
        dates_to_scan = set()
        try:
            for _, r in (matched.iterrows() if not matched.empty else []):
                try:
                    if 'Date' in r and pd.notna(r['Date']):
                        try:
                            d = pd.to_datetime(r['Date']).date()
                            dates_to_scan.add(d)
                        except Exception:
                            pass
                    for col in ('FirstSwipe','LastSwipe'):
                        if col in r and pd.notna(r[col]):
                            try:
                                d = pd.to_datetime(r[col]).date()
                                dates_to_scan.add(d)
                            except Exception:
                                pass
                except Exception:
                    continue
        except Exception:
            pass
        if not dates_to_scan:
            dates_to_scan = {None}  # indicates scan all swipes files

        # ---------- helper: find swipe files for a date (robust) ----------
        def _find_swipes_for_date(date_obj=None):
            try:
                include_shifted = True
                try:
                    if city_slug and str(city_slug).strip().lower() == 'pune':
                        include_shifted = False
                except Exception:
                    include_shifted = True

                if '_find_swipe_files' in globals() and callable(globals().get('_find_swipe_files')):
                    try:
                        cand = _find_swipe_files(str(base_out), date_obj=date_obj, city_slug=city_slug if city_slug else None, include_shifted=include_shifted)
                        if cand:
                            return cand
                    except Exception:
                        logging.exception("_find_swipe_files helper failed; falling back to glob search.")

                files = []
                if date_obj is None:
                    files = list(base_out.glob("swipes_*_*.csv")) + list(base_out.glob("swipes_*.csv")) + list(base_out.glob("*swipe*.csv"))
                else:
                    ymd = date_obj.strftime('%Y-%m-%d')
                    ymd2 = date_obj.strftime('%Y%m%d')
                    cand1 = [p for p in base_out.glob("swipes_*_*.csv") if (ymd in p.name or ymd2 in p.name)]
                    cand2 = [p for p in base_out.glob("swipes_*.csv") if (ymd in p.name or ymd2 in p.name)]
                    files = cand1 + cand2
                files = sorted(list({p for p in files if p.exists()}), key=lambda f: f.stat().st_mtime if f.exists() else 0, reverse=True)
                if not include_shifted:
                    files = [f for f in files if 'shift' not in f.name.lower()]
                return files
            except Exception:
                logging.exception("Error while searching for swipe files for date=%s city=%s", date_obj, city_slug)
                return []


        # ---------- scan swipe files for the target person (dates_to_scan computed earlier) ----------
        raw_files_set = set()
        raw_swipes_out = []
        seen_keys = set()

        def _append_row_for_evidence(out_row, source_name):
            # avoid exact duplicate rows from same file
            key = (
                str(out_row.get('LocaleMessageTime') or ''),
                str(out_row.get('DateOnly') or ''),
                str(out_row.get('Swipe_Time') or ''),
                str(out_row.get('Door') or '').strip(),
                str(out_row.get('Direction') or '').strip(),
                str(out_row.get('CardNumber') or '').strip()
            )
            if key in seen_keys:
                return False
            seen_keys.add(key)
            out_row['_source'] = source_name
            raw_swipes_out.append(out_row)
            return True

        # helper to format datetime to requested display formats
        def _format_time_fields(ts):
            # ts is a pandas Timestamp or datetime or None
            if ts is None or (isinstance(ts, float) and math.isnan(ts)):
                return (None, None, None)
            try:
                dt = pd.to_datetime(ts)
            except Exception:
                return (None, None, None)
            try:
                locale_iso = dt.isoformat()
            except Exception:
                locale_iso = str(dt)
            try:
                date_only = dt.strftime("%d-%b-%y")  # e.g. 17-Nov-25
            except Exception:
                try:
                    date_only = dt.date().isoformat()
                except Exception:
                    date_only = None
            try:
                # 12-hour time with AM/PM, strip leading zero
                swipe_time = dt.strftime("%I:%M:%S %p").lstrip("0")
            except Exception:
                swipe_time = None
            return (locale_iso, date_only, swipe_time)

        # loop over each date we want to scan (these are violation dates if matched rows existed)
        for d in dates_to_scan:
            swipe_candidates = _find_swipes_for_date(d)
            if d is not None and not swipe_candidates:
                # fallback to scanning all swipe files if none found for the exact date pattern
                swipe_candidates = _find_swipes_for_date(None)

            for fp in swipe_candidates:
                try:
                    sdf = _safe_read(fp, parse_dates=['LocaleMessageTime'])
                except Exception:
                    try:
                        sdf = _safe_read(fp)
                    except Exception:
                        continue
                if sdf is None or sdf.empty:
                    continue

                # minimal column-normalization for detection (case-insensitive)
                cols_lower = {c.lower(): c for c in sdf.columns}
                tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or \
                       cols_lower.get('timestamp') or cols_lower.get('time') or None
                emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or \
                          cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
                name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or \
                           cols_lower.get('employee_name') or None
                card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or \
                           cols_lower.get('chuid') or cols_lower.get('value') or None
                door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
                dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
                admit_cols = [c for c in ('admitcode','admit','admit_code','admit_type','admitstatus') if c in cols_lower]
                admit_col = cols_lower.get(admit_cols[0]) if admit_cols else None
                personnel_col = cols_lower.get('personneltype') or cols_lower.get('personneltypename') or None
                location_col = cols_lower.get('partitionname2') or cols_lower.get('location') or cols_lower.get('partitionname') or None
                person_uid_col = cols_lower.get('person_uid')

                # build boolean mask for rows that match the query identifier q_str
                try:
                    mask = pd.Series(False, index=sdf.index)
                except Exception:
                    mask = pd.Series([False] * len(sdf))

                try:
                    if person_uid_col and person_uid_col in sdf.columns:
                        mask = mask | (sdf[person_uid_col].astype(str).str.strip() == q_str)
                except Exception:
                    pass
                try:
                    if emp_col and emp_col in sdf.columns:
                        mask = mask | (sdf[emp_col].astype(str).str.strip() == q_str)
                except Exception:
                    pass
                # numeric fallback for employee id
                if (not mask.any()) and emp_col and emp_col in sdf.columns:
                    try:
                        q_numeric = float(q_str)
                        emp_numeric = pd.to_numeric(sdf[emp_col], errors='coerce')
                        mask = mask | (emp_numeric == q_numeric)
                    except Exception:
                        pass
                # name fallback
                if (not mask.any()) and name_col and name_col in sdf.columns:
                    try:
                        mask = mask | (sdf[name_col].astype(str).str.strip().str.lower() == q_str.lower())
                    except Exception:
                        pass

                if not mask.any():
                    # no matching rows in this file -> skip adding this file
                    continue

                filtered = sdf[mask].copy()
                if filtered.empty:
                    continue

                # parse/normalize timestamp column if available
                if tcol and tcol in filtered.columns:
                    try:
                        filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
                    except Exception:
                        pass
                else:
                    # attempt common fallback column names to produce a timestamp
                    for cand in ('MessageUTC', 'MessageTime', 'Timestamp', 'timestamp', 'Date'):
                        if cand in filtered.columns:
                            try:
                                filtered['LocaleMessageTime'] = pd.to_datetime(filtered[cand], errors='coerce')
                                tcol = 'LocaleMessageTime'
                                break
                            except Exception:
                                pass

                # sort by timestamp for consistent timeline order
                if tcol and tcol in filtered.columns:
                    try:
                        filtered = filtered.sort_values(by=tcol)
                    except Exception:
                        pass

                    # --- compute swipe gaps (preserve previous logic) ---
                    try:
                        filtered['_prev_ts'] = filtered[tcol].shift(1)
                        filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
                        # reset gap at day boundary or when previous is NaT
                        try:
                            cur_dates = filtered[tcol].dt.date
                            prev_dates = cur_dates.shift(1)
                            day_boundary_mask = (prev_dates != cur_dates) | (filtered['_prev_ts'].isna())
                            filtered.loc[day_boundary_mask, '_swipe_gap_seconds'] = 0.0
                        except Exception:
                            pass
                    except Exception:
                        filtered['_swipe_gap_seconds'] = 0.0
                else:
                    # no timestamp column -> defaults
                    filtered['_swipe_gap_seconds'] = 0.0

                # For each matching swipe row, build the slim evidence record expected by frontend
                added_any = False
                for _, r in filtered.iterrows():
                    # timestamp conversions
                    ts_val = None
                    if tcol and tcol in filtered.columns:
                        ts_val = r.get(tcol)
                    else:
                        # fallback: try Date column
                        if 'Date' in filtered.columns:
                            ts_val = r.get('Date')
                    locale_iso, date_only, swipe_time = _format_time_fields(ts_val)

                    # EmployeeID: prefer emp_col, then Int1/Text12, then fallback to matched trends row
                    emp_val = None
                    try:
                        if emp_col and emp_col in filtered.columns:
                            emp_val = _to_python_scalar(r.get(emp_col))
                        else:
                            for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                                cl = cols_lower.get(cand.lower())
                                if cl and cl in filtered.columns:
                                    emp_val = _to_python_scalar(r.get(cl))
                                    if emp_val:
                                        break
                            if emp_val in (None, '', 'nan'):
                                emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)
                    except Exception:
                        emp_val = _to_python_scalar(matched.iloc[0].get('EmployeeID') if not matched.empty else None)

                    # ObjectName1 / EmployeeName (human name)
                    obj_name = None
                    try:
                        if name_col and name_col in filtered.columns:
                            obj_name = _to_python_scalar(r.get(name_col))
                        elif 'ObjectName1' in filtered.columns:
                            obj_name = _to_python_scalar(r.get('ObjectName1'))
                        elif 'EmployeeName' in filtered.columns:
                            obj_name = _to_python_scalar(r.get('EmployeeName'))
                        else:
                            obj_name = _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else None)
                    except Exception:
                        obj_name = _to_python_scalar(matched.iloc[0].get('EmployeeName') if not matched.empty else None)

                    # PersonnelType
                    personnel_val = _to_python_scalar(r.get(personnel_col)) if (personnel_col and personnel_col in filtered.columns) else None
                    # Location / Partition
                    location_val = _to_python_scalar(r.get(location_col)) if (location_col and location_col in filtered.columns) else _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None

                    # CardNumber
                    card_val = None
                    try:
                        if card_col and card_col in filtered.columns:
                            card_val = _to_python_scalar(r.get(card_col))
                        else:
                            for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                                cl = cols_lower.get(cand.lower())
                                if cl and cl in filtered.columns:
                                    card_val = _to_python_scalar(r.get(cl))
                                    if card_val not in (None, '', 'nan'):
                                        break
                            if card_val in (None, '', 'nan'):
                                card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                    except Exception:
                        card_val = _to_python_scalar(matched.iloc[0].get('CardNumber') if not matched.empty else None)
                    if card_val is not None:
                        try:
                            card_val = str(card_val).strip()
                        except Exception:
                            pass

                    # AdmitCode / Note
                    admit_val = _to_python_scalar(r.get(admit_col)) if (admit_col and admit_col in filtered.columns) else None
                    # some logs store admit/rejection text in 'Note' or 'Rejection_Type'
                    if not admit_val:
                        for cand in ('Admit','AdmitCode','Admit_Type','Rejection_Type','Note','NoteType','Source'):
                            cl = cols_lower.get(cand.lower())
                            if cl and cl in filtered.columns:
                                admit_val = _to_python_scalar(r.get(cl))
                                if admit_val:
                                    break

                    # Direction & Door
                    direction_val = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in filtered.columns else None
                    door_val = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else _to_python_scalar(r.get('Door')) if 'Door' in filtered.columns else None

                    # Zone: prefer precomputed _zone, else map using map_door_to_zone if available
                    zone_val = None
                    try:
                        if '_zone' in r and r.get('_zone') not in (None, '', 'nan'):
                            zone_val = _to_python_scalar(r.get('_zone'))
                        else:
                            if 'map_door_to_zone' in globals():
                                try:
                                    zone_val = map_door_to_zone(door_val, direction_val)
                                except Exception:
                                    zone_val = None
                    except Exception:
                        zone_val = None

                    # Swipe gap
                    try:
                        swipe_gap_seconds = float(r.get('_swipe_gap_seconds') or 0.0)
                    except Exception:
                        swipe_gap_seconds = 0.0
                    swipe_gap_str = format_seconds_to_hms(swipe_gap_seconds)

                    # build output row: include EmployeeName (frontend expects this), plus legacy keys
                    row_out = {
                        "EmployeeName": obj_name,
                        "ObjectName1": obj_name,
                        "EmployeeID": emp_val,
                        "CardNumber": card_val,
                        "Card": card_val,
                        "LocaleMessageTime": locale_iso,
                        "DateOnly": date_only,
                        "Date": date_only,
                        "Time": swipe_time,
                        "Swipe_Time": swipe_time,
                        "SwipeGapSeconds": swipe_gap_seconds,
                        "SwipeGap": swipe_gap_str,
                        "Door": door_val,
                        "Direction": direction_val,
                        "Zone": zone_val,
                        "Note": admit_val,
                        "PersonnelType": personnel_val,
                        "Location": location_val,
                        "PartitionName2": _to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None,
                        "_source_file": fp.name
                    }

                    # Attempt to attach an email for the evidence row too
                    try:
                        if not row_out.get('EmployeeEmail') and row_out.get('EmployeeID'):
                            pinfo = {}
                            try:
                                pinfo = get_personnel_info(row_out.get('EmployeeID')) or {}
                            except Exception:
                                pinfo = {}
                            if pinfo:
                                if pinfo.get('EmployeeEmail') and not row_out.get('EmployeeEmail'):
                                    row_out['EmployeeEmail'] = pinfo.get('EmployeeEmail')
                                elif pinfo.get('EmailAddress') and not row_out.get('EmployeeEmail'):
                                    row_out['EmployeeEmail'] = pinfo.get('EmailAddress')
                                elif pinfo.get('Email') and not row_out.get('EmployeeEmail'):
                                    row_out['EmployeeEmail'] = pinfo.get('Email')
                    except Exception:
                        pass

                    added = _append_row_for_evidence(row_out, fp.name)
                    if added:
                        added_any = True

                # only add file name to available evidence list if we actually added rows from it
                if added_any:
                    raw_files_set.add(fp.name)

        raw_swipe_files = sorted(list(raw_files_set))

        # --- ENRICH: attach meta and image_url for frontend convenience ---
        meta = {}
        image_url = None
        try:
            # pick a candidate identifier for personnel lookup:
            # prefer explicit query token q_str, else fallback to first matched aggregated row
            candidate = None
            if q_str:
                candidate = q_str
            elif cleaned_matched and len(cleaned_matched) > 0:
                first = cleaned_matched[0]
                candidate = first.get('EmployeeID') or first.get('person_uid') or first.get('ObjectID') or first.get('GUID') or None

            if candidate:
                try:
                    pinfo = {}
                    if 'get_personnel_info' in globals() and callable(globals().get('get_personnel_info')):
                        pinfo = get_personnel_info(candidate) or {}
                    # populate meta keys commonly used by frontend
                    if pinfo:
                        meta['email'] = pinfo.get('EmployeeEmail') or pinfo.get('EmailAddress') or pinfo.get('Email') or None
                        meta['objectid'] = pinfo.get('ObjectID') or None
                        meta['guid'] = pinfo.get('GUID') or None
                    # convenient image_url: prefer ObjectID -> GUID -> candidate id fallback
                    if meta.get('objectid'):
                        image_url = f"/employee/{meta['objectid']}/image"
                    elif meta.get('guid'):
                        image_url = f"/employee/{meta['guid']}/image"
                    else:
                        # avoid setting GUID-like identifiers as EmployeeID image path
                        cand_s = str(candidate).strip()
                        if cand_s:
                            image_url = f"/employee/{cand_s}/image"
                except Exception:
                    # swallow personnel lookup errors (frontend will still try fallback)
                    logging.debug("record: personnel enrichment failed for candidate=%s", candidate)
        except Exception:
            pass

        # Prepare response with enrichment fields
        resp_payload = {
            "aggregated_rows": cleaned_matched,
            "raw_swipe_files": raw_swipe_files,
            "raw_swipes": raw_swipes_out,
            "meta": meta or None,
            "image_url": image_url
        }
        return jsonify(resp_payload), 200


    except Exception as e:
        # Close the try-block above with a proper except handler to avoid SyntaxError
        logging.exception("Unhandled exception in /record endpoint")
        return jsonify({"error": "internal server error in /record", "details": str(e)}), 500


@app.route('/record/export', methods=['GET'])
def export_record_excel():
    q = request.args.get('employee_id') or request.args.get('person_uid')
    date_str = request.args.get('date')
    city_param = request.args.get('city') or request.args.get('site') or 'pune'
    city_slug = _slug_city(city_param)

    if not q:
        return jsonify({"error":"employee_id or person_uid is required"}), 400

    q_str = str(q).strip()

    # Helper: load trend CSV(s) and build set of flagged (id, date) tuples
    def _load_flagged_map(target_date=None):
        flagged_pairs = set()
        try:
            p = Path(DEFAULT_OUTDIR)
            candidates = []
            if target_date:
                # try city/date specific trend file first
                ymd = target_date.strftime('%Y%m%d')
                f = p / f"trend_{city_slug}_{ymd}.csv"
                if f.exists():
                    candidates = [f]
            if not candidates:
                # fallback to any trend_*.csv in outputs
                candidates = sorted(p.glob("trend_*.csv"), reverse=True)
            for fp in candidates:
                try:
                    tdf = pd.read_csv(fp)
                except Exception:
                    try:
                        tdf = pd.read_csv(fp, dtype=str)
                    except Exception:
                        continue
                if tdf is None or tdf.empty:
                    continue
                tdf = _replace_placeholder_strings(tdf)
                # ensure Date is normalized to iso date string
                if 'Date' in tdf.columns:
                    try:
                        tdf['Date'] = pd.to_datetime(tdf['Date'], errors='coerce').dt.date
                        tdf['DateISO'] = tdf['Date'].apply(lambda d: d.isoformat() if pd.notna(d) else None)
                    except Exception:
                        tdf['DateISO'] = tdf['Date'].astype(str)
                else:
                    tdf['DateISO'] = None
                # find flagged rows (IsFlagged True or AnomalyScore >= threshold)
                if 'IsFlagged' in tdf.columns:
                    sel = tdf[tdf['IsFlagged'] == True]
                else:
                    # fallback: consider AnomalyScore >= ANOMALY_THRESHOLD as flagged
                    if 'AnomalyScore' in tdf.columns:
                        try:
                            sel = tdf[pd.to_numeric(tdf['AnomalyScore'], errors='coerce').fillna(0) >= ANOMALY_THRESHOLD]
                        except Exception:
                            sel = pd.DataFrame()
                    else:
                        sel = pd.DataFrame()

                if sel is None or sel.empty:
                    continue

                for _, rr in sel.iterrows():
                    date_iso = None
                    try:
                        date_iso = rr.get('DateISO') or (pd.to_datetime(rr.get('Date'), errors='coerce').date().isoformat() if rr.get('Date') is not None else None)
                    except Exception:
                        date_iso = None
                    # collect EmployeeID and person_uid if present
                    for idcol in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'Int1', 'Text12', 'CardNumber'):
                        try:
                            if idcol in rr and rr.get(idcol) not in (None, '', float('nan')):
                                val = str(rr.get(idcol)).strip()
                                if val:
                                    flagged_pairs.add((idcol, val, date_iso))
                                    # also add date-agnostic tuple for easy contains checks
                                    flagged_pairs.add((idcol, val, None))
                        except Exception:
                            continue
            return flagged_pairs
        except Exception:
            return set()

    # parse date param for targeted checking (optional)
    target_date_obj = None
    if date_str:
        try:
            target_date_obj = pd.to_datetime(date_str).date()
        except Exception:
            return jsonify({"error":"invalid date format, expected YYYY-MM-DD"}), 400

    flagged_map = _load_flagged_map(target_date_obj)

    # Quick check: ensure the requested employee/q is flagged for the requested date (or flagged at all)
    def _is_q_flagged(qtoken, date_iso=None):
        if not qtoken:
            return False
        # check across multiple id columns recorded in trend files
        for idcol in ('EmployeeID', 'person_uid', 'EmployeeIdentity', 'Int1', 'Text12', 'CardNumber'):
            if (idcol, qtoken, date_iso) in flagged_map or (idcol, qtoken, None) in flagged_map:
                return True
        return False

    # if date provided require flagged on that date; otherwise accept flagged any-date
    q_flagged = _is_q_flagged(q_str, target_date_obj.isoformat() if target_date_obj else None)
    if not q_flagged:
        # if not flagged with exact id, try numeric-normalized attempts (strip trailing .0 etc)
        try:
            if '.' in q_str:
                fq = None
                try:
                    f = float(q_str)
                    if math.isfinite(f) and f.is_integer():
                        fq = str(int(f))
                except Exception:
                    fq = None
                if fq and _is_q_flagged(fq, target_date_obj.isoformat() if target_date_obj else None):
                    q_flagged = True
        except Exception:
            pass

    if not q_flagged:
        return jsonify({"error": "employee not flagged (no evidence rows for requested employee/date)"}), 404

    # find swipe files to scan
    p = Path(DEFAULT_OUTDIR)
    files_to_scan = []
    if target_date_obj:
        files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=target_date_obj, city_slug=city_slug, include_shifted=False if city_slug == 'pune' else True)
    else:
        files_to_scan = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=city_slug)
    if not files_to_scan:
        avail = _find_swipe_files(DEFAULT_OUTDIR, date_obj=None, city_slug=None, include_shifted=False)
        avail_names = [f.name for f in avail] if avail else []
        logging.info("export_record_excel: no files matched for date=%s city=%s; available swipe files=%s", date_str, city_slug, avail_names)
        return jsonify({"error": "no raw swipe files found for requested date / outputs", "available_swipe_files": avail_names}), 404

    all_rows = []
    for fp in files_to_scan:
        try:
            raw_df = pd.read_csv(fp, dtype=str, parse_dates=['LocaleMessageTime'])
        except Exception:
            try:
                raw_df = pd.read_csv(fp, dtype=str)
            except Exception:
                continue

        raw_df = _replace_placeholder_strings(raw_df)
        cols_lower = {c.lower(): c for c in raw_df.columns}
        tcol = cols_lower.get('localemessagetime') or cols_lower.get('messagetime') or cols_lower.get('timestamp') or cols_lower.get('time') or None
        emp_col = cols_lower.get('int1') or cols_lower.get('employeeid') or cols_lower.get('employeeidentity') or cols_lower.get('employee_id') or None
        name_col = cols_lower.get('employeename') or cols_lower.get('objectname1') or cols_lower.get('employee_name') or None
        card_col = cols_lower.get('cardnumber') or cols_lower.get('card') or cols_lower.get('chuid') or cols_lower.get('value') or None
        door_col = cols_lower.get('door') or cols_lower.get('doorname') or cols_lower.get('door_name') or None
        dir_col = cols_lower.get('direction') or cols_lower.get('directionname') or cols_lower.get('direction_name') or None
        admit_cols = [c for c in ('admitcode','admit','admit_code','admit_type','admitstatus') if c in cols_lower]
        admit_col = cols_lower.get(admit_cols[0]) if admit_cols else None
        personnel_col = cols_lower.get('personneltype') or cols_lower.get('personneltypename') or None
        location_col = cols_lower.get('partitionname2') or cols_lower.get('location') or cols_lower.get('partitionname') or None
        person_uid_col = cols_lower.get('person_uid')

        mask = pd.Series(False, index=raw_df.index)
        if person_uid_col and person_uid_col in raw_df.columns:
            mask = mask | (raw_df[person_uid_col].astype(str).str.strip() == q_str)
        if emp_col and emp_col in raw_df.columns:
            mask = mask | (raw_df[emp_col].astype(str).str.strip() == q_str)
        if not mask.any() and emp_col and emp_col in raw_df.columns:
            try:
                q_numeric = float(q_str)
                emp_numeric = pd.to_numeric(raw_df[emp_col], errors='coerce')
                mask = mask | (emp_numeric == q_numeric)
            except Exception:
                pass
        if not mask.any() and name_col and name_col in raw_df.columns:
            try:
                mask = mask | (raw_df[name_col].astype(str).str.strip().str.lower() == q_str.lower())
            except Exception:
                pass

        if not mask.any():
            continue

        filtered = raw_df[mask].copy()
        if filtered.empty:
            continue

        if tcol and tcol in filtered.columns:
            try:
                filtered[tcol] = pd.to_datetime(filtered[tcol], errors='coerce')
            except Exception:
                pass

        if tcol and tcol in filtered.columns:
            filtered = filtered.sort_values(by=tcol)
            filtered['_prev_ts'] = filtered[tcol].shift(1)
            try:
                filtered['_swipe_gap_seconds'] = (filtered[tcol] - filtered['_prev_ts']).dt.total_seconds().fillna(0).astype(float)
            except Exception:
                filtered['_swipe_gap_seconds'] = 0.0
        else:
            filtered['_swipe_gap_seconds'] = 0.0

        try:
            if door_col and door_col in filtered.columns:
                if dir_col and dir_col in filtered.columns:
                    filtered['_zone'] = filtered.apply(lambda rr: map_door_to_zone(rr.get(door_col), rr.get(dir_col)), axis=1)
                else:
                    filtered['_zone'] = filtered[door_col].apply(lambda dv: map_door_to_zone(dv, None))
            else:
                filtered['_zone'] = filtered.get('PartitionName2', None)
        except Exception:
            filtered['_zone'] = None

        for _, r in filtered.iterrows():
            # compute timestamp variants
            ts_val = None
            if tcol and tcol in filtered.columns:
                ts_val = r.get(tcol)
            else:
                if 'Date' in filtered.columns:
                    ts_val = r.get('Date')
            try:
                dt = pd.to_datetime(ts_val)
                locale_iso = dt.isoformat() if pd.notna(dt) else None
            except Exception:
                locale_iso = str(ts_val) if ts_val is not None else None
            try:
                date_only = dt.strftime("%d-%b-%y") if pd.notna(dt) else None
            except Exception:
                date_only = None
            try:
                swipe_time = dt.strftime("%I:%M:%S %p").lstrip("0") if pd.notna(dt) else None
            except Exception:
                swipe_time = None

            # EmployeeID resolution
            emp_val = None
            try:
                if emp_col and emp_col in filtered.columns:
                    emp_val = _to_python_scalar(r.get(emp_col))
                else:
                    for cand in ('Int1','Text12','EmployeeID','EmployeeIdentity','empid','id'):
                        cl = cols_lower.get(cand.lower())
                        if cl and cl in filtered.columns:
                            emp_val = _to_python_scalar(r.get(cl))
                            if emp_val:
                                break
            except Exception:
                emp_val = None
            if emp_val is not None:
                try:
                    s = str(emp_val).strip()
                    if '.' in s:
                        try:
                            f = float(s)
                            if math.isfinite(f) and f.is_integer():
                                s = str(int(f))
                        except Exception:
                            pass
                    emp_val = s
                except Exception:
                    pass

            # ObjectName1 / EmployeeName
            obj_name = None
            try:
                if name_col and name_col in filtered.columns:
                    obj_name = _to_python_scalar(r.get(name_col))
                elif 'ObjectName1' in filtered.columns:
                    obj_name = _to_python_scalar(r.get('ObjectName1'))
                elif 'EmployeeName' in filtered.columns:
                    obj_name = _to_python_scalar(r.get('EmployeeName'))
            except Exception:
                obj_name = None

            personnel_val = _to_python_scalar(r.get(personnel_col)) if (personnel_col and personnel_col in filtered.columns) else None
            location_val = _to_python_scalar(r.get(location_col)) if (location_col and location_col in filtered.columns) else (_to_python_scalar(r.get('PartitionName2')) if 'PartitionName2' in filtered.columns else None)

            # Card Number
            card_val = None
            try:
                if card_col and card_col in filtered.columns:
                    card_val = _to_python_scalar(r.get(card_col))
                else:
                    for cand in ('CardNumber','CHUID','Card','card_no','cardnum','value','xmlmessage'):
                        cl = cols_lower.get(cand.lower())
                        if cl and cl in filtered.columns:
                            card_val = _to_python_scalar(r.get(cl))
                            if card_val not in (None, '', 'nan'):
                                break
                if card_val is not None:
                    card_val = str(card_val).strip()
            except Exception:
                card_val = None

            admit_val = _to_python_scalar(r.get(admit_col)) if (admit_col and admit_col in filtered.columns) else None
            if not admit_val:
                for cand in ('Admit','AdmitCode','Admit_Type','Rejection_Type','Note','NoteType','Source'):
                    cl = cols_lower.get(cand.lower())
                    if cl and cl in filtered.columns:
                        admit_val = _to_python_scalar(r.get(cl))
                        if admit_val:
                            break

            direction_val = _to_python_scalar(r.get(dir_col)) if (dir_col and dir_col in filtered.columns) else _to_python_scalar(r.get('Direction')) if 'Direction' in filtered.columns else None
            door_val = _to_python_scalar(r.get(door_col)) if (door_col and door_col in filtered.columns) else _to_python_scalar(r.get('Door')) if 'Door' in filtered.columns else None

            row_out = {
                "LocaleMessageTime": locale_iso,
                "DateOnly": date_only,
                "Swipe_Time": swipe_time,
                "EmployeeID": emp_val,
                "ObjectName1": obj_name,
                "PersonnelType": personnel_val,
                "Location": location_val,
                "CardNumber": card_val,
                "AdmitCode": admit_val,
                "Direction": direction_val,
                "Door": door_val,
                "_source_file": fp.name
            }

            all_rows.append(row_out)

    if not all_rows:
        return jsonify({"error":"no swipe rows matched the requested employee/date"}), 404

    df_out = pd.DataFrame(all_rows)

    # Further restrict to only rows that match a flagged trend row on the same Date & EmployeeID/person_uid
    # Build flagged lookup using trend CSV(s) again but now keyed by EmployeeID/Date
    flagged_dates = set()
    try:
        p = Path(DEFAULT_OUTDIR)
        # load relevant trend csv(s)
        trend_candidates = []
        if target_date_obj:
            fp_try = p / f"trend_{city_slug}_{target_date_obj.strftime('%Y%m%d')}.csv"
            if fp_try.exists():
                trend_candidates = [fp_try]
        if not trend_candidates:
            trend_candidates = sorted(p.glob("trend_*.csv"), reverse=True)
        for tf in trend_candidates:
            try:
                tdf = pd.read_csv(tf)
            except Exception:
                try:
                    tdf = pd.read_csv(tf, dtype=str)
                except Exception:
                    continue
            if tdf is None or tdf.empty:
                continue
            tdf = _replace_placeholder_strings(tdf)
            if 'IsFlagged' in tdf.columns:
                tsel = tdf[tdf['IsFlagged'] == True]
            else:
                if 'AnomalyScore' in tdf.columns:
                    try:
                        tsel = tdf[pd.to_numeric(tdf['AnomalyScore'], errors='coerce').fillna(0) >= ANOMALY_THRESHOLD]
                    except Exception:
                        tsel = pd.DataFrame()
                else:
                    tsel = pd.DataFrame()
            if tsel is None or tsel.empty:
                continue
            for _, tt in tsel.iterrows():
                try:
                    dval = None
                    if 'Date' in tt and pd.notna(tt.get('Date')):
                        try:
                            dval = pd.to_datetime(tt.get('Date')).date().isoformat()
                        except Exception:
                            dval = str(tt.get('Date'))
                    # collect candidate ids
                    for idcol in ('EmployeeID','person_uid','EmployeeIdentity','Int1','Text12','CardNumber'):
                        if idcol in tt and tt.get(idcol) not in (None, '', float('nan')):
                            try:
                                ival = str(tt.get(idcol)).strip()
                                if ival:
                                    flagged_dates.add((idcol, ival, dval))
                                    flagged_dates.add((idcol, ival, None))
                            except Exception:
                                continue
                except Exception:
                    continue
    except Exception:
        flagged_dates = set()

    # Keep rows where employee/date is present in flagged_dates
    def _row_is_flagged(r):
        emp = r.get('EmployeeID') or ''
        date_only = r.get('DateOnly')
        # try iso date from DateOnly (it is in DD-MMM-YY), so translate to iso for matching if possible
        date_iso = None
        try:
            if date_only:
                # parse using day-month-year short form
                date_iso = pd.to_datetime(date_only, format="%d-%b-%y", errors='coerce')
                if pd.notna(date_iso):
                    date_iso = date_iso.date().isoformat()
                else:
                    date_iso = None
        except Exception:
            date_iso = None
        # match by EmployeeID & date or EmployeeID with any date
        if emp and ((('EmployeeID', emp, date_iso) in flagged_dates) or (('EmployeeID', emp, None) in flagged_dates)):
            return True
        # also try matching by person_uid if employee string includes emp:/uid: patterns
        for idcol in ('person_uid', 'EmployeeIdentity', 'Int1', 'Text12', 'CardNumber'):
            if ((idcol, q_str, date_iso) in flagged_dates) or ((idcol, q_str, None) in flagged_dates):
                return True
        # fallback: if q matched and we earlier accepted q_flagged, accept rows for q
        try:
            if str(q_str) and (str(q_str) == str(emp) or str(q_str) == str(r.get('CardNumber'))):
                return True
        except Exception:
            pass
        return False

    df_filtered = df_out[df_out.apply(_row_is_flagged, axis=1)].copy()
    if df_filtered.empty:
        return jsonify({"error":"no flagged swipe rows found for the requested employee/date"}), 404

    # final column ordering and ensure only the columns requested
    final_cols = ["LocaleMessageTime","DateOnly","Swipe_Time","EmployeeID","ObjectName1","PersonnelType","Location","CardNumber","AdmitCode","Direction","Door"]
    # ensure all final_cols exist in df_filtered (create missing as None)
    for c in final_cols:
        if c not in df_filtered.columns:
            df_filtered[c] = None
    df_final = df_filtered[final_cols].copy()

    # write excel with single sheet "Evidence" (strict columns only)
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df_final.to_excel(writer, sheet_name='Evidence', index=False)
            writer.save()
            output.seek(0)
    except Exception as e:
        logging.exception("Failed to create Excel: %s", e)
        return jsonify({"error":"failed to create excel"}), 500

    if OPENPYXL_AVAILABLE:
        try:
            wb = load_workbook(output)
            thin = Side(border_style="thin", color="000000")
            thick = Side(border_style="medium", color="000000")
            for ws in wb.worksheets:
                header = ws[1]
                for cell in header:
                    cell.font = Font(bold=True)
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                    cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                        cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)
                for col in ws.columns:
                    max_len = 0
                    col_letter = col[0].column_letter
                    for cell in col:
                        try:
                            v = str(cell.value) if cell.value is not None else ""
                        except Exception:
                            v = ""
                        if len(v) > max_len:
                            max_len = len(v)
                    width = min(max(10, max_len + 2), 50)
                    ws.column_dimensions[col_letter].width = width
            out2 = io.BytesIO()
            wb.save(out2)
            out2.seek(0)
            return send_file(out2, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        except Exception:
            logging.exception("Excel styling failed, returning raw file")
            output.seek(0)
            return send_file(output, as_attachment=True,
                             download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    else:
        output.seek(0)
        return send_file(output, as_attachment=True,
                         download_name=f"evidence_{str(q).replace(' ','_')}_{date_str or 'all'}.xlsx",
                         mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")



@app.route('/swipes/<filename>', methods=['GET'])
def download_swipes(filename):
    fp = DEFAULT_OUTDIR / filename
    if not fp.exists():
        return jsonify({"error":"file not found"}), 404
    return send_from_directory(str(DEFAULT_OUTDIR), filename, as_attachment=True)


@app.route('/train', methods=['GET'])
def build_training_endpoint():
    end_date_str = request.args.get('end_date')
    months = int(request.args.get('months') or 3)
    min_unique = int(request.args.get('min_unique') or 1000)
    try:
        if end_date_str:
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
        else:
            end_date = datetime.now().date()
    except Exception as e:
        return jsonify({"error": f"invalid end_date: {e}"}), 400

    try:
        if build_monthly_training is None:
            raise RuntimeError("build_monthly_training not available")
        csv_path = build_monthly_training(end_date=end_date, months=months, min_unique_employees=min_unique, outdir=str(DEFAULT_OUTDIR))
        if csv_path is None:
            return jsonify({"error":"no training CSV produced (no data)"}), 500
        return jsonify({"training_csv": str(csv_path)})
    except Exception as e:
        logging.exception("build_monthly_training failed")
        return jsonify({"error": str(e)}), 500




# chatbot helpers (kept mostly as-is)
try:
    from trend_runner import _read_past_trend_csvs, _normalize_id_val, SCENARIO_EXPLANATIONS
except Exception:
    _read_past_trend_csvs = None
    _normalize_id_val = None
    SCENARIO_EXPLANATIONS = {}

def _load_latest_trend_df(outdir: Path, city: str = "pune"):
    city_slug = _slug_city(city)
    csvs = sorted(outdir.glob(f"trend_{city_slug}_*.csv"), reverse=True)
    if not csvs:
        csvs = sorted(outdir.glob("trend_*.csv"), reverse=True)
    if not csvs:
        return None, None
    latest = csvs[0]
    try:
        df = pd.read_csv(latest)
    except Exception:
        df = pd.read_csv(latest, dtype=str)
    df = _replace_placeholder_strings(df)
    return df, latest.name




def _find_person_rows(identifier: str, days: int | None = None, outdir: Path = DEFAULT_OUTDIR):
    """
    Find person rows in past trend CSVs. `days` is optional; if None the
    DEFAULT_VIOLATION_DAYS is used from configuration/env.
    """
    # resolve days to use
    try:
        days = int(days) if days is not None else int(DEFAULT_VIOLATION_DAYS)
    except Exception:
        days = int(DEFAULT_VIOLATION_DAYS)

    if _normalize_id_val:
        norm = _normalize_id_val(identifier)
    else:
        norm = str(identifier).strip()
        if '.' in norm:
            try:
                f = float(norm)
                if f.is_integer():
                    norm = str(int(f))
            except Exception:
                pass
    today = datetime.now().date()
    try:
        if _read_past_trend_csvs:
            past = _read_past_trend_csvs(str(outdir), days, today)

            
        else:
            files = sorted(Path(outdir).glob("trend_pune_*.csv"), reverse=True)
            dfs = []
            cutoff = today - timedelta(days=days)
            for fp in files:
                try:
                    tmp = pd.read_csv(fp, parse_dates=['Date'])
                    tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                    tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                    dfs.append(tmp)
                except Exception:
                    try:
                        tmp = pd.read_csv(fp, dtype=str)
                        if 'Date' in tmp.columns:
                            tmp['Date'] = pd.to_datetime(tmp['Date'], errors='coerce').dt.date
                            tmp = tmp[tmp['Date'].apply(lambda d: d is not None and d >= cutoff and d <= today)]
                            dfs.append(tmp)
                    except Exception:
                        continue
            past = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()


    except Exception:
        past = pd.DataFrame()

    if past is None or past.empty:
        return pd.DataFrame()

    past = _replace_placeholder_strings(past)
    match_mask = pd.Series(False, index=past.index)
    for col in ('EmployeeID','person_uid','EmployeeIdentity','CardNumber','Int1','Text12'):
        if col in past.columns:
            try:
                match_mask = match_mask | (past[col].astype(str).fillna('').str.strip() == str(norm).strip())
            except Exception:
                continue

    if not match_mask.any():
        try:
            qnum = float(norm)
            for col in ('EmployeeID','Int1'):
                if col in past.columns:
                    try:
                        numcol = pd.to_numeric(past[col], errors='coerce')
                        match_mask = match_mask | (numcol == qnum)
                    except Exception:
                        continue
        except Exception:
            pass

    if not match_mask.any() and 'EmployeeName' in past.columns:
        names = past['EmployeeName'].dropna().astype(str).unique().tolist()
        close = difflib.get_close_matches(str(identifier), names, n=5, cutoff=0.7)
        if close:
            match_mask = match_mask | past['EmployeeName'].astype(str).isin(close)

    return past[match_mask].copy()



def _explain_scenario_code(code):
    if not code:
        return None
    code = str(code).strip()
    if code in SCENARIO_EXPLANATIONS:
        try:
            fn = SCENARIO_EXPLANATIONS.get(code)
            try:
                txt = fn({})
                return txt
            except Exception:
                return code.replace("_", " ").replace(">= ", "≥ ")
        except Exception:
            return code.replace("_", " ").replace(">= ", "≥ ")
    return code.replace("_", " ").replace(">=", "≥")

def _map_score_to_label_fallback(score: float):
    try:
        s = float(score)
    except Exception:
        return (0.0, "Low")
    if s >= 0.75:
        return (s, "High")
    if s >= 0.4:
        return (s, "Medium")
    return (s, "Low")

@app.route('/chatbot/query', methods=['POST'])
def chatbot_query():
    payload = request.get_json(force=True)
    q = (payload.get('q') or '').strip()
    if not q:
        return jsonify({"error":"query text 'q' required"}), 400
    lang = payload.get('lang')
    q_l = q.lower().strip()

    if re.search(r"\bwho is (high|low) risk\b", q_l) or re.search(r"\b(high|low) risk (people|persons|people) (today)?\b", q_l):
        want = 'high' if 'high' in q_l else 'low' if 'low' in q_l else None
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer": "No trend data available.", "evidence": []})
        if 'RiskLevel' not in df.columns:
            if 'RiskScore' in df.columns:
                def _map_rs(s):
                    try:
                        if pd.isna(s):
                            return 'Low'
                    except Exception:
                        pass
                    try:
                        if 'map_score_to_label' in globals() and callable(globals().get('map_score_to_label')):
                            try:
                                return globals().get('map_score_to_label')(float(s))[1]
                            except Exception:
                                pass
                        return _map_score_to_label_fallback(float(s))[1]
                    except Exception:
                        return 'Low'
                df['RiskLevel'] = df['RiskScore'].apply(lambda s: _map_rs(s))
            else:
                df['RiskLevel'] = df.get('RiskLevel', 'Low')
        if want == 'high':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'high']
        elif want == 'low':
            sel = df[df['RiskLevel'].astype(str).str.lower() == 'low']
        else:
            sel = df
        names = sel['EmployeeName'].dropna().astype(str).unique().tolist()
        if not names:
            ans = f"No {want} risk persons found in the latest data." if want else "No persons found."
            return jsonify({"answer": ans, "evidence": []})
        else:
            ans = f"{want.capitalize()} risk persons today: " + ", ".join(names[:40])
            sample = _clean_sample_df(sel.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bexplain\s+([A-Za-z0-9_\-]+)\b.*", q_l)
    if m:
        code = m.group(1).strip()
        explanation = _explain_scenario_code(code)
        ans = f"Explanation for '{code}': {explanation}"
        return jsonify({"answer": ans, "evidence": []})

    if 'trend details' in q_l or 'top reasons' in q_l or 'trend details for today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'Reasons' in df.columns:
            reasons = {}
            for v in df['Reasons'].dropna().astype(str):
                for part in re.split(r'[;,\|]', v):
                    key = part.strip()
                    if key and not _is_placeholder_str(key):
                        reasons[key] = reasons.get(key, 0) + 1
            top = sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:10]
            if not top:
                return jsonify({"answer":"No reason counts available today.","evidence":[]})
            ans = "Top reasons today: " + ", ".join([f"{k} ({c})" for k,c in top])
            sample = []
            try:
                top_reasons = [k for k,_ in top]
                mask = df['Reasons'].astype(str).apply(lambda s: any(tr in s for tr in top_reasons))
                sample_df = df[mask].head(10)
                sample = _clean_sample_df(sample_df, max_rows=10)
            except Exception:
                sample = []
            return jsonify({"answer": ans, "evidence": sample})

    m = re.match(r".*\bshow (?:me )?([A-Za-z0-9\-\:\s]+?) (?:for )?(?:last )?(\d+)\s*days\b", q_l)
    if not m:
        m = re.match(r".*\b(show|display)\s+(?:me\s+)?([A-Za-z0-9\-\:\s]+?)\s+last\s+(\d+)\s*days\b", q_l)
    if m:
        if len(m.groups()) == 2:
            identifier, days = m.group(1).strip(), int(m.group(2))
        else:
            identifier = m.group(1).strip()
            days = int(m.group(2))
        rows = _find_person_rows(identifier, days=days, outdir=DEFAULT_OUTDIR)
        if rows is None or rows.empty:
            return jsonify({"answer": f"No records found for '{identifier}' in last {days} days.", "evidence": []})
        flagged = rows[rows.get('IsFlagged', False) == True] if 'IsFlagged' in rows.columns else pd.DataFrame()
        flagged_count = int(len(flagged))
        total_days = int(len(rows))
        latest_row = rows.sort_values('Date', ascending=False).iloc[0].to_dict()
        name = latest_row.get('EmployeeName') or latest_row.get('person_uid') or latest_row.get('EmployeeID')
        ans = f"Found {total_days} day(s) for {name} in the last {days} days. Flagged days: {flagged_count}."
        sample = _clean_sample_df(rows.sort_values('Date', ascending=False).head(10), max_rows=10)
        return jsonify({"answer": ans, "evidence": sample})

    if 'present today' in q_l or 'who is present today' in q_l:
        df, fname = _load_latest_trend_df(DEFAULT_OUTDIR)
        if df is None:
            return jsonify({"answer":"No trend data available.","evidence":[]})
        if 'PresentToday' in df.columns:
            present = df[df['PresentToday'] == True]
            names = present['EmployeeName'].dropna().unique().tolist()
            ans = f"Present today: {', '.join(names[:40]) if names else 'None'}"
            sample = _clean_sample_df(present.head(10), max_rows=10)
            return jsonify({"answer": ans, "evidence": sample})
        else:
            return jsonify({"answer":"PresentToday field not available in latest trends.","evidence":[]})

    
    #hint = "I can answer: 'Who is high risk today', 'Who is low risk today', 'Show me <EmployeeID|Name> last 90 days', 'Explain <scenario_code>', 'Trend details for today — top reasons'."
    hint = f"I can answer: 'Who is high risk today', 'Who is low risk today', 'Show me <EmployeeID|Name> last {DEFAULT_VIOLATION_DAYS} days', 'Explain <scenario_code>', 'Trend details for today — top reasons'."

    return jsonify({"answer": f"I can help with trend & risk questions. I recognized: {q}. Try: {hint}", "evidence":[]})


# run
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)
