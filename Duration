# backend/duration_report.py
from __future__ import annotations

import logging
import os
import re
import warnings
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

import pandas as pd
import numpy as np
import hashlib

try:
    import pyodbc
except Exception:
    pyodbc = None

# ODBC driver name 
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")

# Try to import shared door/zone helpers from config; fall back to a small local implementation if missing.
try:
    from config.door_zone import map_door_to_zone, BREAK_ZONES, OUT_OF_OFFICE_ZONE
except Exception:
    # fallback — keep behaviour if config file unavailable
    BREAK_ZONES = set(["East Outdoor Area", "West Outdoor Area", "Assembly Area"])
    OUT_OF_OFFICE_ZONE = "Out of office"

    def map_door_to_zone(door: object, direction: object = None) -> str:
        """
        Fallback: Map a raw Door string (and optionally Direction) to a logical zone.
        """
        try:
            if door is None:
                return None
            s = str(door).strip()
            if not s:
                return None
            s_l = s.lower()
            if direction and isinstance(direction, str):
                d = direction.strip().lower()
                if "out" in d:
                    return OUT_OF_OFFICE_ZONE
                if "in" in d:
                    return "Reception Area"
            if "out" in s_l or "exit" in s_l or ("turnstile" in s_l and "out" in s_l):
                return OUT_OF_OFFICE_ZONE
            return "Working Area"
        except Exception:
            return None

# REGION configuration - databases list used to build UNION queries
REGION_CONFIG = {
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUPNQ0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["APAC.Default", "SG.Singapore", "PH.Manila","IN.HYD"]
    },
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUFRA0986V",
        "databases": [
            "ACVSUJournal_00011029","ACVSUJournal_00011028","ACVSUJournal_00011027",
            "ACVSUJournal_00011026","ACVSUJournal_00011025","ACVSUJournal_00011024",
            "ACVSUJournal_00011023","ACVSUJournal_00011022","CVSUJournal_00011021"
        ],
        "partitions": ["LT.Vilnius","AUT.Vienna","MA.Casablanca","RU.Moscow","IT.Rome","UK.London","IE.DUblin",
                        "DU.Abu Dhab", "ES.Madrid"]
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUSJO0986V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["AR.Cordoba", "BR.Sao Paulo", "CR.Costa Rica Partition","PA.Panama City","PE.Lima", "MX.Mexico City"]
    },
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccuredb@2026",
        "server": "SRVWUDEN0891V",
        "databases": [
            "ACVSUJournal_00010030","ACVSUJournal_00010029","ACVSUJournal_00010028",
            "ACVSUJournal_00010027","ACVSUJournal_00010026","ACVSUJournal_00010025"
        ],
        "partitions": ["US.CO.OBS", "USA/Canada Default", "US.FL.Miami", "US.NYC"],
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    }
}

# logic in this file no longer depends on it for date assignment.
GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t3.[Name] IN ('Contractor','Terminated Contractor') THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    t2.[Int1] AS Int1,
    t2.[Text12] AS Text12,
    t_xml.XmlMessage AS XmlMessage,
    sc.value AS XmlShredValue,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      NULLIF(CAST(t2.[Int1] AS NVARCHAR),'0'),
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    DATEADD(HOUR, -2, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) AS AdjustedMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
LEFT JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
LEFT JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND t3.[Name] = 'Employee'
  {date_condition}
  {region_filter}
"""

# Helpers
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out




# GUID / placeholders helpers
_GUID_RE = re.compile(r'^[0-9A-Fa-f]{8}-(?:[0-9A-Fa-f]{4}-){3}[0-9A-Fa-f]{12}$')
_PLACEHOLDER_STRS = set(['', 'nan', 'na', 'n/a', '-', '—', '–', 'none', 'null'])



def _strip_person_uid_prefix(token: object) -> Optional[str]:
    """
    If token is like 'emp:123' or 'uid:GUID' or 'name:xxxxx' return the suffix;
    otherwise return plain stripped string.
    Returns None for empty/placeholder tokens.
    """
    if token is None:
        return None
    try:
        s = str(token).strip()
        if not s:
            return None
        # common canonical prefixes used by duration_report: emp:, uid:, name:
        if ':' in s:
            prefix, rest = s.split(':', 1)
            if prefix.lower() in ('emp', 'uid', 'name'):
                rest = rest.strip()
                if rest:
                    return rest
        return s
    except Exception:
        return None

# keep existing _strip_person_uid_prefix (unchanged) and expose a shorter alias
def _strip_person_uid_prefix(token: object) -> Optional[str]:
    """
    Existing behaviour: if token like 'emp:123' or 'uid:GUID' or 'name:xxxxx' return suffix;
    otherwise return plain stripped string. Returns None for empty/placeholder tokens.
    """
    if token is None:
        return None
    try:
        s = str(token).strip()
        if not s:
            return None
        # common canonical prefixes used by duration_report: emp:, uid:, name:
        if ':' in s:
            prefix, rest = s.split(':', 1)
            if prefix.lower() in ('emp', 'uid', 'name'):
                rest = rest.strip()
                if rest:
                    return rest
        return s
    except Exception:
        return None

# Alias for modules that expect `_strip_uid_prefix`
def _strip_uid_prefix(token: object) -> Optional[str]:
    """
    Backwards-compatible alias for _strip_person_uid_prefix.
    """
    return _strip_person_uid_prefix(token)



def _looks_like_guid(s: object) -> bool:
    try:
        if s is None:
            return False
        st = str(s).strip()
        if not st:
            return False
        return bool(_GUID_RE.match(st))
    except Exception:
        return False

def _is_placeholder_str(s: object) -> bool:
    try:
        if s is None:
            return True
        st = str(s).strip().lower()
        return st in _PLACEHOLDER_STRS
    except Exception:
        return False

def _pick_first_non_guid_value(series):
    for v in series:
        if v is None:
            continue
        try:
            s = str(v).strip()
        except Exception:
            continue
        if not s:
            continue
        if _is_placeholder_str(s):
            continue
        if _looks_like_guid(s):
            continue
        return s
    return None

def _canonical_person_uid_from_row(row):
    """
    Produce canonical person_uid in the form:
      - 'emp:<employeeid>' (if a sensible non-GUID EmployeeID present),
      - 'uid:<EmployeeIdentity>' (if present),
      - 'name:<sha1 10chars>' fallback when name present,
      - otherwise None.
    """
    empid = None
    for cand in ('EmployeeID', 'Int1', 'Text12'):
        if cand in row and row.get(cand) not in (None, '', float('nan')):
            empid = row.get(cand)
            break
    empident = row.get("EmployeeIdentity", None)
    name = row.get("EmployeeName", None)

    # normalize empid numeric floats -> ints
    if empid is not None:
        try:
            s = str(empid).strip()
            if '.' in s:
                f = float(s)
                if f.is_integer():
                    s = str(int(f))
            if s and not _looks_like_guid(s) and not _is_placeholder_str(s):
                return f"emp:{s}"
        except Exception:
            pass

    if empident not in (None, '', float('nan')):
        try:
            si = str(empident).strip()
            if si:
                return f"uid:{si}"
        except Exception:
            pass

    if name not in (None, '', float('nan')):
        try:
            sn = str(name).strip()
            if sn and not _looks_like_guid(sn) and not _is_placeholder_str(sn):
                h = hashlib.sha1(sn.lower().encode('utf8')).hexdigest()[:10]
                return f"name:{h}"
        except Exception:
            pass

    return None

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    if pyodbc is None:
        logging.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None
    try:
        conn_str = (
            f"DRIVER={{{ODBC_DRIVER}}};"
            f"SERVER={rc['server']};DATABASE=master;UID={rc['user']};PWD={rc['password']};"
            "TrustServerCertificate=Yes;"
        )
        return pyodbc.connect(conn_str, autocommit=True)
    except Exception:
        logging.exception("Failed to connect to master DB for server %s", rc.get("server"))
        return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logging.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logging.exception("Error checking existence for database %s", db)
        cursor.close()
        logging.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def build_region_query(region_key: str, target_date: date) -> str:
    rc = REGION_CONFIG[region_key]
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", [])
        parts_sql = ", ".join(f"'{p}'" for p in partitions)
        region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        partitions = rc.get("partitions", [])
        if partitions:
            parts_sql = ", ".join(f"'{p}'" for p in partitions)
            region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
        else:
            likes = rc.get("logical_like", [])
            if likes:
                like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
                region_filter = f"AND ({like_sql})"
            else:
                region_filter = ""
    else:
        region_filter = ""

    # NOTE: AdjustedMessageTime / 2AM boundary logic removed from date selection.
    date_condition = "AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'".format(date=date_str)

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")]

    valid_dbs = _filter_existing_databases(rc, candidates)

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter))

    if not union_parts:
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date_condition=date_condition, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# DB connection & fetch
def get_connection(region_key: str):
    if pyodbc is None:
        raise RuntimeError("pyodbc is not available. Install it with 'pip install pyodbc'.")

    rc = REGION_CONFIG[region_key]
    db = rc.get("databases", [rc.get("database")])[0]
    conn_str = (
        f"DRIVER={{{ODBC_DRIVER}}};"
        f"SERVER={rc['server']};DATABASE={db};UID={rc['user']};PWD={rc['password']};"
        "TrustServerCertificate=Yes;"
    )
    return pyodbc.connect(conn_str, autocommit=True)

def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    sql = build_region_query(region_key, target_date)
    logging.info("Built SQL for region %s, date %s", region_key, target_date)
    cols = [
        "EmployeeName", "Door", "EmployeeID", "Int1", "Text12", "XmlMessage", "XmlShredValue", "CardNumber",
        "PersonnelTypeName", "EmployeeIdentity", "PartitionName2", "LocaleMessageTime", "AdjustedMessageTime", "MessageType",
        "Direction", "CompanyName", "PrimaryLocation"
    ]

    if pyodbc is None:
        logging.warning("pyodbc not available - returning empty DataFrame skeleton for region %s", region_key)
        return pd.DataFrame(columns=cols)

    conn = None
    try:
        conn = get_connection(region_key)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
    except Exception:
        logging.exception("Failed to run query for region %s", region_key)
        df = pd.DataFrame(columns=cols)
    finally:
        try:
            if conn is not None:
                conn.close()
        except Exception:
            pass

    # ensure expected columns exist
    for c in cols:
        if c not in df.columns:
            df[c] = None

    # Dates parsing
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # Keep AdjustedMessageTime if present for debugging but we do NOT use it for date boundaries anymore.
    try:
        if "AdjustedMessageTime" in df.columns:
            df["AdjustedMessageTime"] = pd.to_datetime(df["AdjustedMessageTime"], errors="coerce")
        else:
            df["AdjustedMessageTime"] = pd.NaT
    except Exception:
        df["AdjustedMessageTime"] = pd.NaT

    # defensive: make text fields strings (avoid object type surprises)
    for tcol in ("Door", "PartitionName2", "PersonnelTypeName", "EmployeeName", "CompanyName", "PrimaryLocation"):
        if tcol in df.columns:
            df[tcol] = df[tcol].fillna("").astype(str)

    # Filter: only Employees (defensive; the SQL template already requests t3.Name = 'Employee')
    try:
        if "PersonnelTypeName" in df.columns:
            df = df[df["PersonnelTypeName"].str.strip().str.lower() == "employee"].copy()
    except Exception:
        logging.debug("Could not apply PersonnelTypeName filter for region %s", region_key)

    # canonical person_uid (consistent with trend_runner)
    def make_person_uid(row):
        try:
            return _canonical_person_uid_from_row(row)
        except Exception:
            # fallback: try simple concatenation if canonical fails
            try:
                eid = row.get("EmployeeIdentity")
                if pd.notna(eid) and str(eid).strip() != "":
                    return str(eid).strip()
            except Exception:
                pass
            parts = []
            for c in ("EmployeeID", "CardNumber", "EmployeeName"):
                try:
                    v = row.get(c)
                    if v not in (None, '', float('nan')):
                        parts.append(str(v).strip())
                except Exception:
                    continue
            return "|".join(parts) if parts else None

    if not df.empty:
        df['person_uid'] = df.apply(make_person_uid, axis=1)

    # APAC partition normalization (more robust)
    if region_key == "apac" and not df.empty:
        def normalize_apac_partition(row):
            """
            Minimal / strict APAC partition mapping (only adjust Taguig / Quezon / KL cases).
            - APAC_PI_*  -> Taguig City
            - APAC_PH_*  -> Quezon City
            - APAC_MY_KL* or APAC_MY + KL -> MY.Kuala Lumpur
            If none match, fall back to previously-known tokens or keep original PartitionName2.
            """
            door = str(row.get("Door") or "") or ""
            part = str(row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()

            # 1) Strict: APAC_PI => Taguig City
            # Matches examples like: "APAC_PI_Manila_DR_MainEntrance"
            if re.search(r'\bAPAC[_\-]?PI\b', d) or re.search(r'\bAPAC[_\-]?PI[_\-]', d):
                return "Taguig City"

            # 2) Strict: APAC_PH => Quezon City
            # Matches examples like: "APAC_PH_Manila_7th Floor_Open Office Door 2-721"
            if re.search(r'\bAPAC[_\-]?PH\b', d) or re.search(r'\bAPAC[_\-]?PH[_\-]', d):
                return "Quezon City"

            # 3) Strict: Kuala Lumpur patterns
            # Accept variants like:
            #   "APAC_MY_KL_MAIN ENTRANCE DOOR", "APAC_MY_KL", "APAC_MY KL MAIN", "APAC_MY_KUALA-LUMPUR"
            # We require APAC + MY + (KL or KUALA) tokens in the door/partition string.
            # if ("APAC" in d) and ("MY" in d) and (
            #     "KL" in d or
            #     "KUALA" in d or
            #     re.search(r'KUALA[^A-Z0-9]*LUMPUR', d)
            # ):
            #     return "MY.Kuala Lumpur"


            # 3) Strict: Kuala Lumpur patterns (robust to underscores, dashes, spaces, extra text)
            # Matches: APAC_MY_KL_MAIN ENTRANCE DOOR, APAC-MY-KL, APAC MY KUALA-LUMPUR, etc.
            if re.search(r'APAC[^A-Z0-9]*MY[^A-Z0-9]*(?:KL\b|KUALA\b|KUALA[^A-Z0-9]*LUMPUR)', d):
                return "MY.Kuala Lumpur"



            # --- Minimal remaining token map (kept small & safe) ---
            token_map = {
                "APAC_IN_PUN": "Pune",
                "APAC_PUN": "Pune",
                "VIS_PUN": "Pune",
                "VIS_PUN_177": "Pune",
                "PUN": "Pune",
                "APAC_IN_HYD": "IN.HYD",
                "APAC_HYD": "IN.HYD",
                "HYD": "IN.HYD",
                "IN.HYD": "IN.HYD",
                "SG": "SG.Singapore",
                "SINGAPORE": "SG.Singapore",
                
            }

            # Prefer explicit tokens already present in PartitionName2
            for key, canonical in token_map.items():
                if key in p:
                    return canonical

            # Check door/partition tokens for any remaining known tokens
            toks = [t for t in re.split(r'[^A-Z0-9]+', d + " " + p) if t]
            for t in toks:
                if t in token_map:
                    return token_map[t]

            # If PartitionName2 already has a meaningful value, keep it
            if p and p.strip():
                return part

            # Unknown -> return empty string (caller applies strict masking)
            return ""

            # helper: split door/partition into alphanumeric tokens (keeps mixed tokens like VIS_PUN)
            def make_tokens(s: str):
                toks = [t for t in re.split(r'[^A-Z0-9\-]+', s or "") if t]
                return toks

            # 1) If PartitionName2 already contains a known canonical token, prefer it.
            for key, canonical in token_map.items():
                if key in p:
                    return canonical

            # 2) Tokenize Door and PartitionName2 and check tokens (exact token match)
            door_tokens = make_tokens(d)
            part_tokens = make_tokens(p)
            all_tokens = door_tokens + part_tokens
            for t in all_tokens:
                if t in token_map:
                    return token_map[t]

            # 3) Substring / regex fallbacks -- more permissive
            # Taguig-specific patterns
            if re.search(r'\bTAGUIG\b', d) or re.search(r'\bTAGUIG\b', p):
                return "Taguig City"
            # Quezon / Manila
            if re.search(r'\bQUEZON\b', d) or re.search(r'\bQUEZON\b', p) or re.search(r'\bMANILA\b', d) or re.search(r'\bMANILA\b', p):
                return "Quezon City"
            # Pune / PUN
            if re.search(r'\bPUN(E)?\b', d) or re.search(r'\bPUN(E)?\b', p):
                return "Pune"
            # Hyderabad
            if re.search(r'\bHYD\b', d) or re.search(r'\bHYD\b', p):
                return "IN.HYD"
            # Singapore
            if re.search(r'\bSINGAPORE\b', d) or re.search(r'\bSG\b', d) or re.search(r'\bSINGAPORE\b', p):
                return "SG.Singapore"
            # Kuala Lumpur / MY
            if re.search(r'\bKUALA\b', d) or re.search(r'\bKUALA\b', p) or re.search(r'\bMY\b', d) or re.search(r'\bMY\b', p) or re.search(r'KUALA.?LUMPUR', d):
                return "MY.Kuala Lumpur"

            # 4) If PartitionName2 is a non-empty meaningful value, keep it (no change)
            if p and p.strip():
                return part

            # 5) Unknown: return empty string (caller can filter strictly if needed)
            return ""






        # apply mapping and log the mapping summary for debugging
        df["PartitionName2"] = df.apply(normalize_apac_partition, axis=1)
        try:
            vc = df["PartitionName2"].value_counts(dropna=False).to_dict()
            logging.info("APAC PartitionName2 mapping counts example: %s", {k: vc.get(k, 0) for k in list(vc)[:10]})
        except Exception:
            logging.debug("APAC partition mapping counts unavailable")


    # NAMER: normalize PartitionName2 and add LogicalLocation per previous behaviour
    if region_key == "namer" and not df.empty:
        def namer_partition_and_logical(row):
            door = (row.get("Door") or "") or ""
            part = (row.get("PartitionName2") or "") or ""
            d = door.upper()
            p = part.upper()
            normalized = part
            logical = "Other"

            if ("US.CO.HQ" in d) or ("HQ" in d and "HQ" in d[:20]) or ("DENVER" in d) or (p == "US.CO.OBS"):
                normalized = "US.CO.OBS"
                logical = "Denver-HQ"
            elif "AUSTIN" in d or "AUSTIN TX" in d or p == "USA/CANADA DEFAULT":
                normalized = "USA/Canada Default"
                logical = "Austin Texas"
            elif "MIAMI" in d or p == "US.FL.MIAMI":
                normalized = "US.FL.Miami"
                logical = "Miami"
            elif "NYC" in d or "NEW YORK" in d or p == "US.NYC":
                normalized = "US.NYC"
                logical = "New York"
            else:
                if p == "US.CO.OBS":
                    normalized = "US.CO.OBS"; logical = "Denver-HQ"
                elif p == "USA/CANADA DEFAULT":
                    normalized = "USA/Canada Default"; logical = "Austin Texas"
                elif p == "US.FL.MIAMI":
                    normalized = "US.FL.Miami"; logical = "Miami"
                elif p == "US.NYC":
                    normalized = "US.NYC"; logical = "New York"
                else:
                    normalized = part
                    logical = "Other"
            return pd.Series({"PartitionName2": normalized, "LogicalLocation": logical})

        mapped = df.apply(namer_partition_and_logical, axis=1)
        df["PartitionName2"] = mapped["PartitionName2"].astype(str)
        df["LogicalLocation"] = mapped["LogicalLocation"].astype(str)

    # ensure PartitionName2 column exists as string
    if "PartitionName2" not in df.columns:
        df["PartitionName2"] = ""

    # ensure LogicalLocation exists
    if "LogicalLocation" not in df.columns:
        df["LogicalLocation"] = ""

    return df[cols + (['person_uid'] if 'person_uid' in df.columns else [])]

# ---------------------------------------------------------------------
# compute_daily_durations (single robust implementation)



def compute_daily_durations(swipes_df: pd.DataFrame) -> pd.DataFrame:
    """
    Robust, defensive implementation that:
     - accepts a swipes dataframe (may be empty)
     - ensures expected columns exist
     - deduplicates near-duplicate swipes (rounded to seconds)
     - assigns Date from LocaleMessageTime (strict local wall-time date)
     - groups by person_uid + Date and computes first/last, counts and durations
     - returns dataframe with stable output columns (same order as earlier code)
    """
    out_cols = [
        "person_uid", "EmployeeIdentity", "EmployeeID", "EmployeeName", "CardNumber",
        "Date", "FirstSwipe", "LastSwipe", "FirstDoor", "LastDoor", "CountSwipes",
        "DurationSeconds", "Duration", "DurationMinutes", "DurationDisplay", "DurationHMS",
        "PersonnelTypeName", "PartitionName2",
        "CompanyName", "PrimaryLocation", "FirstDirection", "LastDirection"
    ]

    # quick return for empty input
    if swipes_df is None or swipes_df.empty:
        return pd.DataFrame(columns=out_cols)

    # work on a copy
    df = swipes_df.copy()

    # ensure expected columns exist so later code can always reference them
    expected = [
        "EmployeeIdentity", "EmployeeID", "CardNumber", "EmployeeName", "LocaleMessageTime", "Door",
        "PersonnelTypeName", "PartitionName2", "CompanyName", "PrimaryLocation", "Direction", "person_uid"
    ]
    for col in expected:
        if col not in df.columns:
            df[col] = None

    # parse datetimes robustly
    try:
        if df["LocaleMessageTime"].dtype == object:
            df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")

    # drop near-duplicate swipes (round to seconds)
    try:
        df["_lts_rounded"] = df["LocaleMessageTime"].dt.floor("S")
        # Less aggressive dedupe: remove 'Door' from subset so multi-door swipes on the same second are kept
        dedupe_subset = ["person_uid", "_lts_rounded", "CardNumber"]
        # If person_uid missing, fallback to EmployeeIdentity + LocaleMessageTime
        if df["person_uid"].isnull().all():
            dedupe_subset = ["EmployeeIdentity", "_lts_rounded", "CardNumber"]
        df = df.drop_duplicates(subset=dedupe_subset, keep="first").copy()
    except Exception:
        # best-effort fallback
        try:
            df = df.drop_duplicates(subset=["EmployeeIdentity", "LocaleMessageTime", "EmployeeID", "CardNumber"], keep="first")
        except Exception:
            pass

    # Date assignment (strict local date)
    try:
        df["Date"] = df["LocaleMessageTime"].dt.date
    except Exception:
        try:
            df["Date"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce").dt.date
        except Exception:
            df["Date"] = None

    # ensure person_uid column exists; fallback to a stable join key if missing
    if "person_uid" not in df.columns or df["person_uid"].isnull().all():
        def make_person_uid(row):
            try:
                # prefer canonical EmployeeID-like tokens
                for cand in ("EmployeeID", "Int1", "Text12"):
                    if cand in row and row.get(cand) not in (None, '', float('nan')):
                        s = str(row.get(cand)).strip()
                        if s:
                            return s
                # fallback to EmployeeIdentity
                if row.get("EmployeeIdentity") not in (None, '', float('nan')):
                    return str(row.get("EmployeeIdentity")).strip()
                # fallback to name
                if row.get("EmployeeName") not in (None, '', float('nan')):
                    return str(row.get("EmployeeName")).strip()
            except Exception:
                pass
            return None
        df["person_uid"] = df.apply(make_person_uid, axis=1)

    # drop rows with no person_uid or no Date (they are not groupable)
    df = df[df["person_uid"].notna() & df["Date"].notna()].copy()
    if df.empty:
        return pd.DataFrame(columns=out_cols)

    # sort then group to pick first/last and other aggregations
    try:
        df = df.sort_values(["person_uid", "Date", "LocaleMessageTime"])
        grouped = df.groupby(["person_uid", "Date"], sort=False).agg(
            FirstSwipe=("LocaleMessageTime", "first"),
            LastSwipe=("LocaleMessageTime", "last"),
            FirstDoor=("Door", "first"),
            LastDoor=("Door", "last"),
            CountSwipes=("LocaleMessageTime", "count"),
            EmployeeIdentity=("EmployeeIdentity", "first"),
            EmployeeID=("EmployeeID", lambda s: _pick_first_non_guid_value(s) if not s.empty else None),
            EmployeeName=("EmployeeName", lambda s: _pick_first_non_guid_value(s) if not s.empty else None),
            CardNumber=("CardNumber", lambda s: _pick_first_non_guid_value(s) if not s.empty else None),
            PersonnelTypeName=("PersonnelTypeName", "first"),
            PartitionName2=("PartitionName2", "first"),
            CompanyName=("CompanyName", "first"),
            PrimaryLocation=("PrimaryLocation", "first"),
            FirstDirection=("Direction", "first"),
            LastDirection=("Direction", "last")
        ).reset_index()
    except Exception:
        # fallback single-group aggregator (safer for unexpected frames)
        def agg_for_group(g):
            g_sorted = g.sort_values("LocaleMessageTime")
            first = g_sorted.iloc[0]
            last = g_sorted.iloc[-1]
            empid = _pick_first_non_guid_value(g_sorted["EmployeeID"]) if "EmployeeID" in g_sorted else first.get("EmployeeID")
            ename = _pick_first_non_guid_value(g_sorted["EmployeeName"]) if "EmployeeName" in g_sorted else first.get("EmployeeName")
            cnum = _pick_first_non_guid_value(g_sorted["CardNumber"]) if "CardNumber" in g_sorted else first.get("CardNumber")
            return pd.Series({
                "person_uid": first["person_uid"],
                "EmployeeIdentity": first.get("EmployeeIdentity"),
                "EmployeeID": empid,
                "EmployeeName": ename,
                "CardNumber": cnum,
                "Date": first["Date"],
                "FirstSwipe": first["LocaleMessageTime"],
                "LastSwipe": last["LocaleMessageTime"],
                "FirstDoor": first.get("Door"),
                "LastDoor": last.get("Door"),
                "CountSwipes": int(len(g_sorted)),
                "PersonnelTypeName": first.get("PersonnelTypeName"),
                "PartitionName2": first.get("PartitionName2"),
                "CompanyName": first.get("CompanyName"),
                "PrimaryLocation": first.get("PrimaryLocation"),
                "FirstDirection": first.get("Direction"),
                "LastDirection": last.get("Direction")
            })
        grouped = df.groupby(["person_uid", "Date"], sort=False).apply(agg_for_group).reset_index(drop=True)

    # compute duration seconds using first/last swipes (wall-clock)
    grouped["DurationSeconds"] = (pd.to_datetime(grouped["LastSwipe"]) - pd.to_datetime(grouped["FirstSwipe"])).dt.total_seconds().clip(lower=0)

    # format Duration as H:MM string (hours can exceed 24)
    def _seconds_to_hhmm(seconds_val):
        try:
            if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
                return None
            total = int(round(float(seconds_val)))
            hours = total // 3600
            minutes = (total % 3600) // 60
            return f"{hours}:{minutes:02d}"
        except Exception:
            return None

    grouped["Duration"] = grouped["DurationSeconds"].apply(_seconds_to_hhmm)

    # human readable minutes and hms representations
    def _format_minutes_to_hhmm(seconds_val):
        try:
            if seconds_val is None or (isinstance(seconds_val, float) and np.isnan(seconds_val)):
                return None
            total_minutes = int(round(float(seconds_val) / 60.0))
            h = total_minutes // 60
            m = total_minutes % 60
            return f"{h}h {m}m"
        except Exception:
            return None

    grouped["DurationMinutes"] = grouped["DurationSeconds"].apply(lambda s: int(round(float(s) / 60.0)) if pd.notna(s) else None)
    grouped["DurationDisplay"] = grouped["DurationSeconds"].apply(_format_minutes_to_hhmm)
    grouped["DurationHMS"] = grouped["DurationSeconds"].apply(lambda s: str(timedelta(seconds=int(s))) if pd.notna(s) else None)

    # ensure all requested out_cols exist (stable ordering)
    for c in out_cols:
        if c not in grouped.columns:
            grouped[c] = None

    # drop helper column if present
    if "_lts_rounded" in grouped.columns:
        try:
            grouped = grouped.drop(columns=["_lts_rounded"])
        except Exception:
            pass

    # final column ordering and return
    return grouped[out_cols]


def run_for_date(target_date: date, regions: List[str], outdir: str, city: Optional[str] = None, durations_only: bool = False) -> Dict[str, Any]:
    outdir_path = Path(outdir)
    outdir_path.mkdir(parents=True, exist_ok=True)

    # Defensive: normalize incoming regions list
    try:
        requested_regions = [r.lower() for r in (regions or []) if r]
    except Exception:
        requested_regions = []

    # If user provided a city/site, try to map that city to one or more region keys
    def _normalize_token(s: str) -> str:
        return re.sub(r'[^a-z0-9]', '', str(s or '').strip().lower())

    if city:
        city_raw = str(city).strip()
        city_norm = _normalize_token(city_raw)

        # find regions whose partitions or logical_like look like this city
        matched_regions = []
        for rkey, rc in (REGION_CONFIG or {}).items():
            parts = rc.get("partitions", []) or []
            likes = rc.get("logical_like", []) or []
            tokens = set()
            for p in parts:
                if not p:
                    continue
                tokens.add(_normalize_token(p))
                # also split on punctuation/dot and add parts (e.g. "LT.Vilnius" -> "vilnius")
                for part_piece in re.split(r'[.\-/\s]', str(p)):
                    if part_piece:
                        tokens.add(_normalize_token(part_piece))
            for lk in likes:
                tokens.add(_normalize_token(lk))
            # also include server/database names as a fallback
            if city_norm and city_norm in tokens:
                matched_regions.append(rkey)

        # If we could map city to specific region(s), only run those
        if matched_regions:
            requested_regions = [m for m in matched_regions]

    # fallback to all regions in config if none requested
    try:
        if not requested_regions:
            requested_regions = [k.lower() for k in list(REGION_CONFIG.keys())]
    except Exception:
        requested_regions = ['apac']

    results: Dict[str, Any] = {}
    for r in requested_regions:
        if not r:
            continue
        rkey = r.lower()
        if rkey not in REGION_CONFIG:
            logging.warning("Unknown region '%s' - skipping", r)
            continue
        logging.info("Fetching swipes for region %s on %s", rkey, target_date)
        try:
            swipes = fetch_swipes_for_region(rkey, target_date)
        except Exception:
            logging.exception("Failed fetching swipes for region %s", rkey)
            swipes = pd.DataFrame()

        # If a city was requested, apply a strict (but defensive) city filter
        if city and not swipes.empty:
            city_raw = str(city).strip()
            city_norm = _normalize_token(city_raw)

            alt_tokens = set()
            alt_tokens.add(city_raw)
            alt_tokens.add(city_raw.replace('-', ' '))
            alt_tokens.add(city_raw.replace('_', ' '))
            alt_tokens.add(city_raw.replace('.', ' '))
            alt_tokens.add(city_raw.replace(' ', '-'))
            alt_tokens.update({t.title() for t in list(alt_tokens)})
            if city_norm:
                alt_tokens.add(city_norm)

            def _norm_for_cmp(s):
                try:
                    if s is None:
                        return ''
                    return re.sub(r'[^a-z0-9]', '', str(s).strip().lower())
                except Exception:
                    return ''

            # precompute normalized columns (safe defaults)
            try:
                part_norm = swipes["PartitionName2"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "PartitionName2" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
                door_norm = swipes["Door"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "Door" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
                pl_norm = swipes["PrimaryLocation"].fillna("").astype(str).str.lower().apply(_norm_for_cmp) if "PrimaryLocation" in swipes.columns else pd.Series([''] * len(swipes), index=swipes.index)
            except Exception:
                part_norm = pd.Series([''] * len(swipes), index=swipes.index)
                door_norm = pd.Series([''] * len(swipes), index=swipes.index)
                pl_norm = pd.Series([''] * len(swipes), index=swipes.index)

            # Build mask: start False
            mask = pd.Series(False, index=swipes.index)



            # 1) Strict: prefer PartitionName2 exact matches (canonical names from normalize)
            for t in alt_tokens:
                t_norm = _norm_for_cmp(t)
                if not t_norm:
                    continue
                try:
                    mask = mask | (part_norm == t_norm)
                except Exception:
                    continue

            # 2) For rows where PartitionName2 is empty / unknown, allow Door token contains match (permissive)
            try:
                no_part_mask = part_norm.fillna('').astype(str) == ''
                if no_part_mask.any():
                    door_mask = pd.Series(False, index=swipes.index)
                    for t in alt_tokens:
                        t_norm = _norm_for_cmp(t)
                        if not t_norm:
                            continue
                        door_mask = door_mask | door_norm.str.contains(t_norm, na=False)
                    # only accept door matches when partition is empty
                    mask = mask | (door_mask & no_part_mask)
            except Exception:
                logging.debug("Door-based fallback match failed for city filter in region %s", rkey)

            # 3) Allow PrimaryLocation match ONLY for rows with empty PartitionName2 and not already matched
            try:
                remaining_mask = ~mask
                pl_mask = pd.Series(False, index=swipes.index)
                for t in alt_tokens:
                    t_norm = _norm_for_cmp(t)
                    if not t_norm:
                        continue
                    pl_mask = pl_mask | pl_norm.str.contains(t_norm, na=False)
                mask = mask | (pl_mask & (part_norm.fillna('') == '') & remaining_mask)
            except Exception:
                logging.debug("PrimaryLocation fallback match failed for city filter in region %s", rkey)



            # --- Special-case matching for MY.Kuala Lumpur requested by user ---
            # Many KL door names are like "APAC_MY_KL_MAIN ENTRANCE DOOR" which normalize to
            # 'apacmyklmainentrancedoor' and won't contain 'mykualalumpur' as a contiguous token.
            # So when user asked for MY.Kuala Lumpur, accept any door that contains APAC + MY + (KL | KUALA | KUALALUMPUR).
            try:
                # city_norm is already normalized (non-alphanum removed). compare against expected KL token
                if city_norm in ("mykualalumpur", "mykuala", "kualalumpur", "kuala"):
                    kl_mask = (
                        door_norm.str.contains("apac", na=False)
                        & door_norm.str.contains("my", na=False)
                        & (
                            door_norm.str.contains("kl", na=False)
                            | door_norm.str.contains("kuala", na=False)
                            | door_norm.str.contains("kualalumpur", na=False)
                        )
                    )
                    # Accept KL door matches even if PartitionName2 is empty (safe & strict)
                    mask = mask | kl_mask
            except Exception:
                logging.debug("KL special-case matching failed for city filter")



            # 4) Finally, also check Door and EmployeeName contains across all rows (additional permissive matches)
            try:
                for col in ("Door", "EmployeeName"):
                    if col in swipes.columns:
                        col_norm = swipes[col].fillna("").astype(str).str.lower().apply(_norm_for_cmp)
                        col_mask = pd.Series(False, index=swipes.index)
                        for t in alt_tokens:
                            t_norm = _norm_for_cmp(t)
                            if not t_norm:
                                continue
                            col_mask = col_mask | col_norm.str.contains(t_norm, na=False)
                        mask = mask | col_mask
            except Exception:
                logging.debug("Door/EmployeeName contains-match fallback failed for city filter in region %s", rkey)

            # Apply the mask strictly
            before = len(swipes)
            swipes = swipes[mask].copy()
            logging.info("City filter '%s' applied for region %s: rows before=%d after=%d", city_raw, rkey, before, len(swipes))







        # compute durations for this region
        try:
            durations = compute_daily_durations(swipes)
        except Exception:
            logging.exception("Failed computing durations for region %s", rkey)
            durations = pd.DataFrame()

        # --- Add compact SwipeSamples (first up to 5 times) so durations CSV contains a mini timeline for frontend ---
        try:
            if not swipes.empty and not durations.empty:
                # ensure LocaleMessageTime exists and is datetime
                swipes["LocaleMessageTime"] = pd.to_datetime(swipes["LocaleMessageTime"], errors="coerce")
                # produce "HH:MM:SS" samples per (person_uid, Date)
                samples = (
                    swipes.sort_values(["person_uid", "LocaleMessageTime"])
                          .groupby(["person_uid", "Date"])["LocaleMessageTime"]
                          .apply(lambda s: ",".join(t.strftime("%H:%M:%S") for t in s.dt.floor("S").tolist()[:5]))
                          .reset_index(name="SwipeSamples")
                )
                # merge compact samples onto durations (safe left merge)
                try:
                    durations = durations.merge(samples, on=["person_uid", "Date"], how="left")
                except Exception:
                    # if merge keys different types, coerce and retry
                    samples["Date"] = samples["Date"].apply(lambda d: pd.to_datetime(d).date() if pd.notna(d) else None)
                    durations["Date"] = durations["Date"].apply(lambda d: pd.to_datetime(d).date() if pd.notna(d) else None)
                    durations = durations.merge(samples, on=["person_uid", "Date"], how="left")
        except Exception:
            logging.debug("Failed to compute SwipeSamples for region %s", rkey)

        # frontend-friendly display columns for swipes (keep these even if we only write durations)
        try:
            if "LocaleMessageTime" in swipes.columns:
                swipes["LocaleMessageTime"] = pd.to_datetime(swipes["LocaleMessageTime"], errors="coerce")
                swipes["DateOnly"] = swipes["LocaleMessageTime"].dt.date
                swipes["Time"] = swipes["LocaleMessageTime"].dt.strftime("%H:%M:%S")
            else:
                if "Date" in swipes.columns and "Time" in swipes.columns:
                    swipes["DateOnly"] = swipes["Date"]
        except Exception:
            logging.debug("Frontend display enrichment failed for region %s", rkey)

        if "AdjustedMessageTime" not in swipes.columns:
            swipes["AdjustedMessageTime"] = pd.NaT

        # write outputs - always write durations file; optionally write swipes file
        try:
            csv_path = outdir_path / f"{rkey}_duration_{target_date.strftime('%Y%m%d')}.csv"
            durations.to_csv(csv_path, index=False)
        except Exception:
            logging.exception("Failed writing durations CSV for %s", rkey)

        if not durations_only:
            try:
                swipes_csv_path = outdir_path / f"{rkey}_swipes_{target_date.strftime('%Y%m%d')}.csv"
                swipes.to_csv(swipes_csv_path, index=False)
            except Exception:
                logging.exception("Failed writing swipes CSV for %s", rkey)
            logging.info("Wrote swipes CSV for %s to %s (rows=%d)", rkey, swipes_csv_path if 'swipes_csv_path' in locals() else '<unknown>', len(swipes) if swipes is not None else 0)

        logging.info("Wrote duration CSV for %s to %s (rows=%d)", rkey, csv_path if 'csv_path' in locals() else '<unknown>', len(durations) if durations is not None else 0)

        results[rkey] = {"swipes": swipes, "durations": durations}

    return results


# end of file



