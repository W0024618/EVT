1) Add near the top of app.py (with other imports)

Add these imports and globals (paste after your existing imports section):



# new imports for duration concurrency control
from concurrent.futures import ThreadPoolExecutor
import shutil
import uuid

# Dedicated executor for duration compute tasks (avoids saturating default executor)
_DUR_EXECUTOR_WORKERS = int(os.getenv("DURATION_EXECUTOR_WORKERS", "12"))
_DURATION_EXECUTOR = ThreadPoolExecutor(max_workers=_DUR_EXECUTOR_WORKERS)

# Global concurrency limiter for /duration requests (quickly fail when overloaded)
_GLOBAL_DURATION_CONCURRENCY = int(os.getenv("DURATION_GLOBAL_CONCURRENCY", "20"))
# NOTE: asyncio.Semaphore created at import-time is fine; it's used within async functions.
_GLOBAL_DURATION_SEMAPHORE = asyncio.Semaphore(_GLOBAL_DURATION_CONCURRENCY)





2) Replace the portion inside api_duration that sets up the request outdir, the run_for_date call, and concurrency usage.

Find the start of api_duration and immediately after you compute outdir_path.mkdir(...) insert the global semaphore acquire/release wrapper and per-request temp directory, and switch run_in_executor to use the dedicated executor.

Replace the section starting from:




        if outdir:
            outdir_path = Path(outdir)
        else:
            outdir_path = OUTPUT_DIR / "duration_reports"
        outdir_path.mkdir(parents=True, exist_ok=True)




through the point where you create loop = asyncio.get_running_loop() (i.e. the next lines) with the following code. (This keeps the rest of your handler intact â€” only changes how the compute is scheduled and how outdir is handled.)


        if outdir:
            outdir_path = Path(outdir)
        else:
            outdir_path = OUTPUT_DIR / "duration_reports"
        outdir_path.mkdir(parents=True, exist_ok=True)

        # Acquire global duration-slot (fail fast if overloaded)
        acquired_global_slot = False
        try:
            try:
                # short timeout: if the server is saturated, return 503 quickly
                await asyncio.wait_for(_GLOBAL_DURATION_SEMAPHORE.acquire(), timeout=5)
                acquired_global_slot = True
            except asyncio.TimeoutError:
                raise HTTPException(status_code=503, detail="Server busy processing other duration requests; please retry shortly")






Then later, right before you build ext_dates_set (i.e. before you create the per-date concurrency semaphore and schedule tasks), create a per-request temporary outdir and change the executor usage. Replace the part where you previously used:




        loop = asyncio.get_running_loop()
        ...
        sem = asyncio.Semaphore(SEM_MAX)
        async def _run_for_single_date(d: date):
            async with sem:
                ...
                task = loop.run_in_executor(None, functools.partial(duration_report.run_for_date, d, regions_list, str(outdir_path), city))
                res = await asyncio.wait_for(task, timeout=COMPUTE_WAIT_TIMEOUT_SECONDS)



with:


        loop = asyncio.get_running_loop()

        # create unique per-request temp outdir so concurrent requests don't collide on CSV filenames
        request_id = uuid.uuid4().hex
        request_outdir = outdir_path / request_id
        request_outdir.mkdir(parents=True, exist_ok=True)

        # bounded concurrency for per-date tasks within this request
        SEM_MAX = 4  # keep per-request bounded concurrency
        sem = asyncio.Semaphore(SEM_MAX)

        async def _run_for_single_date(d: date):
            async with sem:
                try:
                    # run duration_report.run_for_date in our dedicated threadpool executor
                    task = loop.run_in_executor(
                        _DURATION_EXECUTOR,
                        functools.partial(duration_report.run_for_date, d, regions_list, str(request_outdir), city)
                    )
                    res = await asyncio.wait_for(task, timeout=COMPUTE_WAIT_TIMEOUT_SECONDS)
                    return (d.isoformat(), res)
                except asyncio.TimeoutError:
                    logger.exception("Duration computation timed out for date %s", d.isoformat())
                    return (d.isoformat(), {"error": "timeout"})
                except Exception as e:
                    logger.exception("duration run_for_date failed for date %s: %s", d, e)
                    return (d.isoformat(), {"error": str(e)})






Finally, very important: ensure the global slot semaphore is released and the temp outdir is cleaned up even on error. After the heavy compute completes and just before returning the final JSONResponse(resp) (i.e. near the end of the handler) add cleanup in a finally block:



        # cache duration result (best-effort)
        try:
            _save_duration_cache(cache_path, resp)
        except Exception:
            logger.exception("Failed to save duration cache for %s", cache_path)

        # respond and then cleanup the temporary per-request folder
        try:
            return JSONResponse(resp)
        finally:
            # release the global slot (if acquired)
            try:
                if acquired_global_slot:
                    _GLOBAL_DURATION_SEMAPHORE.release()
            except Exception:
                pass
            # cleanup per-request temp files to avoid disk growth
            try:
                shutil.rmtree(request_outdir, ignore_errors=True)
            except Exception:
                logger.exception("Failed to cleanup request outdir %s", request_outdir)








