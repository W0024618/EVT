"""
Generate Denver Monthly Attendance Excel report.

Usage:
    from denverAttendance import generate_monthly_denver_report
    path = generate_monthly_denver_report(year=2025, month=9, outdir='/path/to/output', city_filter='Denver')
"""
from datetime import date, datetime, timedelta
from pathlib import Path
import pandas as pd
import math
import logging
import re
from typing import Optional, Tuple, Dict

logger = logging.getLogger("denverAttendance")

# Import duration_report (must be importable in same package/dir)
try:
    import duration_report
except Exception:
    duration_report = None

def _month_date_range(year: int, month: int) -> Tuple[date, date]:
    start = date(year, month, 1)
    if month == 12:
        end = date(year + 1, 1, 1) - timedelta(days=1)
    else:
        end = date(year, month + 1, 1) - timedelta(days=1)
    return start, end

def _to_date(v):
    if v is None:
        return None
    if isinstance(v, date):
        return v
    if isinstance(v, datetime):
        return v.date()
    try:
        return datetime.strptime(str(v)[:10], "%Y-%m-%d").date()
    except Exception:
        try:
            return pd.to_datetime(v).date()
        except Exception:
            return None

def _find_active_employee_file() -> Optional[Path]:
    """
    Search common locations for canonical active_employee file saved by the upload endpoint.
    Looks for names like active_employee(.xlsx|.xls|.csv) and active_employee_*.*
    """
    candidates = []
    # candidate directories
    cand_dirs = [Path.cwd(), Path.cwd() / "data", Path(__file__).resolve().parent, Path(__file__).resolve().parent / "data"]
    seen = set()
    for d in cand_dirs:
        try:
            if not d.exists():
                continue
            for pattern in ("active_employee.*", "active_employees.*", "active_employee_*.*"):
                for p in d.glob(pattern):
                    if p.is_file() and p not in seen:
                        candidates.append(p)
                        seen.add(p)
        except Exception:
            continue
    # prefer xlsx/xls then csv
    for ext in (".xlsx", ".xls"):
        for p in candidates:
            if p.suffix.lower() == ext:
                return p
    for p in candidates:
        if p.suffix.lower() == ".csv":
            return p
    if candidates:
        return candidates[0]
    return None

def _load_active_employees_df() -> Optional[pd.DataFrame]:
    p = _find_active_employee_file()
    if not p:
        return None
    try:
        if p.suffix.lower() in (".xlsx", ".xls"):
            df = pd.read_excel(p, dtype=str)
        else:
            df = pd.read_csv(p, dtype=str)
        # normalize column names (strip, keep original as well)
        df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]
        return df
    except Exception:
        logger.exception("Failed to load active employee file %s", p)
        return None

def _pick_first_column_match(df: pd.DataFrame, candidates):
    """
    Given a df and list of candidate column-names (strings or regex patterns),
    return first matching actual column name from df.columns (case-insensitive).
    """
    cols = list(df.columns)
    lower_map = {c.lower(): c for c in cols}
    for cand in candidates:
        if not cand:
            continue
        # exact try
        lc = cand.lower()
        if lc in lower_map:
            return lower_map[lc]
        # fuzzy contains
        for c in cols:
            if lc in c.lower():
                return c
        # regex style (if cand contains special chars)
        try:
            rx = re.compile(cand, re.I)
            for c in cols:
                if rx.search(c):
                    return c
        except Exception:
            pass
    return None

def _build_enrichment_map(active_df: pd.DataFrame) -> Dict[str, Dict]:
    """
    Build lookup maps keyed by EmployeeID (preferred) and also by FullName fallback.
    Returns dict: { employee_id_str: {...row...}, '__name__': {fullname: {...}} }
    """
    out = {"__id__": {}, "__name__": {}}
    if active_df is None or active_df.empty:
        return out

    # identify likely columns
    id_col = _pick_first_column_match(active_df, ["Employee ID", "EmployeeID", "EmployeeId", "Text12", "employeeid"])
    # For name fields we prefer "Full Name" or "FullName" or combine First/Last
    full_name_col = _pick_first_column_match(active_df, ["Full Name", "FullName", "Full_Name", "Full name", "FullName"])
    if not full_name_col:
        # fallback build from First/Last
        first_c = _pick_first_column_match(active_df, ["First Name", "FirstName", "First"])
        last_c = _pick_first_column_match(active_df, ["Last Name", "LastName", "Last"])
        if first_c and last_c:
            active_df["__full_name__"] = active_df[first_c].fillna("").astype(str).str.strip() + " " + active_df[last_c].fillna("").astype(str).str.strip()
            full_name_col = "__full_name__"

    # other common enrichment columns (try many variants)
    business_col = _pick_first_column_match(active_df, ["Business Title", "Business_Title", "Job Title", "JobTitle", "BusinessTitle", "Business Title"])
    manager_col = _pick_first_column_match(active_df, ["Manager Name", "Manager_Name", "Manager", "Manager's Name", "ManagerName"])
    n1_col = _pick_first_column_match(active_df, ["N1_Sup_Organization", "Reporting Level 1 Name", "Reporting Level 1", "Supervisory Organization", "SupervisoryOrganization"])
    loc_col = _pick_first_column_match(active_df, ["Location Description", "Location_Description", "Location Description", "Location"])
    status_col = _pick_first_column_match(active_df, ["Current Status", "Current_Status", "Employee Status", "Status"])
    hire_col = _pick_first_column_match(active_df, ["Hire Date", "Hire_Date", "HireDate", "Hire date"])
    # keep also any other email/manager email if needed
    for idx, row in active_df.iterrows():
        try:
            rec = {}
            if id_col:
                eid = row.get(id_col)
                if pd.notna(eid):
                    eid = str(eid).strip()
                else:
                    eid = None
            else:
                eid = None
            if full_name_col:
                fname = row.get(full_name_col)
                fname = None if pd.isna(fname) else str(fname).strip()
            else:
                fname = None
            # enrichment fields
            rec["Business_Title"] = None if business_col is None or pd.isna(row.get(business_col)) else str(row.get(business_col)).strip()
            rec["Manager_Name"] = None if manager_col is None or pd.isna(row.get(manager_col)) else str(row.get(manager_col)).strip()
            rec["N1_Sup_Organization"] = None if n1_col is None or pd.isna(row.get(n1_col)) else str(row.get(n1_col)).strip()
            rec["Location_Description"] = None if loc_col is None or pd.isna(row.get(loc_col)) else str(row.get(loc_col)).strip()
            rec["Current_Status"] = None if status_col is None or pd.isna(row.get(status_col)) else str(row.get(status_col)).strip()
            rec["Hire_Date"] = None if hire_col is None or pd.isna(row.get(hire_col)) else _to_date(row.get(hire_col))
            rec["Full_Name"] = fname
            rec["EmployeeID"] = eid
            # store by id and by name lower-case
            if eid:
                out["__id__"][str(eid).strip()] = rec
            if fname:
                out["__name__"][fname.strip().lower()] = rec
        except Exception:
            continue
    return out

def _date_label(d: date) -> str:
    # Format like "1-Jan-25"
    return d.strftime("%-d-%b-%y") if hasattr(d, "strftime") and ("% -d" in "%-d") else d.strftime("%-d-%b-%y") if False else d.strftime("%-d-%b-%y")  # placeholder for cross-platform
    # NOTE: above is to ensure single-digit day (on linux). If running on windows where %-d not supported, we'll fallback below.

def _date_label_safe(d: date) -> str:
    # cross platform single-digit day formatting
    try:
        # try %-d first (works on unix)
        return d.strftime("%-d-%b-%y")
    except Exception:
        # fallback to strip leading zero from %d
        s = d.strftime("%d-%b-%y")
        if s.startswith("0"):
            s = s[1:]
        return s




def generate_monthly_denver_report(year: int = None, month: int = None, start_date: date = None, end_date: date = None,
                                   outdir: str = None, city_filter: str = "Denver", region: str = "namer") -> str:
    """
    Generate the Denver monthly attendance report Excel file.
    Accepts either year+month or start_date+end_date. Returns path to the xlsx file.
    This version ensures we include *any* employee who physically swiped at a Denver door,
    even if their primary location/PartitionName2 is not Denver.
    """
    if start_date is not None or end_date is not None:
        start_date = _to_date(start_date)
        end_date = _to_date(end_date)

    if (start_date is None or end_date is None):
        if year is None or month is None:
            raise ValueError("Either (year and month) or (start_date and end_date) must be provided")
        start_date, end_date = _month_date_range(int(year), int(month))

    if not outdir:
        outdir = Path.cwd() / "output"
    outdir = Path(outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    # build full date list
    current = start_date
    dates = []
    while current <= end_date:
        dates.append(current)
        current = current + timedelta(days=1)

    if not dates:
        raise ValueError("Empty date range")

    # canonical month-range label (used for output filename) â€” always set so it's available in all code paths
    mon_label = f"{start_date.strftime('%Y%m')}_{end_date.strftime('%Y%m')}"

    # helper: decide if a swipe row is a Denver visit based on Door/PartitionName2/PrimaryLocation
    def _is_denver_visit_row(row) -> bool:
        try:
            # combine a few fields that might indicate the door/location
            door = (row.get("Door") or "") if isinstance(row, dict) else (row.get("Door") if "Door" in row else "")
            partition = (row.get("PartitionName2") or "") if isinstance(row, dict) else (row.get("PartitionName2") if "PartitionName2" in row else "")
            prim = (row.get("PrimaryLocation") or "") if isinstance(row, dict) else (row.get("PrimaryLocation") if "PrimaryLocation" in row else "")
            # lowercase join
            s = " ".join([str(door), str(partition), str(prim)]).lower()
            # check mapping/keywords: user mapping used '%HQ%' -> Denver; include direct 'denver' and abbreviations
            keywords = ["denver", "hq", " hq ", " den ", "den."]
            for kw in keywords:
                if kw in s:
                    return True
            # also match common variants like 'co-denver' or 'denver - w'
            if "denver -" in s or "denver " in s:
                return True
            return False
        except Exception:
            return False

    # collect durations for each date from (A) normal namer run and (B) supplemental scans of all regions for door hits
    all_durations = []

    # regions to scan for supplemental door-based visits
    regions_to_scan = list(getattr(duration_report, "REGION_CONFIG", {}).keys()) if duration_report and hasattr(duration_report, "REGION_CONFIG") else ["apac","emea","laca","namer"]

    for d in dates:
        # 1) primary (fast path) - existing behavior: run_for_date for the 'namer' region (this will include Denver-native people)
        try:
            res = duration_report.run_for_date(d, regions=[region], outdir=str(outdir), city=city_filter)
            region_obj = res.get(region, {}) if isinstance(res, dict) else {}
            durations_df = region_obj.get("durations") if isinstance(region_obj, dict) else None
            if isinstance(durations_df, pd.DataFrame) and not durations_df.empty:
                durations_df['Date'] = pd.to_datetime(durations_df['Date']).dt.date
                all_durations.append(durations_df)
        except Exception:
            logger.exception("Primary run_for_date failed for date %s", d)

        # 2) supplemental: scan ALL regions and pick swipe rows that indicate a Denver door
        #    use fetch_swipes_for_region (lighter-weight) + compute_daily_durations()
        for r in regions_to_scan:
            try:
                swipes = duration_report.fetch_swipes_for_region(r, d)
            except Exception:
                logger.exception("fetch_swipes_for_region failed for region %s date %s", r, d)
                swipes = pd.DataFrame()

            if swipes is None or swipes.empty:
                continue

            # normalize column names and ensure door/partition/primary exist
            for c in ("Door", "PartitionName2", "PrimaryLocation", "EmployeeName"):
                if c not in swipes.columns:
                    swipes[c] = None

            # filter swipes to only those that look like Denver visits (based on door/partition/primary)
            try:
                # run row-wise check (vectorize minimal)
                mask = swipes.apply(lambda row: _is_denver_visit_row(row), axis=1)
                denver_swipes = swipes[mask].copy()
            except Exception:
                denver_swipes = swipes  # fallback: if failure, be conservative and keep them (better to include than miss)
            
            if denver_swipes is None or denver_swipes.empty:
                continue

            # compute durations for the filtered swipes (this yields person_uid, DurationSeconds, Date, etc.)
            try:
                durations_df = duration_report.compute_daily_durations(denver_swipes)
                if isinstance(durations_df, pd.DataFrame) and not durations_df.empty:
                    # ensure Date is date type
                    if 'Date' in durations_df.columns:
                        durations_df['Date'] = pd.to_datetime(durations_df['Date']).dt.date
                    else:
                        durations_df['Date'] = d
                    all_durations.append(durations_df)
            except Exception:
                logger.exception("compute_daily_durations failed for supplemental denver swipes region=%s date=%s", r, d)
                continue

    # concat and dedupe person/date to avoid double-counting (keep first occurrence)
    if not all_durations:
        filename = outdir / f"denver_attendance_{mon_label}.xlsx"
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            pd.DataFrame([{"Note": f"No durations found for {start_date.isoformat()} -> {end_date.isoformat()}"}]).to_excel(writer, sheet_name="Summary", index=False)
        return str(filename)

    df_all = pd.concat(all_durations, ignore_index=True, sort=False)

    # normalize Date & person_uid and dedupe by person_uid+Date (keep first)
    if "Date" in df_all.columns:
        df_all["Date"] = pd.to_datetime(df_all["Date"]).dt.date
    else:
        # should not happen but set to start_date
        df_all["Date"] = start_date

    if "person_uid" not in df_all.columns:
        def make_person_uid(row):
            eid = row.get("EmployeeIdentity")
            if pd.notna(eid) and str(eid).strip():
                return str(eid).strip()
            parts = []
            for k in ("EmployeeID", "CardNumber", "EmployeeName"):
                v = row.get(k)
                if pd.notna(v) and str(v).strip():
                    parts.append(str(v).strip())
            return "|".join(parts) or None
        df_all["person_uid"] = df_all.apply(make_person_uid, axis=1)

    # drop duplicates keeping first occurrence (this prevents the same person/date appearing twice)
    df_all = df_all.drop_duplicates(subset=["person_uid", "Date"], keep="first").reset_index(drop=True)




    # Normalize DurationSeconds
    if "DurationSeconds" not in df_all.columns:
        if "FirstSwipe" in df_all.columns and "LastSwipe" in df_all.columns:
            try:
                df_all["DurationSeconds"] = (pd.to_datetime(df_all["LastSwipe"]) - pd.to_datetime(df_all["FirstSwipe"])).dt.total_seconds().clip(lower=0)
            except Exception:
                df_all["DurationSeconds"] = None
        else:
            df_all["DurationSeconds"] = None

    # Normalize PersonnelType column name
    if "PersonnelTypeName" in df_all.columns and "PersonnelType" not in df_all.columns:
        df_all = df_all.rename(columns={"PersonnelTypeName": "PersonnelType"})
    if "PersonnelType" not in df_all.columns:
        df_all["PersonnelType"] = None

    # FILTER: keep only Employees and Terminated Personnel
    def _is_employee_or_terminated(v):
        try:
            if v is None:
                return False
            s = str(v).strip().lower()
            if "employee" in s:
                return True
            if "terminated" in s and "personnel" in s:
                return True
            if s == "terminated personnel" or s == "terminated":
                return True
            return False
        except Exception:
            return False

    df_all = df_all[df_all["PersonnelType"].apply(_is_employee_or_terminated)].copy()

    # Build pivot (one row per person_uid, columns per date -> DurationSeconds)
    df_all["DurationSeconds"] = pd.to_numeric(df_all["DurationSeconds"], errors="coerce")
    pivot_secs = df_all.pivot_table(index="person_uid", columns="Date", values="DurationSeconds", aggfunc="first")

    # ensure all requested dates present
    for d in dates:
        if d not in pivot_secs.columns:
            pivot_secs[d] = pd.NA
    pivot_secs = pivot_secs.reindex(sorted(pivot_secs.columns), axis=1)

    # Representative meta (first values) for each person
    agg_first = df_all.groupby("person_uid", sort=False).agg({
        "EmployeeName": "first",
        "EmployeeID": "first",
        "PersonnelType": "first",
        "CardNumber": "first",
        "PartitionName2": "first",
        "PrimaryLocation": "first",
        "FirstSwipe": "first",
        "LastSwipe": "last"
    }).rename(columns={
        "EmployeeName": "EmployeeName",
        "EmployeeID": "EmployeeID"
    })

    # Days present / Days Ge8 / DaysInMonth
    days_present = (pivot_secs.notna() & (pivot_secs.astype(float) > 0)).sum(axis=1)
    days_ge8 = (pivot_secs.astype(float) >= 28800).sum(axis=1)  # >= 8 hours
    days_in_range = len(dates)

    agg_first["DaysPresent"] = days_present.fillna(0).astype(int)
    agg_first["DaysGe8"] = days_ge8.fillna(0).astype(int)
    agg_first["DaysInMonth"] = int(days_in_range)

    # Build enrichment map from active employees file (best-effort)
    active_df = _load_active_employees_df()
    enrichment = _build_enrichment_map(active_df) if active_df is not None else {"__id__": {}, "__name__": {}}

    # Build Summary DataFrame columns as requested (mapping/enriching)
    summary_rows = []
    for uid, meta in agg_first.reset_index().set_index("person_uid").to_dict(orient="index").items():
        try:
            empid = meta.get("EmployeeID") or ""
            empname = meta.get("EmployeeName") or ""
            personnel_type = meta.get("PersonnelType") or ""
            days_present_val = int(meta.get("DaysPresent") or 0)
            # enrichment lookup
            enrich_rec = None
            if empid and str(empid).strip() in enrichment.get("__id__", {}):
                enrich_rec = enrichment["__id__"].get(str(empid).strip())
            elif empname and str(empname).strip().lower() in enrichment.get("__name__", {}):
                enrich_rec = enrichment["__name__"].get(str(empname).strip().lower())

            business = enrich_rec.get("Business_Title") if enrich_rec else None
            manager = enrich_rec.get("Manager_Name") if enrich_rec else None
            n1_org = enrich_rec.get("N1_Sup_Organization") if enrich_rec else None
            loc_desc = enrich_rec.get("Location_Description") if enrich_rec else None
            current_status = enrich_rec.get("Current_Status") if enrich_rec else None
            hire_date = enrich_rec.get("Hire_Date") if enrich_rec else None

            row = {
                "Employee ID": empid,
                "Full_Name": empname,
                "Personnel Type": personnel_type,
                "Business_Title": business,
                "Manager_Name": manager,
                "N1_Sup_Organization": n1_org,
                "Location_Description": loc_desc or meta.get("PrimaryLocation"),
                "Current_Status": current_status,
                "Report Start Date": start_date.isoformat(),
                "Report End Date": end_date.isoformat(),
                "Hire_Date": hire_date.isoformat() if isinstance(hire_date, (date, datetime)) else (hire_date if hire_date else None),
                "Actual_No_Of_Days_Attended": days_present_val
            }
            summary_rows.append(row)
        except Exception:
            logger.exception("Failed building summary row for uid=%s", uid)
            continue

    summary_df = pd.DataFrame(summary_rows, columns=[
        "Employee ID", "Full_Name", "Personnel Type", "Business_Title", "Manager_Name", "N1_Sup_Organization",
        "Location_Description", "Current_Status", "Report Start Date", "Report End Date", "Hire_Date", "Actual_No_Of_Days_Attended"
    ])

    # Build Attendance sheet: Emp ID, Emp Name, date cols (formatted), monthly totals
    # Start with pivot_secs -> binary presence (1/0)
    presence_df = pivot_secs.copy()
    # binary: 1 if >0 else 0 (treat NaN as 0)
    presence_df = presence_df.applymap(lambda v: 1 if (pd.notna(v) and float(v) > 0) else 0)
    # ensure index aligns with agg_first
    presence_df = presence_df.reindex(agg_first.index)

    # rename date columns to required format (e.g. 1-Jan-25)
    date_labels = [_date_label_safe(d) for d in presence_df.columns]
    col_map = dict(zip(presence_df.columns, date_labels))
    presence_df.columns = [col_map[c] for c in presence_df.columns]

    # compute month totals: group dates by month label
    month_groups = {}
    for d in dates:
        mon_label = d.strftime("%b")  # e.g. Jan
        lbl = _date_label_safe(d)
        month_groups.setdefault(mon_label, []).append(lbl)

    # For final attendance frame, create index columns Emp ID and Emp Name
    att_index = pd.DataFrame({
        "Emp ID": agg_first["EmployeeID"].fillna("").astype(str),
        "Emp Name": agg_first["EmployeeName"].fillna("").astype(str)
    }, index=presence_df.index)

    # attach presence columns
    attendance_df = pd.concat([att_index.reset_index(drop=True), presence_df.reset_index(drop=True)], axis=1)

    # add monthly totals columns (e.g. "Jan Total") computed as sum across that month's date columns
    for mon, cols in month_groups.items():
        try:
            attendance_df[f"{mon} Total"] = attendance_df[cols].sum(axis=1).astype(int)
        except Exception:
            attendance_df[f"{mon} Total"] = 0

    # also compute overall monthly totals in summary (if you want it there)
    # Save to Excel
    filename = outdir / f"denver_attendance_{mon_label}.xlsx"
    try:
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # Summary sheet
            summary_df.to_excel(writer, sheet_name="Summary", index=False)

            # Attendance sheet
            # Ensure columns order: Emp ID, Emp Name, all date cols in chronological order, then month totals
            date_col_order = [ _date_label_safe(d) for d in sorted(dates) ]
            month_totals_order = [f"{m} Total" for m in sorted(month_groups.keys(), key=lambda m: datetime.strptime(m, "%b").month)]
            attendance_cols = ["Emp ID", "Emp Name"] + date_col_order + month_totals_order
            # Defensive: ensure only existing columns are used
            attendance_cols = [c for c in attendance_cols if c in attendance_df.columns]
            attendance_df.to_excel(writer, sheet_name="Attendance", index=False, columns=attendance_cols)

            # Optional: include DurationsRaw if helpful
            try:
                tidy = df_all[["person_uid", "Date", "DurationSeconds", "Duration"]].copy()
                tidy = tidy.rename(columns={"person_uid": "PersonUID"})
                tidy = tidy.sort_values(["PersonUID", "Date"])
                tidy.to_excel(writer, sheet_name="DurationsRaw", index=False)
            except Exception:
                logger.debug("Could not write DurationsRaw sheet (non-fatal)", exc_info=True)
        logger.info("Wrote Denver attendance report to %s", filename)
    except Exception as e:
        logger.exception("Failed writing Excel report to %s: %s", filename, e)
        raise

    return str(filename)

























Make all Above Changes in below file carefully and share me fully updated file so i can easily swap file with each other ..

# denverAttendance.py
"""
Generate Denver Monthly Attendance Excel report.

Usage:
    from denverAttendance import generate_monthly_denver_report
    path = generate_monthly_denver_report(year=2025, month=9, outdir='/path/to/output', city_filter='Denver')
"""
from datetime import date, datetime, timedelta
from pathlib import Path
import pandas as pd
import math
import logging
import re
from typing import Optional, Tuple, Dict

logger = logging.getLogger("denverAttendance")

# Import duration_report (must be importable in same package/dir)
try:
    import duration_report
except Exception:
    duration_report = None

def _month_date_range(year: int, month: int) -> Tuple[date, date]:
    start = date(year, month, 1)
    if month == 12:
        end = date(year + 1, 1, 1) - timedelta(days=1)
    else:
        end = date(year, month + 1, 1) - timedelta(days=1)
    return start, end

def _to_date(v):
    if v is None:
        return None
    if isinstance(v, date):
        return v
    if isinstance(v, datetime):
        return v.date()
    try:
        return datetime.strptime(str(v)[:10], "%Y-%m-%d").date()
    except Exception:
        try:
            return pd.to_datetime(v).date()
        except Exception:
            return None

def _find_active_employee_file() -> Optional[Path]:
    """
    Search common locations for canonical active_employee file saved by the upload endpoint.
    Looks for names like active_employee(.xlsx|.xls|.csv) and active_employee_*.*
    """
    candidates = []
    # candidate directories
    cand_dirs = [Path.cwd(), Path.cwd() / "data", Path(__file__).resolve().parent, Path(__file__).resolve().parent / "data"]
    seen = set()
    for d in cand_dirs:
        try:
            if not d.exists():
                continue
            for pattern in ("active_employee.*", "active_employees.*", "active_employee_*.*"):
                for p in d.glob(pattern):
                    if p.is_file() and p not in seen:
                        candidates.append(p)
                        seen.add(p)
        except Exception:
            continue
    # prefer xlsx/xls then csv
    for ext in (".xlsx", ".xls"):
        for p in candidates:
            if p.suffix.lower() == ext:
                return p
    for p in candidates:
        if p.suffix.lower() == ".csv":
            return p
    if candidates:
        return candidates[0]
    return None

def _load_active_employees_df() -> Optional[pd.DataFrame]:
    p = _find_active_employee_file()
    if not p:
        return None
    try:
        if p.suffix.lower() in (".xlsx", ".xls"):
            df = pd.read_excel(p, dtype=str)
        else:
            df = pd.read_csv(p, dtype=str)
        # normalize column names (strip, keep original as well)
        df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]
        return df
    except Exception:
        logger.exception("Failed to load active employee file %s", p)
        return None

def _pick_first_column_match(df: pd.DataFrame, candidates):
    """
    Given a df and list of candidate column-names (strings or regex patterns),
    return first matching actual column name from df.columns (case-insensitive).
    """
    cols = list(df.columns)
    lower_map = {c.lower(): c for c in cols}
    for cand in candidates:
        if not cand:
            continue
        # exact try
        lc = cand.lower()
        if lc in lower_map:
            return lower_map[lc]
        # fuzzy contains
        for c in cols:
            if lc in c.lower():
                return c
        # regex style (if cand contains special chars)
        try:
            rx = re.compile(cand, re.I)
            for c in cols:
                if rx.search(c):
                    return c
        except Exception:
            pass
    return None

def _build_enrichment_map(active_df: pd.DataFrame) -> Dict[str, Dict]:
    """
    Build lookup maps keyed by EmployeeID (preferred) and also by FullName fallback.
    Returns dict: { employee_id_str: {...row...}, '__name__': {fullname: {...}} }
    """
    out = {"__id__": {}, "__name__": {}}
    if active_df is None or active_df.empty:
        return out

    # identify likely columns
    id_col = _pick_first_column_match(active_df, ["Employee ID", "EmployeeID", "EmployeeId", "Text12", "employeeid"])
    # For name fields we prefer "Full Name" or "FullName" or combine First/Last
    full_name_col = _pick_first_column_match(active_df, ["Full Name", "FullName", "Full_Name", "Full name", "FullName"])
    if not full_name_col:
        # fallback build from First/Last
        first_c = _pick_first_column_match(active_df, ["First Name", "FirstName", "First"])
        last_c = _pick_first_column_match(active_df, ["Last Name", "LastName", "Last"])
        if first_c and last_c:
            active_df["__full_name__"] = active_df[first_c].fillna("").astype(str).str.strip() + " " + active_df[last_c].fillna("").astype(str).str.strip()
            full_name_col = "__full_name__"

    # other common enrichment columns (try many variants)
    business_col = _pick_first_column_match(active_df, ["Business Title", "Business_Title", "Job Title", "JobTitle", "BusinessTitle", "Business Title"])
    manager_col = _pick_first_column_match(active_df, ["Manager Name", "Manager_Name", "Manager", "Manager's Name", "ManagerName"])
    n1_col = _pick_first_column_match(active_df, ["N1_Sup_Organization", "Reporting Level 1 Name", "Reporting Level 1", "Supervisory Organization", "SupervisoryOrganization"])
    loc_col = _pick_first_column_match(active_df, ["Location Description", "Location_Description", "Location Description", "Location"])
    status_col = _pick_first_column_match(active_df, ["Current Status", "Current_Status", "Employee Status", "Status"])
    hire_col = _pick_first_column_match(active_df, ["Hire Date", "Hire_Date", "HireDate", "Hire date"])
    # keep also any other email/manager email if needed
    for idx, row in active_df.iterrows():
        try:
            rec = {}
            if id_col:
                eid = row.get(id_col)
                if pd.notna(eid):
                    eid = str(eid).strip()
                else:
                    eid = None
            else:
                eid = None
            if full_name_col:
                fname = row.get(full_name_col)
                fname = None if pd.isna(fname) else str(fname).strip()
            else:
                fname = None
            # enrichment fields
            rec["Business_Title"] = None if business_col is None or pd.isna(row.get(business_col)) else str(row.get(business_col)).strip()
            rec["Manager_Name"] = None if manager_col is None or pd.isna(row.get(manager_col)) else str(row.get(manager_col)).strip()
            rec["N1_Sup_Organization"] = None if n1_col is None or pd.isna(row.get(n1_col)) else str(row.get(n1_col)).strip()
            rec["Location_Description"] = None if loc_col is None or pd.isna(row.get(loc_col)) else str(row.get(loc_col)).strip()
            rec["Current_Status"] = None if status_col is None or pd.isna(row.get(status_col)) else str(row.get(status_col)).strip()
            rec["Hire_Date"] = None if hire_col is None or pd.isna(row.get(hire_col)) else _to_date(row.get(hire_col))
            rec["Full_Name"] = fname
            rec["EmployeeID"] = eid
            # store by id and by name lower-case
            if eid:
                out["__id__"][str(eid).strip()] = rec
            if fname:
                out["__name__"][fname.strip().lower()] = rec
        except Exception:
            continue
    return out

def _date_label(d: date) -> str:
    # Format like "1-Jan-25"
    return d.strftime("%-d-%b-%y") if hasattr(d, "strftime") and ("% -d" in "%-d") else d.strftime("%-d-%b-%y") if False else d.strftime("%-d-%b-%y")  # placeholder for cross-platform
    # NOTE: above is to ensure single-digit day (on linux). If running on windows where %-d not supported, we'll fallback below.

def _date_label_safe(d: date) -> str:
    # cross platform single-digit day formatting
    try:
        # try %-d first (works on unix)
        return d.strftime("%-d-%b-%y")
    except Exception:
        # fallback to strip leading zero from %d
        s = d.strftime("%d-%b-%y")
        if s.startswith("0"):
            s = s[1:]
        return s




def generate_monthly_denver_report(year: int = None, month: int = None, start_date: date = None, end_date: date = None,
                                   outdir: str = None, city_filter: str = "Denver", region: str = "namer") -> str:
    """
    Generate the Denver monthly attendance report Excel file.
    Accepts either year+month or start_date+end_date. Returns path to the xlsx file.
    This version ensures we include *any* employee who physically swiped at a Denver door,
    even if their primary location/PartitionName2 is not Denver.
    """
    if start_date is not None or end_date is not None:
        start_date = _to_date(start_date)
        end_date = _to_date(end_date)

    if (start_date is None or end_date is None):
        if year is None or month is None:
            raise ValueError("Either (year and month) or (start_date and end_date) must be provided")
        start_date, end_date = _month_date_range(int(year), int(month))

    if not outdir:
        outdir = Path.cwd() / "output"
    outdir = Path(outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    # build full date list
    current = start_date
    dates = []
    while current <= end_date:
        dates.append(current)
        current = current + timedelta(days=1)

    if not dates:
        raise ValueError("Empty date range")

    # helper: decide if a swipe row is a Denver visit based on Door/PartitionName2/PrimaryLocation
    def _is_denver_visit_row(row) -> bool:
        try:
            # combine a few fields that might indicate the door/location
            door = (row.get("Door") or "") if isinstance(row, dict) else (row.get("Door") if "Door" in row else "")
            partition = (row.get("PartitionName2") or "") if isinstance(row, dict) else (row.get("PartitionName2") if "PartitionName2" in row else "")
            prim = (row.get("PrimaryLocation") or "") if isinstance(row, dict) else (row.get("PrimaryLocation") if "PrimaryLocation" in row else "")
            # lowercase join
            s = " ".join([str(door), str(partition), str(prim)]).lower()
            # check mapping/keywords: user mapping used '%HQ%' -> Denver; include direct 'denver' and abbreviations
            keywords = ["denver", "hq", " hq ", " den ", "den."]
            for kw in keywords:
                if kw in s:
                    return True
            # also match common variants like 'co-denver' or 'denver - w'
            if "denver -" in s or "denver " in s:
                return True
            return False
        except Exception:
            return False

    # collect durations for each date from (A) normal namer run and (B) supplemental scans of all regions for door hits
    all_durations = []

    # regions to scan for supplemental door-based visits
    regions_to_scan = list(getattr(duration_report, "REGION_CONFIG", {}).keys()) if duration_report and hasattr(duration_report, "REGION_CONFIG") else ["apac","emea","laca","namer"]

    for d in dates:
        # 1) primary (fast path) - existing behavior: run_for_date for the 'namer' region (this will include Denver-native people)
        try:
            res = duration_report.run_for_date(d, regions=[region], outdir=str(outdir), city=city_filter)
            region_obj = res.get(region, {}) if isinstance(res, dict) else {}
            durations_df = region_obj.get("durations") if isinstance(region_obj, dict) else None
            if isinstance(durations_df, pd.DataFrame) and not durations_df.empty:
                durations_df['Date'] = pd.to_datetime(durations_df['Date']).dt.date
                all_durations.append(durations_df)
        except Exception:
            logger.exception("Primary run_for_date failed for date %s", d)

        # 2) supplemental: scan ALL regions and pick swipe rows that indicate a Denver door
        #    use fetch_swipes_for_region (lighter-weight) + compute_daily_durations()
        for r in regions_to_scan:
            try:
                swipes = duration_report.fetch_swipes_for_region(r, d)
            except Exception:
                logger.exception("fetch_swipes_for_region failed for region %s date %s", r, d)
                swipes = pd.DataFrame()

            if swipes is None or swipes.empty:
                continue

            # normalize column names and ensure door/partition/primary exist
            for c in ("Door", "PartitionName2", "PrimaryLocation", "EmployeeName"):
                if c not in swipes.columns:
                    swipes[c] = None

            # filter swipes to only those that look like Denver visits (based on door/partition/primary)
            try:
                # run row-wise check (vectorize minimal)
                mask = swipes.apply(lambda row: _is_denver_visit_row(row), axis=1)
                denver_swipes = swipes[mask].copy()
            except Exception:
                denver_swipes = swipes  # fallback: if failure, be conservative and keep them (better to include than miss)
            
            if denver_swipes is None or denver_swipes.empty:
                continue

            # compute durations for the filtered swipes (this yields person_uid, DurationSeconds, Date, etc.)
            try:
                durations_df = duration_report.compute_daily_durations(denver_swipes)
                if isinstance(durations_df, pd.DataFrame) and not durations_df.empty:
                    # ensure Date is date type
                    if 'Date' in durations_df.columns:
                        durations_df['Date'] = pd.to_datetime(durations_df['Date']).dt.date
                    else:
                        durations_df['Date'] = d
                    all_durations.append(durations_df)
            except Exception:
                logger.exception("compute_daily_durations failed for supplemental denver swipes region=%s date=%s", r, d)
                continue

    # concat and dedupe person/date to avoid double-counting (keep first occurrence)
    if not all_durations:
        mon_label = f"{start_date.strftime('%Y%m')}_{end_date.strftime('%Y%m')}"
        filename = outdir / f"denver_attendance_{mon_label}.xlsx"
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            pd.DataFrame([{"Note": f"No durations found for {start_date.isoformat()} -> {end_date.isoformat()}"}]).to_excel(writer, sheet_name="Summary", index=False)
        return str(filename)

    df_all = pd.concat(all_durations, ignore_index=True, sort=False)

    # normalize Date & person_uid and dedupe by person_uid+Date (keep first)
    if "Date" in df_all.columns:
        df_all["Date"] = pd.to_datetime(df_all["Date"]).dt.date
    else:
        # should not happen but set to start_date
        df_all["Date"] = start_date

    if "person_uid" not in df_all.columns:
        def make_person_uid(row):
            eid = row.get("EmployeeIdentity")
            if pd.notna(eid) and str(eid).strip():
                return str(eid).strip()
            parts = []
            for k in ("EmployeeID", "CardNumber", "EmployeeName"):
                v = row.get(k)
                if pd.notna(v) and str(v).strip():
                    parts.append(str(v).strip())
            return "|".join(parts) or None
        df_all["person_uid"] = df_all.apply(make_person_uid, axis=1)

    # drop duplicates keeping first occurrence (this prevents the same person/date appearing twice)
    df_all = df_all.drop_duplicates(subset=["person_uid", "Date"], keep="first").reset_index(drop=True)




    # Normalize DurationSeconds
    if "DurationSeconds" not in df_all.columns:
        if "FirstSwipe" in df_all.columns and "LastSwipe" in df_all.columns:
            try:
                df_all["DurationSeconds"] = (pd.to_datetime(df_all["LastSwipe"]) - pd.to_datetime(df_all["FirstSwipe"])).dt.total_seconds().clip(lower=0)
            except Exception:
                df_all["DurationSeconds"] = None
        else:
            df_all["DurationSeconds"] = None

    # Normalize PersonnelType column name
    if "PersonnelTypeName" in df_all.columns and "PersonnelType" not in df_all.columns:
        df_all = df_all.rename(columns={"PersonnelTypeName": "PersonnelType"})
    if "PersonnelType" not in df_all.columns:
        df_all["PersonnelType"] = None

    # FILTER: keep only Employees and Terminated Personnel
    def _is_employee_or_terminated(v):
        try:
            if v is None:
                return False
            s = str(v).strip().lower()
            if "employee" in s:
                return True
            if "terminated" in s and "personnel" in s:
                return True
            if s == "terminated personnel" or s == "terminated":
                return True
            return False
        except Exception:
            return False

    df_all = df_all[df_all["PersonnelType"].apply(_is_employee_or_terminated)].copy()

    # Build pivot (one row per person_uid, columns per date -> DurationSeconds)
    df_all["DurationSeconds"] = pd.to_numeric(df_all["DurationSeconds"], errors="coerce")
    pivot_secs = df_all.pivot_table(index="person_uid", columns="Date", values="DurationSeconds", aggfunc="first")

    # ensure all requested dates present
    for d in dates:
        if d not in pivot_secs.columns:
            pivot_secs[d] = pd.NA
    pivot_secs = pivot_secs.reindex(sorted(pivot_secs.columns), axis=1)

    # Representative meta (first values) for each person
    agg_first = df_all.groupby("person_uid", sort=False).agg({
        "EmployeeName": "first",
        "EmployeeID": "first",
        "PersonnelType": "first",
        "CardNumber": "first",
        "PartitionName2": "first",
        "PrimaryLocation": "first",
        "FirstSwipe": "first",
        "LastSwipe": "last"
    }).rename(columns={
        "EmployeeName": "EmployeeName",
        "EmployeeID": "EmployeeID"
    })

    # Days present / Days Ge8 / DaysInMonth
    days_present = (pivot_secs.notna() & (pivot_secs.astype(float) > 0)).sum(axis=1)
    days_ge8 = (pivot_secs.astype(float) >= 28800).sum(axis=1)  # >= 8 hours
    days_in_range = len(dates)

    agg_first["DaysPresent"] = days_present.fillna(0).astype(int)
    agg_first["DaysGe8"] = days_ge8.fillna(0).astype(int)
    agg_first["DaysInMonth"] = int(days_in_range)

    # Build enrichment map from active employees file (best-effort)
    active_df = _load_active_employees_df()
    enrichment = _build_enrichment_map(active_df) if active_df is not None else {"__id__": {}, "__name__": {}}

    # Build Summary DataFrame columns as requested (mapping/enriching)
    summary_rows = []
    for uid, meta in agg_first.reset_index().set_index("person_uid").to_dict(orient="index").items():
        try:
            empid = meta.get("EmployeeID") or ""
            empname = meta.get("EmployeeName") or ""
            personnel_type = meta.get("PersonnelType") or ""
            days_present_val = int(meta.get("DaysPresent") or 0)
            # enrichment lookup
            enrich_rec = None
            if empid and str(empid).strip() in enrichment.get("__id__", {}):
                enrich_rec = enrichment["__id__"].get(str(empid).strip())
            elif empname and str(empname).strip().lower() in enrichment.get("__name__", {}):
                enrich_rec = enrichment["__name__"].get(str(empname).strip().lower())

            business = enrich_rec.get("Business_Title") if enrich_rec else None
            manager = enrich_rec.get("Manager_Name") if enrich_rec else None
            n1_org = enrich_rec.get("N1_Sup_Organization") if enrich_rec else None
            loc_desc = enrich_rec.get("Location_Description") if enrich_rec else None
            current_status = enrich_rec.get("Current_Status") if enrich_rec else None
            hire_date = enrich_rec.get("Hire_Date") if enrich_rec else None

            row = {
                "Employee ID": empid,
                "Full_Name": empname,
                "Personnel Type": personnel_type,
                "Business_Title": business,
                "Manager_Name": manager,
                "N1_Sup_Organization": n1_org,
                "Location_Description": loc_desc or meta.get("PrimaryLocation"),
                "Current_Status": current_status,
                "Report Start Date": start_date.isoformat(),
                "Report End Date": end_date.isoformat(),
                "Hire_Date": hire_date.isoformat() if isinstance(hire_date, (date, datetime)) else (hire_date if hire_date else None),
                "Actual_No_Of_Days_Attended": days_present_val
            }
            summary_rows.append(row)
        except Exception:
            logger.exception("Failed building summary row for uid=%s", uid)
            continue

    summary_df = pd.DataFrame(summary_rows, columns=[
        "Employee ID", "Full_Name", "Personnel Type", "Business_Title", "Manager_Name", "N1_Sup_Organization",
        "Location_Description", "Current_Status", "Report Start Date", "Report End Date", "Hire_Date", "Actual_No_Of_Days_Attended"
    ])

    # Build Attendance sheet: Emp ID, Emp Name, date cols (formatted), monthly totals
    # Start with pivot_secs -> binary presence (1/0)
    presence_df = pivot_secs.copy()
    # binary: 1 if >0 else 0 (treat NaN as 0)
    presence_df = presence_df.applymap(lambda v: 1 if (pd.notna(v) and float(v) > 0) else 0)
    # ensure index aligns with agg_first
    presence_df = presence_df.reindex(agg_first.index)

    # rename date columns to required format (e.g. 1-Jan-25)
    date_labels = [_date_label_safe(d) for d in presence_df.columns]
    col_map = dict(zip(presence_df.columns, date_labels))
    presence_df.columns = [col_map[c] for c in presence_df.columns]

    # compute month totals: group dates by month label
    month_groups = {}
    for d in dates:
        mon_label = d.strftime("%b")  # e.g. Jan
        lbl = _date_label_safe(d)
        month_groups.setdefault(mon_label, []).append(lbl)

    # For final attendance frame, create index columns Emp ID and Emp Name
    att_index = pd.DataFrame({
        "Emp ID": agg_first["EmployeeID"].fillna("").astype(str),
        "Emp Name": agg_first["EmployeeName"].fillna("").astype(str)
    }, index=presence_df.index)

    # attach presence columns
    attendance_df = pd.concat([att_index.reset_index(drop=True), presence_df.reset_index(drop=True)], axis=1)

    # add monthly totals columns (e.g. "Jan Total") computed as sum across that month's date columns
    for mon, cols in month_groups.items():
        try:
            attendance_df[f"{mon} Total"] = attendance_df[cols].sum(axis=1).astype(int)
        except Exception:
            attendance_df[f"{mon} Total"] = 0

    # also compute overall monthly totals in summary (if you want it there)
    # Save to Excel
    filename = outdir / f"denver_attendance_{mon_label}.xlsx"
    try:
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # Summary sheet
            summary_df.to_excel(writer, sheet_name="Summary", index=False)

            # Attendance sheet
            # Ensure columns order: Emp ID, Emp Name, all date cols in chronological order, then month totals
            date_col_order = [ _date_label_safe(d) for d in sorted(dates) ]
            month_totals_order = [f"{m} Total" for m in sorted(month_groups.keys(), key=lambda m: datetime.strptime(m, "%b").month)]
            attendance_cols = ["Emp ID", "Emp Name"] + date_col_order + month_totals_order
            # Defensive: ensure only existing columns are used
            attendance_cols = [c for c in attendance_cols if c in attendance_df.columns]
            attendance_df.to_excel(writer, sheet_name="Attendance", index=False, columns=attendance_cols)

            # Optional: include DurationsRaw if helpful
            try:
                tidy = df_all[["person_uid", "Date", "DurationSeconds", "Duration"]].copy()
                tidy = tidy.rename(columns={"person_uid": "PersonUID"})
                tidy = tidy.sort_values(["PersonUID", "Date"])
                tidy.to_excel(writer, sheet_name="DurationsRaw", index=False)
            except Exception:
                logger.debug("Could not write DurationsRaw sheet (non-fatal)", exc_info=True)
        logger.info("Wrote Denver attendance report to %s", filename)
    except Exception:
        logger.exception("Failed writing Excel report")
        raise

    return str(filename)

