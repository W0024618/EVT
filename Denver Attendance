Now strickly use below Query carefully ..
and Built Denver attendance Report from jan 1 2025 to current date 

/* Combined Headcount - updated with Denver / PrimaryLocation HQ filter
   - EmployeeName (ObjectName1), DoorName (ObjectName2), PrimaryLocation (Text5)
   - Filter PersonnelType to Employee OR Terminated Personnel (case-insensitive)
   - Location filter: LogicalLocation = 'Denver' OR (PrimaryLocation contains 'denver' AND contains 'hq')
   - Add additional DB names to @DBList if you want to scan more DBs
*/

SET NOCOUNT ON;
GO

DECLARE @targetDate DATE = '2025-09-09';  -- adjust as required

-- ---------- List of databases (add more if needed) ----------
DECLARE @DBList TABLE (db SYSNAME);
INSERT INTO @DBList (db) VALUES
 ('ACVSUJournal_00010021');  -- add other ACVSUJournal_* DB names here as additional rows

-- ---------- Combined list of PartitionName2 values you care about (kept for earlier broad filtering if needed) ----------
DECLARE @PartitionList NVARCHAR(MAX) =
    '''LT.Vilnius'',''AUT.Vienna'',''IE.DUblin'',''DU.Abu Dhab'',''ES.Madrid'',''IT.Rome'',''MA.Casablanca'',''RU.Moscow'',''UK.London'','
  + '''APAC.Default'',''IN.HYD'',''JP.Tokyo'',''PH.Manila'',''MY.Kuala Lumpur'','
  + '''AR.Cordoba'',''BR.Sao Paulo'',''CR.Costa Rica Partition'',''MX.Mexico City'',''PA.Panama City'',''PE.Lima''';

-- ---------- Drop temp table if left over ----------
IF OBJECT_ID('tempdb..#CombinedEmployeeData') IS NOT NULL
    DROP TABLE #CombinedEmployeeData;

-- Create temp table explicitly
CREATE TABLE #CombinedEmployeeData (
    SourceDB SYSNAME,
    ObjectName1 NVARCHAR(255),   -- EmployeeName source
    ObjectName2 NVARCHAR(255),   -- DoorName source
    EmployeeID NVARCHAR(200),
    PersonnelTypeID INT,
    PersonnelTypeName NVARCHAR(255),
    Text5 NVARCHAR(255),         -- PrimaryLocation source
    PartitionName2 NVARCHAR(255),
    LocaleMessageTime DATETIME2,
    MessageType NVARCHAR(100),
    EmployeeIdentity NVARCHAR(200),
    CardNumber NVARCHAR(200),
    LogicalLocation NVARCHAR(255)
);

-- ---------- Build dynamic SQL that inserts from each DB ----------
DECLARE @sql NVARCHAR(MAX) = N'';
DECLARE @db SYSNAME;

DECLARE db_cursor CURSOR FAST_FORWARD FOR
    SELECT db FROM @DBList;

OPEN db_cursor;
FETCH NEXT FROM db_cursor INTO @db;

WHILE @@FETCH_STATUS = 0
BEGIN
    SET @sql = @sql + N'
    INSERT INTO #CombinedEmployeeData (
        SourceDB, ObjectName1, ObjectName2, EmployeeID, PersonnelTypeID, PersonnelTypeName,
        Text5, PartitionName2, LocaleMessageTime, MessageType, EmployeeIdentity, CardNumber, LogicalLocation
    )
    SELECT
        N' + QUOTENAME(@db,'''') + N' AS SourceDB,
        t1.[ObjectName1],
        t1.[ObjectName2],
        CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR(200)) END AS EmployeeID,
        t2.[PersonnelTypeID],
        t3.[Name] AS PersonnelTypeName,
        t2.[Text5],
        t1.[PartitionName2],
        DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
        t1.[MessageType],
        CAST(t1.ObjectIdentity1 AS NVARCHAR(200)) AS EmployeeIdentity,
        NULL AS CardNumber,
        CASE
            WHEN t1.[ObjectName2] LIKE ''%HQ%'' THEN ''Denver''
            WHEN t1.[ObjectName2] LIKE ''%Austin%'' THEN ''Austin''
            WHEN t1.[ObjectName2] LIKE ''%Miami%'' THEN ''Miami''
            WHEN t1.[ObjectName2] LIKE ''%NYC%'' THEN ''New York''
            WHEN t1.[ObjectName2] LIKE ''APAC_PI%'' THEN ''Taguig City''
            WHEN t1.[ObjectName2] LIKE ''APAC_PH%'' THEN ''Quezon City''
            WHEN t1.[ObjectName2] LIKE ''%PUN%'' THEN ''Pune''
            WHEN t1.[ObjectName2] LIKE ''%HYD%'' THEN ''Hyderabad''
            ELSE t1.[PartitionName2]
        END AS LogicalLocation
    FROM ' + QUOTENAME(@db) + N'.dbo.ACVSUJournalLog AS t1
    INNER JOIN [ACVSCore].[Access].[Personnel] AS t2
        ON t1.ObjectIdentity1 = t2.GUID
    INNER JOIN [ACVSCore].[Access].[PersonnelType] AS t3
        ON t2.PersonnelTypeID = t3.ObjectID
    WHERE
        t1.MessageType = ''CardAdmitted''
        AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = ' + QUOTENAME(CONVERT(NVARCHAR(10), @targetDate, 120),'''') + N'
        AND (
            t1.PartitionName2 IN (' + @PartitionList + N')
            OR t1.ObjectName2 LIKE ''%HQ%'' OR t1.ObjectName2 LIKE ''%Austin%'' OR t1.ObjectName2 LIKE ''%Miami%'' OR t1.ObjectName2 LIKE ''%NYC%'' 
            OR t1.ObjectName2 LIKE ''APAC_PI%'' OR t1.ObjectName2 LIKE ''APAC_PH%'' OR t1.ObjectName2 LIKE ''%PUN%'' OR t1.ObjectName2 LIKE ''%HYD%''
        );
    ';

    FETCH NEXT FROM db_cursor INTO @db;
END

CLOSE db_cursor;
DEALLOCATE db_cursor;

-- Execute the dynamic insert SQL
EXEC sp_executesql @sql;

-- ---------- De-duplicate & pick latest swipe per person per date ----------
;WITH LatestPerPerson AS (
    SELECT
        SourceDB,
        ObjectName1,
        ObjectName2,
        EmployeeID,
        PersonnelTypeID,
        PersonnelTypeName,
        Text5,
        PartitionName2,
        LocaleMessageTime,
        MessageType,
        EmployeeIdentity,
        CardNumber,
        LogicalLocation,
        CONVERT(DATE, LocaleMessageTime) AS [DateOnly],
        ROW_NUMBER() OVER (
            PARTITION BY 
              COALESCE(NULLIF(EmployeeIdentity, ''), NULLIF(EmployeeID, ''), ObjectName1),
              CONVERT(DATE, LocaleMessageTime)
            ORDER BY LocaleMessageTime DESC
        ) AS rn
    FROM #CombinedEmployeeData
)
SELECT
   
    ObjectName1    AS EmployeeName,
    ObjectName2    AS DoorName,
    PersonnelTypeName AS PersonnelType,
    EmployeeID,
    Text5          AS PrimaryLocation,
    PartitionName2,
    LogicalLocation,
    MessageType,
    [DateOnly]     AS [Date],
    LocaleMessageTime
FROM LatestPerPerson
WHERE rn = 1
    -- Personnel type filter (case-insensitive): keep only Employee and Terminated Personnel
    AND LOWER(RTRIM(LTRIM(PersonnelTypeName))) IN ('employee','terminated personnel')
    -- LOCATION filter: either logical 'Denver' OR PrimaryLocation contains both 'denver' and 'hq' (case-insensitive)
    AND (
         LogicalLocation = 'Denver'
         OR (
             Text5 IS NOT NULL
             AND LOWER(Text5) LIKE '%denver%'
             AND LOWER(Text5) LIKE '%hq%'         -- covers 'WU HQ', 'WU-HQ', 'HQ', etc.
         )
    )
ORDER BY LogicalLocation, PersonnelTypeName, ObjectName1;

-- cleanup
IF OBJECT_ID('tempdb..#CombinedEmployeeData') IS NOT NULL
    DROP TABLE #CombinedEmployeeData;

GO



use last 4 database 21, 20 , 19, 18 
amd make Denver attendance report carefully....



Remove all Unnecessary logic from below File carefully 


# denverAttendance.py
"""
Denver attendance report (presence-only).

Notes:
- Presence logic: one unique row per person per date (GUID preferred).
- This file intentionally uses a local fallback region config (no import of duration_report).
- Improvements: connection retries (tcp / port), SQLAlchemy fallback for pd.read_sql to avoid pandas warning,
  corrected EMEA server name, clearer logging.
"""

from datetime import date, datetime, timedelta
from pathlib import Path
import pandas as pd
import logging
import re
import os
from typing import Optional, Tuple, Dict, Any, List

# logger = logging.getLogger("denverAttendance")
# logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

# Use the application logger name so all attendance logs use the same handler/format
# Do NOT call logging.basicConfig() from library modules.
logger = logging.getLogger("attendance_app")

# ODBC driver fallback (mirrors duration_report's default)
ODBC_DRIVER = os.getenv("ODBC_DRIVER", "ODBC Driver 17 for SQL Server")


GENERIC_SQL_TEMPLATE = r"""
SELECT
    t1.[ObjectName1] AS EmployeeName,
    t1.[ObjectName2] AS Door,
    CASE WHEN t2.[Int1] = 0 THEN t2.[Text12] ELSE CAST(t2.[Int1] AS NVARCHAR) END AS EmployeeID,
    COALESCE(
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID/Card)[1]','varchar(50)'),
      TRY_CAST(t_xml.XmlMessage AS XML).value('(/LogMessage/CHUID)[1]','varchar(50)'),
      sc.value,
      t2.[Text12]
    ) AS CardNumber,
    t3.[Name] AS PersonnelTypeName,
    t1.ObjectIdentity1 AS EmployeeIdentity,
    t1.PartitionName2,
    DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC]) AS LocaleMessageTime,
    t1.MessageType,
    t5d.value AS Direction,
    t2.Text4 AS CompanyName,
    t2.Text5 AS PrimaryLocation
FROM [{db}].dbo.ACVSUJournalLog AS t1
INNER JOIN ACVSCore.Access.Personnel AS t2 ON t1.ObjectIdentity1 = t2.GUID
INNER JOIN ACVSCore.Access.PersonnelType AS t3 ON t2.PersonnelTypeID = t3.ObjectID
LEFT JOIN [{db}].dbo.ACVSUJournalLogxmlShred t5d
  ON t1.XmlGUID = t5d.GUID AND t5d.Value IN ('InDirection','OutDirection')
LEFT JOIN [{db}].dbo.ACVSUJournalLogxml t_xml
  ON t1.XmlGUID = t_xml.GUID
LEFT JOIN (
  SELECT GUID, value
  FROM [{db}].dbo.ACVSUJournalLogxmlShred
  WHERE Name IN ('Card','CHUID')
) AS sc
  ON t1.XmlGUID = sc.GUID
WHERE t1.MessageType = 'CardAdmitted'
  AND CONVERT(DATE, DATEADD(MINUTE, -1 * t1.[MessageLocaleOffset], t1.[MessageUTC])) = '{date}'
  {region_filter}
"""

# fallback region config: corrected EMEA server to match duration_report's config
FALLBACK_REGION_CONFIG = {
    "namer": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUDEN0891V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 5,
        "logical_like": ["%HQ%", "%Austin%", "%Miami%", "%NYC%"]
    },
    "apac": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUPNQ0986V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 5,
        "partitions": []
    },
    # <-- FIXED server entry here (was SRVWUFRA0981V in older fallback)
    "emea": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUFRA0986V",
        "database": "ACVSUJournal_00011028",
        "last_n_databases": 5,
        "partitions": []
    },
    "laca": {
        "user": "GSOC_Test",
        "password": "Westernccure@2025",
        "server": "SRVWUSJO0986V",
        "database": "ACVSUJournal_00010029",
        "last_n_databases": 5,
        "partitions": []
    }
}

# -------------------------
# DB discovery / helpers
# -------------------------
def _split_db_name(dbname: str):
    m = re.match(r"^(.*?)(\d+)$", dbname)
    if not m:
        return dbname, None
    return m.group(1), m.group(2)

def _expand_databases_from_base(db_base: str, last_n: int) -> List[str]:
    prefix, digits = _split_db_name(db_base)
    if digits is None:
        return [db_base]
    width = len(digits)
    try:
        cur = int(digits)
    except Exception:
        return [db_base]
    out = []
    for i in range(last_n):
        num = cur - i
        if num < 0:
            break
        out.append(f"{prefix}{str(num).zfill(width)}")
    return out

def _get_candidate_databases(rc: Dict[str, Any]) -> List[str]:
    if "databases" in rc and isinstance(rc["databases"], list) and rc["databases"]:
        return rc["databases"]
    base_db = rc.get("database")
    if not base_db:
        return []
    last_n = int(rc.get("last_n_databases", 1) or 1)
    if last_n <= 1:
        return [base_db]
    return _expand_databases_from_base(base_db, last_n)

def _connect_master(rc: Dict[str, Any]):
    """
    Try to open a connection to 'master' DB for the given server.
    Try several server formats (raw, tcp:..., server,1433) to be tolerant of network config.
    Returns pyodbc connection or None.
    """
    try:
        import pyodbc
    except Exception:
        logger.debug("pyodbc not available; cannot connect to master for DB discovery.")
        return None

    server = rc.get("server")
    user = rc.get("user")
    pwd = rc.get("password")
    candidates = [
        server,
        f"tcp:{server}",
        f"{server},1433",
        f"tcp:{server},1433"
    ]
    for srv in candidates:
        try:
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={srv};DATABASE=master;UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;Connection Timeout=5;"
            )
            logger.debug("Attempting master connection to %s", srv)
            conn = pyodbc.connect(conn_str, autocommit=True)
            logger.info("Connected to master on %s", srv)
            return conn
        except Exception as exc:
            logger.debug("Master connection attempt to %s failed: %s", srv, exc)
            continue
    logger.exception("Failed to connect to master DB for server %s after multiple attempts", server)
    return None

def _filter_existing_databases(rc: Dict[str, Any], candidates: List[str]) -> List[str]:
    if not candidates:
        return []
    master_conn = _connect_master(rc)
    if master_conn is None:
        logger.warning("Unable to validate DB existence (no master connection). Proceeding with candidate list: %s", candidates)
        return candidates
    try:
        exists = []
        cursor = master_conn.cursor()
        for db in candidates:
            try:
                cursor.execute("SELECT COUNT(1) FROM sys.databases WHERE name = ?", (db,))
                row = cursor.fetchone()
                if row and row[0] and int(row[0]) > 0:
                    exists.append(db)
            except Exception:
                logger.exception("Error checking existence for database %s", db)
        cursor.close()
        logger.info("Databases present for server %s: %s", rc.get("server"), exists)
        return exists if exists else candidates
    finally:
        try:
            master_conn.close()
        except Exception:
            pass

def _build_region_query_local(region_key: str, target_date: date, rcfgs: Dict[str, Any]) -> str:
    rc = rcfgs.get(region_key, {})
    date_str = target_date.strftime("%Y-%m-%d")
    region_filter = ""

    if region_key in ("apac", "emea", "laca"):
        partitions = rc.get("partitions", []) or []
        if partitions:
            parts_sql = ", ".join(f"'{p}'" for p in partitions)
            region_filter = f"AND t1.PartitionName2 IN ({parts_sql})"
    elif region_key == "namer":
        likes = rc.get("logical_like", []) or []
        if likes:
            like_sql = " OR ".join(f"t1.[ObjectName2] LIKE '{p}'" for p in likes)
            region_filter = f"AND ({like_sql})"
    else:
        region_filter = ""

    candidates = _get_candidate_databases(rc)
    if not candidates:
        candidates = [rc.get("database")] if rc.get("database") else []

    valid_dbs = _filter_existing_databases(rc, candidates) if candidates else []

    union_parts = []
    for dbname in valid_dbs:
        union_parts.append(GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter))

    if not union_parts and rc.get("database"):
        dbname = rc.get("database")
        return GENERIC_SQL_TEMPLATE.format(db=dbname, date=date_str, region_filter=region_filter)

    sql = "\nUNION ALL\n".join(union_parts)
    return sql

# -------------------------
# Fetch using pyodbc (with retries) + pd.read_sql
# -------------------------
def _try_pyodbc_connection(rc: Dict[str, Any]) -> Optional[Any]:
    """
    Return a pyodbc connection object, trying several server representations.
    """
    try:
        import pyodbc
    except Exception:
        logger.debug("pyodbc not installed")
        return None

    server = rc.get("server")
    user = rc.get("user")
    pwd = rc.get("password")
    candidates = [
        server,
        f"tcp:{server}",
        f"{server},1433",
        f"tcp:{server},1433"
    ]
    for srv in candidates:
        try:
            conn_str = (
                f"DRIVER={{{ODBC_DRIVER}}};"
                f"SERVER={srv};DATABASE={rc.get('database')};UID={user};PWD={pwd};"
                "TrustServerCertificate=Yes;Connection Timeout=5;"
            )
            logger.debug("Attempting pyodbc connection to %s", srv)
            conn = pyodbc.connect(conn_str, autocommit=True)
            logger.info("pyodbc connected to %s", srv)
            return conn
        except Exception as exc:
            logger.debug("pyodbc attempt to %s failed: %s", srv, exc)
            continue
    logger.warning("All pyodbc connection attempts failed for server %s", server)
    return None

def _fetch_swipes_using_sql(rc: Dict[str, Any], sql: str) -> pd.DataFrame:
    """
    Execute `sql` against rc's server/database and return DataFrame.
    Tries: (1) SQLAlchemy engine (if available) using ODBC connect string; (2) pyodbc connection attempts.
    """
    # try SQLAlchemy first (quietly) - pandas will accept an engine and avoid the DBAPI warning
    try:
        from urllib.parse import quote_plus
        from sqlalchemy import create_engine
        # Build ODBC connection string and create engine (if SQLAlchemy + pyodbc installed)
        odbc_str = f"DRIVER={{{ODBC_DRIVER}}};SERVER={rc.get('server')};DATABASE={rc.get('database')};UID={rc.get('user')};PWD={rc.get('password')};TrustServerCertificate=Yes;"
        conn_uri = "mssql+pyodbc:///?odbc_connect=" + quote_plus(odbc_str)
        logger.debug("Attempting SQLAlchemy engine connection to %s", rc.get("server"))
        engine = create_engine(conn_uri, connect_args={"connect_timeout": 5})
        with engine.connect() as conn:
            try:
                df = pd.read_sql(sql, conn)
                logger.info("SQL executed via SQLAlchemy engine for server %s", rc.get("server"))
                return df
            except Exception:
                logger.exception("pd.read_sql via SQLAlchemy engine failed; falling back to pyodbc")
    except Exception:
        logger.debug("SQLAlchemy engine path not available or failed - falling back to pyodbc")

    # Fallback: try pyodbc direct connection attempts
    conn = _try_pyodbc_connection(rc)
    if conn is None:
        logger.warning("No DB connection available for server %s - returning empty DataFrame", rc.get("server"))
        return pd.DataFrame()

    try:
        import warnings as _warnings
        with _warnings.catch_warnings():
            _warnings.filterwarnings("ignore", message="pandas only supports SQLAlchemy connectable")
            df = pd.read_sql(sql, conn)
        logger.info("SQL executed via pyodbc for server %s", rc.get("server"))
    except Exception:
        logger.exception("SQL execution failed for swipes query on server %s", rc.get("server"))
        df = pd.DataFrame()
    finally:
        try:
            conn.close()
        except Exception:
            pass

    return df

# -------------------------
# Public fetch wrapper
# -------------------------
def fetch_swipes_for_region(region_key: str, target_date: date) -> pd.DataFrame:
    """
    Unified fetch function. Uses local FALLBACK_REGION_CONFIG and expands last_n_databases.
    """
    rcfgs = FALLBACK_REGION_CONFIG
    rc = rcfgs.get(region_key, {})
    if not rc:
        logger.error("No region config available for %s", region_key)
        return pd.DataFrame()

    sql = _build_region_query_local(region_key, target_date, rcfgs)
    logger.info("Built fallback SQL for region %s date %s", region_key, target_date)
    df = _fetch_swipes_using_sql(rc, sql)

    # ensure expected columns exist
    cols = [
        "EmployeeName", "Door", "EmployeeID", "CardNumber", "PersonnelTypeName", "EmployeeIdentity",
        "PartitionName2", "LocaleMessageTime", "MessageType", "Direction", "CompanyName", "PrimaryLocation"
    ]
    for c in cols:
        if c not in df.columns:
            df[c] = None
    try:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"], errors="coerce")
    except Exception:
        df["LocaleMessageTime"] = pd.to_datetime(df["LocaleMessageTime"].astype(str), errors="coerce")
    return df[cols]

# -------------------------
# Utility helpers
# -------------------------
def _to_date(v):
    if v is None:
        return None
    if isinstance(v, date):
        return v
    if isinstance(v, datetime):
        return v.date()
    try:
        return datetime.strptime(str(v)[:10], "%Y-%m-%d").date()
    except Exception:
        try:
            return pd.to_datetime(v).date()
        except Exception:
            return None

def _date_label_safe(d: date) -> str:
    try:
        return d.strftime("%-d-%b-%y")
    except Exception:
        s = d.strftime("%d-%b-%y")
        if s.startswith("0"):
            s = s[1:]
        return s

_date_label = _date_label_safe

def _find_active_employee_file() -> Optional[Path]:
    candidates = [
        Path.cwd(),
        Path.cwd() / "data",
        Path(__file__).resolve().parent,
        Path(__file__).resolve().parent / "data",
    ]
    seen = set()
    for d in candidates:
        try:
            if not d.exists():
                continue
            for pattern in ("active_employee.*", "active_employees.*", "active_employee_*.*"):
                for p in d.glob(pattern):
                    if p.is_file() and p not in seen:
                        seen.add(p)
                        return p
        except Exception:
            continue
    return None

def _load_active_employees_df() -> Optional[pd.DataFrame]:
    p = _find_active_employee_file()
    if not p:
        return None
    try:
        if p.suffix.lower() in (".xlsx", ".xls"):
            df = pd.read_excel(p, dtype=str)
        else:
            df = pd.read_csv(p, dtype=str)
        df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]
        return df
    except Exception:
        logger.exception("Failed to load active employee file %s", p)
        return None

def _pick_first_column_match(df: pd.DataFrame, candidates):
    cols = list(df.columns)
    lower_map = {c.lower(): c for c in cols}
    for cand in candidates:
        if not cand:
            continue
        lc = cand.lower()
        if lc in lower_map:
            return lower_map[lc]
        for c in cols:
            if lc in c.lower():
                return c
        try:
            rx = re.compile(cand, re.I)
            for c in cols:
                if rx.search(c):
                    return c
        except Exception:
            pass
    return None

def _build_enrichment_map(active_df: pd.DataFrame) -> Dict[str, Dict]:
    out = {"__id__": {}, "__name__": {}}
    if active_df is None or active_df.empty:
        return out
    id_col = _pick_first_column_match(active_df, ["Employee ID", "EmployeeID", "EmployeeId", "Text12", "employeeid"])
    full_name_col = _pick_first_column_match(active_df, ["Full Name", "FullName", "Full_Name", "Full name", "FullName"])
    if not full_name_col:
        first_c = _pick_first_column_match(active_df, ["First Name", "FirstName", "First"])
        last_c = _pick_first_column_match(active_df, ["Last Name", "LastName", "Last"])
        if first_c and last_c:
            active_df["__full_name__"] = (
                active_df[first_c].fillna("").astype(str).str.strip()
                + " "
                + active_df[last_c].fillna("").astype(str).str.strip()
            )
            full_name_col = "__full_name__"

    business_col = _pick_first_column_match(active_df, ["Business Title", "Business_Title", "Job Title", "JobTitle", "BusinessTitle"])
    manager_col = _pick_first_column_match(active_df, ["Manager Name", "Manager_Name", "Manager", "Manager's Name", "ManagerName"])
    n1_col = _pick_first_column_match(active_df, ["N1_Sup_Organization", "Reporting Level 1 Name", "Reporting Level 1", "Supervisory Organization", "SupervisoryOrganization"])
    loc_col = _pick_first_column_match(active_df, ["Location Description", "Location_Description", "Location Description", "Location"])
    status_col = _pick_first_column_match(active_df, ["Current Status", "Current_Status", "Employee Status", "Status"])
    hire_col = _pick_first_column_match(active_df, ["Hire Date", "Hire_Date", "HireDate", "Hire date"])

    for idx, row in active_df.iterrows():
        try:
            rec = {}
            eid = None
            if id_col:
                v = row.get(id_col)
                if pd.notna(v):
                    eid = str(v).strip()
            fname = None
            if full_name_col:
                v = row.get(full_name_col)
                if pd.notna(v):
                    fname = str(v).strip()
            rec["Business_Title"] = None if business_col is None or pd.isna(row.get(business_col)) else str(row.get(business_col)).strip()
            rec["Manager_Name"] = None if manager_col is None or pd.isna(row.get(manager_col)) else str(row.get(manager_col)).strip()
            rec["N1_Sup_Organization"] = None if n1_col is None or pd.isna(row.get(n1_col)) else str(row.get(n1_col)).strip()
            rec["Location_Description"] = None if loc_col is None or pd.isna(row.get(loc_col)) else str(row.get(loc_col)).strip()
            rec["Current_Status"] = None if status_col is None or pd.isna(row.get(status_col)) else str(row.get(status_col)).strip()
            rec["Hire_Date"] = None if hire_col is None or pd.isna(row.get(hire_col)) else _to_date(row.get(hire_col))
            rec["Full_Name"] = fname
            rec["EmployeeID"] = eid
            if eid:
                out["__id__"][str(eid).strip()] = rec
            if fname:
                out["__name__"][fname.strip().lower()] = rec
        except Exception:
            continue
    return out

# safe Excel writer with CSV fallback
def _write_workbook(filename: Path, sheets: Dict[str, pd.DataFrame]) -> Path:
    try:
        filename.parent.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass

    fn = Path(filename)
    tried = []
    for engine in ("openpyxl", "xlsxwriter"):
        try:
            if fn.suffix.lower() not in (".xlsx", ".xls"):
                fn_xlsx = fn.with_suffix(".xlsx")
            else:
                fn_xlsx = fn
            with pd.ExcelWriter(str(fn_xlsx), engine=engine) as writer:
                for sheet_name, df in (sheets or {}).items():
                    try:
                        if df is None or (isinstance(df, pd.DataFrame) and df.empty):
                            pd.DataFrame([]).to_excel(writer, sheet_name=sheet_name, index=False)
                        else:
                            df.to_excel(writer, sheet_name=sheet_name, index=False)
                    except Exception:
                        try:
                            pd.DataFrame(df).to_excel(writer, sheet_name=sheet_name, index=False)
                        except Exception:
                            continue
            return fn_xlsx
        except Exception as exc:
            tried.append((engine, str(exc)))
            continue

    # CSV fallback
    try:
        csv_path = fn.with_suffix(".csv")
        first_sheet = None
        for name, df in (sheets or {}).items():
            first_sheet = df
            break
        if first_sheet is None:
            pd.DataFrame([{"Note": "Report generation could not create .xlsx (no excel engine); this is a CSV fallback."}]).to_csv(csv_path, index=False)
        else:
            try:
                if isinstance(first_sheet, pd.DataFrame):
                    first_sheet.to_csv(csv_path, index=False)
                else:
                    pd.DataFrame(first_sheet).to_csv(csv_path, index=False)
            except Exception:
                pd.DataFrame([{"Note": "Failed to write sheet to CSV (fallback)."}]).to_csv(csv_path, index=False)
        return csv_path
    except Exception:
        try:
            txt_path = fn.with_suffix(".txt")
            txt_path.write_text("Report generation failed and no writer available.\nTried engines: " + str(tried))
            return txt_path
        except Exception:
            raise

# month date range helper
def _month_date_range(year: int, month: int) -> Tuple[date, date]:
    start = date(int(year), int(month), 1)
    if int(month) == 12:
        end = date(int(year) + 1, 1, 1) - timedelta(days=1)
    else:
        end = date(int(year), int(month) + 1, 1) - timedelta(days=1)
    return start, end

# -------------------------
# Main report generator
# -------------------------
def generate_monthly_denver_report(year: int = None, month: int = None, start_date: date = None, end_date: date = None,
                                   outdir: str = None, city_filter: str = "Denver", region: str = "namer") -> str:
    """
    Generate the Denver monthly attendance report Excel file (presence-only).
    Returns path to the written file.
    """
    # Accept either start/end or year/month
    if start_date is not None or end_date is not None:
        start_date = _to_date(start_date)
        end_date = _to_date(end_date)

    if (start_date is None or end_date is None):
        if year is None or month is None:
            raise ValueError("Either (year and month) or (start_date and end_date) must be provided")
        start_date, end_date = _month_date_range(int(year), int(month))

    mon_label = f"{start_date.strftime('%Y%m')}_{end_date.strftime('%Y%m')}"

    if not outdir:
        outdir = Path.cwd() / "output"
    outdir = Path(outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    # build full date list
    current = start_date
    dates = []
    while current <= end_date:
        dates.append(current)
        current = current + timedelta(days=1)
    if not dates:
        raise ValueError("Empty date range")

    # Denver-matching helper
    def _is_denver_visit_row(row) -> bool:
        try:
            # row can be Series or dict; use .get where possible
            door = row.get("Door", "") if hasattr(row, "get") else (row["Door"] if "Door" in row else "")
            partition = row.get("PartitionName2", "") if hasattr(row, "get") else (row["PartitionName2"] if "PartitionName2" in row else "")
            prim = row.get("PrimaryLocation", "") if hasattr(row, "get") else (row["PrimaryLocation"] if "PrimaryLocation" in row else "")
            s = " ".join([str(door), str(partition), str(prim)]).lower()
            keywords = ["denver", "hq", " us.co.obs", "co.denver"]
            for kw in keywords:
                if kw.strip() and kw in s:
                    return True
            if "denver" in s:
                return True
            return False
        except Exception:
            return False

    # Collect unique DENVER swipes (one row per person per date).
    all_rows = []
    regions_to_scan = list(FALLBACK_REGION_CONFIG.keys())

    def _make_person_uid_row(r):
        try:
            eid = r.get("EmployeeIdentity") if hasattr(r, "get") else (r["EmployeeIdentity"] if "EmployeeIdentity" in r else None)
            if pd.notna(eid) and str(eid).strip():
                return str(eid).strip()
        except Exception:
            pass
        parts = []
        for k in ("EmployeeID", "CardNumber", "EmployeeName"):
            try:
                v = r.get(k) if hasattr(r, "get") else (r[k] if k in r else None)
                if pd.notna(v) and str(v).strip():
                    parts.append(str(v).strip())
            except Exception:
                continue
        return "|".join(parts) or None

    for d in dates:
        for rgn in regions_to_scan:
            try:
                swipes = fetch_swipes_for_region(rgn, d)
            except Exception:
                logger.exception("fetch_swipes_for_region failed for region %s date %s", rgn, d)
                swipes = pd.DataFrame()

            if swipes is None or swipes.empty:
                continue

            # ensure expected columns exist
            for c in ("Door", "PartitionName2", "PrimaryLocation", "EmployeeName", "EmployeeIdentity", "EmployeeID", "CardNumber", "PersonnelTypeName", "LocaleMessageTime"):
                if c not in swipes.columns:
                    swipes[c] = None

            try:
                mask = swipes.apply(lambda row: _is_denver_visit_row(row), axis=1)
                denver_swipes = swipes[mask].copy()
            except Exception:
                denver_swipes = swipes.copy()

            if denver_swipes is None or denver_swipes.empty:
                continue

            # normalize date
            try:
                denver_swipes["LocaleMessageTime"] = pd.to_datetime(denver_swipes.get("LocaleMessageTime"), errors="coerce")
            except Exception:
                denver_swipes["LocaleMessageTime"] = None
            denver_swipes["Date"] = denver_swipes["LocaleMessageTime"].dt.date.fillna(d)

            # compute person_uid (prefer EmployeeIdentity/GUID)
            denver_swipes["person_uid"] = denver_swipes.apply(lambda row: _make_person_uid_row(row), axis=1)
            denver_swipes = denver_swipes[denver_swipes["person_uid"].notna()].copy()
            if denver_swipes.empty:
                continue

            # keep only first unique swipe per person per date
            denver_unique = denver_swipes.drop_duplicates(subset=["person_uid", "Date"], keep="first")

            for _, srow in denver_unique.iterrows():
                all_rows.append({
                    "person_uid": _make_person_uid_row(srow),
                    "EmployeeName": srow.get("EmployeeName") if pd.notna(srow.get("EmployeeName")) else None,
                    "EmployeeID": srow.get("EmployeeID") if pd.notna(srow.get("EmployeeID")) else None,
                    "PersonnelType": srow.get("PersonnelTypeName") if pd.notna(srow.get("PersonnelTypeName")) else None,
                    "CardNumber": srow.get("CardNumber") if pd.notna(srow.get("CardNumber")) else None,
                    "PartitionName2": srow.get("PartitionName2") if pd.notna(srow.get("PartitionName2")) else None,
                    "PrimaryLocation": srow.get("PrimaryLocation") if pd.notna(srow.get("PrimaryLocation")) else None,
                    "Date": srow.get("Date") if not pd.isna(srow.get("Date")) else d,
                    "Presence": 1
                })

    if not all_rows:
        filename = outdir / f"denver_attendance_{mon_label}.xlsx"
        note_df = pd.DataFrame([{"Note": f"No attendance swipes found for {start_date.isoformat()} -> {end_date.isoformat()} (Denver presence logic)"}])
        try:
            written = _write_workbook(filename, {"Summary": note_df})
            return str(written)
        except Exception:
            logger.exception("Failed to write 'no attendance' report fallback")
            raise

    df_all = pd.DataFrame(all_rows)
    try:
        df_all["Date"] = pd.to_datetime(df_all["Date"]).dt.date
    except Exception:
        df_all["Date"] = df_all["Date"].apply(lambda x: _to_date(x) or start_date)

    # Filter: keep only Employees and Terminated Personnel (defensive)
    def _is_employee_or_terminated(v):
        try:
            if v is None:
                return False
            s = str(v).strip().lower()
            if "employee" in s:
                return True
            if "terminated" in s:
                return True
            return False
        except Exception:
            return False

    try:
        if "PersonnelType" in df_all.columns:
            df_all = df_all[df_all["PersonnelType"].apply(_is_employee_or_terminated)].copy()
    except Exception:
        logger.exception("PersonnelType filtering failed; continuing without PersonnelType filter")

    # pivot to presence matrix
    try:
        pivot_presence = df_all.pivot_table(index="person_uid", columns="Date", values="Presence", aggfunc="first").fillna(0)
    except Exception:
        pivot_presence = pd.DataFrame(index=df_all["person_uid"].unique())

    for d in dates:
        if d not in pivot_presence.columns:
            pivot_presence[d] = 0
    try:
        pivot_presence = pivot_presence.reindex(sorted(pivot_presence.columns), axis=1)
    except Exception:
        pass

    # meta
    try:
        agg_first = df_all.groupby("person_uid", sort=False).agg({
            "EmployeeName": "first",
            "EmployeeID": "first",
            "PersonnelType": "first",
            "CardNumber": "first",
            "PartitionName2": "first",
            "PrimaryLocation": "first"
        })
    except Exception:
        agg_first = pd.DataFrame(index=pivot_presence.index)

    # Days present & DaysGe8 (DaysGe8 set = DaysPresent due to no duration)
    try:
        days_present = (pivot_presence.astype(float) > 0).sum(axis=1)
    except Exception:
        days_present = pd.Series(0, index=pivot_presence.index)
    days_ge8 = days_present.copy()
    days_in_range = len(dates)

    agg_first = agg_first.rename(columns={"EmployeeName": "EmployeeName", "EmployeeID": "EmployeeID"})
    agg_first["DaysPresent"] = days_present.fillna(0).astype(int)
    agg_first["DaysGe8"] = days_ge8.fillna(0).astype(int)
    agg_first["DaysInMonth"] = int(days_in_range)

    # enrichment map
    active_df = _load_active_employees_df()
    enrichment = _build_enrichment_map(active_df) if active_df is not None else {"__id__": {}, "__name__": {}}

    # build summary rows
    summary_rows = []
    try:
        for uid, meta in agg_first.reset_index().set_index("person_uid").to_dict(orient="index").items():
            try:
                empid = meta.get("EmployeeID") or ""
                empname = meta.get("EmployeeName") or ""
                personnel_type = meta.get("PersonnelType") or ""
                days_present_val = int(meta.get("DaysPresent") or 0)
                enrich_rec = None
                if empid and str(empid).strip() in enrichment.get("__id__", {}):
                    enrich_rec = enrichment["__id__"].get(str(empid).strip())
                elif empname and str(empname).strip().lower() in enrichment.get("__name__", {}):
                    enrich_rec = enrichment["__name__"].get(str(empname).strip().lower())

                business = enrich_rec.get("Business_Title") if enrich_rec else None
                manager = enrich_rec.get("Manager_Name") if enrich_rec else None
                n1_org = enrich_rec.get("N1_Sup_Organization") if enrich_rec else None
                loc_desc = enrich_rec.get("Location_Description") if enrich_rec else None
                current_status = enrich_rec.get("Current_Status") if enrich_rec else None
                hire_date = enrich_rec.get("Hire_Date") if enrich_rec else None

                row = {
                    "Employee ID": empid,
                    "Full_Name": empname,
                    "Personnel Type": personnel_type,
                    "Business_Title": business,
                    "Manager_Name": manager,
                    "N1_Sup_Organization": n1_org,
                    "Location_Description": loc_desc or meta.get("PrimaryLocation"),
                    "Current_Status": current_status,
                    "Report Start Date": start_date.isoformat(),
                    "Report End Date": end_date.isoformat(),
                    "Hire_Date": hire_date.isoformat() if isinstance(hire_date, (date, datetime)) else (hire_date if hire_date else None),
                    "Actual_No_Of_Days_Attended": days_present_val
                }
                summary_rows.append(row)
            except Exception:
                logger.exception("Failed building summary row for uid=%s", uid)
                continue
    except Exception:
        logger.exception("Failed iterating agg_first to build summary rows")

    summary_df = pd.DataFrame(summary_rows, columns=[
        "Employee ID", "Full_Name", "Personnel Type", "Business_Title", "Manager_Name", "N1_Sup_Organization",
        "Location_Description", "Current_Status", "Report Start Date", "Report End Date", "Hire_Date", "Actual_No_Of_Days_Attended"
    ])

    # Attendance sheet
    presence_df = pivot_presence.copy()
    try:
        presence_df = presence_df.applymap(lambda v: 1 if (pd.notna(v) and float(v) > 0) else 0)
    except Exception:
        for c in presence_df.columns:
            presence_df[c] = presence_df[c].apply(lambda v: 1 if (pd.notna(v) and (float(v) if str(v).strip() else 0) > 0) else 0)

    try:
        presence_df = presence_df.reindex(agg_first.index)
    except Exception:
        pass

    # rename date columns to labels
    try:
        date_labels = [_date_label_safe(d) for d in presence_df.columns]
        col_map = dict(zip(presence_df.columns, date_labels))
        presence_df.columns = [col_map[c] for c in presence_df.columns]
    except Exception:
        pass

    # month groups
    month_groups = {}
    for d in dates:
        mon_label_short = d.strftime("%b")
        lbl = _date_label_safe(d)
        month_groups.setdefault(mon_label_short, []).append(lbl)

    # final attendance frame
    try:
        att_index = pd.DataFrame({
            "Emp ID": agg_first["EmployeeID"].fillna("").astype(str),
            "Emp Name": agg_first["EmployeeName"].fillna("").astype(str)
        }, index=presence_df.index)
    except Exception:
        att_index = pd.DataFrame(index=presence_df.index)
        att_index["Emp ID"] = ""
        att_index["Emp Name"] = ""

    attendance_df = pd.concat([att_index.reset_index(drop=True), presence_df.reset_index(drop=True)], axis=1)

    for mon, cols in month_groups.items():
        try:
            attendance_df[f"{mon} Total"] = attendance_df[cols].sum(axis=1).astype(int)
        except Exception:
            attendance_df[f"{mon} Total"] = 0

    filename = outdir / f"denver_attendance_{mon_label}.xlsx"
    sheets_to_write = {}
    # Summary
    try:
        sheets_to_write["Summary"] = summary_df
    except Exception:
        sheets_to_write["Summary"] = pd.DataFrame([{"Note": "Failed to build Summary; see logs"}])
    # Attendance
    try:
        sheets_to_write["Attendance"] = attendance_df
    except Exception:
        sheets_to_write["Attendance"] = pd.DataFrame([{"Note": "Failed to build Attendance sheet; see logs"}])
    # DurationsRaw (presence-only)
    try:
        cols = ["person_uid", "Date", "Presence"] if set(["person_uid", "Date", "Presence"]).issubset(df_all.columns) else [c for c in ("person_uid", "Date") if c in df_all.columns]
        tidy = df_all[cols].copy() if cols else pd.DataFrame()
        if not tidy.empty:
            tidy = tidy.rename(columns={"person_uid": "PersonUID"})
            tidy = tidy.sort_values(["PersonUID", "Date"])
            sheets_to_write["DurationsRaw"] = tidy
    except Exception:
        pass

    try:
        written = _write_workbook(filename, sheets_to_write)
        logger.info("Wrote Denver attendance report to %s", written)
        return str(written)
    except Exception:
        logger.exception("Failed writing workbook to %s", filename)
        raise

# If executed as script for quick local runs:
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--year", type=int, default=None)
    parser.add_argument("--month", type=int, default=None)
    parser.add_argument("--start", default=None)
    parser.add_argument("--end", default=None)
    parser.add_argument("--outdir", default="./output")
    args = parser.parse_args()

    if args.start and args.end:
        start = _to_date(args.start)
        end = _to_date(args.end)
    elif args.year and args.month:
        start, end = _month_date_range(args.year, args.month)
    else:
        raise SystemExit("Please provide either --year & --month or --start & --end")

    path = generate_monthly_denver_report(start_date=start, end_date=end, outdir=args.outdir)
    print("Report written to:", path)









Also check app.py for APi endpoint..




@app.get("/api/reports/denver-attendance")
@app.get("/reports/denver-attendance")  # alias (helps when proxy isn't configured)
async def api_denver_attendance(
    year: Optional[int] = Query(None, description="Year (e.g. 2025)"),
    month: Optional[int] = Query(None, description="Month 1..12"),
    from_date: Optional[str] = Query(None, description="YYYY-MM-DD (optional alternative)"),
    to_date: Optional[str] = Query(None, description="YYYY-MM-DD (optional alternative)"),
    refresh: bool = Query(False, description="If true, do not use cache (not implemented)"),
):
    """
    Generate Denver Monthly Attendance Excel report (returns file).
    Provide either year+month OR from_date+to_date (YYYY-MM-DD).
    If neither provided, defaults to previous month relative to Asia/Kolkata.
    """
    request_id = uuid.uuid4().hex[:8]
    logger.info("[%s] denver-attendance request START year=%s month=%s from_date=%s to_date=%s refresh=%s",
                request_id, year, month, from_date, to_date, refresh)
    try:
        # robust import: try normal import, then file-based fallback
        denver_mod = None
        try:
            denver_mod = importlib.import_module("denverAttendance")
            logger.debug("[%s] Imported denverAttendance via normal import", request_id)
        except Exception as imp_exc:
            logger.warning("[%s] Normal import of denverAttendance failed: %s. Attempting file-based import...", request_id, str(imp_exc))
            try:
                # try to import from same directory as app.py
                candidate = Path(__file__).resolve().parent / "denverAttendance.py"
                if candidate.exists():
                    spec = importlib.util.spec_from_file_location("denverAttendance", str(candidate))
                    denver_mod = importlib.util.module_from_spec(spec)
                    if spec and spec.loader:
                        spec.loader.exec_module(denver_mod)
                        logger.debug("[%s] Imported denverAttendance from file %s", request_id, candidate)
                else:
                    # last-ditch: look up module path in sys.path
                    denver_mod = importlib.import_module("denverAttendance")
            except Exception as ex2:
                logger.exception("[%s] Failed to import denverAttendance (both normal and file fallback)", request_id)
                raise HTTPException(status_code=500, detail=f"denverAttendance import failed: {ex2}")

        # compute date range
        tz = ZoneInfo("Asia/Kolkata")
        today = datetime.now(tz).date()

        if from_date and to_date:
            try:
                s = datetime.strptime(str(from_date)[:10], "%Y-%m-%d").date()
                e = datetime.strptime(str(to_date)[:10], "%Y-%m-%d").date()
            except Exception:
                logger.error("[%s] Invalid from_date/to_date format: %s / %s", request_id, from_date, to_date)
                raise HTTPException(status_code=400, detail="Invalid from_date/to_date format. Use YYYY-MM-DD")
            start_date, end_date = s, e
        else:
            if year is not None and month is not None:
                # if module provides helper use it; else fallback compute
                if hasattr(denver_mod, "_month_date_range"):
                    try:
                        start_date, end_date = denver_mod._month_date_range(int(year), int(month))
                    except Exception:
                        logger.exception("[%s] denver_mod._month_date_range threw an error, falling back to manual compute", request_id)
                        start_date = date(int(year), int(month), 1)
                        if int(month) == 12:
                            end_date = date(int(year) + 1, 1, 1) - timedelta(days=1)
                        else:
                            end_date = date(int(year), int(month) + 1, 1) - timedelta(days=1)
                else:
                    start_date = date(int(year), int(month), 1)
                    if int(month) == 12:
                        end_date = date(int(year) + 1, 1, 1) - timedelta(days=1)
                    else:
                        end_date = date(int(year), int(month) + 1, 1) - timedelta(days=1)
            else:
                # default: previous month
                prev_month_end = (today.replace(day=1) - timedelta(days=1))
                if hasattr(denver_mod, "_month_date_range"):
                    try:
                        start_date, end_date = denver_mod._month_date_range(prev_month_end.year, prev_month_end.month)
                    except Exception:
                        start_date = prev_month_end.replace(day=1)
                        end_date = prev_month_end
                else:
                    start_date = prev_month_end.replace(day=1)
                    end_date = prev_month_end

        logger.info("[%s] denver-attendance will generate for %s -> %s", request_id, start_date.isoformat(), end_date.isoformat())

        outdir = str(OUTPUT_DIR)
        try:
            # call generator
            path = denver_mod.generate_monthly_denver_report(
                start_date=start_date,
                end_date=end_date,
                outdir=outdir,
                city_filter="Denver",
                region="namer"
            )
        except TypeError:
            # fallback call-signature support (older code paths)
            try:
                path = denver_mod.generate_monthly_denver_report(year=getattr(start_date, 'year', None),
                                                                 month=getattr(start_date, 'month', None),
                                                                 outdir=outdir,
                                                                 city_filter="Denver",
                                                                 region="namer")
            except Exception as e:
                logger.exception("[%s] denverAttendance.generate_monthly_denver_report TypeError fallback failed", request_id)
                raise HTTPException(status_code=500, detail=f"Report generation failed: {e}")
        except Exception as e:
            logger.exception("[%s] denverAttendance generator failed", request_id)
            raise HTTPException(status_code=500, detail=f"Report generation failed: {e}")

        if not path:
            logger.error("[%s] Report generation returned empty path", request_id)
            raise HTTPException(status_code=500, detail="Report generation failed (no path returned)")

        p = Path(path)
        if not p.exists() or not p.is_file():
            logger.error("[%s] Generated file not found at %s", request_id, p)
            raise HTTPException(status_code=500, detail="Generated file not found")

        logger.info("[%s] denver-attendance generated file %s; returning to client", request_id, p.name)
        # return FileResponse(
        #     str(p),
        #     media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        #     filename=p.name
        # )

       # pick media type by extension
        suffix = p.suffix.lower()
        if suffix in (".xlsx",):
            media = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        elif suffix in (".xls",):
            media = "application/vnd.ms-excel"
        elif suffix in (".csv",):
            media = "text/csv"
        else:
            media = "application/octet-stream"
        return FileResponse(
            str(p),
            media_type=media,
            filename=p.name
        )


    except HTTPException:
        # FastAPI will handle this; ensure we log
        logger.exception("[%s] api_denver_attendance HTTPException raised", request_id)
        raise
    except Exception as e:
        logger.exception("[%s] api_denver_attendance unknown failure", request_id)
        raise HTTPException(status_code=500, detail=f"denver attendance error: {e}")







Report format is 

in 1st Sheet 

Summary(Jan- Oct )Employees
Employee ID	Full_Name	Personnel Type	Business_Title	Manager_Name	N1_Sup_Organization	Location_Description	Current_Status	Report Start Date	Report End Date 	Hire_Date	Actual_No_Of_Days_Attended


This Column 

Employee ID , Name as Employee Name , Personnel Type  ( Employee ot Terminted personnel )  Business_Title	Manager_Name	N1_Sup_Organization Pull this data using 
Active Employee Sheet .
Current_Status ( Active or Deactive ) , Start Date	Report End Date Start date 1 jan end date Current date , Hire date ( from Active Employee Sheet)

Actual_No_Of_Days_Attended ( Total Attendance Count..)

for ex-
as per above Column responce like 

72072	Galligan, Michelle	Employee [Global]	Vice President, Executive Finance	Cagwin, Matthew L	Cagwin, Matthew L	Denver - WU HQ	Active	01-01-2025	30-09-2025	30-12-1996	114



2nd Sheet like 

Attendance Sheet


		Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri		Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri		Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon		Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed		Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat		Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon		Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu		Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun		Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue	Wed	Thu	Fri	Sat	Sun	Mon	Tue		
Emp ID	Emp Name	1-Jan-25	2-Jan-25	3-Jan-25	4-Jan-25	5-Jan-25	6-Jan-25	7-Jan-25	8-Jan-25	9-Jan-25	10-Jan-25	11-Jan-25	12-Jan-25	13-Jan-25	14-Jan-25	15-Jan-25	16-Jan-25	17-Jan-25	18-Jan-25	19-Jan-25	20-Jan-25	21-Jan-25	22-Jan-25	23-Jan-25	24-Jan-25	25-Jan-25	26-Jan-25	27-Jan-25	28-Jan-25	29-Jan-25	30-Jan-25	31-Jan-25	Jan Total	1-Feb-25	2-Feb-25	3-Feb-25	4-Feb-25	5-Feb-25	6-Feb-25	7-Feb-25	8-Feb-25	9-Feb-25	10-Feb-25	11-Feb-25	12-Feb-25	13-Feb-25	14-Feb-25	15-Feb-25	16-Feb-25	17-Feb-25	18-Feb-25	19-Feb-25	20-Feb-25	21-Feb-25	22-Feb-25	23-Feb-25	24-Feb-25	25-Feb-25	26-Feb-25	27-Feb-25	28-Feb-25	Feb Total	1-Mar-25	2-Mar-25	3-Mar-25	4-Mar-25	5-Mar-25	6-Mar-25	7-Mar-25	8-Mar-25	9-Mar-25	10-Mar-25	11-Mar-25	12-Mar-25	13-Mar-25	14-Mar-25	15-Mar-25	16-Mar-25	17-Mar-25	18-Mar-25	19-Mar-25	20-Mar-25	21-Mar-25	22-Mar-25	23-Mar-25	24-Mar-25	25-Mar-25	26-Mar-25	27-Mar-25	28-Mar-25	29-Mar-25	30-Mar-25	31-Mar-25	Mar Total	1-Apr-25	2-Apr-25	3-Apr-25	4-Apr-25	5-Apr-25	6-Apr-25	7-Apr-25	8-Apr-25	9-Apr-25	10-Apr-25	11-Apr-25	12-Apr-25	13-Apr-25	14-Apr-25	15-Apr-25	16-Apr-25	17-Apr-25	18-Apr-25	19-Apr-25	20-Apr-25	21-Apr-25	22-Apr-25	23-Apr-25	24-Apr-25	25-Apr-25	26-Apr-25	27-Apr-25	28-Apr-25	29-Apr-25	30-Apr-25	Apr Total	1-May-25	2-May-25	3-May-25	4-May-25	5-May-25	6-May-25	7-May-25	8-May-25	9-May-25	10-May-25	11-May-25	12-May-25	13-May-25	14-May-25	15-May-25	16-May-25	17-May-25	18-May-25	19-May-25	20-May-25	21-May-25	22-May-25	23-May-25	24-May-25	25-May-25	26-May-25	27-May-25	28-May-25	29-May-25	30-May-25	31-May-25	May total	1-Jun-25	2-Jun-25	3-Jun-25	4-Jun-25	5-Jun-25	6-Jun-25	7-Jun-25	8-Jun-25	9-Jun-25	10-Jun-25	11-Jun-25	12-Jun-25	13-Jun-25	14-Jun-25	15-Jun-25	16-Jun-25	17-Jun-25	18-Jun-25	19-Jun-25	20-Jun-25	21-Jun-25	22-Jun-25	23-Jun-25	24-Jun-25	25-Jun-25	26-Jun-25	27-Jun-25	28-Jun-25	29-Jun-25	30-Jun-25	Jun Total	1-Jul-25	2-Jul-25	3-Jul-25	4-Jul-25	5-Jul-25	6-Jul-25	7-Jul-25	8-Jul-25	9-Jul-25	10-Jul-25	11-Jul-25	12-Jul-25	13-Jul-25	14-Jul-25	15-Jul-25	16-Jul-25	17-Jul-25	18-Jul-25	19-Jul-25	20-Jul-25	21-Jul-25	22-Jul-25	23-Jul-25	24-Jul-25	25-Jul-25	26-Jul-25	27-Jul-25	28-Jul-25	29-Jul-25	30-Jul-25	31-Jul-25	July Total	1-Aug-25	2-Aug-25	3-Aug-25	4-Aug-25	5-Aug-25	6-Aug-25	7-Aug-25	8-Aug-25	9-Aug-25	10-Aug-25	11-Aug-25	12-Aug-25	13-Aug-25	14-Aug-25	15-Aug-25	16-Aug-25	17-Aug-25	18-Aug-25	19-Aug-25	20-Aug-25	21-Aug-25	22-Aug-25	23-Aug-25	24-Aug-25	25-Aug-25	26-Aug-25	27-Aug-25	28-Aug-25	29-Aug-25	30-Aug-25	31-Aug-25	August Total	1-Sep-25	2-Sep-25	3-Sep-25	4-Sep-25	5-Sep-25	6-Sep-25	7-Sep-25	8-Sep-25	9-Sep-25	10-Sep-25	11-Sep-25	12-Sep-25	13-Sep-25	14-Sep-25	15-Sep-25	16-Sep-25	17-Sep-25	18-Sep-25	19-Sep-25	20-Sep-25	21-Sep-25	22-Sep-25	23-Sep-25	24-Sep-25	25-Sep-25	26-Sep-25	27-Sep-25	28-Sep-25	29-Sep-25	30-Sep-25	September Total	Grand Total



Make Exact same Column dont make unnecessary chanes 

and share me Updateed file for Denver attendance carefully







